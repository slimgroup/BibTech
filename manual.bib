% This file was created with JabRef 2.6.
% Encoding: MacRoman
@MANUAL{Haber2010emp,
  title = {An effective method for parameter estimation with PDE constraints
	with multiple right hand sides},
  author = {Eldad Haber and Matthias Chung and Felix J. Herrmann},
  year = {2010},
  abstract = {Many parameter estimation problems involve with a parameter-dependent
	PDEs with multiple right hand sides. The computational cost and memory
	requirements of such problems increases linearly with the number
	of right hand sides. For many applications this is the main bottleneck
	of the computation. In this paper we show that problems with multiple
	right hand sides can be reformulated as stochastic optimization problems
	that are much cheaper to solve. We discuss the solution methodology
	and use the direct current resistivity and seismic tomography as
	model problems to show the effectiveness of our approach.},
  keywords = {SLIM},
  number = {TR-2010-4},
  publisher = {UBC-Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/Haber2010emp.pdf}
}

@MANUAL{hennenfent2010nct,
  title = {Nonequispaced curvelet transform for seismic data reconstruction:
	a sparsity-promoting approach},
  author = {Gilles Hennenfent and Lloyd Fenelon and Felix J. Herrmann},
  year = {2010},
  abstract = {Seismic data are typically irregularly sampled along spatial axes.
	This irregular sampling may adversely affect some key steps, e.g.,
	multiple prediction/attenuation or imaging, in the processing workflow.
	To overcome this problem almost every large dataset is regularized
	and/or interpolated. Our contribution is twofold. Firstly, we extend
	our earlier work on the nonequispaced fast discrete curvelet transform
	(NFDCT) and introduce a second generation of the transform. This
	new generation differs from the previous one by the approach taken
	to compute accurate curvelet coefficients from irregularly sampled
	data. The first generation relies on accurate Fourier coefficients
	obtained by an $\ell_2$-regularized inversion of the nonequispaced
	fast Fourier transform, while the second is based on a direct, $\ell_1$
	-regularized inversion of the operator that links curvelet coefficients
	to irregular data. Also, by construction, the NFDCT second generation
	is lossless, unlike the NFDCT first generation. This property is
	particularly attractive for processing irregularly sampled seismic
	data in the curvelet domain and bringing them back to their irregular
	recording locations with high fidelity. Secondly, we combine the
	NFDCT second generation with the standard fast discrete curvelet
	transform (FDCT) to form a new curvelet-based method, coined nonequispaced
	curvelet reconstruction with sparsity-promoting inversion (NCRSI),
	for the regularization and interpolation of irregularly sampled data.
	We demonstrate that, for a pure regularization problem, the reconstruction
	is very accurate. The signal-to-reconstruction error ratio is, in
	our example, above 40 dB. We also conduct combined interpolation
	and regularization experiments. The reconstructions for synthetic
	data are accurate, particularly when the recording locations are
	optimally jittered. The reconstruction in our real data example shows
	amplitudes along the main wavefronts smoothly varying with no obvious
	acquisition imprint; a result very competitive with results from
	other reconstruction methods overall.},
  number = {TR-2010-2},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http\://slim.eos.ubc.ca/Publications/Private/Journals/hennenfent2010nct}
}

@MANUAL{herrmann10erc,
  title = {Emperical recovery conditions for seismic sampling},
  author = {Felix J. Herrmann},
  year = {2010},
  abstract = {In this paper, we offer an alternative sampling method leveraging
	recent insights from compressive sensing towards seismic acquisition
	and processing for data that are traditionally considered to be undersampled.
	The main outcome of this approach is a new technology where acquisition
	and processing related costs are no longer determined by overly stringent
	sampling criteria, such as Nyquist. At the heart of our approach
	lies randomized incoherent sampling that breaks subsampling related
	interferences by turning them into harmless noise, which we subsequently
	remove by promoting transform-domain sparsity. Now, costs no longer
	grow with resolution and dimensionality of the survey area, but instead
	depend on transform-domain sparsity only. Our contribution is twofold.
	First, we demonstrate by means of carefully designed numerical experiments
	that compressive sensing can successfully be adapted to seismic acquisition.
	Second, we show that accurate recovery can be accomplished for compressively
	sampled data volumes sizes that exceed the size of conventional transform-domain
	data volumes by only a small factor. Because compressive sensing
	combines transformation and encoding by a single linear encoding
	step, this technology is directly applicable to acquisition and to
	dimensionality reduction during processing. In either case, sampling,
	storage, and processing costs scale with transform-domain sparsity.
	We illustrate this principle by means of number of case studies.},
  keywords = {SEG},
  publisher = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/herrmann10erc.pdf}
}

@MANUAL{herrmann2010rsa,
  title = {Randomized sampling and sparsity: getting more information from fewer
	samples},
  author = {Felix J. Herrmann},
  year = {2010},
  abstract = {Many seismic exploration techniques rely on the collection of massive
	data volumes that are subsequently mined for information during processing.
	While this approach has been extremely successful in the past, current
	efforts toward higher- resolution images in increasingly complicated
	regions of the Earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly amongst these is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which disproportionately
	strains current acquisition and processing systems as the size and
	desired resolution of our survey areas continues to increase. In
	this paper, we offer an alternative sampling method leveraging recent
	insights from compressive sensing towards seismic acquisition and
	processing for data that are traditionally considered to be undersampled.
	The main outcome of this approach is a new technology where acquisition
	and processing related costs are no longer determined by overly stringent
	sampling criteria, such as Nyquist. At the heart of our approach
	lies randomized incoherent sampling that breaks subsampling related
	interferences by turning them into harmless noise, which we subsequently
	remove by promoting transform-domain sparsity. Now, costs no longer
	grow significantly with resolution and dimensionality of the survey
	area, but instead depend on transform-domain sparsity only. Our contribution
	is twofold. First, we demonstrate by means of carefully designed
	numerical experiments that compressive sensing can successfully be
	adapted to seismic exploration. Second, we show that accurate recovery
	can be accomplished for compressively sampled data volumes sizes
	that exceed the size of conventional transform-domain data volumes
	by only a small factor. Because compressive sensing combines transformation
	and encoding by a single linear encoding step, this technology is
	directly applicable to acquisition and to dimensionality reduction
	during processing. In either case, sampling, storage, and processing
	costs scale with transform-domain sparsity. We illustrate this principle
	by means of number of case studies.},
  number = {TR-2010-1},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann2010rsa.pdf}
}

@MANUAL{tang09dtr,
  title = {Design of two-dimensional randomized sampling schemes for curvelet-based
	sparsity-promoting seismic data recovery},
  author = {Gang Tang and Reza Shahidi and Jianwei Ma},
  year = {2009},
  abstract = {The tasks of sampling, compression and reconstruction are very common
	and often necessary in seismic data processing due to the large size
	of seismic data. Curvelet-based Recovery by Sparsity-promoting Inversion,
	motivated by the newly developed theory of compressive sensing, is
	among the best recovery strategies for seismic data. The incomplete
	data input to this curvelet-based recovery is determined by randomized
	sampling of the original complete data. Unlike usual regular undersampling,
	randomized sampling can convert aliases to easy-to-eliminate noise,
	thus facilitating the process of reconstruction of the complete data
	from the incomplete data. Randomized sampling methods such as jittered
	sampling have been developed in the past that are suitable for curvelet-based
	recovery, however most have only been applied to sampling in one
	dimension. Considering that seismic datasets are usually higher dimensional
	and extremely large, in the present paper, we extend the 1D version
	of jittered sampling to two dimensions, both with underlying Cartesian
	and hexagonal grids. We also study separable and non-separable two
	dimensional jittered sampling, the former referring to the Kronecker
	product of two one-dimensional jittered samplings. These different
	categories of jittered sampling are compared against one another
	in terms of signal-to-noise ratio and visual quality, from which
	we find that jittered hexagonal sampling is better than jittered
	Cartesian sampling, while fully non-separable jittered sampling is
	better than separable sampling. Because in the image processing and
	computer graphics literature, sampling patterns with blue-noise spectra
	are found to be ideal to avoid aliasing, we also introduce two other
	randomized sampling methods, possessing sampling spectra with beneficial
	blue noise characteristics, Poisson Disk sampling and Farthest Point
	sampling. We compare these methods, and apply the introduced sampling
	methodologies to higher dimensional curvelet-based reconstruction.
	These sampling schemes are shown to lead to better results from CRSI
	compared to the other more traditional sampling protocols, e.g. regular
	subsampling.},
  number = {TR-2009-03},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/tang09dtr.pdf}
}

@MANUAL{Erlangga08oam,
  title = {On a multilevel projection Krylov method for the Helmholtz equation
	preconditioned by shifted Laplacian},
  author = {Yogi A. Erlangga and R. Nabben},
  year = {2008},
  file = {:http\\\://slim.eos.ubc.ca/Publications/Public/Journals/Yogi/erlangga-nabben-helmholtz.pdf:PDF},
  journal = {Elec. Trans. Numer. Anal.},
  keywords = {SLIM},
  number = {TR-2008-2},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http\://slim.eos.ubc.ca/Publications/Public/Journals/Yogi/erlangga-nabben-helmholtz.pdf},
  volume = {submitted}
}

@MANUAL{hennenfent08onr,
  title = {One-norm regularized inversion: Learning form the Pareto curve},
  author = {Gilles Hennenfent and Felix J. Herrmann},
  month = {August},
  year = {2008},
  abstract = {Geophysical inverse problems typically involve a trade off between
	data misfit and some prior. Pareto curves trace the optimal trade
	off between these two competing aims. These curves are commonly used
	in problems with two-norm priors where they are plotted on a log-log
	scale and are known as L-curves. For other priors, such as the sparsity-promoting
	one norm, Pareto curves remain relatively unexplored. First, we show
	how these curves provide an objective criterion to gauge how robust
	one-norm solvers are when they are limited by a maximum number of
	matrix-vector products that they can perform. Second, we use Pareto
	curves and their properties to define and compute one-norm compressibilities.
	We argue this notion is key to understand one-norm regularized inversion.
	Third, we illustrate the correlation between the one-norm compressibility
	and the performance of Fourier and curvelet reconstructions with
	sparsity promoting inversion.},
  keywords = {SLIM},
  number = {TR-2008-5},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/hennenfent08seg.pdf}
}

@MANUAL{hennenfent08rap,
  title = {TR2008-7 Repro: a Python package for automating reproducible research
	in scientific computing},
  author = {Gilles Hennenfent and Sean Ross-Ross},
  month = {August},
  year = {2008},
  abstract = {repro is a Python package for automating reproducible research in
	scientific computing. repro works in combination with SCons, a next-generation
	build tool. The package is freely available over the Internet. Downloading
	and installation instructions are provided in this gui de. The repro
	package is documented in various ways (many comments in source code,
	this guide{\textendash}-written using repro itself!{\textendash}-and
	a reference guide ). In this user{\textquoteright}s guide, we present
	a few pedagogical examples that uses Matlab, Python, Seismic Unix
	(SU), and Madagascar. We also include demo pa pers. These papers
	are written in {\L}aTeX\ and compiled using repro. The figures they
	contain are automatically generated from the source codes prov ided.
	In that sense, the demo papers are a model of self-contained documents
	that are fully reproducible. The repro package is largely inspired
	by some parts of Madagascar, a geophysical software package for reproducible
	research. However, the repro package is intended for a broad audience
	co ming from a wide spectrum of interest areas.},
  keywords = {SLIM},
  publisher = {The University of British Columbia},
  type = {software package}
}

@MANUAL{herrmann08cdm,
  title = {Curvelet-domain matched filtering},
  author = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
  month = {August},
  year = {2008},
  abstract = {Matching seismic wavefields and images lies at the heart of many pre-/post-processing
	steps part of seismic imaging{\textendash}- whether one is matching
	predicted wavefield components, such as multiples, to the actual
	to-be-separated wavefield components present in the data or whether
	one is aiming to restore migration amplitudes by scaling, using an
	image-to-remigrated-image matching procedure to calculate the scaling
	coefficients. The success of these wavefield matching procedures
	depends on our ability to (i) control possible overfitting, which
	may lead to accidental removal of energy or to inaccurate image-amplitude
	corrections, (ii) handle data or images with nonunique dips, and
	(iii) apply subsequent wavefield separations or migraton amplitude
	corrections stably. In this paper, we show that the curvelet transform
	allows us to address all these issues by imposing smoothness in phase
	space, by using their capability to handle conflicting dips, and
	by leveraging their ability to represent seismic data and images
	sparsely. This latter property renders curvelet-domain sparsity promotion
	an effective prior.},
  keywords = {SLIM},
  number = {TR-2008-6},
  presentation = { http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08cmf.pdf
	},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08segmat.pdf}
}

@MANUAL{lebed08ahg,
  title = {A Hitchhiker{\textquoteright}s guide to the galaxy of transform-domain
	sparsifiction},
  author = {Evgeniy Lebed and Felix J. Herrmann},
  month = {August},
  year = {2008},
  abstract = {The ability to efficiently and sparsely represent seismic data is
	becoming an increasingly important problem in geophysics. Over the
	last decade many transforms such as wavelets, curervelets, contourlets,
	surfacelets, shearlets, and many other types of {\textquoteright}x-lets{\textquoteright}
	have been developed to try to resolve this issue. In this abstract
	we compare the properties of four of these commonly used transforms,
	namely the shift-invariant wavelets, complex wavelets, curvelets
	and surfacelets. We also briefly explore the performance of these
	transforms for the problem of recovering seismic wavefields from
	incomplete measurements.},
  file = {:http\\\://slim.eos.ubc.ca/Publications/Public/TechReports/lebed08seg.pdf:PDF},
  keywords = {SLIM},
  number = {TR-2008-4},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/lebed08seg.pdf}
}

@MANUAL{slimpy08,
  title = {{SLIMPY}: a python interface for unix-pipe based coordinate-free
	scientific computing},
  author = {Sean Ross-Ross and Henryk Modzelewski and Felix J. Herrmann},
  month = {July},
  year = {2008},
  abstract = {SLIMpy is a Python interface that exposes the functionality of seismic
	data processing packages, such as MADAGASCAR, through oper ator overloading.
	SLIMpy provides a concrete coordinate-free implementation of classes
	for out-of-core linear (implicit matrix-vector), and element -wise
	operations, including calculation of norms and other basic vector
	operations. The library is intended to provide the user with an abstract
	sc ripting language to program iterative algorithms from numerical
	linear algebra. These algorithms require repeated evaluation of operators
	that were initially designed to be run as part of batch-oriented
	processing flows. The current implementation supports a plugin for
	Madagascar{\textquoteright}s out-of-core UNIX pipe-based applications
	and is extenable to pipe-based collections of programs such as Seismic
	Un*x, SEPLib, and FreeUSP. To optimize perform ance, SLIMpy uses
	an Abstract Syntax Tree that parses the algorithm and optimizes the
	pipes.},
  url = {http://slim.eos.ubc.ca/SLIMpy/}
}

@MANUAL{vandenberg08ptp,
  title = {Probing the {P}areto frontier for basis pursuit solutions},
  author = {E. van den Berg and Michael P. Friedlander},
  month = {January},
  year = {2008},
  abstract = {The basis pursuit problem seeks a minimum one-norm solution of an
	underdetermined least-squares problem. Basis pursuit denoise (BPDN)
	fits the least-squares problem only approximately, and a single parameter
	determines a curve that traces the optimal trade-off between the
	least-squares fit and the one-norm of the solution. We prove that
	this curve is convex and continuously differentiable over all points
	of interest, and show that it gives an explicit relationship to two
	other optimization problems closely related to BPDN. We describe
	a root-finding algorithm for finding arbitrary points on this curve;
	the algorithm is suitable for problems that are large scale and for
	those that are in the complex domain. At each iteration, a spectral
	gradient-projection method approximately minimizes a least-squares
	problem with an explicit one-norm constraint. Only matrix-vector
	operations are required. The primal-dual solution of this problem
	gives function and derivative information needed for the root-finding
	method. Numerical experiments on a comprehensive set of test problems
	demonstrate that the method scales well to large problems.},
  file = {:http\://www.optimization-online.org/DB_FILE/2008/07/2056.pdf:PDF},
  keywords = {one-norm solver, Pareto, SLIM},
  number = {TR-2008-01},
  publisher = {UBC Computer Science Department},
  url = {http://www.optimization-online.org/DB_HTML/2008/01/1889.html}
}

@MANUAL{wang08bss,
  title = {Bayesian-signal separation by sparsity promotion: application to
	primary-multiple separation},
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  month = {January},
  year = {2008},
  abstract = {Successful removal of coherent noise sources greatly determines the
	quality of seismic imaging. Major advances were made in this direction,
	e.g., Surface-Related Multiple Elimination (SRME) and interferometric
	ground-roll removal. Still, moderate phase, timing, amplitude errors
	and clutter in the predicted signal components can be detrimental.
	Adopting a Bayesian approach along with the assumption of approximate
	curvelet-domain independence of the to-be-separated signal components,
	we construct an iterative algorithm that takes the predictions produced
	by for example SRME as input and separates these components in a
	robust fashion. In addition, the proposed algorithm controls the
	energy mismatch between the separated and predicted components. Such
	a control, which was lacking in earlier curvelet-domain formulations,
	produces improved results for primary-multiple separation on both
	synthetic and real data.},
  keywords = {signal separation, SLIM},
  number = {TR-2008-1},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/wang08bayes/paper_html/paper.html}
}

@MANUAL{rossross07sda,
  title = {{SLIMpy} development and programming interface for seismic processing},
  author = {Sean Ross-Ross and Henryk Modzelewski and Cody R. Brown and Felix
	J. Herrmann},
  year = {2007},
  abstract = {Inverse problems in (exploration) seismology are known for their large
	to very large scale. For instance, certain sparsity-promoting inversion
	techniques involve vectors that easily exceed unknowns while seismic
	imaging involves the construction and application of matrix-free
	discretized operators where single matrix-vector evaluations may
	require hours, days or even weeks on large compute clusters. For
	these reasons, software development in this field has remained the
	domain of highly technical codes programmed in low-level languages
	with little eye for easy development, code reuse and integration
	with (nonlinear) programs that solve inverse problems.Following ideas
	from the Symes{\textquoteright} Rice Vector Library and Bartlett{\textquoteright}s
	C++ object-oriented interface, Thyra, and Reduction/Transformation
	operators (both part of the Trilinos software package), we developed
	a software-development environment based on overloading. This environment
	provides a pathway from in-core prototype development to out-of-core
	and MPI {\textquoteright}production{\textquoteright} code with a
	high level of code reuse. This code reuse is accomplished by integrating
	the out-of-core and MPI functionality into the dynamic object-oriented
	programming language Python. This integration is implemented through
	operator overloading and allows for the development of a coordinate-free
	solver framework that (i) promotes code reuse; (ii) analyses the
	statements in an abstract syntax tree and (iii) generates executable
	statements. In the current implementation, we developed an interface
	to generate executable statements for the out-of-core unix-pipe based
	(seismic) processing package RSF-Madagascar (rsf.sf.net). The modular
	design allows for interfaces to other seismic processing packages
	and to in-core Python packages such as numpy. So far, the implementation
	overloads linear operators and element-wise reduction/transformation
	operators. We are planning extensions towards nonlinear operators
	and integration with existing (parallel) solver frameworks such as
	Trilinos.},
  keywords = {SLIM, software},
  url = {http://slim.eos.ubc.ca/SLIMpy}
}

@MANUAL{sparco07,
  title = {SPARCO: A toolbox for testing sparse reconstruction algorithms},
  author = {E. van den Berg and Michael P. Friedlander},
  month = {October},
  year = {2007},
  abstract = {Sparco is a suite of problems for testing and benchmarking algorithms
	for sparse signal reconstruction. It is also an environment for creating
	new test problems, and a suite of standard linear operators is provided
	from which new problems can be assembled. Sparco is implement ed
	entirely in Matlab and is self contained. (A few optional test problems
	are based on the CurveLab toolbox, which can be installed separately.)
	At the core of the sparse recovery problem is the linear system $Ax+r=b$,
	where $A$ is an $m$-by-$n$ linear operator and the $m$-vector $b$
	is the observed signal. The goal is to find a sparse $n$-vector $x$
	such that $r$ is small in norm.},
  keywords = {SLIM},
  url = {http://www.cs.ubc.ca/labs/scl/sparco/}
}

@MANUAL{vandendberg07ipo,
  title = {In Pursuit of a Root},
  author = {E. van den Berg and Michael P. Friedlander},
  month = {June},
  year = {2007},
  abstract = {The basis pursuit technique is used to find a minimum one-norm solution
	of an un- derdetermined least-squares problem. Basis pursuit denoise
	fits the least-squares problem only approximately, and a single parameter
	determines a curve that traces the trade-off between the least-squares
	fit and the one-norm of the solution. We show that the function that
	describes this curve is convex and continuously differ- entiable
	over all points of interest. The dual solution of a least-squares
	problem with an explicit one-norm constraint gives function and derivative
	information needed for a root-finding method. As a result, we can
	compute arbitrary points on this curve. Numerical experiments demonstrate
	that our method, which relies on only matrix-vector operations, scales
	well to large problems.},
  keywords = {SLIM, SPGL1},
  publisher = {UBC},
  url = {http://www.cs.ubc.ca/~mpf/downloads/BergFrie07.pdf}
}

@MANUAL{vandenberg07sat,
  title = {Sparco: a testing framework for sparse reconstruction},
  author = {E. van den Berg and Michael P. Friedlander and Gilles Hennenfent
	and Felix J. Herrmann and Rayan Saab and Ozgur Yilmaz},
  year = {2007},
  abstract = {Sparco is a framework for testing and benchmarking algorithms for
	sparse reconstruction. It includes a large collection of sparse reconstruction
	problems drawn from the imaging, compressed sensing, and geophysics
	literature. Sparco is also a framework for implementing new test
	problems and can be used as a tool for reproducible research. Sparco
	is implemented entirely in Matlab, and is released as open-source
	software under the GNU Public License.},
  file = {:http\://www.cs.ubc.ca/labs/scl/sparco/uploads/Main/sparco.pdf:PDF},
  keywords = {SLIM, Sparco},
  number = {TR-2007-20},
  publisher = {UBC Computer Science Department},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf}
}

%@MANUAL{slimp08,
%  title = {SLIMpy},
%  year = {Submitted},
%  abstract = {SLIMpy is a Python interface that exposes the functionality of seismic
%	data processing packages, such as MADAGASCAR, through operator overloading.
%	SLIMpy provides a concrete coordinate-free implementation of classes
%	for out-of-core linear (implicit matrix-vector), and element-wise
%	operations, including calculation of norms and other basic vector
%	operations. The library is intended to provide the user with an abstract
%	scripting language to program iterative algorithms from numerical
%	linear algebra. These algorithms require repeated evaluation of operators
%	that were initially designed to be run as part of batch-oriented
%	processing flows. The current implementation supports a plugin for
%	Madagascar{\textquoteright}s out-of-core UNIX pipe-based applications
%	and is extenable to pipe- based collections of programs such as Seismic
%	Un*x, SEPLib, and FreeUSP. To optimize performance, SLIMpy uses an
%	Abstract Syntax Tree that parses the algorithm and optimizes the
%	pipes.}
%}

%@MANUAL{BergFrieHennHerrSaab:2007,
%  title = {Sparco: {A} testing framework for sparse reconstruction},
%  year = {2008},
%  note = {To appear in {\i}t ACM Trans. Math. Soft.},
%  number = {TR-2007-20},
%  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf}
}

