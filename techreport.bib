% This file was created with JabRef 2.9.
% Encoding: MacRoman

@TECHREPORT{dasilva2014htuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Optimization on the Hierarchical Tucker manifold - applications to tensor completion},
  year = {2014},
  month = {03},
  institution = {UBC},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker (HT) tensors, an efficient
                  structured tensor format based on recursive subspace
                  factorizations. By exploiting the smooth manifold
                  structure of these tensors, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient for completing tensors from
                  missing entries. Our algorithmic framework is fast
                  and scalable to large problem sizes as we do not
                  require SVDs on the ambient tensor space, as
                  required by other methods. Moreover, we exploit the
                  structure of the Gramian matrices associated with
                  the HT format to regularize our problem, reducing
                  overfitting for high subsampling ratios. We also
                  find that the organization of the tensor can have a
                  major impact on completion from realistic seismic
                  acquisition geometries. These samplings are far from
                  idealized randomized samplings that are usually
                  considered in the literature but are realizable in
                  practical scenarios. Using these algorithms, we
                  successfully interpolate large-scale seismic data
                  sets and demonstrate the competitive computational
                  scaling of our algorithms as the problem sizes
                  grow.},
  keywords = {hierarchical tucker, structured tensor, tensor interpolation, differential geometry, riemannian optimization, gauss newton},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/dasilva2014htuck/dasilva2014htuck.pdf}
}


@TECHREPORT{slim2011NSERCpr,
  author = {Felix J. Herrmann},
  title = {NSERC 2011 DNOISE Progress Report},
  year = {2011},
  institution = {UBC},
  abstract = {The main thrust of the DNOISE project is focused on the
                    following researth themes: [x] seismic acquisition
                    design and recovery from incomplete data with the
                    goal to reduce acquisition costs while increasing
                    the spatial bandwidth and aperture of seismic data;
                    [x] Removal of the 'surface nonlinearity' by
                    simultaneous estimation of the source signature and
                    the surface-free Green's function by inverting the
                    surface-related multiple prediction operator; [x]
                    Reduction of the computational complexity of
                    full-waveform inversion (FWI) by randomized
                    dimensionality reduction; [x] "Convexification" of
                    FWI to remove or at least diminish the adverse
                    effects of non-uniqueness that has plagued FWI
                    since its inception; The first three themes are
                    directed towards removing major impediments faced
                    by FWI related to the costs of acquiring data, the
                    computational costs of processing and inverting
                    data, and to issues with source calibration and
                    surface-related multiples. The final theme is more
                    'blue sky' and tries to incorporate ideas from
                    migration-velocity analysis into the formulation of
                    full-waveform inversion. Aside from these themes,
                    we will continue to work on seismic data
                    acquisition schemes that favor sparsity-promoting
                    recovery and on the development of large-scale
                    solvers using recent developments in convex and
                    stochastic optimization.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2011/nserc-2011-dnoise-progress-report.pdf}
}


@TECHREPORT{slim2010NSERCapp,
  author = {Felix J. Herrmann},
  title = {NSERC 2010 DNOISE Application},
  year = {2010},
  institution = {UBC},
  abstract = {DNOISE II: Dynamic Nonlinear Optimization for Imaging in
                    Seismic Exploration is a multidisciplinary research
                    project that involves faculty from the Mathematics,
                    Computer Science, and Earth and Ocean Sciences
                    Departments at the University of British Columbia.
                    DNOISE II constitutes a transformative research
                    program towards a new paradigm in seismic
                    exploration where the acquisition- and
                    processing-related costs are no longer determined
                    by the survey area and discretization but by
                    transform-domain sparsity of the final result. In
                    this approach, we rid ourselves from the
                    confinements of conventional overly stringent
                    sampling criteria that call for regular sampling
                    with sequentiual sources at Nyquist rates. By
                    adapting the principles of compressive sensing,
                    DNOISE II promotes a ground-up formulation for
                    seismic imaging where adverse subsampling-related
                    artifacts are removed by intelligent
                    simultaneous-acquisition design and recovery by
                    transform-domain sparsity promotion. This
                    development---in conjunction with our track records
                    in sparse recovery and time-harmonic Helmholtz
                    solvers---puts us in an unique position to deliver
                    on fundamental breakthroughs in the development and
                    implementation of the next-generation of
                    processing, imaging, and full-waveform inversion
                    solutions.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2010/nserc-2010-dnoise-application.pdf}
}


@TECHREPORT{oghenekohwo2013SEGtlswrs,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Time-lapse seismics with randomized sampling},
  year = {2013},
  institution = {UBC},
  abstract = {In time-lapse or 4D seismics, repeatability of the
                  acquisition is a very crucial step, as we do not
                  want spurious events that are not there. In this
                  paper, we propose an approach which avoids any
                  requirement to repeat the surveys, by using
                  randomized sampling technique which allows us to be
                  more efficient in the acquisition. Our method
                  applies to sampling data using ocean bottom nodes
                  (OBN) as receivers. We test the efficacy of our
                  proposed randomized acquisition geometry for
                  time-lapse survey on two different models. In the
                  first example, model properties does not change with
                  time, while in the second example, model exhibit a
                  time-lapse effect which may be caused by the
                  migration of fluid within the reservoir. We perform
                  two types of randomized sampling - uniform
                  randomized sampling and jittered sampling to
                  visualize the effects of non-repeatability in
                  time-lapse survey. We observe that jittered
                  randomized sampling is a more efficient method
                  compared to randomized sampling, due to it's
                  requirement to control the maximum spacing between
                  the receivers. The results are presented, in the
                  image space, as a least-squares migration of the
                  model perturbation and they are shown for a subset
                  of a synthetic model - the Marmousi model},
  keywords = {acquisition,time-lapse,migration,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/oghenekohwo2013SEGtlswrs/oghenekohwo2013SEGtlswrs.pdf}
}


@TECHREPORT{slim2013NSERCpr,
  author = {Felix J. Herrmann},
  title = {NSERC 2013 DNOISE Progress Report},
  year = {2013},
  institution = {UBC},
  abstract = {As we enter the second half of the DNOISE II project, we
                  are happy to report that we have made significant
                  progress on several fronts. Firstly, our work on
                  seismic data acquisition with compressive sensing is
                  becoming widely recognized, reflected in adaptations
                  of this technology by industry and in this yearâ€™s
                  SEG Karcher award, which went to Gilles Hennenfent,
                  who was one of the researchers who started working
                  in this area in our group. As this report shows, we
                  continued to make progress on this topic with
                  numerous presentations, publications, and software
                  releases. Secondly, our work on large-scale
                  optimization is also widely adapted and instrumental
                  to the different research areas on the grant. In
                  particular, we are excited about new directions that
                  go beyond sparsity promotion and which allow us to
                  exploit other types of structure within the data,
                  such as low-rank. Over the near future, we expect to
                  see a body of new research based on these findings
                  touching acquisition as well as the wave-equation
                  based inversion aspects of our research
                  program. Thirdly, we are also very happy to report
                  that we continued to make substantial progress in
                  wave-equation base inversion. In particular, we
                  would like to mention successes in the areas of
                  acceleration of sparsity-promoting imaging with
                  source estimation and multiples and in theoretical
                  as well as practical aspects of full-waveform
                  inversion. We derived a highly practical and
                  economic formulation of 3-D FWI and we also came up
                  with a complete new formulation of FWI, which
                  mitigates issues related to cycle skipping. Finally,
                  we made a lot of progress applying our algorithm to
                  industrial datasets, which has been well received by
                  industry. Our findings show that FWI is still an
                  immature technology calling for more theoretical
                  input and for the development of practical
                  workflows. Over the last year our work cumulated in
                  14 peer-reviewed journal publications, 5 submitted
                  journal publications, 13 (+ 9) extended abstracts,
                  32 talks at international conferences, and 6
                  software packages. Finally, we are happy to report
                  that we have been joined by several new companies,
                  namely, ION Geophysical, CGG, and Woodside. At this
                  midpoint of the Grant, we are also happy to report
                  that we are well on schedule to meet the milestones
                  included in the original proposal. Given our wide
                  range of expertise and our plans to replace our
                  compute cluster, we continue to be in an excellent
                  position to make fundamental contributions to the
                  fields of seismic data acquisition, processing, and
                  wave-equation based inversion.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2013/Progress_Report_2013.pdf}
}


@TECHREPORT{slim2012NSERCpr,
  author = {Felix J. Herrmann},
  title = {NSERC 2012 DNOISE Progress Report},
  year = {2012},
  institution = {UBC},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2012/Progress_Report_2012.pdf}
}


@TECHREPORT{petrenko2013SEGsaoc,
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of CARP, an iterative linear solver
                  and preconditioner},
  year = {2013},
  institution = {UBC},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {Helmholtz equation, Kaczmarz, software, wave
                  propagation, frequency-domain, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/petrenko2013SEGsaoc/petrenko2013SEGsaoc.pdf}
}


@TECHREPORT{kumar2013ICMLlr,
  author   = {Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title    = {An SVD-free Pareto curve approach to rank minimization},
  year     = {2013},
  institution = {UBC},
  abstract = {Recent SVD-free matrix factorization formulations have
                  enabled rank optimization for extremely large-scale
                  systems (millions of rows and columns). In this
                  paper, we consider rank-regularized formulations
                  that only require a target data-fitting error level,
                  and propose an algorithm for the corresponding
                  problem. We illustrate the advantages of the new
                  approach using the Netflix problem, and use it to
                  obtain high quality results for seismic trace
                  interpolation, a key application in exploration
                  geophysics. We show that factor rank can be easily
                  adjusted as the inversion proceeds, and propose a
                  weighted extension that allows known subspace
                  information to improve the results of matrix
                  completion formulations. Using these methods, we
                  obtain high-quality reconstructions for large scale
                  seismic interpolation problems with real data.},
  keywords = {Interpolation, low-rank},
  month = {02},
  url      = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2013/kumar2013ICMLlr/kumar2013ICMLlr.pdf}
}


@TECHREPORT{li2013EAGEwebmplijsp,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Wave-equation based multi-parameter linearized inversion with joint-sparsity promotion},
  year = {2013},
  institution = {UBC},
  abstract = {The successful application of linearized inversion is
                  affected by the prohibitive size of the data,
                  computational resources required, and how accurately
                  the model parameters reflects the real Earth
                  properties. The issue of data size and computational
                  resources can be addressed by combining ideas from
                  sparsity promoting and stochastic optimization,
                  which can allow us to invert model perturbation with
                  a small subset of the data, yielding a few PDE
                  solves for the inversion. In this abstract, we are
                  aiming at addressing the issue of accuracy of model
                  parameters by inverting density and velocity
                  simultaneously rather than only using velocity. As a
                  matter of face, the effects of density and velocity
                  variations towards the wavefield are very similar,
                  which will cause energy leakage between density and
                  velocity images. To overcome this issue, we proposed
                  a incoherence enhanced method that can reduce the
                  similarity between the effect of density and
                  velocity. Moreover, the location of structural
                  variations in velocity and density are often
                  overlapped in geological setting, thus in this
                  abstract, we also exploit this property with
                  joint-sparsity promoting to further improve the
                  imaging result.},
  keywords = {EAGE, Linearized inversion, incoherence enhancement, joint-sparsity},
  month = {01},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2013/li2013EAGEwebmplijsp/li2013EAGEwebmplijsp.pdf}
}


@TECHREPORT{vanleeuwen2012CGMN,
  author = {Tristan van Leeuwen},
  title = {Fourier analysis of the CGMN method for solving the Helmholtz equation},
  year = {2012},
  institution = {Department of Earth, Ocean and Atmospheric Sciences},
  address = {The University of British Columbia, Vancouver},
  abstract = {The Helmholtz equation arises in many applications, such
                  as seismic and medical imaging. These application
                  are characterized by the need to propagate many
                  wavelengths through an inhomogeneous medium. The
                  typical size of the problems in 3D applications
                  precludes the use of direct factorization to solve
                  the equation and hence iterative methods are used in
                  practice. For higher wavenumbers, the system becomes
                  increasingly indefinite and thus good
                  preconditioners need to be constructed. In this note
                  we consider an accelerated Kazcmarz method (CGMN)
                  and present an expression for the resulting
                  iteration matrix. This iteration matrix can be used
                  to analyze the convergence of the CGMN method. In
                  particular, we present a Fourier analysis for the
                  method applied to the 1D Helmholtz equation. This
                  analysis suggests an optimal choice of the
                  relaxation parameter. Finally, we present some
                  numerical experiments.},
  keywords = {Helmholtz equation, modelling},
  url = {http://arxiv.org/abs/1210.2644},
}
  

@TECHREPORT{almatar10SEGesfd,
  author = {Mufeed H. AlMatar and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of surface-free data by curvelet-domain matched filtering
	and sparse inversion},
  institution = {Department of Earth and Ocean Sciences},
  year = {2010},
  address = {University of British Columbia, Vancouver},
  abstract = {Matching seismic wavefields and images lies at the heart of many pre-post-processing
	steps part of seismic imaging- whether one is matching predicted
	wavefield components, such as multiples, to the actual to-be-separated
	wavefield compo- nents present in the data or whether one is aiming
	to restore migration amplitudes by scaling, using an image-to-remigrated-
	image matching procedure to calculate the scaling coefficients. The
	success of these wavefield matching procedures depends on our ability
	to (i) control possible overfitting, which may lead to accidental
	removal of energy or to inaccurate image-amplitude corrections, (ii)
	handle data or images with nonunique dips, and (iii) apply subsequent
	wavefield separations or migraton amplitude corrections stably. In
	this paper, we show that the curvelet transform allows us to address
	all these issues by im- posing smoothness in phase space, by using
	their capability to handle conflicting dips, and by leveraging their
	ability to represent seismic data and images sparsely. This latter
	property renders curvelet-domain sparsity promotion an effective
	prior.},
  keywords = {SEG},
  organization = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/almatar10SEGesfd/almatar10SEGesfd.pdf}
}


@TECHREPORT{herrmann10SEGerc,
  author = {Felix J. Herrmann},
  title = {Empirical recovery conditions for seismic sampling},
  institution = {Department of Earth and Ocean Sciences, UBC},
  year = {2010},
  abstract = {In this paper, we offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  towards seismic acquisition and processing for data
                  that are traditionally considered to be
                  undersampled.  The main outcome of this approach is
                  a new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria, such as Nyquist. At the
                  heart of our approach lies randomized incoherent
                  sampling that breaks subsampling related
                  interferences by turning them into harmless noise,
                  which we subsequently remove by promoting
                  transform-domain sparsity. Now, costs no longer grow
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold.  First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  acquisition. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.},
  keywords = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/herrmann10SEGerc/herrmann10SEGerc.pdf}
}


@TECHREPORT{hennenfent08TRori,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the Pareto curve},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-5},
  abstract = {Geophysical inverse problems typically involve a trade off between
	data misfit and some prior. Pareto curves trace the optimal trade
	off between these two competing aims. These curves are commonly used
	in problems with two-norm priors where they are plotted on a log-log
	scale and are known as L-curves. For other priors, such as the sparsity-promoting
	one norm, Pareto curves remain relatively unexplored. First, we show
	how these curves provide an objective criterion to gauge how robust
	one-norm solvers are when they are limited by a maximum number of
	matrix-vector products that they can perform. Second, we use Pareto
	curves and their properties to define and compute one-norm compressibilities.
	We argue this notion is key to understand one-norm regularized inversion.
	Third, we illustrate the correlation between the one-norm compressibility
	and the performance of Fourier and curvelet reconstructions with
	sparsity promoting inversion.},
  keywords = {SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2008/hennenfent08TRori/hennenfent08TRori.pdf}
}


@TECHREPORT{rajiv2012SEGFRM,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Fast Methods for Rank Minimization with Applications to Seismic-Data
	Interpolation},
  institution = {Department of Earth and Ocean Sciences},
  year = {2012},
  number = {TR-2012-04},
  address = {University of British Columbia, Vancouver},
  abstract = {Rank penalizing techniques are an important direction in seismic inverse
	problems, since they allow improved recovery by exploiting low-rank
	structure. A major downside of current state of the art techniques
	is their reliance on the SVD of seismic data structures, which can
	be prohibitively expensive. Fortunately, recent work allows us to
	circumvent this problem by working with matrix factorizations. We
	review a novel approach to rank penalization, and successfully apply
	it to the seismic interpolation problem by exploiting the low-rank
	structure of seismic data in the midpoint-offset domain. Experiments
	for the recovery of 2D monochromatic data matrices and seismic lines
	represented as 3D volumes support the feasibility and potential of
	the new approach.},
  keywords = {Rank, optimization, Seismic data Interpolation},
  month = {04},
  organization = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications//Public/TechReport/2012/rajiv2012SEGFRM/rajiv2012SEGFRM.pdf}
}

@TECHREPORT{lebed08TRhgg,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker's guide to the galaxy of transform-domain sparsification},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-4},
  abstract = {The ability to efficiently and sparsely represent seismic data is
	becoming an increasingly important problem in geophysics. Over the
	last decade many transforms such as wavelets, curervelets, contourlets,
	surfacelets, shearlets, and many other types of 'x-lets' have been
	developed to try to resolve this issue. In this abstract we compare
	the properties of four of these commonly used transforms, namely
	the shift-invariant wavelets, complex wavelets, curvelets and surfacelets.
	We also briefly explore the performance of these transforms for the
	problem of recovering seismic wavefields from incomplete measurements.},
  keywords = {SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2008/lebed08TRhgg/lebed08TRhgg.pdf}
}

@TECHREPORT{tang09TRdtr,
  author = {Gang Tang and Reza Shahidi and Jianwei Ma},
  title = {Design of two-dimensional randomized sampling schemes for curvelet-based
	sparsity-promoting seismic data recovery},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2009},
  number = {TR-2009-03},
  abstract = {The tasks of sampling, compression and reconstruction are very common
	and often necessary in seismic data processing due to the large size
	of seismic data. Curvelet-based Recovery by Sparsity-promoting Inversion,
	motivated by the newly developed theory of compressive sensing, is
	among the best recovery strategies for seismic data. The incomplete
	data input to this curvelet-based recovery is determined by randomized
	sampling of the original complete data. Unlike usual regular undersampling,
	randomized sampling can convert aliases to easy-to-eliminate noise,
	thus facilitating the process of reconstruction of the complete data
	from the incomplete data. Randomized sampling methods such as jittered
	sampling have been developed in the past that are suitable for curvelet-based
	recovery, however most have only been applied to sampling in one
	dimension. Considering that seismic datasets are usually higher dimensional
	and extremely large, in the present paper, we extend the 1D version
	of jittered sampling to two dimensions, both with underlying Cartesian
	and hexagonal grids. We also study separable and non-separable two
	dimensional jittered sampling, the former referring to the Kronecker
	product of two one-dimensional jittered samplings. These different
	categories of jittered sampling are compared against one another
	in terms of signal-to-noise ratio and visual quality, from which
	we find that jittered hexagonal sampling is better than jittered
	Cartesian sampling, while fully non-separable jittered sampling is
	better than separable sampling. Because in the image processing and
	computer graphics literature, sampling patterns with blue-noise spectra
	are found to be ideal to avoid aliasing, we also introduce two other
	randomized sampling methods, possessing sampling spectra with beneficial
	blue noise characteristics, Poisson Disk sampling and Farthest Point
	sampling. We compare these methods, and apply the introduced sampling
	methodologies to higher dimensional curvelet-based reconstruction.
	These sampling schemes are shown to lead to better results from CRSI
	compared to the other more traditional sampling protocols, e.g. regular
	subsampling.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Journals/2009/tang09TRdtr/tang09TRdtr.pdf}
}

@TECHREPORT{vandenberg10TRsol,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Sparse optimization with least-squares constraints},
  institution = {Department of Computer Science},
  year = {2010},
  type = {Tech. Rep.},
  number = {TR-2010-02},
  address = {University of British Columbia, Vancouver},
  abstract = {The use of convex optimization for the recovery of sparse signals
	from incomplete or compressed data is now common practice. Motivated
	by the success of basis pursuit in recovering sparse vectors, new
	formulations have been proposed that take advantage of different
	types of sparsity. In this paper we propose an efficient algorithm
	for solving a general class of sparsifying formulations. For several
	common types of sparsity we provide applications, along with details
	on how to apply the algorithm, and experimental results.},
  month = {01},
  url = {http://www.cs.ubc.ca/~mpf/papers/2011BergFriedlander.pdf}
}

@TECHREPORT{vandenberg09TRter,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Theoretical and empirical results for recovery from multiple measurements},
  institution = {Department of Computer Science},
  year = {2009},
  type = {Tech. Rep.},
  number = {TR-2009-7},
  address = {University of British Columbia, Vancouver},
  abstract = {The joint-sparse recovery problem aims to recover, from sets of compressed
	measurements, unknown sparse matrices with nonzero entries restricted
	to a subset of rows. This is an extension of the single-measurement-vector
	(SMV) problem widely studied in compressed sensing. We analyze the
	recovery properties for two types of recovery algorithms. First,
	we show that recovery using sum-of-norm minimization cannot exceed
	the uniform recovery rate of sequential SMV using L1 minimization,
	and that there are problems that can be solved with one approach
	but not with the other. Second, we analyze the performance of the
	ReMBo algorithm [M. Mishali and Y. Eldar, IEEE Trans. Sig. Proc.,
	56 (2008)] in combination with L1 minimization, and show how recovery
	improves as more measurements are taken. From this analysis it follows
	that having more measurements than number of nonzero rows does not
	improve the potential theoretical recovery rate.},
  month = {Sept},
  url = {http://www.cs.ubc.ca/~mpf/papers/vdBergFriedlander09.pdf}
}

@TECHREPORT{vandenberg07TRipr,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {In pursuit of a root},
  institution = {Department of Computer Science},
  year = {2007},
  type = {Tech. Rep.},
  number = {TR-2007-19},
  address = {University of British Columbia, Vancouver},
  month = {06},
  abstract = {The basis pursuit technique is used to find a minimum one-norm solution
	of an un- derdetermined least-squares problem. Basis pursuit denoise
	fits the least-squares problem only approximately, and a single parameter
	determines a curve that traces the trade-off between the least-squares
	fit and the one-norm of the solution. We show that the function that
	describes this curve is convex and continuously differ- entiable
	over all points of interest. The dual solution of a least-squares
	problem with an explicit one-norm constraint gives function and derivative
	information needed for a root-finding method. As a result, we can
	compute arbitrary points on this curve. Numerical experiments demonstrate
	that our method, which relies on only matrix-vector operations, scales
	well to large problems.},
  url = {http://www.cs.ubc.ca/~mpf/papers/2007BergFriedlander.pdf}
}

@TECHREPORT{vandenberg07TRsat,
  author = {Ewout {van den Berg} and Michael P. Friedlander and Gilles Hennenfent
	and Felix J. Herrmann and Rayan Saab and Ozgur Yilmaz},
  title = {Sparco: a testing framework for sparse reconstruction},
  institution = {UBC Computer Science Department},
  year = {2007},
  number = {TR-2007-20},
  abstract = {Sparco is a framework for testing and benchmarking algorithms for
	sparse reconstruction. It includes a large collection of sparse reconstruction
	problems drawn from the imaging, compressed sensing, and geophysics
	literature. Sparco is also a framework for implementing new test
	problems and can be used as a tool for reproducible research. Sparco
	is implemented entirely in Matlab, and is released as open-source
	software under the GNU Public License.},
  keywords = {SLIM, Sparco},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Tech%20Report/SINBAD/2007/vandenberg07TRsat.pdf},
  url = {http://www.cs.ubc.ca/labs/scl/sparco/downloads.php?filename=sparco.pdf}
}

@TECHREPORT{vanleeuwen2012SEGparallel,
  author = {Tristan {van Leeuwen} and Felix J. Herrmann},
  title = {A parallel, object-oriented framework for frequency-domain wavefield
	imaging and inversion.},
  institution = {Department of Earth and Ocean Sciences},
  year = {2012},
  number = {TR-2012-03},
  address = {University of British Columbia, Vancouver},
  abstract = {We present a parallel object-oriented matrix-free framework for frequency-domain
	seismic modeling, imaging and inversion. The key aspects of the framework
	are its modularity and level of abstraction, which allows us to write
	code that reflects the underlying mathematical structure and develop
	unit-tests that guarantee the fidelity of the code. By overloading
	standard linear-algebra operations, such as matrix-vector multiplications,
	we can use standard optimization packages to work with our code without
	any modification. This leads to a scalable testbed on which new methods
	can be rapidly prototyped and tested on medium-sized 2D problems.
	Although our current implementation uses (parallel) Matlab, all of
	these design principles can also be met by using lower-level languages
	which is important when we want to scale to realistic 3D problems.
	We present some numerical examples on synthetic data.},
  keywords = {modeling, imaging, inversion, SEG},
  month = {04},
  organization = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2012/vanleeuwen2012SEGparallel/vanleeuwen2012SEGparallel.pdf}
}

@TECHREPORT{wang08TRbss,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Bayesian-signal separation by sparsity promotion: application to
	primary-multiple separation},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-1},
  abstract = {Successful removal of coherent noise sources greatly determines the
	quality of seismic imaging. Major advances were made in this direction,
	e.g., Surface-Related Multiple Elimination (SRME) and interferometric
	ground-roll removal. Still, moderate phase, timing, amplitude errors
	and clutter in the predicted signal components can be detrimental.
	Adopting a Bayesian approach along with the assumption of approximate
	curvelet-domain independence of the to-be-separated signal components,
	we construct an iterative algorithm that takes the predictions produced
	by for example SRME as input and separates these components in a
	robust fashion. In addition, the proposed algorithm controls the
	energy mismatch between the separated and predicted components. Such
	a control, which was lacking in earlier curvelet-domain formulations,
	produces improved results for primary-multiple separation on both
	synthetic and real data.},
  keywords = {signal separation, SLIM},
  month = {01},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Journals/2008/wang08TRbss/paper_html/paper.html}
}

