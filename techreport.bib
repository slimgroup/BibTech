% This file was created with JabRef 2.9.
% Encoding: MacRoman

@TECHREPORT{esser2014SEGsgp,
  author = {Ernie Esser and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {A scaled gradient projection method for total variation regularized full waveform inversion},
  year = {2014},
  month = {04},
  institution = {UBC},
  abstract = {We propose an extended full waveform inversion
                  formulation that includes convex constraints on the
                  model. In particular, we show how to simultaneously
                  constrain the total variation of the slowness
                  squared while enforcing bound constraints to keep it
                  within a physically realistic range. Synthetic
                  experiments show that including total variation
                  regularization can improve the recovery of a high
                  velocity perturbation to a smooth background model.},
  keywords = {full waveform inversion, convex constraints, total variation regularization},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/esser2014SEGsgp/esser2014SEGsgp.html}
}


@TECHREPORT{zfang2014SEGsqn,
  author = {Zhilong Fang and Felix J. Herrmann},
  title = {A stochastic quasi-Newton {McMC} method for uncertainty quantification of full-waveform inversion},
  year = {2014},
  month = {04},
  institution = {UBC},
  abstract = {In this work we propose a stochastic quasi-Newton Markov
                  chain Monte Carlo (McMC) method to quantify the
                  uncertainty of full-waveform inversion (FWI). We
                  formulate the uncertainty quantification problem in
                  the framework of the Bayesian inference, which
                  formulates the posterior probability as the
                  conditional probability of the model given the
                  observed data. The Metropolis-Hasting algorithm is
                  used to generate samples satisfying the posterior
                  probability density function (pdf) to quantify the
                  uncertainty. However it suffers from the challenge
                  to construct a proposal distribution that
                  simultaneously provides a good representation of the
                  true posterior pdf and is easy to manipulate. To
                  address this challenge, we propose a stochastic
                  quasi-Newton McMC method, which relies on the fact
                  that the Hessian of the deterministic problem is
                  equivalent to the inverse of the covariance matrix
                  of the posterior pdf. The l-BFGS (limited-memory
                  Broyden–Fletcher–Goldfarb–Shanno) Hessian is used to
                  approximate the inverse of the covariance matrix
                  efficiently, and the randomized source sub-sampling
                  strategy is used to reduce the computational cost of
                  evaluating the posterior pdf and constructing the
                  l-BFGS Hessian. Numerical experiments show the
                  capability of this stochastic quasi-Newton McMC
                  method to quantify the uncertainty of FWI with a
                  considerable low cost.},
  keywords = {FWI, uncertainty quantification, quasi-Newton, McMC},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/zfang2014SEGsqn/zfang2014SEGsqn.html}
}


@TECHREPORT{wang2014SEGfwi,
  author = {Rongrong Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Full waveform inversion with interferometric measurements},
  year = {2014},
  month = {04},
  institution = {UBC},
  abstract = {In this note, we design new misfit functions for
                  full-waveform inversion by using interferometric
                  measurements to reduce sensitivity to phase
                  errors. Though established within a completely
                  different setting from the linear case, we obtain a
                  similar observation: the interferometry can improve
                  robustness under certain modeling errors. Moreover,
                  in order to deal with errors on both source and
                  receiver sides, we propose a higher order
                  interferometry, which, as a generalization of the
                  usual definition, involves the cross correlation of
                  four traces. A proof of principle simulations is
                  included on a stylized example.},
  keywords = {FWI},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/wang2014SEGfwi/wang2014SEGfwi.html}
}


@TECHREPORT{kumar2014SEGmcu,
  author = {Rajiv Kumar and Oscar Lopez and Ernie Esser and Felix J. Herrmann},
  title = {Matrix completion on unstructured grids : 2-D seismic data regularization and interpolation},
  year = {2014},
  month = {04},
  institution = {UBC},
  abstract = {Seismic data interpolation via rank-minimization
                  techniques has been recently introduced in the
                  seismic community. All the existing
                  rank-minimization techniques assume the recording
                  locations to be on a regular grid, e.g. sampled
                  periodically, but seismic data are typically
                  irregularly sampled along spatial axes. Other than
                  the irregularity of the sampled grid, we often have
                  missing data. In this paper, we study the effect of
                  grid irregularity to conduct matrix completion on a
                  regular grid for unstructured data. We propose an
                  improvement of existing rank-minimization techniques
                  to do regularization. We also demonstrate that we
                  can perform seismic data regularization and
                  interpolation simultaneously. We illustrate the
                  advantages of the modification using a real seismic
                  line from the Gulf of Suez to obtain high quality
                  results for regularization and interpolation, a key
                  application in exploration geophysics.},
  keywords = {regularization, interpolation, matrix completion, NFFT},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/kumar2014SEGmcu/kumar2014SEGmcu.html}
}


@TECHREPORT{smithyman2014SEGjfw,
  author = {Brendan R. Smithyman and Bas Peters and Bryan DeVault and Felix J. Herrmann},
  title = {Joint full-waveform inversion of on-land surface and {VSP} data from the {Permian} {Basin}},
  year = {2014},
  month = {04},
  institution = {UBC},
  abstract = {Full-waveform Inversion is applied to generate a
                  high-resolution model of P-wave velocity for a site
                  in the Permian Basin, Texas, USA. This investigation
                  jointly inverts seismic waveforms from a surface 3-D
                  vibroseis surface seismic survey and a co-located
                  3-D Vertical Seismic Profiling (VSP) survey, which
                  shared common source Vibration Points (VPs). The
                  resulting velocity model captures features that were
                  not resolvable by conventional migration velocity
                  analysis.},
  keywords = {full-waveform inversion, seismic, land, vibroseis, downhole receivers},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/smithyman2014SEGjfw/smithyman2014SEGjfw.html}
}


@TECHREPORT{slim2014NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2014 {DNOISE} progress report},
  year = {2014},
  institution = {UBC},
  abstract = {As we entered the second half of the DNOISE II project,
                  we are happy to report that we have made significant
                  progress on several fronts. Firstly, our work on
                  seismic data acquisition with compressive sensing is
                  becoming widely recognized. For instance,
                  ConocoPhilips ran a highly successful field trial on
                  Marine acquisition with compressive sensing and
                  obtained significant improvements compared to
                  standard production (see figure below). Moreover,
                  one of the main outcomes of this year’s EAGE
                  workshop was that industry is ready to adapt
                  randomized sampling as a new acquisition
                  paradigm. Needless to say this is a big success for
                  what we have been trying to accomplish with DNOISE
                  II. Finally, we have made a breakthrough in the
                  application of randomized sampling in 4-D seismic,
                  which is receiving a lot of interest from
                  industry. Secondly, our work on large-scale
                  optimization in the context of wave-equation based
                  inversion is also increasingly widely adapted. For
                  instance, our batching techniques are making the
                  difference between making a loss or profit for a
                  large contractor company active in the area of
                  full-waveform inversion. We also continued to make
                  progress in exciting new directions that go beyond
                  sparsity promotion and which allow us to exploit
                  other types of structure within the data, such as
                  low-rank for matrices or hierarchical Tucker formats
                  for tensors. Application of these techniques show
                  excellent results and in certain cases, such as
                  source separation problems with small dithering,
                  show significant improvements over transform-domain
                  methods. Thirdly, we continued to make significant
                  progress in wave-equation based inversion. We
                  extended our new penalty-based formulation now
                  called Wavefield Reconstruction Inversion/Imaging to
                  include total-variation regularization and density
                  variations. We also continued to make progress on
                  multiples, imaging with multiples and 3-D
                  full-waveform inversion. Statoil is the latest
                  company to join and we have several other companies
                  that have shown a keen interest. We also received
                  substantial in-kind contributions including a
                  license to WesternGeco’s iOmega and HPC equipment
                  discounts. After many years of support BP decided
                  unfortunately to no longer support SINBAD quoting
                  financial headwind related to the Deep horizon
                  disaster. On a more positive note, we are extremely
                  happy to report major progress on our efforts to
                  secure access to high-performance compute, including
                  renewed funding from NSERC and our involvement in
                  the International Inversion Initiative in Brazil. 9
                  peer-reviewed journal publications have resulted
                  from our work within the reporting period, with a
                  further 6 submitted, and DNOISE members disseminated
                  the results of our research at 49 major national and
                  international conference presentations. On the HQP
                  training side, 4 MSc students have recently
                  graduated, with one obtaining a position with CGG
                  Calgary, and we added 4 postdocs and 3 PhD students
                  to our team in September 2014, greatly increasing
                  our research capacity. As can be seen from the
                  report below, we are well on schedule and on certain
                  topics well beyond the milestones included in the
                  original proposal. With the purchase of the new
                  cluster we expect to see a surge of activity in
                  extending our algorithms to 3D. With this increased
                  capacity, we continue to be in an excellent position
                  to make fundamental contributions to the fields of
                  seismic data acquisition, processing, and
                  wave-equation based inversion. In the sections
                  below, we give a detailed overview of the research
                  and publication activities of the different members
                  of the group and how these relate to the objectives
                  of the grant, to industrial uptake, and to
                  outreach. Unless stated otherwise the students and
                  PDFs are (co)-supervised by the PI. We refer to the
                  publications section 4.0 for a complete list of our
                  presentations, conference proceedings, and journal
                  publications. We also refer to our mindmap, which
                  clearly establishes connections between the
                  different research topics we have embarked upon as
                  part of the DNOISE II project.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Tech%20Report/NSERC/2014/Progress_Report_2014.html}
}


@TECHREPORT{dasilva2014htuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Optimization on the {Hierarchical} {Tucker} manifold - applications to tensor completion},
  year = {2014},
  month = {03},
  institution = {UBC},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker (HT) tensors, an efficient
                  structured tensor format based on recursive subspace
                  factorizations. By exploiting the smooth manifold
                  structure of these tensors, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient for completing tensors from
                  missing entries. Our algorithmic framework is fast
                  and scalable to large problem sizes as we do not
                  require SVDs on the ambient tensor space, as
                  required by other methods. Moreover, we exploit the
                  structure of the Gramian matrices associated with
                  the HT format to regularize our problem, reducing
                  overfitting for high subsampling ratios. We also
                  find that the organization of the tensor can have a
                  major impact on completion from realistic seismic
                  acquisition geometries. These samplings are far from
                  idealized randomized samplings that are usually
                  considered in the literature but are realizable in
                  practical scenarios. Using these algorithms, we
                  successfully interpolate large-scale seismic data
                  sets and demonstrate the competitive computational
                  scaling of our algorithms as the problem sizes
                  grow.},
  keywords = {hierarchical tucker, structured tensor, tensor interpolation, differential geometry, riemannian optimization, gauss newton},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2014/dasilva2014htuck/dasilva2014htuck.pdf}
}


@TECHREPORT{slim2011NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2011 {DNOISE} progress report},
  year = {2011},
  institution = {UBC},
  abstract = {The main thrust of the DNOISE project is focused on the
                  following researth themes: [1] seismic acquisition
                  design and recovery from incomplete data with the
                  goal to reduce acquisition costs while increasing
                  the spatial bandwidth and aperture of seismic data;
                  [2] Removal of the 'surface nonlinearity' by
                  simultaneous estimation of the source signature and
                  the surface-free Green's function by inverting the
                  surface-related multiple prediction operator; [3]
                  Reduction of the computational complexity of
                  full-waveform inversion (FWI) by randomized
                  dimensionality reduction; [4] "Convexification" of
                  FWI to remove or at least diminish the adverse
                  effects of non-uniqueness that has plagued FWI since
                  its inception; The first three themes are directed
                  towards removing major impediments faced by FWI
                  related to the costs of acquiring data, the
                  computational costs of processing and inverting
                  data, and to issues with source calibration and
                  surface-related multiples. The final theme is more
                  'blue sky' and tries to incorporate ideas from
                  migration-velocity analysis into the formulation of
                  full-waveform inversion. Aside from these themes, we
                  will continue to work on seismic data acquisition
                  schemes that favor sparsity-promoting recovery and
                  on the development of large-scale solvers using
                  recent developments in convex and stochastic
                  optimization.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2011/nserc-2011-dnoise-progress-report.pdf}
}


@TECHREPORT{slim2010NSERCapp,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2010 {DNOISE} application},
  year = {2010},
  institution = {UBC},
  abstract = {DNOISE II: Dynamic Nonlinear Optimization for Imaging in
                  Seismic Exploration is a multidisciplinary research
                  project that involves faculty from the Mathematics,
                  Computer Science, and Earth and Ocean Sciences
                  Departments at the University of British Columbia.
                  DNOISE II constitutes a transformative research
                  program towards a new paradigm in seismic
                  exploration where the acquisition- and
                  processing-related costs are no longer determined by
                  the survey area and discretization but by
                  transform-domain sparsity of the final result. In
                  this approach, we rid ourselves from the
                  confinements of conventional overly stringent
                  sampling criteria that call for regular sampling
                  with sequentiual sources at Nyquist rates. By
                  adapting the principles of compressive sensing,
                  DNOISE II promotes a ground-up formulation for
                  seismic imaging where adverse subsampling-related
                  artifacts are removed by intelligent
                  simultaneous-acquisition design and recovery by
                  transform-domain sparsity promotion. This
                  development---in conjunction with our track records
                  in sparse recovery and time-harmonic Helmholtz
                  solvers---puts us in an unique position to deliver
                  on fundamental breakthroughs in the development and
                  implementation of the next-generation of processing,
                  imaging, and full-waveform inversion solutions.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2010/nserc-2010-dnoise-application.pdf}
}


@TECHREPORT{oghenekohwo2013SEGtlswrs,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Time-lapse seismics with randomized sampling},
  year = {2013},
  institution = {UBC},
  abstract = {In time-lapse or 4D seismics, repeatability of the
                  acquisition is a very crucial step, as we do not
                  want spurious events that are not there. In this
                  paper, we propose an approach which avoids any
                  requirement to repeat the surveys, by using
                  randomized sampling technique which allows us to be
                  more efficient in the acquisition. Our method
                  applies to sampling data using ocean bottom nodes
                  (OBN) as receivers. We test the efficacy of our
                  proposed randomized acquisition geometry for
                  time-lapse survey on two different models. In the
                  first example, model properties does not change with
                  time, while in the second example, model exhibit a
                  time-lapse effect which may be caused by the
                  migration of fluid within the reservoir. We perform
                  two types of randomized sampling - uniform
                  randomized sampling and jittered sampling to
                  visualize the effects of non-repeatability in
                  time-lapse survey. We observe that jittered
                  randomized sampling is a more efficient method
                  compared to randomized sampling, due to it's
                  requirement to control the maximum spacing between
                  the receivers. The results are presented, in the
                  image space, as a least-squares migration of the
                  model perturbation and they are shown for a subset
                  of a synthetic model - the Marmousi model},
  keywords = {acquisition, time-lapse, migration, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/oghenekohwo2013SEGtlswrs/oghenekohwo2013SEGtlswrs.pdf}
}


@TECHREPORT{slim2013NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2013 {DNOISE} progress report},
  year = {2013},
  institution = {UBC},
  abstract = {As we enter the second half of the DNOISE II project, we
                  are happy to report that we have made significant
                  progress on several fronts. Firstly, our work on
                  seismic data acquisition with compressive sensing is
                  becoming widely recognized, reflected in adaptations
                  of this technology by industry and in this year’s
                  SEG Karcher award, which went to Gilles Hennenfent,
                  who was one of the researchers who started working
                  in this area in our group. As this report shows, we
                  continued to make progress on this topic with
                  numerous presentations, publications, and software
                  releases. Secondly, our work on large-scale
                  optimization is also widely adapted and instrumental
                  to the different research areas on the grant. In
                  particular, we are excited about new directions that
                  go beyond sparsity promotion and which allow us to
                  exploit other types of structure within the data,
                  such as low-rank. Over the near future, we expect to
                  see a body of new research based on these findings
                  touching acquisition as well as the wave-equation
                  based inversion aspects of our research
                  program. Thirdly, we are also very happy to report
                  that we continued to make substantial progress in
                  wave-equation base inversion. In particular, we
                  would like to mention successes in the areas of
                  acceleration of sparsity-promoting imaging with
                  source estimation and multiples and in theoretical
                  as well as practical aspects of full-waveform
                  inversion. We derived a highly practical and
                  economic formulation of 3-D FWI and we also came up
                  with a complete new formulation of FWI, which
                  mitigates issues related to cycle skipping. Finally,
                  we made a lot of progress applying our algorithm to
                  industrial datasets, which has been well received by
                  industry. Our findings show that FWI is still an
                  immature technology calling for more theoretical
                  input and for the development of practical
                  workflows. Over the last year our work cumulated in
                  14 peer-reviewed journal publications, 5 submitted
                  journal publications, 13 (+ 9) extended abstracts,
                  32 talks at international conferences, and 6
                  software packages. Finally, we are happy to report
                  that we have been joined by several new companies,
                  namely, ION Geophysical, CGG, and Woodside. At this
                  midpoint of the Grant, we are also happy to report
                  that we are well on schedule to meet the milestones
                  included in the original proposal. Given our wide
                  range of expertise and our plans to replace our
                  compute cluster, we continue to be in an excellent
                  position to make fundamental contributions to the
                  fields of seismic data acquisition, processing, and
                  wave-equation based inversion.},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2013/Progress_Report_2013.pdf}
}


@TECHREPORT{slim2012NSERCpr,
  author = {Felix J. Herrmann},
  title = {{NSERC} 2012 {DNOISE} progress report},
  year = {2012},
  institution = {UBC},
  keywords = {NSERC, DNOISE, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/TechReport/NSERC/2012/Progress_Report_2012.pdf}
}


@TECHREPORT{petrenko2013SEGsaoc,
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of {CARP}, an iterative linear solver and preconditioner},
  year = {2013},
  institution = {UBC},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7\% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {Helmholtz equation, Kaczmarz, software, wave propagation, frequency-domain, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/petrenko2013SEGsaoc/petrenko2013SEGsaoc.pdf}
}


@TECHREPORT{kumar2013ICMLlr,
  author = {Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {An {SVD}-free {Pareto} curve approach to rank minimization},
  year = {2013},
  institution = {UBC},
  abstract = {Recent SVD-free matrix factorization formulations have
                  enabled rank optimization for extremely large-scale
                  systems (millions of rows and columns). In this
                  paper, we consider rank-regularized formulations
                  that only require a target data-fitting error level,
                  and propose an algorithm for the corresponding
                  problem. We illustrate the advantages of the new
                  approach using the Netflix problem, and use it to
                  obtain high quality results for seismic trace
                  interpolation, a key application in exploration
                  geophysics. We show that factor rank can be easily
                  adjusted as the inversion proceeds, and propose a
                  weighted extension that allows known subspace
                  information to improve the results of matrix
                  completion formulations. Using these methods, we
                  obtain high-quality reconstructions for large scale
                  seismic interpolation problems with real data.},
  keywords = {interpolation, low-rank},
  month = {02},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2013/kumar2013ICMLlr/kumar2013ICMLlr.pdf}
}


@TECHREPORT{li2013EAGEwebmplijsp,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Wave-equation based multi-parameter linearized inversion with joint-sparsity promotion},
  year = {2013},
  institution = {UBC},
  abstract = {The successful application of linearized inversion is
                  affected by the prohibitive size of the data,
                  computational resources required, and how accurately
                  the model parameters reflects the real Earth
                  properties. The issue of data size and computational
                  resources can be addressed by combining ideas from
                  sparsity promoting and stochastic optimization,
                  which can allow us to invert model perturbation with
                  a small subset of the data, yielding a few PDE
                  solves for the inversion. In this abstract, we are
                  aiming at addressing the issue of accuracy of model
                  parameters by inverting density and velocity
                  simultaneously rather than only using velocity. As a
                  matter of face, the effects of density and velocity
                  variations towards the wavefield are very similar,
                  which will cause energy leakage between density and
                  velocity images. To overcome this issue, we proposed
                  a incoherence enhanced method that can reduce the
                  similarity between the effect of density and
                  velocity. Moreover, the location of structural
                  variations in velocity and density are often
                  overlapped in geological setting, thus in this
                  abstract, we also exploit this property with
                  joint-sparsity promoting to further improve the
                  imaging result.},
  keywords = {linearized inversion, incoherence enhancement, joint-sparsity},
  month = {01},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2013/li2013EAGEwebmplijsp/li2013EAGEwebmplijsp.pdf}
}


@TECHREPORT{vanLeeuwen2013Penalty2,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for {PDE}-constrained optimization},
  year = {2013},
  institution = {UBC},
  abstract = {We present a method for solving PDE constrained
                  optimization problems based on a penalty
                  formulation. This method aims to combine advantages
                  of both full-space and reduced methods by exploiting
                  a large search-space (consisting of both control and
                  state variables) while allowing for an efficient
                  implementation that avoids storing and updating the
                  state-variables. This leads to a method that has
                  roughly the same per-iteration complexity as
                  conventional reduced approaches while dening an
                  objective that is less non-linear in the control
                  variable by implicitly relaxing the constraint. We
                  apply the method to a seismic inverse problem where
                  it leads to a particularly ecient implementation
                  when compared to a conventional reduced approach as
                  it avoids the use of adjoint
                  state-variables. Numerical examples illustrate the
                  approach and suggest that the proposed formulation
                  can indeed mitigate some of the well-known problems
                  with local minima in the seismic inverse problem.},
  keywords = {waveform inversion, optimization, private},
  month = {04},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Tech%20Report/2013/vanLeeuwen2013Penalty2/vanLeeuwen2013Penalty2.pdf}
}


@TECHREPORT{vanleeuwen2012CGMN,
  author = {Tristan van Leeuwen},
  title = {Fourier analysis of the {CGMN} method for solving the {Helmholtz} equation},
  year = {2012},
  institution = {Department of Earth, Ocean and Atmospheric Sciences},
  address = {The University of British Columbia, Vancouver},
  abstract = {The Helmholtz equation arises in many applications, such
                  as seismic and medical imaging. These application
                  are characterized by the need to propagate many
                  wavelengths through an inhomogeneous medium. The
                  typical size of the problems in 3D applications
                  precludes the use of direct factorization to solve
                  the equation and hence iterative methods are used in
                  practice. For higher wavenumbers, the system becomes
                  increasingly indefinite and thus good
                  preconditioners need to be constructed. In this note
                  we consider an accelerated Kazcmarz method (CGMN)
                  and present an expression for the resulting
                  iteration matrix. This iteration matrix can be used
                  to analyze the convergence of the CGMN method. In
                  particular, we present a Fourier analysis for the
                  method applied to the 1D Helmholtz equation. This
                  analysis suggests an optimal choice of the
                  relaxation parameter. Finally, we present some
                  numerical experiments.},
  keywords = {Helmholtz equation, modelling},
  url = {http://arxiv.org/abs/1210.2644},
}
  

@TECHREPORT{almatar10SEGesfd,
  author = {Mufeed H. AlMatar and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of surface-free data by curvelet-domain matched filtering and sparse inversion},
  institution = {Department of Earth and Ocean Sciences},
  year = {2010},
  address = {University of British Columbia, Vancouver},
  abstract = {Matching seismic wavefields and images lies at the heart
                  of many pre-post-processing steps part of seismic
                  imaging- whether one is matching predicted wavefield
                  components, such as multiples, to the actual
                  to-be-separated wavefield components present in the
                  data or whether one is aiming to restore migration
                  amplitudes by scaling, using an image-to-remigrated-
                  image matching procedure to calculate the scaling
                  coefficients. The success of these wavefield
                  matching procedures depends on our ability to (i)
                  control possible overfitting, which may lead to
                  accidental removal of energy or to inaccurate
                  image-amplitude corrections, (ii) handle data or
                  images with nonunique dips, and (iii) apply
                  subsequent wavefield separations or migraton
                  amplitude corrections stably. In this paper, we show
                  that the curvelet transform allows us to address all
                  these issues by im- posing smoothness in phase
                  space, by using their capability to handle
                  conflicting dips, and by leveraging their ability to
                  represent seismic data and images sparsely. This
                  latter property renders curvelet-domain sparsity
                  promotion an effective prior.},
  keywords = {SEG},
  organization = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/almatar10SEGesfd/almatar10SEGesfd.pdf}
}


@TECHREPORT{herrmann2010SEGerc,
  author = {Felix J. Herrmann},
  title = {Empirical recovery conditions for seismic sampling},
  institution = {Department of Earth and Ocean Sciences, UBC},
  year = {2010},
  abstract = {In this paper, we offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  towards seismic acquisition and processing for data
                  that are traditionally considered to be
                  undersampled. The main outcome of this approach is a
                  new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria, such as Nyquist. At the
                  heart of our approach lies randomized incoherent
                  sampling that breaks subsampling related
                  interferences by turning them into harmless noise,
                  which we subsequently remove by promoting
                  transform-domain sparsity. Now, costs no longer grow
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  acquisition. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/herrmann10SEGerc/herrmann10SEGerc.pdf}
}


@TECHREPORT{hennenfent08TRori,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the {Pareto} curve},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-5},
  abstract = {Geophysical inverse problems typically involve a trade
                  off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain relatively
                  unexplored. First, we show how these curves provide
                  an objective criterion to gauge how robust one-norm
                  solvers are when they are limited by a maximum
                  number of matrix-vector products that they can
                  perform. Second, we use Pareto curves and their
                  properties to define and compute one-norm
                  compressibilities. We argue this notion is key to
                  understand one-norm regularized inversion. Third, we
                  illustrate the correlation between the one-norm
                  compressibility and the performance of Fourier and
                  curvelet reconstructions with sparsity promoting
                  inversion.},
  keywords = {SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2008/hennenfent08TRori/hennenfent08TRori.pdf}
}


@TECHREPORT{rajiv2012SEGFRM,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Fast methods for rank minimization with applications to seismic-data interpolation},
  institution = {Department of Earth and Ocean Sciences},
  year = {2012},
  number = {TR-2012-04},
  address = {University of British Columbia, Vancouver},
  abstract = {Rank penalizing techniques are an important direction in
                  seismic inverse problems, since they allow improved
                  recovery by exploiting low-rank structure. A major
                  downside of current state of the art techniques is
                  their reliance on the SVD of seismic data
                  structures, which can be prohibitively
                  expensive. Fortunately, recent work allows us to
                  circumvent this problem by working with matrix
                  factorizations. We review a novel approach to rank
                  penalization, and successfully apply it to the
                  seismic interpolation problem by exploiting the
                  low-rank structure of seismic data in the
                  midpoint-offset domain. Experiments for the recovery
                  of 2D monochromatic data matrices and seismic lines
                  represented as 3D volumes support the feasibility
                  and potential of the new approach.},
  keywords = {rank, optimization, seismic data interpolation},
  month = {04},
  organization = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications//Public/TechReport/2012/rajiv2012SEGFRM/rajiv2012SEGFRM.pdf}
}


@TECHREPORT{lebed08TRhgg,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker's guide to the galaxy of transform-domain sparsification},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-4},
  abstract = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last decade many
                  transforms such as wavelets, curvelets, contourlets,
                  surfacelets, shearlets, and many other types of
                  'x-lets' have been developed to try to resolve this
                  issue. In this abstract we compare the properties of
                  four of these commonly used transforms, namely the
                  shift-invariant wavelets, complex wavelets,
                  curvelets and surfacelets. We also briefly explore
                  the performance of these transforms for the problem
                  of recovering seismic wavefields from incomplete
                  measurements.},
  keywords = {SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2008/lebed08TRhgg/lebed08TRhgg.pdf}
}


@TECHREPORT{tang09TRdtr,
  author = {Gang Tang and Reza Shahidi and Jianwei Ma},
  title = {Design of two-dimensional randomized sampling schemes for curvelet-based sparsity-promoting seismic data recovery},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2009},
  number = {TR-2009-03},
  abstract = {The tasks of sampling, compression and reconstruction
                  are very common and often necessary in seismic data
                  processing due to the large size of seismic
                  data. Curvelet-based Recovery by Sparsity-promoting
                  Inversion, motivated by the newly developed theory
                  of compressive sensing, is among the best recovery
                  strategies for seismic data. The incomplete data
                  input to this curvelet-based recovery is determined
                  by randomized sampling of the original complete
                  data. Unlike usual regular undersampling, randomized
                  sampling can convert aliases to easy-to-eliminate
                  noise, thus facilitating the process of
                  reconstruction of the complete data from the
                  incomplete data. Randomized sampling methods such as
                  jittered sampling have been developed in the past
                  that are suitable for curvelet-based recovery,
                  however most have only been applied to sampling in
                  one dimension. Considering that seismic datasets are
                  usually higher dimensional and extremely large, in
                  the present paper, we extend the 1D version of
                  jittered sampling to two dimensions, both with
                  underlying Cartesian and hexagonal grids. We also
                  study separable and non-separable two dimensional
                  jittered sampling, the former referring to the
                  Kronecker product of two one-dimensional jittered
                  samplings. These different categories of jittered
                  sampling are compared against one another in terms
                  of signal-to-noise ratio and visual quality, from
                  which we find that jittered hexagonal sampling is
                  better than jittered Cartesian sampling, while fully
                  non-separable jittered sampling is better than
                  separable sampling. Because in the image processing
                  and computer graphics literature, sampling patterns
                  with blue-noise spectra are found to be ideal to
                  avoid aliasing, we also introduce two other
                  randomized sampling methods, possessing sampling
                  spectra with beneficial blue noise characteristics,
                  Poisson Disk sampling and Farthest Point
                  sampling. We compare these methods, and apply the
                  introduced sampling methodologies to higher
                  dimensional curvelet-based reconstruction. These
                  sampling schemes are shown to lead to better results
                  from CRSI compared to the other more traditional
                  sampling protocols, e.g. regular subsampling.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Journals/2009/tang09TRdtr/tang09TRdtr.pdf}
}


@TECHREPORT{vandenberg07TRipr,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {In pursuit of a root},
  institution = {Department of Computer Science},
  year = {2007},
  month = {06},
  number = {TR-2007-19},
  address = {University of British Columbia, Vancouver},
  abstract = {The basis pursuit technique is used to find a minimum
                  one-norm solution of an underdetermined
                  least-squares problem. Basis pursuit denoise fits
                  the least-squares problem only approximately, and a
                  single parameter determines a curve that traces the
                  trade-off between the least-squares fit and the
                  one-norm of the solution. We show that the function
                  that describes this curve is convex and continuously
                  differentiable over all points of interest. The dual
                  solution of a least-squares problem with an explicit
                  one-norm constraint gives function and derivative
                  information needed for a root-finding method. As a
                  result, we can compute arbitrary points on this
                  curve. Numerical experiments demonstrate that our
                  method, which relies on only matrix-vector
                  operations, scales well to large problems.},
  url = {http://www.optimization-online.org/DB_HTML/2007/06/1708.html}
}


@TECHREPORT{vanleeuwen2012SEGparallel,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A parallel, object-oriented framework for frequency-domain wavefield imaging and inversion.},
  institution = {Department of Earth and Ocean Sciences},
  year = {2012},
  number = {TR-2012-03},
  address = {University of British Columbia, Vancouver},
  abstract = {We present a parallel object-oriented matrix-free
                  framework for frequency-domain seismic modeling,
                  imaging and inversion. The key aspects of the
                  framework are its modularity and level of
                  abstraction, which allows us to write code that
                  reflects the underlying mathematical structure and
                  develop unit-tests that guarantee the fidelity of
                  the code. By overloading standard linear-algebra
                  operations, such as matrix-vector multiplications,
                  we can use standard optimization packages to work
                  with our code without any modification. This leads
                  to a scalable testbed on which new methods can be
                  rapidly prototyped and tested on medium-sized 2D
                  problems. Although our current implementation uses
                  (parallel) Matlab, all of these design principles
                  can also be met by using lower-level languages which
                  is important when we want to scale to realistic 3D
                  problems. We present some numerical examples on
                  synthetic data.},
  keywords = {modeling, imaging, inversion, SEG},
  month = {04},
  organization = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2012/vanleeuwen2012SEGparallel/vanleeuwen2012SEGparallel.pdf}
}


@TECHREPORT{vanleeuwen2012smii,
  author = {Tristan van Leeuwen},
  title = {A parallel matrix-free framework for frequency-domain seismic modelling, imaging and inversion in Matlab},
  year = {2012},
  abstract = {I present a parallel matrix-free framework for
                  frequency-domain seismic modeling, imaging and
                  inversion. The framework provides basic building
                  blocks for designing and testing optimization-based
                  formulations of both linear and non-linear seismic
                  in- verse problems. By overloading standard
                  linear-algebra operations, such as matrix-vector
                  multiplications, standard optimization packages can
                  be used to work with the code without any
                  modification. This leads to a scalable testbed on
                  which new methods can be rapidly prototyped and
                  tested on medium-sized 2D problems. I present some
                  numerical examples on both linear and non-linear
                  seismic inverse problems.},
  keywords = {seismic imaging, optimization, Matlab, object-oriented programming},
  month = {07},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2012/vanleeuwen2012smii/vanleeuwen2012smii.pdf}
}


@TECHREPORT{wang08TRbss,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Bayesian-signal separation by sparsity promotion: application to primary-multiple separation},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-1},
  abstract = {Successful removal of coherent noise sources greatly
                  determines the quality of seismic imaging. Major
                  advances were made in this direction, e.g.,
                  Surface-Related Multiple Elimination (SRME) and
                  interferometric ground-roll removal. Still, moderate
                  phase, timing, amplitude errors and clutter in the
                  predicted signal components can be detrimental.
                  Adopting a Bayesian approach along with the
                  assumption of approximate curvelet-domain
                  independence of the to-be-separated signal
                  components, we construct an iterative algorithm that
                  takes the predictions produced by for example SRME
                  as input and separates these components in a robust
                  fashion. In addition, the proposed algorithm
                  controls the energy mismatch between the separated
                  and predicted components. Such a control, which was
                  lacking in earlier curvelet-domain formulations,
                  produces improved results for primary-multiple
                  separation on both synthetic and real data.},
  keywords = {signal separation, SLIM},
  month = {01},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Journals/2008/wang08TRbss/paper_html/paper.html}
}

