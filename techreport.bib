% This file was created with JabRef 2.6.
% Encoding: MacRoman

@TECHREPORT{Friedlander11hds,
  author = {Michael P. Friedlander and Mark Schmidt},
  title = {Hybrid deterministic-stochastic methods for data fitting},
  institution = {Department of Computer Science},
  year = {2011},
  type = {Tech. Rep.},
  address = {University of British Columbia, Vancouver},
  month = {April},
  abstract = {Many structured data-fitting applications require the solution of
	an optimization problem involving a sum over a potentially large
	number of measurements. Incremental gradient algorithms (both deterministic
	and randomized) offer inexpensive iterations by sampling only subsets
	of the terms in the sum. These methods can make great progress initially,
	but often slow as they approach a solution. In contrast, full gradient
	methods achieve steady convergence at the expense of evaluating the
	full objective and gradient on each iteration. We explore hybrid
	methods that exhibit the benefits of both approaches. Rate of convergence
	analysis and numerical experiments illustrate the potential for the
	approach.},
  publisher = {Department of Computer Science},
  url = {http://www.cs.ubc.ca/~mpf/papers/FriedlanderSchmidt2011.pdf}
}

@TECHREPORT{Mansour:2011,
  author = {Michael P. Friedlander and Hassan Mansour and Rayan Saab and Ozgur
	Yilmaz},
  title = {Recovering compressively sampled signals using partial support information},
  institution = {Department of Computer Science},
  year = {2011},
  type = {Tech. Rep.},
  address = {University of British Columbia, Vancouver},
  month = {July},
  abstract = {We study recovery conditions of weighted $\ell_1$ minimization for signal
	reconstruction from compressed sensing measurements when partial
	support information is available. We show that if at least 50\% of
	the (partial) support information is accurate, then weighted $\ell_1$ minimization
	is stable and robust under weaker sufficient conditions than the
	analogous conditions for standard $\ell_1$ minimization. Moreover, weighted
	$\ell_1$ minimization provides better upper bounds on the reconstruction
	error in terms of the measurement noise and the compressibility of
	the signal to be recovered. We illustrate our results with extensive
	numerical experiments on synthetic data and real audio and video
	signals.},
  url = {http://slim/Publications/Public/Journals/mansour2011.pdf}
}

@TECHREPORT{BergFriedlander:2010,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Sparse optimization with least-squares constraints},
  institution = {Department of Computer Science},
  year = {2010},
  type = {Tech. Rep.},
  number = {TR-2010-02},
  address = {University of British Columbia, Vancouver},
  month = {January},
  abstract = {The use of convex optimization for the recovery of sparse signals
	from incomplete or compressed data is now common practice. Motivated
	by the success of basis pursuit in recovering sparse vectors, new
	formulations have been proposed that take advantage of different
	types of sparsity. In this paper we propose an efficient algorithm
	for solving a general class of sparsifying formulations. For several
	common types of sparsity we provide applications, along with details
	on how to apply the algorithm, a nd experimental results.}
}

@ TECHREPORT{Haber2010emp,
  title = {An effective method for parameter estimation with PDE constraints
	with multiple right hand sides},
  author = {Eldad Haber and Matthias Chung and Felix J. Herrmann},
  year = {2010},
  abstract = {Many parameter estimation problems involve with a parameter-dependent
	PDEs with multiple right hand sides. The computational cost and memory
	requirements of such problems increases linearly with the number
	of right hand sides. For many applications this is the main bottleneck
	of the computation. In this paper we show that problems with multiple
	right hand sides can be reformulated as stochastic optimization problems
	that are much cheaper to solve. We discuss the solution methodology
	and use the direct current resistivity and seismic tomography as
	model problems to show the effectiveness of our approach.},
  keywords = {SLIM},
  number = {TR-2010-4},
  publisher = {UBC-Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/Haber2010emp.pdf}
}

@TECHREPORT{BergFriedlander:2009,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Theoretical and empirical results for recovery from multiple measurements},
  institution = {Department of Computer Science},
  year = {2009},
  type = {Tech. Rep.},
  number = {TR-2009-7},
  address = {University of British Columbia, Vancouver},
  month = {September},
  note = {to appear in {\it IEEE Trans. Info. Th.}},
  abstract = {The joint-sparse recovery problem aims to recover, from sets of compressed
	measurements, unknown sparse matrices with nonzero entries restricted
	to a subset of rows. This is an extension of the single-measurement-vector
	(SMV) problem widely studied in compressed sensing. We analyze the
	recovery properties for two types of recovery algorithms. First,
	we show that recovery using sum-of-norm minimization cannot exceed
	the uniform recovery rate of sequential SMV using L1 minimization,
	and that there are problems that can be solved with one approach
	but not with the other. Second, we analyze the performance of the
	ReMBo algorithm [M. Mishali and Y. Eldar, IEEE Trans. Sig. Proc.,
	56 (2008)] in combination with L1 minimization, and show how recovery
	improves as more measurements are taken. From this analysis it follows
	that having more measurements than number of nonzero rows does not
	improve the potential theoretical recovery rate.}
}

@TECHREPORT{hennenfent08,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the Pareto curve},
  number = {TR-2008-5},
  year = {2008},
  abstract = {Geophysical inverse problems typically involve a trade off between
	data misfit and some prior. Pareto curves trace the optimal trade
	off between these two competing aims. These curves are commonly used
	in problems with two-norm priors where they are plotted on a log-log
	scale and are known as L-curves. For other priors, such as the sparsity-promoting
	one norm, Pareto curves remain relatively unexplored. First, we show
	how these curves provide an objective criterion to gauge how robust
	one-norm solvers are when they are limited by a maximum number of
	matrix-vector products that they can perform. Second, we use Pareto
	curves and their properties to define and compute one-norm compressibilities.
	We argue this notion is key to understand one-norm regularized inversion.
	Third, we illustrate the correlation between the one-norm compressibility
	and the performance of Fourier and curvelet reconstructions with
	sparsity promoting inversion.},
  keywords = {SLIM},
  number = {TR-2008-5},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/hennenfent08seg.pdf}
}

@TECHREPORT{lebed08,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker's guide to the galaxy of transform-domain sparsification},
  number = {TR-2008-4},
  year = {2008},
  abstract = {The ability to efficiently and sparsely represent seismic data is
	becoming an increasingly important problem in geophysics. Over the
	last decade many transforms such as wavelets, curervelets, contourlets,
	surfacelets, shearlets, and many other types of 'x-lets' have been
	developed to try to resolve this issue. In this abstract we compare
	the properties of four of these commonly used transforms, namely
	the shift-invariant wavelets, complex wavelets, curvelets and surfacelets.
	We also briefly explore the performance of these transforms for the
	problem of recovering seismic wavefields from incomplete measurements.},
  keywords = {SLIM},
  number = {TR-2008-4},
  publisher = {UBC Earth and Ocean Sciences Department},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/lebed08seg.pdf}
}

@TECHREPORT{wang08,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
  title = {Curvelet-domain matched filtering}
  number = {TR-2008-6.},
  year = {2008},
  abstract = {Matching seismic wavefields and images lies at the heart of many pre-/post-processing
	steps part of seismic imaging--- whether one is matching predicted
	wavefield components, such as multiples, to the actual to-be-separated
	wavefield components present in the data or whether one is aiming
	to restore migration amplitudes by scaling, using an image-to-remigrated-image
	matching procedure to calculate the scaling coefficients. The success
	of these wavefield matching procedures depends on our ability to
	(i) control possible overfitting, which may lead to accidental removal
	of energy or to inaccurate image-amplitude corrections, (ii) handle
	data or images with nonunique dips, and (iii) apply subsequent wavefield
	separations or migraton amplitude corrections stably. In this paper,
	we show that the curvelet transform allows us to address all these
	issues by imposing smoothness in phase space, by using their capability
	to handle conflicting dips, and by leveraging their ability to represent
	seismic data and images sparsely. This latter property renders curvelet-domain
	sparsity promotion an effective prior.},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08segmat.pdf}
}

 @ TECHREPORT{BergFrie:2007,
  Address = {University of British Columbia, Vancouver},
  Author = {Ewout {van den} Berg and Michael P. Friedlander},
  Institution = {Department of Computer Science},
  Month = {June},
  Number = {TR-2007-19},
  Title = {In pursuit of a root},
  Type = {Tech. Rep.},
  Url = {http://www.optimization-online.org/DB_HTML/2007/06/1708.html},
  Year = 2007
 }
 
 @ TECHREPORT{vandenberg07sat,
  title = {Sparco: a testing framework for sparse reconstruction},
  author = {E. van den Berg and Michael P. Friedlander and Gilles Hennenfent
	and Felix J. Herrmann and Rayan Saab and Ozgur Yilmaz},
  year = {2007},
  abstract = {Sparco is a framework for testing and benchmarking algorithms for
	sparse reconstruction. It includes a large collection of sparse reconstruction
	problems drawn from the imaging, compressed sensing, and geophysics
	literature. Sparco is also a framework for implementing new test
	problems and can be used as a tool for reproducible research. Sparco
	is implemented entirely in Matlab, and is released as open-source
	software under the GNU Public License.},
  keywords = {SLIM, Sparco},
  number = {TR-2007-20},
  publisher = {UBC Computer Science Department},
  url ={http://www.cs.ubc.ca/labs/scl/sparco/downloads.php?filename=sparco.pdf},
  presentation = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf}
}