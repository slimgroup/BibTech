% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2020-----%

@UNPUBLISHED{yang2020tdsp,
  author = {Mengmeng Yang and Zhilong Fang and Philipp A. Witte and Felix J. Herrmann},
  title = {Time-domain sparsity promoting least-squares reverse time migration
with source estimation},
  year = {2020},
  month = {2},
  abstract = {Least-squares reverse time migration is well-known for its capability to generate artifact-free true-amplitude subsurface images through fitting observed data in the least-squares sense. However, when applied to realistic imaging problems, this approach is faced with issues related to overfitting and excessive computational costs induced by many wave-equation solves. The fact that the source function is unknown complicates this situation even further. Motivated by recent results in stochastic optimization and transform-domain sparsity-promotion, we demonstrate that the computational costs of inversion can be reduced significantly while avoiding imaging artifacts and restoring amplitudes. While powerfull, these new approaches do require accurate information on the source-time function, which is often lacking. Without this information, the imaging quality deteriorates rapidly. We address this issue by presenting an approach where the source-time function is estimated on the fly through a technique known as variable projection. Aside from introducing negligible computational overhead, the proposed method is shown to perform well on imaging problems with noisy data and problems that involve complex settings such as salt. In either case, the presented method produces high resolution high-amplitude fidelity images including an estimates for the source-time function. In addition, due to its use of stochastic optimization, we arrive at these images at roughly one to two times the cost of conventional reverse time migration involving all data.},
  keywords = {sparsity inversion, source estimation, penalty},
  note = {Submitted to Geophysical Prospecting},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/yang2020tdsp/yang2020tdsp.html}
}

@UNPUBLISHED{rizzuti2020EAGEtwri,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Felix J. Herrmann},
  title = {Time-domain wavefield reconstruction inversion for large-scale
seismics},
  year = {2020},
  month = {1},
  abstract = {Wavefield reconstruction inversion is an imaging technique akin to
full-waveform inversion, albeit based on a relaxed version of the wave
equation. This relaxation aims to beat the multimodality typical of
full-waveform inversion. However it prevents the use of time-marching solvers
for the augmented equation and, as a consequence, cannot be straightforwardly
employed to large 3D problems. In this work, we formulate a dual version of
wavefield reconstruction inversion amenable to explicit time-domain solvers,
yielding a robust and scalable inversion technique.},
  keywords = {3D, Full-waveform inversion, EAGE, Time-domain},
  note = {Submitted to EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/rizzuti2020EAGEtwri/rizzuti2020EAGEtwri.html}
}

@UNPUBLISHED{siahkoohi2020EAGEdlb,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {A deep-learning based Bayesian approach to seismic imaging and
uncertainty quantification},
  year = {2020},
  month = {1},
  abstract = {Uncertainty quantification is essential when dealing with ill-conditioned
inverse problems due to the inherent nonuniqueness of the solution. Bayesian
approaches allow us to determine how likely an estimation of the unknown
parameters is via formulating the posterior distribution. Unfortunately, it
is often not possible to formulate a prior distribution that precisely
encodes our prior knowledge about the unknown. Furthermore, adherence to
handcrafted priors may greatly bias the outcome of the Bayesian analysis. To
address this issue, we propose to use the functional form of a randomly
initialized convolutional neural network as an implicit structured prior,
which is shown to promote natural images and excludes images with unnatural
noise. In order to incorporate the model uncertainty into the final estimate,
we sample the posterior distribution using stochastic gradient Langevin
dynamics and perform Bayesian model averaging on the obtained samples. Our
synthetic numerical experiment verifies that deep priors combined with
Bayesian model averaging are able to partially circumvent imaging artifacts
and reduce the risk of overfitting in the presence of extreme noise. Finally,
we present pointwise variance of the estimates as a measure of uncertainty,
which coincides with regions that are difficult to image.},
  keywords = {seismic imaging, uncertainty quantification, stochastic gradient
Langevin dynamics, deep learning, EAGE},
  note = {Submitted to EAGE},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/siahkoohi2020EAGEdlb/siahkoohi2020EAGEdlb.html}
}

%-----2019-----%

@UNPUBLISHED{daskalakis2019SIAMJISasr,
  author = {Emmanouil Daskalakis and Felix J. Herrmann and Rachel Kuske},
  title = {ACCELERATING SPARSE RECOVERY BY REDUCING CHATTER},
  year = {2019},
  abstract = {Compressive Sensing has driven a resurgence of sparse recovery algorithms with
  l_1-norm minimization. While these minimizations are relatively well understood for small underdetermined, possibly inconsistent systems, their behavior for large over-determined and inconsistent systems has received much less attention. Specifically, we focus on large systems where computational restrictions call for algorithms that use randomized subsets of rows that are touched a limited number of times. In that regime, l_1-norm minimization algorithms exhibit unwanted fluctuations near the desired solution, and the Linear Bregman iterations are no exception. We explain this observed lack of performance in terms of chatter, a well-known phenomena observed in non-smooth dynamical systems, where intermediate solutions wander between different states stifling convergence. By identifying chatter as the culprit, we modify the Bregman iterations with chatter reducing adaptive element-wise step lengths in combination with potential support detection via threshold crossing. We demonstrate the performance of our algorithm on carefully selected stylized examples and a realistic seismic imaging problem involving millions of unknowns and matrix-free matrix-vector products that involve expensive wave-equation solves.},
  keywords = {sparsity promotion, inconsistent linear systems, Kacmarz, linearized Bregman
dynamical systems, non-smooth dynamics, chatter},
  note = {Submitted to SIAM Journal on Imaging Sciences},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2019/daskalakis2019SIAMJISasr/daskalakis2019SIAMJISasr.pdf}
}

@UNPUBLISHED{witte2019RHPCssi,
  author = {Philipp A. Witte and Mathias Louboutin and Charles Jones and Felix J. Herrmann},
  title = {Serverless seismic imaging in the cloud},
  year = {2019},
  abstract = {This talk presents a serverless approach to seismic imaging in the
cloud based on high-throughput containerized batch processing, event-driven
computations and a  domain-specific language compiler for solving the
underlying wave equations. A 3D case study on Azure demonstrates that this
approach allows reducing the operating cost of up to a factor of 6, making
the cloud a viable alternative to on-premise HPC clusters for seismic
imaging.},
  keywords = {cloud, hpc, reverse-time migration, serverless},
  note = {Submitted to Rice Oil and Gas High Performance Computing Conference 2020 on November 27, 2019},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2019/witte2019RHPCssi/witte2019RHPCssi.html}
}

@UNPUBLISHED{witte2019TPDedas,
  author = {Philipp A. Witte and Mathias Louboutin and Henryk Modzelewski and Charles Jones and James Selvage and Felix J. Herrmann},
  title = {An Event-Driven Approach to Serverless Seismic Imaging in the Cloud},
  year = {2019},
  abstract = {Adapting the cloud for high-performance computing (HPC) is a
challenging task, as software for HPC applications hinges on fast network
connections and is sensitive to hardware failures. Using cloud infrastructure
to recreate conventional HPC clusters is therefore in many cases an
infeasible solution for migrating HPC applications to the cloud. As an
alternative to the generic lift and shift approach, we consider the specific
application of seismic imaging and demonstrate a serverless and event-driven
pproach for running large-scale instances of this problem in the cloud.
Instead of permanently running compute instances, our workflow is based on a
serverless architecture with high throughput batch computing and event-driven
computations, in which computational resources are only running as long as
they are utilized. We demonstrate that this approach is very flexible and
allows for resilient and nested levels of parallelization, including domain
decomposition for solving the underlying partial differential equations.
While the event-driven approach introduces some overhead as computational
resources are repeatedly restarted, it inherently provides resilience to
instance shut-downs and allows a significant reduction of cost by avoiding
idle instances, thus making the cloud a viable alternative to on-premise
clusters for large-scale seismic imaging.},
  keywords = {cloud, imaging, serverless, event-driven, lsrtm},
  note = {Submitted to IEEE Transactions on Parallel and Distributed Systems on August 20, 2019},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2019/witte2019TPDedas/witte2019TPDedas.html}
}

@UNPUBLISHED{kukreja2019PASCccd,
  author = {Navjot Kukreja and Jan Huckelheim and Mathias Louboutin and Kaiyuan Hou and Paul Hovland and Gerard Gorman},
  title = {Combining checkpointing and data compression to accelerate adjoint-based optimization problems},
  year = {2019},
  abstract = {Seismic inversion and imaging are adjoint-based optimization problems that processes up to terabytes of data, regularly exceeding the memory capacity of available computers. Data compression is an effective strategy to reduce this memory requirement by a certain factor, particularly if some loss in accuracy is acceptable. A popular alternative is checkpointing, where data is stored at selected points in time, and values at other times are recomputed as needed from the last stored state. This allows arbitrarily large adjoint computations with limited memory, at the cost of additional recomputations. In this paper we combine compression and checkpointing for the first time to compute a realistic seismic inversion. The combination of checkpointing and compression allows larger adjoint computations compared to using only compression, and reduces the recomputation overhead significantly compared to using only checkpointing.},
  keywords = {Adjoint-state, FD, checkpointing, compression, HPC, inverse problems},
  note = {Submitted to PASC19 on January 16, 2019},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2019/kukreja2019PASCccd/kukreja2019PASCccd.pdf}
}



%-----2018-----%

@UNPUBLISHED{luporini2018aap,
  author = {FABIO LUPORINI and MICHAEL LANGE and MATHIAS LOUBOUTIN and NAVJOT KUKREJA and JAN HUCKELHEIM and CHARLES YOUNT and PHILIPP A. WITTE and PAUL H. J. KELLY and GERARD J. GORMAN and FELIX J. HERRMANN},
  title = {Architecture and performance of Devito, a system for automated stencil computation},
  year = {2018},
  abstract = {Stencil computations are a key part of many high-performance computing applications, such as image processing, convolutional neural networks, and finite-difference solvers for partial differential equations. Devito is a framework capable of generating highly-optimized code given symbolic equations expressed in Python, specialized in, but not limited to, affine (stencil) codes. The lowering process – from mathematical equations down to C++ code – is
performed by the Devito compiler through a series of intermediate representations. Several performance optimizations are introduced, including advanced common sub-expressions elimination, tiling and parallelization. Some of these are obtained through well-established stencil optimizers, integrated in the back-end of the Devito compiler. The architecture of the Devito compiler, as well as the performance optimizations that are applied when generating code, are presented. The effectiveness of such performance optimizations is demonstrated using operators drawn from seismic imaging applications.},
  keywords = {Stencil, finite difference method, symbolic processing, structured grid, compiler, performance optimization},
  note = {Submitted to SIAM Journal on Scientific Computing on July 9, 2018.},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2018/luporini2018aap/luporini2018aap.pdf}
}
