% This file was created with JabRef 2.6.
% Encoding: MacRoman

@unpublished{dasilva2012EAGEprobingprecond,
author = {Curt {Da Silva} and Felix J. Herrmann},
title = {Matrix Probing and Simultaneous Sources: A New Approach for Preconditioning the Hessian},
year = {2012},
month = {01/2012},
keywords = {EAGE, matrix probing, pseudo-differential operator},
abstract = {Recent advances based on the mathematical understanding 
of the Hessian as, under certain conditions, a pseudo-differential operator 
have resulted in a new preconditioner by L. Demanet et al. Basing their 
approach on a suitable basis expansion for the Hessian, by suitably 
'probing' the Hessian, i.e. applying the Hessian to a small number 
of randomized model perturbations, one can obtain an approximation 
to the inverse Hessian in an efficient manner.
Building upon this approach, we consider this preconditioner in the 
context of least-squares migration and Full Waveform Inversion and 
specifically dimensionality reduction techniques in these domains. 
By utilizing previous work in simultaneous sources, we are able to 
develop an efficient least-squares migration scheme which recovers 
higher quality images and hence higher quality search directions 
in the context of a Gauss-Newton method for Full Waveform Inversion
while simultaneously avoiding inordinate amounts of additional work.},
url = {NA}
}

@unpublished{wason2012EAGEode,
author = {Haneet Wason, Felix J. Herrmann},
title = {Only dither: efficient simultaneous marine acquisition},
year = {2012},
month = {01/2012},
keywords = {EAGE, acquisition, marine, simultaneous},
abstract = {Simultaneous-source acquisition is an emerging technology that is
stimulating both geophysical research and commercial efforts. The focus here
is on simultaneous-source marine acquisition design and sparsity-promoting
sequential-source data recovery. We propose a pragmatic simultaneous-source,
randomized marine acquisition scheme where multiple vessels sail across an
ocean-bottom array firing airguns at—sequential locations and randomly
time-dithered instances. Within the context of compressive sensing, where the
choice of the sparsifying transform needs to be incoherent with the
compressive sampling matrix, we can significantly impact the reconstruction quality, and
demonstrate that the compressive sampling matrix resulting from the proposed
sampling scheme is sufficiently incoherent with the curvelet transform to
yield successful recovery by sparsity promotion. Results are illustrated with
simulations of “purely” random marine acquisition, which requires an
airgun to be located at each source location, and random time-dithering marine
acquisition with one and two source vessels. Size of the collected data
volumes in all cases is the same. Compared to the recovery from the former
acquisition scheme (SNR = 10.5dB), we get good results by dithering with only one source
vessel (SNR = 8.06dB) in the latter scheme, which improve at the cost of
having an additional source vessel (SNR = 9.44dB).},
url = {NA}
}

@unpublished{tu2011EAGElmf,
author = {Ning Tu and Felix J. Herrmann},
title = {Least-squares migration of full wavefield with source encoding},
year = {2012},
month = {01/2012},
keywords = {EAGE, depth migration, surface-related multiples},
abstract = {Multiples can provide valuable information that is missing in
primaries, and there is a growing interest in using them for seismic imaging.
In our earlier work, we proposed to combine primary estimation and migration to
image from the total up-going wavefield. The method proves to be effective
but computationally expensive. In this abstract, we propose to reduce the
computational cost by removing the multi-dimensional convolution required by
primary estimation, and reducing the number of PDE solves in migration by
introducing simultaneous sources with source renewal. We gain great
performance boost without compromising the quality of the image.},
url = {NA}
}

@unpublished{aravkin2012EAGErobust,
author = {Aleksandr Y. Aravkin and Tristan {van Leeuwen} and Henri Calandra and Felix J.Herrmann},
title = {Source estimation for frequency-domain FWI with robust penalties},
year = {2012},
month = {01/2012},
keywords = {EAGE},
abstract = {Source estimation is an essential component of full waveform inversion. In
the standard frequency domain formulation, there is closed form solution for the the optimal source
weights, which can thus be cheaply estimated on the fly. A growing body of work underscores the
importance of robust modeling for data with large outliers or artifacts that are not captured by
the forward model. Effectively, the least-squares penalty on the residual is replaced by a robust penalty,
such as Huber, Hybrid `1-`2 or Student’s t. As we will demonstrate, it is essential to use
the same robust penalty for source estimation. In this abstract, we present a general approach to
robust waveform inversion with robust source estimation. In this general formulation, there is no
closed form solution for the optimal source weights so we need to solve a scalar optimization problem to
obtain these weights. We can efficiently solve this optimization problem with a Newton-like method
in a few iterations. The computational cost involved is of the same order as the usual
least-squares source estimation procedure. We show numerical examples illustrating robust source estimation
and robust waveform inversion on synthetic data with outliers.},
url = {NA}
}

@unpublished{vanleeuwen2012EAGEext,
author = {Tristan {van Leeuwen} and Felix J. Herrmann},
title = {Wave-equation extended images: computation and velocity continuation},
year = {2012},
month = {01/2012},
keywords = {EAGE},
abstract = {An extended image is a multi-dimensional correlation of source and receiver
wavefields. For a kinematically correct velocity, most of the energy will be concentrated at
zero offset. Because of the computational cost involved in correlating the wavefields for all
offsets, such exteded images are computed for a subsurface offset that is aligned with the local dip. In
this paper, we present an efficient way to compute extended images for all subsurface offsets without
explicitly calculating the receiver wavefields, thus making it computationally feasible to compute
such extended images.
We show how more conventional image gathers, where the offset is aligned with
the dip, can be extracted from this extended image. We also present a velocity continuation
procedure that allows us to compute the extended image for a given velocity without recomputing all
the source wavefields.},
url = {NA}
}

@unpublished{vanleeuwen2012EAGEcarpcg,
author = {Tristan {van Leeuwen} and Dan Gordon and Rachel Gordon and Felix J. Herrmann},
title = {Preconditioning the Helmholtz equation via row-projections},
year = {2012},
month = {01/2012},
keywords = {EAGE},
abstract = {3D frequency-domain full waveform inversion relies on being able to
efficiently solve the 3D Helmholtz equation. Iterative methods require sophisticated preconditioners
because the Helmholtz matrix is typically indefinite. We review a preconditioning technique that is
based on row-projections.
Notable advantages of this preconditioner over existing ones are that it has
low algorithmic complexity, is easily parallelizable and extendable to time-harmonic vector equations.},
url = {NA}
}

@unpublished{
author = {},
title = {},
year = {},
month = {},
keywords = {},
url = {}
}

@unpublished{
author = {},
title = {},
year = {},
month = {},
keywords = {},
url = {}
}

@unpublished{Friedlander11TRhdm,
  author = {Michael P. Friedlander and Mark Schmidt},
  title = {Hybrid deterministic-stochastic methods for data fitting},
  institution = {Department of Computer Science},
  year = {2011},
  address = {University of British Columbia, Vancouver},
  month = {04/2011},
  abstract = {Many structured data-fitting applications require the
                  solution of an optimization problem involving a sum
                  over a potentially large number of
                  measurements. Incremental gradient algorithms (both
                  deterministic and randomized) offer inexpensive
                  iterations by sampling only subsets of the terms in
                  the sum. These methods can make great progress
                  initially, but often slow as they approach a
                  solution. In contrast, full gradient methods achieve
                  steady convergence at the expense of evaluating the
                  full objective and gradient on each iteration. We
                  explore hybrid methods that exhibit the benefits of
                  both approaches. Rate of convergence analysis and
                  numerical experiments illustrate the potential for
                  the approach.},
  publisher = {Department of Computer Science},
  keywords ={Optimization},
  url = {http://www.cs.ubc.ca/~mpf/papers/FriedlanderSchmidt2011.pdf}
}

% This paper is accepted for publication and will appear in Geophysical Prospecting
@unpublished{Herrmann11TRLlsqIm,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares imaging with sparsity promotion and compressive
        sensing},
  year = {2011},
  notes = { TR-2011-03},
  address = {University of British Columbia, Vancouver},
  month = {08/2011},
  abstract = {Seismic imaging is a linearized inversion problem
                  relying on the minimization of a least-squares
                  misfit functional as a function of the medium
                  perturbation. The success of this procedure hinges
                  on our ability to handle large systems of
                  equations√ëwhose size grows exponentially with the
                  demand for higher reso- lution images in more and
                  more complicated areas√ëand our ability to invert
                  these systems given a limited amount of
                  computational resources. To overcome this √ícurse of
                  dimensionality√ì in problem size and computational
                  complexity, we propose a combination of randomized
                  dimensionality-reduction and divide-and- conquer
                  techniques.  This approach allows us to take
                  advantage of sophisticated sparsity-promoting
                  solvers that work on a series of smaller subproblems
                  each in- volving a small randomized subset of
                  data. These subsets correspond to artificial
                  simultaneous-source experiments made of random
                  superpositions of sequential- source experiments. By
                  changing these subsets after each subproblem is
                  solved, we are able to attain an inversion quality
                  that is competitive while requiring fewer
                  computational, and possibly, fewer acquisition
                  resources. Application of this con- cept to a
                  controlled series of experiments showed the validity
                  of our approach and the relationship between its
                  efficiency√ëby reducing the number of sources and
                  hence the number of wave-equation solves√ëand the
                  image quality.  Application of our
                  dimensionality-reduction methodology with sparsity
                  promotion to a com- plicated synthetic with
                  well-constrained structure also yields excellent
                  results underlining the importance of sparsity
                  promotion.},
  notes = {To appear in Geophysical Prospecting},
  keywords ={SLIM,Imaging,Optimization,Compressive Sensing},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/leastsquaresimag.pdf}
}

@unpublished{Herrmann11TRfcd,
  author = {Felix J. Herrmann and Michael P. Friedlander and Ozgur Yilmaz},
  title = {Fighting the curse of dimensionality: compressive sensing in exploration
        seismology},
  year = {2011},
  address = {University of British Columbia, Vancouver},
  month = {08/2011},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are mined
                  for infor- mation during processing.  This approach
                  has been extremely successful, but current efforts
                  toward higher-resolution images in increasingly
                  complicated regions of the Earth continue to reveal
                  fundamental shortcomings in our typical
                  workflows. The √ícurse of dimensionality√ì is the main
                  roadblock, and is exemplified by Nyquist√ïs sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. We offer an alternative sampling strategy
                  that leverages recent insights from compressive
                  sensing towards seismic acquisition and processing
                  for data that are traditionally considered to be
                  undersampled. The main outcome of this approach is a
                  new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria. Compressive sensing is
                  a novel nonlinear sampling paradigm, effective for
                  acquiring signals that have a sparse representation
                  in some transform domain. We review basic facts
                  about this new sampling paradigm that revolutionized
                  various areas of signal processing, and illustrate
                  how it can be successfully exploited in various
                  problems in seismic exploration to effectively fight
                  the curse of dimensionality.},
  keywords ={SLIM,Acquisition,Processing,Imaging,Full-waveform inversion,Optimization,Compressive Sensing},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/sigprocmag.pdf}
}

@unpublished{Mansour11TRwmmw,
  author = {Hassan Mansour and Ozgur Yilmaz.},
  title = {Weighted -$\ell_1$ minimization with multiple weighting sets},
  year = {2011},
  type = {Tech. Rep.},
  address = {University of British Columbia, Vancouver},
  month = {09/2011},
notes= {TR-2011-07 },
  abstract = {In this paper, we study the support In this paper, we
                  study the support recovery conditions of weighted
                  -$\ell_1$ minimization for signal reconstruction
                  from compressed sensing measurements when multiple
                  support estimate sets with different accuracy are
                  available. We identify a class of signals for which
                  the recovered vector from -$\ell_1$ minimization
                  provides an accurate support estimate. We then
                  derive stability and robustness guarantees for the
                  weighted -$\ell_1$ minimization problem with more
                  than one support estimate. We show that applying a
                  smaller weight to support estimate that enjoy higher
                  accuracy improves the recovery conditions compared
                  with the case of a single support estimate and the
                  case with standard, i.e., non-weighted,-$\ell_1$
                  minimization. Our theoretical results are supported
                  by numerical simulations on synthetic signals and
                  real audio signals.},
keywords={Compressive Sensing,Optimization},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/MansourYilmaz2011.pdf }
}

@unpublished{Mansour11TRssma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Simultaneous-source marine acquisition with compressive sampling
        matrices},
  year = {2011},
notes= { TR-2011-04 },
  type = {Tech. Rep.},
  address = {University of British Columbia, Vancouver},
  month = {08/2011},
  abstract = {Seismic data acquisition in marine environments is a
                  costly process that compels the adoption of
                  simultaneous-source acquisition - an emerging
                  technology that is stimu- lating both geophysical
                  research and commercial efforts. In this paper, we
                  discuss the properties of randomized simultaneous
                  acquisition matrices and demonstrate that
                  sparsity-promoting recovery improves the quality of
                  the reconstructed seismic data volumes. Leveraging
                  established findings from the field of compressive
                  sensing, we demonstrate that the choice of the
                  sparsifying transform that is incoherent with the
                  compressive sampling matrix can significantly impact
                  the reconstruction quality. Si- multaneous marine
                  acquisition calls for the development of a new set
                  of design principles and post-processing tools. We
                  propose to use random time dithering where
                  sequential acquisition with a single airgun is
                  replaced by continuous acquisition with multiple
                  airguns firing at random times and at random
                  locations. We then demonstrate that the resulting
                  compressive sampling matrix is incoherent with the
                  curvelet transform and the combined measurement
                  matrix exhibits better isometry properties than
                  other transform bases such as a non-localized
                  multidimensional Fourier transform. We il- lustrate
                  our results with simulations of simultaneous-source
                  marine acquisition using periodic and randomized
                  time dithering.},
  keywords ={SLIM,Acquisition,Processing,Compressive Sensing},
  notes ={In revision},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/simmarineacq.pdf}
}

@unpublished{VanLeeuwen11TRfwiwse,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast waveform inversion without source encoding},
  year = {2011},
  type = {Tech. Rep.},
notes = {TR-2011-06 },
  address = {University of British Columbia, Vancouver},
  month = {09/2011},
  abstract = {Randomized source encoding has recently been proposed as
                  a way to dramatically reduce the costs of full
                  waveform inversion. The main idea is to replace all
                  sequential sources by a small number of simultaneous
                  sources. This introduces random crosstalk in the
                  model updates and special stochastic optimization
                  strategies are required to deal with this. Two
                  problems arise with this approach: i) source
                  encoding can only be applied to fixed-spread
                  acquisition setups, and ii) stochastic optimization
                  methods tend to converge very slowly, relying on
                  averaging to get rid of the cross-talk. Although the
                  slow convergence is partly offset by the low
                  iteration cost, we show that conventional
                  optimization strategies are bound to outperform
                  stochastic methods in the long run. In this paper we
                  argue that we don¬øt need randomized source encoding
                  to reap the benefits of stochastic optimization and
                  we review an optimization strategy that combines the
                  benefits of both conventional and stochastic
                  optimization. The method uses a gradually increasing
                  batch of sources. Thus, iterations are very cheap
                  initially and this allows the method to make fast
                  progress in the beginning. As the batch size grows,
                  the method behaves like conventional optimization,
                  allowing for fast convergence. Numerical examples
                  suggest that the stochastic and hybrid method
                  perform equally well with and without source
                  encoding and that the hybrid method outperforms both
                  conventional and stochastic optimization. The method
                  does not rely on source encoding techniques and can
                  thus be applied to non fixed-spread data.},
  keywords = {SLIM,Full-waveform inversion,Optimization},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/TR201106.pdf }
}

@unpublished{Li11TRfrfwi,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix
        J. Herrmann},
  title = {Fast randomized full-waveform inversion with compressive sensing},
  year = {2011},
notes = {TR-2011-12 },
  address = {University of British Columbia, Vancouver},
  month = {10/2011},
  abstract = { Wave-equation based seismic inversion can be formulated
                  as a nonlinear inverse problem where the medium
                  properties are obtained via minimization of a least-
                  squares misfit functional. The demand for higher
                  resolution models in more geologically complex areas
                  drives the need to develop techniques that explore
                  the special structure of full-waveform inversion to
                  reduce the computational burden and to regularize
                  the inverse problem. We meet these goals by using
                  ideas from compressive sensing and stochastic
                  optimization to design a novel Gauss-Newton method,
                  where the updates are computed from random subsets
                  of the data via curvelet-domain sparsity
                  promotion. Application of this idea to a realistic
                  synthetic shows improved results compared to
                  quasi-Newton methods, which require passes through
                  all data. Two different subset sampling strategies
                  are considered: randomized source encoding, and
                  drawing sequential shots firing at random source
                  locations from marine data with missing near and far
                  offsets. In both cases, we obtain excellent
                  inversion results compared to conventional methods
                  at reduced computational costs. },
  keywords ={SLIM,Full-waveform inversion,Compressive Sensing,Optimization},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/LiAravkinLeeuwenHerrmann.pdf}
}

@unpublished{Aravkin11TRridr,
  author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Felix J. Herrmann and Tristan
        van Leeuwen},
  title = {Robust inversion, dimensionality reduction, and randomized sampling},
  year = {2011},
notes = {TR-2011-13},  
address = {University of British Columbia, Vancouver},
  month = {11/2011},
  abstract = {We consider a class of inverse problems in which the
                  forward model is the solution operator to linear
                  ODEs or PDEs. This class admits several
                  dimensionality-reduction techniques based on data
                  averaging or sampling, which are especially useful
                  for large-scale problems.  We survey these
                  approaches and their connection to stochastic
                  optimization.  The data-averaging approach is only
                  viable, however, for a least-squares misfit, which
                  is sensitive to outliers in the data and artifacts
                  unexplained by the forward model. This motivates us
                  to propose a robust formulation based on the
                  Student's t-distribution of the error.  We
                  demonstrate how the corresponding penalty function,
                  together with the sampling approach, can obtain good
                  results for a large-scale seismic inverse problem
                  with 50% corrupted data.},
  keywords = {SLIM,Optimization,Full-waveform inversion},
  url = {http://www.optimization-online.org/DB_FILE/2011/11/3243.pdf}
}

% This abstract has been accepted. Move to published abstracts
@unpublished{aravkin12ICASSProbustb,
author={Aleksandr Y. Aravkin and Michael P. Friedlander and Tristan van Leeuwen },
Keywords={ICASSP},
Organization={ICASSP},
Title={ Robust inversion via semistochastic dimensionality reduction},
Year={2011},
Month={10/2011},
notes = {Submitted to ICASSP 2012, 9/27/2011 },
Abstract={In this paper, we offer an alternative sampling method
                  leveraging recent insights from compressive sensing
                  towards seismic acquisition and processing for data
                  that are traditionally considered to be
                  undersampled. The main outcome of this approach is a
                  new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria, such as Nyquist. At the
                  heart of our approach lies randomized incoherent
                  sampling that breaks subsampling related
                  interferences by turning them into harmless noise,
                  which we subsequently remove by promoting
                  transform-domain sparsity. Now, costs no longer grow
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  acquisition. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity. We illustrate this
                  principle by means of number of case studies. },
keywords ={SLIM,Optimization,Full-waveform inversion},
URL= {http://slim.eos.ubc.ca/Publications/Private/Journals/ICASSP/AravkinFriedlanderLeeuwen.pdf }
}

% This abstract has been accepted. Move to published abstracts
@unpublished{Aravkin12ICASSPfastseis,
Abstract = {Seismic imaging can be formulated as a linear inverse
                  problem where a medium perturbation is obtained via
                  minimization of a least-squares misfit
                  functional. The demand for higher resolution images
                  in more geophysically complex areas drives the need
                  to develop techniques that handle problems of
                  tremendous size with limited computational
                  resources. While seismic imaging is amenable to
                  dimensionality reduction techniques that collapse
                  the data volume into a smaller set of ‚Äúsuper-shots‚Äù,
                  these techniques break down for complex acquisition
                  geometries such as marine acquisition, where sources
                  and receivers move during acquisition. To meet these
                  challenges, we propose a novel method that combines
                  sparsity-promoting (SP) solvers with random sub- set
                  selection of sequential shots, yielding a SP
                  algorithm that only ever sees a small portion of the
                  full data, enabling its application to very
                  large-scale problems. Application of this technique
                  yields excellent results for a complicated
                  synthetic, which underscores the robustness of
                  sparsity promotion and its suitability for seismic
                  imaging.},
Author = {Aleksandr Y. Aravkin and Xiang Li and Felix J. Herrmann },
Keywords = {ICASSP},
Organization = {ICASSP},
Publisher = {},
Title = {Fast seismic imaging for marine data},
Date-Added = {2011-09-27 15:00:00 -0700},
Year = {2011},
Month={09/2011},
keywords={SLIM,Imaging,Optimization,Compressive Sensing},
url={http://slim.eos.ubc.ca/Publications/Private/Journals/ICASSP/AravkinLiHerrmann.pdf }
}

% This abstract has been accepted. Move to published abstracts
@unpublished{Mansour12ICASSadapt,
Abstract = {In this paper, we propose an adaptive compressed sensing
                  scheme that utilizes a support estimate to focus the
                  measurements on the large valued coefficients of a
                  compressible signal. We embed a ‚Äúsparse-filtering‚Äù
                  stage into the measure- ment matrix by weighting
                  down the contribution of signal coefficients that
                  are outside the support estimate. We present an
                  application which can benefit from the proposed
                  sampling scheme, namely, video compressive
                  acquisition. We demonstrate that our proposed
                  adaptive CS scheme results in a significant
                  improvement in reconstruction quality compared with
                  standard CS as well as adaptive recovery using
                  weighted $\ell$1 minimization.},
Author = {Hassan Mansour and Ozgur Yilmaz},
Keywords = {ICASSP},
Organization = {ICASSP},
Publisher = {},
Title = {Adaptive compressed sensing for video acquisition},
Date-Added = {2011-09-27 15:00:00 -0700},
Year = {2011},
Month={09/2011},
keywords ={Compressive Sensing},
url={http://slim.eos.ubc.ca/Publications/Private/Journals/ICASSP/MansourYilmazICASSPaCS.pdf }
}

% This abstract has been accepted. Move to published abstracts
@unpublished{Mansour12ICASSsupport,
Abstract = {In this paper, we propose a support driven reweighted
                  $\ell$_Ôøø1 minimization algorithm (SDRL1) that solves
                  a sequence of weighted $\ell$_Ôøø1 problems and relies
                  on the support estimate accu- racy. Our SDRL1
                  algorithm is related to the IRL1 algorithm proposed
                  by Cande`s, Wakin, and Boyd. We demonstrate that it
                  is sufficient to find support estimates with good
                  accuracy and apply constant weights instead of using
                  the inverse coefficient magnitudes to achieve gains
                  similar to those of IRL1. We then prove that given a
                  support estimate with sufficient accuracy, if the
                  signal decays according to a specific rate, the
                  solution to the weighted Ôøø1 minimization problem
                  results in a support estimate with higher accuracy
                  than the initial estimate. We also show that under
                  certain conditions, it is possible to achieve higher
                  estimate accuracy when the inter- section of support
                  estimates is considered. We demonstrate the
                  performance of SDRL1 through numerical simulations
                  and compare it with that of IRL1 and standard
                  Ôøø$\ell$_Ôøø1 minimization.},
Author = {Hassan Mansour and Ozgur Yilmaz},
Keywords = {ICASSP},
Organization = {ICASSP},
Publisher = {},
Title = {Support driven reweighted $\ell_1$ minimization},
Date-Added = {2011-09-27 15:00:00 -0700},
Year = {2011},
Month={09/2011},
keywords ={Compressive Sensing},
url={http://slim.eos.ubc.ca/Publications/Private/Journals/ICASSP/MansourYilmazICASSPwL1.pdf }
}

@unpublished{haber10TRemp,
  author = {Eldad Haber and Matthias Chung and Felix J. Herrmann},
  title = {An effective method for parameter estimation with PDE constraints
        with multiple right hand sides},
  institution = {UBC-Earth and Ocean Sciences Department},
  year = {2010},
  number = {TR-2010-4},
  abstract = {Many parameter estimation problems involve with a
                  parameter-dependent PDEs with multiple right hand
                  sides. The computational cost and memory
                  requirements of such problems increases linearly
                  with the number of right hand sides. For many
                  applications this is the main bottleneck of the
                  computation. In this paper we show that problems
                  with multiple right hand sides can be reformulated
                  as stochastic optimization problems that are much
                  cheaper to solve. We discuss the solution
                  methodology and use the direct current resistivity
                  and seismic tomography as model problems to show the
                  effectiveness of our approach.},
  keywords = {SLIM,Full-waveform inversion,Optimization},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/Haber2010emp.pdf}
}
