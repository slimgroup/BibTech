% This file was created with JabRef 2.6.
% Encoding: MacRoman

@unpublished{vanleeuwen2012SEGparallel,
author = {Tristan {van Leeuwen} and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
title = {A parallel, object-oriented framework for frequency-domain wavefield 
imaging and inversion.},
abstract = {We present a parallel object-oriented matrix-free framework for frequency-domain 
seismic modeling, imaging and inversion. 
The key aspects of the framework are its modularity and level of abstraction, 
which allows us to write code that reflects the underlying mathematical 
structure and develop unit-tests that guarantee the fidelity of the code. 
By overloading standard linear-algebra operations, such as matrix-vector 
multiplications, we can use standard optimization packages to work with our code 
without any modification. This leads to a scalable testbed on 
which new methods can be rapidly prototyped and tested on medium-sized 2D problems. 
Although our current implementation uses (parallel) Matlab, all of 
these design principles can also be met by using lower-level languages 
which is important when we want to scale to realistic 3D problems. 
We present some numerical examples on synthetic data.},
keywords = {modeling, imaging, inversion, SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/SEG/2012/vanleeuwen2012SEGparallel/vanleeuwen2012SEGparallel.pdf}
}

@unpublished{herrmann2012SEGals,
author = {Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
title = {Accelerated large-scale inversion with message passing},
abstract = {To meet current-day challenges, exploration seismology increasingly 
relies on more and more sophisticated algorithms that require 
multiple paths through all data. This requirement leads to problems 
because the size of seismic data volumes is increasing exponentially, 
exposing bottlenecks in IO and computational capability. To overcome 
these bottlenecks, we follow recent trends in machine learning and 
compressive sensing by proposing a sparsity-promoting inversion 
technique that works on small randomized subsets of data only. We 
boost the performance of this algorithm significantly by modifying a 
state-of-the-art ?1-norm solver to benefit from message 
passing, which breaks the build up of correlations between model 
iterates and the randomized linear forward model. We demonstrate the 
performance of this algorithm on a toy sparse-recovery problem and 
on a realistic reverse-time-migration example with random source 
encoding. The improvements in speed, memory use, and output quality 
are truly remarkable.},
keywords = {Imaging, Optimization, Compressive Sensing},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/SEG/2012/herrmann2012SEGals/herrmann2012SEGals.pdf}
}

@unpublished{vanderneut2012SEGirs,
author = {Joost {van der Neut} and Felix Herrmann and Kees Wapenaar},
year = {2012},
optmonth = {04/2012},
title = {Interferometric redatuming with simultaneous and missing 
sources using sparsity promotion in the curvelet domain},
abstract = {Interferometric redatuming is a velocity-independent 
method to turn downhole receivers into virtual sources. Accurate 
redatuming involves solving an inverse problem, which can be 
highly ill-posed, especially in the presence of noise, incomplete 
data and limited aperture. We address these issues by combining 
interferometric redatuming with transform-domain sparsity 
promotion, leading to a formulation that deals with data imperfections. 
We show that sparsity promotion improves the retrieval of virtual 
shot records under a salt flank. To reduce acquisition costs, it can 
be beneficial to reduce the number of sources or shoot them 
simultaneously. It is shown that sparse inversion can still provide 
a stable solution in such cases.},
keywords = {Processing,Imaging,Optimization,Interferometry},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/SEG/2012/vanderneut2012SEGirs/vanderneut2012SEGirs.pdf}
}

@unpublished{li2012SEGspmamp,
author = {Xiang Li and Felix J.Herrmann},
year = {2012},
optmonth = {04/2012},
title = {Sparsity-promoting migration accelerated by message passing},
abstract = {Seismic imaging via linearized inversion requires multiple 
iterations to minimize the least-squares misfit as a function of the 
medium perturbation. Unfortunately, the cost for these iterations 
are prohibitive because each iteration requires many wave-equation 
simulations, which without direct solvers require an expensive 
separate solve for each source. To overcome this problem, we use 
dimensionality-reduction to decrease the size of seismic imaging 
problem by turning the large number of sequential shots into a much 
small number of simultaneous shots. In our approach, we take 
advantage of sparsifying transforms to remove source crosstalk 
resulting from randomly weighting and stacking sequential shots into 
a few super shots. We also take advantage of the fact that the 
convergence of large-scale sparsity-promoting solvers can be 
improved significantly by borrowing ideas from message passing, 
which are designed to break correlation built up between the linear 
system and the model iterate. In this way, we arrive at a 
formulation where we run the sparsity-promoting solver for a 
relatively large number of very iterations. Aside from leading to a 
significant speed up, our approach had the advantage of greatly 
reducing the memory imprint and IO requirements. We demonstrate this 
feature by solving a sparsity-promoting imaging problem with 
operators of reverse-time migration, which is computationally 
infeasible without the dimensionality reduction.},
keywords = {SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/SEG/2012/li2012SEGspmamp/li2012SEGspmamp.pdf}
}

@unpublished{rajiv2012SEGFRM,
author = {Rajiv Kumar and Aleksandr Y. Aravkin and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
title = {Fast Methods for Rank Minimization with Applications to Seismic-Data Interpolation},
abstract = {Rank penalizing techniques are an important direction 
in seismic inverse problems, since they allow improved recovery by 
exploiting low-rank structure. A major downside of current state of 
the art techniques is their reliance on the SVD of seismic data 
structures, which can be prohibitively expensive. Fortunately, recent 
work allows us to circumvent this problem by working with matrix 
factorizations. We review a novel approach to rank penalization, and 
successfully apply it to the seismic interpolation problem by exploiting 
the low-rank structure of seismic data in the midpoint-offset domain. 
Experiments for the recovery of 2D monochromatic data matrices and 
seismic lines represented as 3D volumes support the feasibility and 
potential of the new approach.},
keywords = {Rank, optimization, Seismic data Interpolation},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/SEG/2012/rajiv2012SEGFRM/rajiv2012SEGFRM.pdf}
}

@unpublished{aravkin2012SEGST,
author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Kenneth Bube and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
title = {On Non-Uniqueness of the Student’s t-formulation for Linear Inverse Problems},
abstract = {We review the statistical interpretation of inverse problem 
formulations, and the motivations for selecting non-convex penalties 
for robust behaviour with respect to measurement outliers or artifacts 
in the data. An important downside of using non- convex formulations 
such as the Student’s t is the potential for non-uniqueness, and we 
present a simple example where the Student’s t penalty can be made 
to have many local minima by appropriately selecting the degrees of 
freedom parameter. 
On the other hand, the non-convexity of the Student’s t is precisely 
what gives it the ability to ignore artifacts in the data. We explain this 
idea, and present a stylized imaging experiment, where the Student’s t 
is able to recover a velocity perturbation from data contaminated by 
a very peculiar artifact — data from a different velocity perturbation. 
The performance of Student’s t inversion is investigated empirically 
for different values of the degrees of freedom parameter, and 
different initial conditions.},
keywords = {student's t, robust, non-convex, uniqueness},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/SEG/2012/aravkin2012SEGST/aravkin2012SEGST.pdf}
}

@unpublished{tu2012SEGima,
author = {Ning Tu and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
title = {Imaging with multiples accelerated by message passing},
abstract = {With the growing realization that multiples can provide 
valuable information, there is a paradigm shift from removing them 
to using them. For instance, primary estimation by sparse inversion 
has demonstrated its superiority over surface-related multiple removal 
in many aspects. Inspired by this shift, we propose a method to 
image directly from the total up-going wavefield, including surface-related 
multiples, by sparse in- version. To address the high computational cost 
associated with this method, we propose to speed up the inversion by 
having the wave-equation solver carry out the multi-dimensional 
convolutions implicitly and cheaply by randomized subsampling. We 
improve the overall performance of this algorithm by selecting new 
independent copies of the randomized modeling operator, which 
leads to a cancellation of correlations that hamper the speed of 
convergence of the solver. We show the merits of our approach on 
a number of examples.},
keywords = {SEG, Imaging, Multiples},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/SEG/2012/tu2012SEGima/tu2012SEGima.pdf}
}

@unpublished{lin2012robustepsi,
author = {Tim T.Y. Lin and Felix J. Herrmann},
year = {2012},
optmonth = {03/2012},
title = {Robust estimation of primaries by sparse inversion via one-norm minimization},
abstract = {Even though contemporary methods for the removal of multiple 
events in seismic data due to a free-surface are built upon reciprocity 
relationships between wavefields, they are often still implemented as 
prediction-subtraction processes. The subtraction process does not always 
compensate for imperfect prediction of multiple events, and itself often 
leads to distortion of primary events. A recently proposed method called 
Estimation of Primaries by Sparse Inversion avoids the subtraction process 
altogether by directly prediction the primary impulse response as a collection 
of band-limited spikes under sparsity-regulated wavefield inversion approach. 
Although it can be shown that the correct primary impulse response is 
obtained through the sparsest possible solution, the Estimation of Primaries 
by Sparse Inversion algorithm was not designed to seek such a solution, instead 
depending on a predetermined degree of sparsity as an inversion parameter. 
This leads to imperfect multiple rejection when the sparsity is overestimated, 
and problems with recovering late primary events when it is underestimated. 
In this paper, we propose a new algorithm where we make obtaining the 
sparsest solution our explicit goal. Our approach remains a gradient-based 
approach like the original algorithm, but is in turn derived from a new 
optimization framework based on an extended basis pursuit denoising formulation. 
We show that the sparsity-minimizing objective of our formulation enables it 
to operate successfully on a wide variety of synthetic and field marine dataset 
without excessive tweaking of inversion parameters. We also demonstrate that 
Robust EPSI produces a more artifact-free impulse response compared to the 
original algorithm, which has interesting implications for broadband seismic 
applications. Finally we demonstrate through field data that recovering the 
primary impulse response under transform domains can significantly improve 
the recovery of weak primary late arrivals, without appreciable change to the 
underlying algorithm.},
keywords = {multiples, optimization, sparsity, waveform inversion, pareto, biconvex, algorithm, EPSI},
notes = {submitted to Geophysics, March 12, 2012},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/lin2012robustepsi/lin2012robustepsi.pdf}
}

@unpublished{mansour2012SSPwspgl1,
author = {Hassan Mansour},
year = {2012},
optmonth = {03/2012},
title = {Beyond $\ell_1$ norm minimization for sparse signal recovery},
abstract = {Sparse signal recovery has been dominated by the basis 
pursuit denoise (BPDN) problem formulation for over a decade. In this 
paper, we propose an algorithm that outperforms BPDN in finding 
sparse solutions to underdetermined linear systems of equations at no 
additional computational cost. Our algorithm, called WSPGL1, is a 
modification of the spectral projected gradient for $\ell_1$ minimization 
(SPGL1) algorithm in which the sequence of LASSO subproblems are 
replaced by a sequence of weighted LASSO subproblems with constant 
weights applied to a support estimate. The support estimate is derived 
from the data and is updated at every iteration. The algorithm also 
modifies the Pareto curve at every iteration to reflect the new weighted 
$\ell_1$ minimization problem that is being solved. We demonstrate through 
extensive simulations that the sparse recovery performance of our 
algorithm is superior to that of $\ell_1$ minimization and approaches the 
recovery performance of iterative re-weighted $\ell_1$ (IRWL1) minimization 
of Cand{\`e}s, Wakin, and Boyd. Moreover, our algorithm has the 
computational cost of a single BPDN problem.},
keywords = {Sparse recovery, compressed sensing, iterative algorithms, weighted $\ell_1$ minimization, partial support recovery},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1.pdf}
}

@unpublished{herrmann2012SSPamp,
author = {Felix J. Herrmann},
year = {2012},
optmonth = {03/2012},
title = {Approximate message passing meets exploration seismology},
abstract = {Data collection, data processing, and imaging in exploration 
seismology increasingly hinge on large-scale sparsity promoting solvers 
to remove artifacts caused by efforts to reduce costs. We show how the 
inclusion of a 'message term' in the calculation of the residuals improves 
the convergence of these iterative solvers by breaking correlations that 
develop between the model iterate and the linear system that needs to 
be inverted. We compare this message-passing scheme to state-of-the-art 
solvers for problems in missing-trace interpo- lation and in 
dimensionality-reduced imaging with phase en- coding.},
keywords = {Exploration seismology, compressive sensing, transform-domain sparsity promotion, seismic imaging},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp.pdf}
}

@unpublished{bander2012dre,
author = {Bander Jumah and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
title = {Dimensionality-reduced estimation of primaries by sparse inversion},
abstract = {Data-driven methods---such as the estimation of primaries by sparse 
inversion---suffer from the 'curse of dimensionality' that leads to 
disproportional growth in computational and storage demands when 
moving to realistic 3D field data. To remove this fundamental 
impediment, we propose a dimensionality-reduction technique where 
the 'data matrix' is approximated adaptively by a randomized 
low-rank factorization. Compared to conventional methods, which need 
for each iteration passage through all data possibly requiring 
on-the-fly interpolation, our randomized approach has the advantage 
that the total number of passes is reduced to only one to three. In 
addition, the low-rank matrix factorization leads to considerable 
reductions in storage and computational costs of the matrix 
multiplies required by the sparse inversion. Application of the 
proposed method to synthetic and real data shows that significant 
performance improvements in speed and memory use are achievable at a 
low computational up-front cost required by the low-rank 
factorization.},
keywords = {multiples,Processing,Optimization},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/bander2012dre/bander2012dre.pdf}
}

@unpublished{min2012CSEGrgfe,
author = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
title = {Re-establishment of gradient in frequency-domain elastic waveform inversion},
abstract = {To obtain solutions close to global minimum in waveform 
inversion, the gradients computed at each frequency need to be 
weighted to appropriately describe the residuals between modeled and 
field data. While the low-frequency components of the gradients should 
be weighted to recover the long- wavelength structures, the high-frequency 
components of the gradients need to be weighted when the short-wavelength 
structures are restored. However, the conventional elastic waveform inversion 
algorithms cannot properly weight the gradients computed at each frequency. 
When gradients are scaled using the pseudo-Hessian matrix inside the 
frequency loop, gradients obtained at high frequencies are over-emphasized. 
When the gradients are scaled outside the frequency loop, gradients are 
weighted by the source spectra. In this study, we propose applying weighting 
factors to the gradients obtained at each frequency so that gradients can 
properly reflect the differences between the true and assumed models satisfying 
the general inverse theory. The weighting factors are composed by the 
backpropagated residuals. Numerical examples for the simple rectangular-shaped 
model and the modified version of the Marmousi-2 model show that the 
weighting method enhances gradient images and inversion results compared 
to the conventional inversion algorithms.},
keywords = {elastic, waveform inversion, frequency-domain, weighting factors},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/CSEG/2012/min2012CSEGrgfe/min2012CSEGrgfe.pdf}
}

@unpublished{wason2012CSEGode,
author = {Haneet Wason and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
title = {Only dither: efficient simultaneous marine acquisition},
abstract = {Simultaneous-source acquisition is an emerging technology
that is stimulating both geophysical research and commercial efforts. 
Simultaneous marine acquisition calls for the development of a new set
of design principles and post-processing tools. The focus here is on 
simultaneous-source marine acquisition design and sparsity-promoting 
sequential-source data recovery. We propose a pragmatic simultaneous-source, 
randomized marine acquisition scheme where multiple vessels sail across 
an ocean-bottom array firing airguns at ‚Äî sequential locations and randomly 
time-dithered instances. By leveraging established findings from the field of 
compressive sensing, where the choice of the sparsifying transform needs 
to be incoherent with the compressive sampling matrix, we can significantly 
impact the reconstruction quality, and demonstrate that the compressive 
sampling matrix resulting from the proposed sampling scheme is sufficiently 
incoherent with the curvelet transform to yield successful recovery by sparsity 
promotion. Results are illustrated with simulations of ‚Äúpurely‚Äù random marine 
acquisition, which requires an airgun to be located at each source location, 
and random time-dithering marine acquisition with one and two source vessels. 
Size of the collected data volumes in all cases is the same. Compared to the 
recovery from the former acquisition scheme (SNR = 10.5dB), we get good 
results by dithering with only one source vessel (SNR = 8.06dB) in the latter 
scheme, which improve at the cost of having an additional source vessel 
(SNR = 9.85dB).},
keywords = {CSEG, acquisition, marine, simultaneous},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/CSEG/2012/wason2012CSEGode/wason2012CSEGode.pdf}
}

@unpublished{li2011CSEGefimag,
author = {Xiang Li and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
title = {Efficient full-waveform inversion with marine acquisition geometry},
abstract = {Full-waveform inversion (FWI) is a nonlinear data fitting procedure 
based on seismic data to derive a accurate velocity model. With the increasing 
demand for high resolution images in complex geological settings, the 
importance of improvements in acquisition and inversion become more and 
more critical. However, these improvements will be obtained at high 
computational cost, as a typical marine survey contains thousands of shot 
and receiver positions, and FWI needs several passes through massive seismic 
data. Computational cost of FWI will grow exponentially as the size of seismic 
data and desired resolution increase. In this paper we present a modified 
Gauss-Newton (GN) method that borrows ideas from compressive sensing, 
where we compute the GN updates from a few randomly selected sequential 
shots. Each subproblem is solved by using a sparsity promoting algorithm. 
With this approach, we dramatically reduce the size and hence the 
computational costs of the problem, whilst we control information loss by 
redrawing a different set of sequential shots for each subproblem.},
keywords = {CSEG},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/CSEG/2012/li2011CSEGefimag/li2011CSEGefimag.pdf}
}


@unpublished{Friedlander11TRhdm,
  author = {Michael P. Friedlander and Mark Schmidt},
  title = {Hybrid deterministic-stochastic methods for data fitting},
  institution = {Department of Computer Science},
  year = {2011},
  address = {University of British Columbia, Vancouver},
  optmonth = {04/2011},
  abstract = {Many structured data-fitting applications require the
                  solution of an optimization problem involving a sum
                  over a potentially large number of
                  measurements. Incremental gradient algorithms (both
                  deterministic and randomized) offer inexpensive
                  iterations by sampling only subsets of the terms in
                  the sum. These methods can make great progress
                  initially, but often slow as they approach a
                  solution. In contrast, full gradient methods achieve
                  steady convergence at the expense of evaluating the
                  full objective and gradient on each iteration. We
                  explore hybrid methods that exhibit the benefits of
                  both approaches. Rate of convergence analysis and
                  numerical experiments illustrate the potential for
                  the approach.},
  publisher = {Department of Computer Science},
  keywords ={Optimization},
  url = {http://www.cs.ubc.ca/~mpf/papers/FriedlanderSchmidt2011.pdf}
}

@unpublished{Herrmann11TRfcd,
  author = {Felix J. Herrmann and Michael P. Friedlander and Ozgur Yilmaz},
  title = {Fighting the curse of dimensionality: compressive sensing in exploration
        seismology},
  year = {2011},
  address = {University of British Columbia, Vancouver},
  optmonth = {08/2011},
  abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are mined
                  for infor- mation during processing.  This approach
                  has been extremely successful, but current efforts
                  toward higher-resolution images in increasingly
                  complicated regions of the Earth continue to reveal
                  fundamental shortcomings in our typical
                  workflows. The ‚àö√≠curse of dimensionality‚àö√¨ is the main
                  roadblock, and is exemplified by Nyquist‚àö√Øs sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. We offer an alternative sampling strategy
                  that leverages recent insights from compressive
                  sensing towards seismic acquisition and processing
                  for data that are traditionally considered to be
                  undersampled. The main outcome of this approach is a
                  new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria. Compressive sensing is
                  a novel nonlinear sampling paradigm, effective for
                  acquiring signals that have a sparse representation
                  in some transform domain. We review basic facts
                  about this new sampling paradigm that revolutionized
                  various areas of signal processing, and illustrate
                  how it can be successfully exploited in various
                  problems in seismic exploration to effectively fight
                  the curse of dimensionality.},
  notes = {To appear in IEEE Signal Processing Magazine},
  keywords ={SLIM,Acquisition,Processing,Imaging,Full-waveform inversion,Optimization,Compressive Sensing},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Submitted/IEEESignalProcessingMagazine/2012/Herrmann11TRfcd/Herrmann11TRfcd.pdf}
}

@unpublished{Mansour11TRwmmw,
  author = {Hassan Mansour and Ozgur Yilmaz.},
  title = {Weighted -$\ell_1$ minimization with multiple weighting sets},
  year = {2011},
  type = {Tech. Rep.},
  address = {University of British Columbia, Vancouver},
  optmonth = {09/2011},
notes= {TR-2011-07 },
  abstract = {In this paper, we study the support In this paper, we
                  study the support recovery conditions of weighted
                  -$\ell_1$ minimization for signal reconstruction
                  from compressed sensing measurements when multiple
                  support estimate sets with different accuracy are
                  available. We identify a class of signals for which
                  the recovered vector from -$\ell_1$ minimization
                  provides an accurate support estimate. We then
                  derive stability and robustness guarantees for the
                  weighted -$\ell_1$ minimization problem with more
                  than one support estimate. We show that applying a
                  smaller weight to support estimate that enjoy higher
                  accuracy improves the recovery conditions compared
                  with the case of a single support estimate and the
                  case with standard, i.e., non-weighted,-$\ell_1$
                  minimization. Our theoretical results are supported
                  by numerical simulations on synthetic signals and
                  real audio signals.},
keywords={Compressive Sensing,Optimization},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/MansourYilmaz2011.pdf }
}

@unpublished{Mansour11TRssma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Simultaneous-source marine acquisition with compressive sampling
        matrices},
  year = {2011},
notes= { TR-2011-04 },
  type = {Tech. Rep.},
  address = {University of British Columbia, Vancouver},
  optmonth = {08/2011},
  abstract = {Seismic data acquisition in marine environments is a
                  costly process that compels the adoption of
                  simultaneous-source acquisition - an emerging
                  technology that is stimu- lating both geophysical
                  research and commercial efforts. In this paper, we
                  discuss the properties of randomized simultaneous
                  acquisition matrices and demonstrate that
                  sparsity-promoting recovery improves the quality of
                  the reconstructed seismic data volumes. Leveraging
                  established findings from the field of compressive
                  sensing, we demonstrate that the choice of the
                  sparsifying transform that is incoherent with the
                  compressive sampling matrix can significantly impact
                  the reconstruction quality. Si- multaneous marine
                  acquisition calls for the development of a new set
                  of design principles and post-processing tools. We
                  propose to use random time dithering where
                  sequential acquisition with a single airgun is
                  replaced by continuous acquisition with multiple
                  airguns firing at random times and at random
                  locations. We then demonstrate that the resulting
                  compressive sampling matrix is incoherent with the
                  curvelet transform and the combined measurement
                  matrix exhibits better isometry properties than
                  other transform bases such as a non-localized
                  multidimensional Fourier transform. We il- lustrate
                  our results with simulations of simultaneous-source
                  marine acquisition using periodic and randomized
                  time dithering.},
  keywords ={SLIM,Acquisition,Processing,Compressive Sensing},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/simmarineacq.pdf}
}

@unpublished{VanLeeuwen11TRfwiwse,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast waveform inversion without source encoding},
  year = {2011},
  type = {Tech. Rep.},
notes = {TR-2011-06 },
  address = {University of British Columbia, Vancouver},
  optmonth = {09/2011},
  abstract = {Randomized source encoding has recently been proposed as
                  a way to dramatically reduce the costs of full
                  waveform inversion. The main idea is to replace all
                  sequential sources by a small number of simultaneous
                  sources. This introduces random crosstalk in the
                  model updates and special stochastic optimization
                  strategies are required to deal with this. Two
                  problems arise with this approach: i) source
                  encoding can only be applied to fixed-spread
                  acquisition setups, and ii) stochastic optimization
                  methods tend to converge very slowly, relying on
                  averaging to get rid of the cross-talk. Although the
                  slow convergence is partly offset by the low
                  iteration cost, we show that conventional
                  optimization strategies are bound to outperform
                  stochastic methods in the long run. In this paper we
                  argue that we don¬¨√∏t need randomized source encoding
                  to reap the benefits of stochastic optimization and
                  we review an optimization strategy that combines the
                  benefits of both conventional and stochastic
                  optimization. The method uses a gradually increasing
                  batch of sources. Thus, iterations are very cheap
                  initially and this allows the method to make fast
                  progress in the beginning. As the batch size grows,
                  the method behaves like conventional optimization,
                  allowing for fast convergence. Numerical examples
                  suggest that the stochastic and hybrid method
                  perform equally well with and without source
                  encoding and that the hybrid method outperforms both
                  conventional and stochastic optimization. The method
                  does not rely on source encoding techniques and can
                  thus be applied to non fixed-spread data.},
  keywords = {SLIM,Full-waveform inversion,Optimization},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/TR201106.pdf }
}

@unpublished{Li11TRfrfwi,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix
        J. Herrmann},
  title = {Fast randomized full-waveform inversion with compressive sensing},
  year = {2011},
notes = {to appear in Geophysics},
  address = {University of British Columbia, Vancouver},
  optmonth = {10/2011},
  abstract = { Wave-equation based seismic inversion can be formulated
                  as a nonlinear inverse problem where the medium
                  properties are obtained via minimization of a least-
                  squares misfit functional. The demand for higher
                  resolution models in more geologically complex areas
                  drives the need to develop techniques that explore
                  the special structure of full-waveform inversion to
                  reduce the computational burden and to regularize
                  the inverse problem. We meet these goals by using
                  ideas from compressive sensing and stochastic
                  optimization to design a novel Gauss-Newton method,
                  where the updates are computed from random subsets
                  of the data via curvelet-domain sparsity
                  promotion. Application of this idea to a realistic
                  synthetic shows improved results compared to
                  quasi-Newton methods, which require passes through
                  all data. Two different subset sampling strategies
                  are considered: randomized source encoding, and
                  drawing sequential shots firing at random source
                  locations from marine data with missing near and far
                  offsets. In both cases, we obtain excellent
                  inversion results compared to conventional methods
                  at reduced computational costs. },
  keywords ={SLIM,Full-waveform inversion,Compressive Sensing,Optimization},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Submitted/Geophysics/Li11TRfrfwi/Li11TRfrfwi.pdf}
}

@unpublished{Aravkin11TRridr,
  author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Felix J. Herrmann and Tristan
        van Leeuwen},
  title = {Robust inversion, dimensionality reduction, and randomized sampling},
  year = {2011},
notes = {TR-2011-13},  
address = {University of British Columbia, Vancouver},
  optmonth = {11/2011},
  abstract = {We consider a class of inverse problems in which the
                  forward model is the solution operator to linear
                  ODEs or PDEs. This class admits several
                  dimensionality-reduction techniques based on data
                  averaging or sampling, which are especially useful
                  for large-scale problems.  We survey these
                  approaches and their connection to stochastic
                  optimization.  The data-averaging approach is only
                  viable, however, for a least-squares misfit, which
                  is sensitive to outliers in the data and artifacts
                  unexplained by the forward model. This motivates us
                  to propose a robust formulation based on the
                  Student's t-distribution of the error.  We
                  demonstrate how the corresponding penalty function,
                  together with the sampling approach, can obtain good
                  results for a large-scale seismic inverse problem
                  with 50% corrupted data.},
  keywords = {SLIM,Optimization,Full-waveform inversion},
  url = {http://www.optimization-online.org/DB_FILE/2011/11/3243.pdf}
}

@unpublished{haber10TRemp,
  author = {Eldad Haber and Matthias Chung and Felix J. Herrmann},
  title = {An effective method for parameter estimation with PDE constraints
        with multiple right hand sides},
  institution = {UBC-Earth and Ocean Sciences Department},
  year = {2010},
  number = {TR-2010-4},
  abstract = {Many parameter estimation problems involve with a
                  parameter-dependent PDEs with multiple right hand
                  sides. The computational cost and memory
                  requirements of such problems increases linearly
                  with the number of right hand sides. For many
                  applications this is the main bottleneck of the
                  computation. In this paper we show that problems
                  with multiple right hand sides can be reformulated
                  as stochastic optimization problems that are much
                  cheaper to solve. We discuss the solution
                  methodology and use the direct current resistivity
                  and seismic tomography as model problems to show the
                  effectiveness of our approach.},
  keywords = {SLIM,Full-waveform inversion,Optimization},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/Haber2010emp.pdf}
}

% Check with Michael whether this will be publsihed or stays as an technical report
@unpublished{vandenberg08gsv,
  author = {Ewout van den Berg and Mark Schmidt and Michael P. Friedlander and K.
	Murphy},
  title = {Group sparsity via linear-time projection},
institution = {UBC-Computer Science Department},
  year = {2008},
    number = {TR-2008-1},
  abstract = {We present an efficient spectral projected-gradient algorithm for
	optimization subject to a group one-norm constraint. Our approach
	is based on a novel linear-time algorithm for Euclidean projection
	onto the one- and group one-norm constraints. Numerical experiments
	on large data sets suggest that the proposed method is substantially
	more efficient and scalable than existing methods.},
  keywords = {SLIM,Optimization},
  url = {http://www.optimization-online.org/DB_FILE/2008/07/2056.pdf}
}

