% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2017-----%

@UNPUBLISHED{alfaraj2017SEGrsw,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Reconstruction of {S-waves} from low-cost randomized and simultaneous acquisition by joint sparse inversion},
  year = {2017},
  abstract = {Spatial Nyquist sampling rate, which is proportional to
                  the apparent subsurface velocity, is the key
                  deciding cost factor for conventional seismic
                  acquisition. Due to the lower shear wave velocity
                  compared with compressional waves, finer spatial
                  sampling is required to properly record the earlier,
                  which increases the acquisition costs. This is one
                  of the reasons why shear waves are usually not
                  considered in practice. To avoid having the Nyquist
                  criterion as the deciding cost factor and to utilize
                  multicomponent data to its available full extent, we
                  propose acquiring randomly undersampled ocean bottom
                  seismic data. Each component can then be
                  interpolated separately, followed by elastic
                  decomposition to recover up- and down-going
                  S-waves. Instead, we jointly interpolate and
                  decompose the recorded multicomponent data by
                  solving one sparsity promoting optimization
                  problem. This way we ensure that the relative
                  amplitudes of the multicomponent data is
                  preserved. We compare two sparsifying transforms:
                  the curvelet transform and the frequency-wavenumber
                  transform. Another key cost deciding factor for
                  seismic acquisition is the efficiency of acquiring
                  data. This calls for simultaneous acquisition, which
                  requires a source separation step. Similarly,
                  instead of taking a two-step approach, we perform a
                  sparsity-promoting joint source separation
                  decomposition. Results on economically and
                  efficiently acquired synthetic data of both joint
                  methods show their ability of reconstructing
                  accurate up- and down-going S-waves.},
  keywords = {SEG, elastic, shear wave, randomized acquisition, interpolation, source separation, decomposition, sparse, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/alfaraj2017SEGrsw/alfaraj2017SEGrsw.html}
}


@UNPUBLISHED{dasilva2017uls,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {A unified {2D/3D} large scale software environment for nonlinear inverse problems},
  year = {2017},
  abstract = {Large scale parameter estimation problems are some of
                  the most computationally demanding problems. An
                  academic researcher's domain-specific knowledge
                  often precludes that of software design, which
                  results in software frameworks for inversion that
                  are technically correct, but not scalable to
                  realistically-sized problems. On the other hand, the
                  computational demands of the problem for realistic
                  problems result in industrial codebases that are
                  geared solely for performance, rather than
                  comprehensibility or flexibility. We propose a new
                  software design that bridges the gap between these
                  two seemingly disparate worlds. A hierarchical and
                  modular design allows a user to delve into as much
                  detail as she desires, while using high performance
                  primitives at the lower levels. Our code has the
                  added benefit of actually reflecting the underlying
                  mathematics of the problem, which lowers the
                  cognitive load on user using it and reduces the
                  initial startup period before a researcher can be
                  fully productive. We also introduce a new
                  preconditioner for the Helmholtz equation that is
                  suitable for fault-tolerant distributed
                  systems. Numerical experiments on a variety of 2D
                  and 3D test problems demonstrate the effectiveness
                  of this approach on scaling algorithms from small to
                  large scale problems with minimal code changes.},
  keywords = {optimization, PDE-constrained inversion, large scale, matlab, private},
  note = {Submitted to ACM Transactions on Mathematical Software on February 14, 2017.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2017/dasilva2017uls/dasilva2017uls.html}
}


@UNPUBLISHED{daskalakis2017SEGdds,
  author = {Emmanouil Daskalakis and Rachel Kuske and Felix J. Herrmann},
  title = {Developments in the direction of solving extremly large problems in {Geophysics}},
  year = {2017},
  abstract = {Often in exploration Geophysics we are forced to work
                  with extremely large problems. Acquisitions via
                  dense grids of receivers translate into very large
                  mathematical systems. Usually, depending on the
                  acquisition, the size of the matrix of the system to
                  be solved, can be measured in the millions. The
                  proper way to address this problem is by
                  subsampling. Even though subsampling can reduce the
                  computational efforts required, it can not address
                  stability problems caused by preconditioning and/or
                  instrumental response errors. In this abstract, we
                  introduce a modification of the linearized Bregman
                  solver for these large problems that resolves
                  stability issues.},
  keywords = {SEG, linearized Bregman, cycling, weighted increment, least-squares migration, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/daskalakis2017SEGdds/daskalakis2017SEGdds.html}
}


@UNPUBLISHED{fang2017WAVESuqf,
  author = {Zhilong Fang and Curt Da Silva and Rachel Kuske and Felix J. Herrmann},
  title = {Uncertainty quantification for inverse problems with a weak wave-equation constraint},
  year = {2017},
  abstract = {In this work, we present a new posterior distribution to
                  quantify uncertainties in solutions of wave-equation
                  based inverse problems. By introducing an auxiliary
                  variable for the wavefields, we weaken the strict
                  wave-equation constraint used by conventional
                  Bayesian approaches. With this weak constraint, the
                  new posterior distribution is a bi-Gaussian
                  distribution with respect to both model parameters
                  and wavefields, which can be directly sampled by the
                  Gibbs sampling method.},
  keywords = {uncertainty, wave equation, constraint, Gibbs sampling, private},
  note = {Submitted to the WAVES 2017 Conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/WAVES/2017/fang2017WAVESuqf/fang2017WAVESuqf.html}
}


@UNPUBLISHED{kumar2017hrc,
  author = {Rajiv Kumar and Haneet Wason and Shashin Sharan and Felix J. Herrmann},
  title = {Highly repeatable {3D} compressive full-azimuth towed-streamer time-lapse acquisition --- a numerical feasibility study at scale},
  year = {2017},
  month = {05},
  abstract = {Most conventional 3D time-lapse (or 4D) acquisitions are
                  ocean-bottom cable (OBC) or ocean-bottom node (OBN)
                  surveys since these surveys are relatively easy to
                  replicate compared to towed-streamer surveys. To
                  attain high degrees of repeatability, survey
                  replicability and dense periodic sampling has become
                  the norm for 4D surveys that renders this technology
                  expensive. Conventional towed-streamer acquisitions
                  suffer from limited illumination of subsurface due
                  to narrow azimuth. Although, acquisition techniques
                  such as multi-azimuth, wide-azimuth, rich-azimuth
                  acquisition, etc., have been developed to illuminate
                  the subsurface from all possible angles, these
                  techniques can be prohibitively expensive for
                  densely sampled surveys. This leads to uneven
                  sampling, i.e., dense receiver and coarse source
                  sampling or vice-versa, in order to make these
                  acquisitions more affordable. Motivated by the
                  design principles of Compressive Sensing (CS), we
                  acquire economic, randomly subsampled (or
                  compressive) and simultaneous towed-streamer
                  time-lapse data without the need of replicating the
                  surveys. We recover densely sampled time-lapse data
                  on one and the same periodic grid by using a
                  joint-recovery model (JRM) that exploits shared
                  information among different time-lapse recordings,
                  coupled with a computationally cheap and scalable
                  rank-minimization technique. The acquisition is low
                  cost since we have subsampled measurements (about
                  70\% subsampled), simulated with a simultaneous
                  long-offset acquisition configuration of two source
                  vessels travelling across a survey area at random
                  azimuths. We analyze the performance of our proposed
                  compressive acquisition and subsequent recovery
                  strategy by conducting a synthetic, at scale,
                  seismic experiment on a 3D time-lapse model
                  containing geological features such as channel
                  systems, dipping and faulted beds, unconformities
                  and a gas cloud. Our findings indicate that the
                  insistence on replicability between surveys and the
                  need for OBC/OBN 4D surveys can, perhaps, be
                  relaxed. Moreover, this is a natural next step
                  beyond the successful CS acquisition examples
                  discussed in this special issue.},
  keywords = {time-lapse seismic, marine, 3D, simultaneous long offset, CS, rank minimization, private},
  note = {Submitted to The Leading Edge},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2017/kumar2017hrc/kumar2017hrc.html}
}


@UNPUBLISHED{oghenekohwo2017hrt,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Highly repeatable time-lapse seismic with distributed {Compressive} {Sensing}---mitigating effects of calibration errors},
  year = {2017},
  month = {05},
  abstract = {Recently, we demonstrated that combining joint recovery
                  with low-cost non-replicated randomized sampling
                  tailored to time-lapse seismic can give us access to
                  high fidelity, highly repeatable, dense prestack
                  vintages, and high-grade time-lapse. To arrive at
                  this result, we assumed well-calibrated
                  surveys---i.e., we presumed accurate post-plot
                  source/receiver positions. Unfortunately, in
                  practice seismic surveys are prone to calibration
                  errors, which are unknown deviations between actual
                  and post-plot acquisition geometry. By means of
                  synthetic experiments, we analyze the possible
                  impact of these errors on vintages and on time-lapse
                  data obtained with our joint recovery model from
                  compressively sampled surveys. Supported by these
                  experiments, we demonstrate that highly repeatable
                  time-lapse vintages are attainable despite the
                  presence of unknown calibration errors in the
                  positions of the shots. We assess the repeatability
                  quantitatively for two scenarios by studying the
                  impact of calibration errors on conventional dense
                  but irregularly sampled surveys and on low-cost
                  compressed surveys. To separate time-lapse effects
                  from calibration issues, we consider the idealized
                  case where the subsurface remains unchanged and the
                  practical situation where time-lapse changes are
                  restricted to a subset of the data. In both cases,
                  the quality of the recovered vintages and time-lapse
                  decreases gracefully for low-cost compressed surveys
                  with increasing calibration errors. Conversely, the
                  quality of vintages from expensive densely
                  periodically sampled surveys decreases more rapidly
                  as unknown and difficult to control calibration
                  errors increase.},
  keywords = {time-lapse seismic, marine, random sampling, calibration errors, joint-recovery method, private},
  note = {Submitted to The Leading Edge},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2017/oghenekohwo2017hrt/oghenekohwo2017hrt.html}
}


@UNPUBLISHED{sharan2017SEGhrf,
  author = {Shashin Sharan and Rongrong Wang and Felix J. Herrmann},
  title = {High resolution fast microseismic source collocation and source time function estimation},
  year = {2017},
  abstract = {Sparsity promotion based joint microseismic source
                  collocation and source time function estimation,
                  using Linearized Bregman algorithm, although simple
                  to implement, suffers from slow convergence. This is
                  due to the fact that Linearized Bregman algorithm
                  has only first order of convergence. In this work,
                  we propose to accelerate the existing Linearized
                  Bregman algorithm using the L-BFGS
                  algorithm. Without any initial guess for the source
                  location or source time function, our method is able
                  to estimate the source location and source time
                  function for kinematically correct smooth velocity
                  model. We demonstrate the effectiveness of our
                  method for multiple sources spaced within half a
                  wavelength. We also compare our results with
                  Linearized Bregman based method in ``2.5``D instead
                  of ``2``D.},
  keywords = {SEG, algorithm, full-waveform inversion, microseismic, passive imaging, wave equation, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/sharan2017SEGhrf/sharan2017SEGhrf.html}
}


@UNPUBLISHED{wang2017SEGdff,
  author = {Rongrong Wang and Felix J. Herrmann},
  title = {A denoising formulation of full-waveform inversion},
  year = {2017},
  abstract = {We propose a wave-equation-based subsurface inversion
                  method that in many cases is more robust than the
                  conventional Full-Waveform Inversion. The new
                  formulation is written in a denoising form that
                  allows the synthetic data to match the observed ones
                  up to a small error. Compared to the Full-Waveform
                  Inversion, our method treats the noise arising from
                  the data measuring/recording process and that from
                  the synthetic modelling process
                  separately. Comparing to the Wavefields
                  Reconstruction Inversion, the new formulation
                  mitigates the difficulty of choosing the penalty
                  parameter $\lambda$. To solve the proposed
                  optimization problem, we develop an efficient
                  frequency domain algorithm that alternatively
                  updates the model and the data. Numerical
                  experiments confirm strong stability of the proposed
                  method by comparisons between the results of our
                  algorithm with that from both plain FWI and a
                  weighted formulation of the FWI.},
  keywords = {SEG, FWI, denoising, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/wang2017SEGdff/wang2017SEGdff.pdf}
}


@UNPUBLISHED{yang2017SEGfsp,
  author = {Mengmeng Yang and Emmanouil Daskalakis and Felix J. Herrmann},
  title = {Fast sparsity-promoting least-squares migration with multiples in time domain},
  year = {2017},
  abstract = {Based on the latest developments of research in
                  inversion technology with optimization, researchers
                  have made significant progress in the implementation
                  of least-squares reverse-time migration (LS-RTM) of
                  primaries. In Marine data however, these
                  applications rely on the success of a pre-imaging
                  separation of primaries and multiples, which can be
                  modeled as a multi-dimensional convolution between
                  the vertical derivative of the surface-free Green’s
                  function and the down-going receiver
                  wavefield. Instead of imaging the primaries and
                  multiples separately, we implement the LS-RTM of the
                  total down-going wavefield by combining areal source
                  injection and linearized Born modelling, where
                  strong surface related multiples are generated from
                  a strong density variation at the ocean bottom. The
                  advantage including surface related multiples in
                  LS-RTM is the extra illumination we obtain from
                  these multiples without incurring additional
                  computational costs related to carrying out
                  multi-dimensional convolutions part of conventional
                  multiple prediction procedures. Even though we are
                  able to avert these computational costs, our
                  approach shares the large costs of LSRTM. We reduce
                  these costs by combining randomized source
                  subsampling with our sparsity-promoting imaging
                  technology, which produces artifact-free,
                  high-resolution images, with the surface-related
                  multiples migrated properly.},
  keywords = {SEG, sparsity, least-squares RTM, multiples, time domain, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/yang2017SEGfsp/yang2017SEGfsp.html}
}


@UNPUBLISHED{zhang2017SEGmsd,
  author = {Yiming Zhang and Curt Da Silva and Rajiv Kumar and Felix J. Herrmann},
  title = {Massive {3D} seismic data compression and inversion with hierarchical {Tucker}},
  year = {2017},
  abstract = {Modern-day oil and gas exploration, especially in areas
                  of complex geology such as fault belts and sub-salt
                  areas, is an increasingly expensive and risky
                  endeavour. Typically long-offset and dense sampling
                  seismic data are required for subsequent shot based
                  processing procedures, e.g. wave-equation based
                  inversion (WEI) and surface-related multiple
                  elimination (SRME). However, these strict
                  requirements result in an exponential growth in data
                  volume size and prohibitive demands on computational
                  resources, given the multidimensional nature of the
                  data volumes. Moreover the physical constraints and
                  cost limitations impose restrictions on acquiring
                  fully sampled data. In this work, we propose to
                  invert our large-scale data from a set of subsampled
                  measurements, resulting in an estimate of the true
                  volume in a compressed low-rank tensor
                  format. Rather than expanding the data to its
                  fully-sampled form for later downstream processes,
                  we demonstrate how to use this compressed data
                  directly via on-the-fly common shot or receiver
                  gathers extraction. The combination of massive
                  compression and fast on demand data reconstruction
                  of 3D shot or receiver gathers leads to a
                  substantial reduction in memory costs but with
                  minimal effects on results in the subsequent
                  processing procedures. We demonstrate the effective
                  implementation of our proposed framework on
                  full-waveform inversion on a 3D seismic synthetic
                  data set generated from a Overthrust model.},
  keywords = {SEG, 3D, tensor algebra, FWI, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/zhang2017SEGmsd/zhang2017SEGmsd.html}
}


%-----2016-----%

@UNPUBLISHED{esser2016tvr,
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Total-variation regularization strategies in full-waveform inversion},
  year = {2016},
  abstract = {We propose an extended full-waveform inversion
                  formulation that includes general convex constraints
                  on the model. Though the full problem is highly
                  nonconvex, the overarching optimization scheme
                  arrives at geologically plausible results by solving
                  a sequence of relaxed and warm-started constrained
                  convex subproblems. The combination of box,
                  total-variation, and successively relaxed asymmetric
                  total-variation constraints allows us to steer free
                  from parasitic local minima while keeping the
                  estimated physical parameters laterally continuous
                  and in a physically realistic range. For accurate
                  starting models, numerical experiments carried out
                  on the challenging 2004 BP velocity benchmark
                  demonstrate that bound and total-variation
                  constraints improve the inversion result
                  significantly by removing inversion artifacts,
                  related to source encoding, and by clearly improved
                  delineation of top, bottom, and flanks of a
                  high-velocity high-contrast salt inclusion. The
                  experiments also show that for poor starting models
                  these two constraints by themselves are insufficient
                  to detect the bottom of high-velocity inclusions
                  such as salt. Inclusion of the one-sided asymmetric
                  total-variation constraint overcomes this issue by
                  discouraging velocity lows to buildup during the
                  early stages of the inversion. To the author's
                  knowledge the presented algorithm is the first to
                  successfully remove the imprint of local minima
                  caused by poor starting models and band-width
                  limited finite aperture data.},
  keywords = {full-waveform inversion, constrained optimization, total variation, salt, hinge loss, private},
  note = {Submitted on July 13, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/esser2016tvr/esser2016tvr.pdf}
}

