% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2015-----%

@UNPUBLISHED{esser2015SEGasd,
  author = {Ernie Esser and Lluís Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Automatic salt delineation --- {Wavefield} {Reconstruction} {Inversion} with convex constraints},
  year = {2015},
  abstract = {We extend full-waveform inversion by Wavefield
                  Reconstruction Inversion by including convex
                  constraints on the model. Contrary to the
                  conventional adjoint-state formulations, Wavefield
                  Reconstruction Inversion has the advantage that the
                  Gauss-Newton Hessian is well approximated by a
                  diagonal scaling, which allows us to add convex
                  constraints, such as the box- and the
                  edge-preserving total-variation constraint, on the
                  square slowness without incurring significant
                  increases in computational costs. As the examples
                  demonstrate, including these constraints yields far
                  superior results in complex geological areas that
                  contain high-velocity high-contrast bodies
                  (e.g. salt or basalt). Without these convex
                  constraints, adjoint-state and Wavefield
                  Reconstruction Inversion get trapped in local minima
                  for poor starting models.},
  keywords = {SEG, full-waveform inversion, convex constraints, total-variation norm, salt, private},
  note = {(submitted to the SEG conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2015/esser2015SEGasd/esser2015SEGasd.html}
}


@UNPUBLISHED{esser2015CAMSAPrsa,
  author = {Ernie Esser and Rongrong Wang and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Resolving scaling ambiguities with the $\ell_1/\ell_2$ norm in a blind deconvolution problem with feedback},
  year = {2015},
  date_submitted = {07/03/2015},
  abstract = {Compared to more mundane blind deconvolution problems,
                  blind deconvolution in seismic applications involves
                  a feedback mechanism related to the free
                  surface. The presence of this feedback mechanism
                  gives us an unique opportunity to remove ambiguities
                  that have plagued blind deconvolution for a long
                  time. While beneficial, this feedback by itself is
                  insufficient to remove the ambiguities even with
                  $\ell_1$ constraints. However, when paired with an
                  $\ell_1/\ell_2$ constraint the feedback allows us to
                  resolve the scaling ambiguity under relatively mild
                  assumptions. Inspired by lifting approaches, we
                  propose to split the sparse signal into positive and
                  negative components and apply an $\ell_1/\ell_2$
                  constraint to the difference, thereby obtaining a
                  constraint that is easy to implement. Numerical
                  experiments demonstrate robustness to the
                  initialization as well as to noise in the data.},
  keywords = {CAMSAP, blind deconvolution, $\ell_1/\ell_2$, lifting, method of multipliers, private},
  note = {submitted to the IEEE workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/CAMSAP/2015/esser2015CAMSAPrsa/esser2015CAMSAPrsa.pdf}
}


@UNPUBLISHED{kumar2015sss,
  author = {Rajiv Kumar and Haneet Wason and Felix J. Herrmann},
  title = {Source separation for simultaneous towed-streamer marine acquisition---a compressed sensing approach},
  year = {2015},
  abstract = {Simultaneous marine acquisition is an economic way to
                  sample seismic data and speedup acquisition, wherein
                  single and/or multiple source vessels fire sources
                  at near-simultaneous or slightly random times,
                  resulting in overlapping shot records. The current
                  paradigm for simultaneous towed-streamer marine
                  acquisition incorporates "low-variability" in source
                  firing times---i.e., $\leq$ 1 or 2 seconds, since
                  both the sources and receivers are moving. This
                  results in low degree of randomness in simultaneous
                  data, which is challenging to separate (into its
                  constituent sources) using compressed sensing based
                  separation techniques since randomization is the key
                  to successful recovery via compressed sensing. In
                  this paper, we address the challenge of source
                  separation for simultaneous towed-streamer
                  acquisitions via two compressed sensing based
                  approaches---i.e., sparsity-promotion and
                  rank-minimization. We illustrate the performance of
                  both the sparsity-promotion and rank-minimization
                  based techniques by simulating two simultaneous
                  towed-streamer acquisition scenarios---i.e.,
                  over/under and simultaneous long offset. We observe
                  that the proposed approaches give good and
                  comparable recovery qualities of the separated
                  sources, but the rank-minimization technique is
                  relatively faster and memory efficient. We also
                  compare these two techniques with the NMO-based
                  median filtering type approach.},
  keywords = {simultaneous marine acquisition, source separation, sparsity, rank, private},
  note = {Submitted to Geophysics on February 16.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/kumar2015sss/kumar2015sss.html}
}


@UNPUBLISHED{li2015GEOPmgn,
  author = {Xiang Li and Ernie Esser and Felix J. Herrmann},
  title = {Modified {Gauss}-{Newton} full-waveform inversion explained—why sparsity-promoting updates do matter},
  year = {2015},
  abstract = {Full-waveform inversion can be formulated as a nonlinear
                  least-squares optimization problem. This non-convex
                  problem can be extremely computationally expensive
                  because it requires repeatedly solving large linear
                  systems that correspond to discretized partial
                  differential equations. Randomized subsampling
                  techniques allow us to work with small subsets of
                  (monochromatic) source experiments, reducing the
                  computational cost. However, this subsampling
                  weakens subsurface illumination and introduces
                  subsampling related incoherent artifacts. These
                  subsampling-related artifacts---in conjunction with
                  local minima that are known to plague full-waveform
                  inversion---motivate us to come up with a technique
                  to "regularize" this problem. Following earlier
                  work, we take advantage of the fact that curvelets
                  represent subsurface models and perturbations
                  parsimoniously. At first impulse promoting sparsity
                  on the model directly seems the most natural way to
                  proceed, but we will demonstrate that in certain
                  cases it can be advantageous to promote sparsity on
                  the Gauss-Newton updates instead. While constraining
                  the one-norm of the descent directions does not
                  change not change the underlying full-waveform
                  inversion objective, the constrained model updates
                  remain descent directions, remove
                  subsampling-related artifacts and improve the
                  overall inversion result. We empirically observe
                  this phenomenon in situations where the different
                  model updates occur at roughly the same locations in
                  the curvelet domain. We further investigate and
                  analyze this phenomenon, where nonlinear inversions
                  benefit from sparsity-promoting constraints on the
                  updates, by means of a set of carefully selected
                  examples including phase retrieval and full-waveform
                  inversion. In all cases, we observe a faster decay
                  of the residual and model error as a function of the
                  number of iterations.},
  keywords = {full-waveform inversion, Gauss-Newton method, sparsity promotion, private},
  note = {Submitted to Geophysics on May 6.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/li2015GEOPmgn/li2015GEOPmgn.pdf}
}


@UNPUBLISHED{lin2015scatterEPSI,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of primaries by sparse inversion with scattering-based multiple predictions for data with large gaps},
  year = {2015},
  abstract = {We propose to solve the Estimation of Primaries by
                  Sparse Inversion problem without any explicit data
                  reconstruction, from a seismic record with missing
                  near-offsets and large holes in the acquisition grid
                  which does not produce aliasing but otherwise causes
                  large errors in the multiple prediction. Exclusion
                  of the unknown data as an inversion variable from
                  the process is desirable, since it sidesteps
                  possible issues arising from fitting observations
                  that are also unknowns in the same inversion
                  problem. Instead, we simulate their multiple
                  contributions by augmenting the forward prediction
                  model for the total wavefield with a scattering
                  series that mimics the action of the free surface
                  reflector within the confines of the unobserved
                  trace locations. Each term in this scattering series
                  simply involves convolution of the predicted
                  wavefield once more with the current estimated
                  surface-free Green's function at these unobserved
                  locations. We investigate the necessary
                  modifications to the primary estimation algorithm to
                  account for the resulting nonlinearity in the
                  modeling operator, and also demonstrate that just a
                  few scattering terms are enough to satisfactorily
                  mitigate the effects of near-offset data gaps during
                  the inversion process. Numerical experiments on
                  synthetic data show that the final derived method
                  can significantly outperform explicit data
                  reconstruction for large near-offset gaps. This is
                  achieved with a similar computational cost and
                  better memory efficiency compared to explicit data
                  reconstruction. We also show on real data that our
                  scheme outperforms pre-interpolation of the
                  near-offset gap.},
  keywords = {multiples, REPSI, EPSI, scattering, sparsity, optimization, private},
  note = {Submitted to Geophysics on May 5.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/lin2015scatterEPSI/lin2015scatterEPSI.html}
}


@UNPUBLISHED{louboutin2015SEGtcs,
  author = {Mathias Louboutin and Felix J. Herrmann},
  title = {Time compressively sampled full-waveform inversion with stochastic optimization},
  year = {2015},
  abstract = {Time-domain Full-Waveform Inversion (FWI) aims to image
                  the subsurface of the earth accurately from field
                  recorded data and can be solved via the reduced
                  adjoint-state method. However, this method requires
                  access to the forward and adjoint wavefields that
                  are meet when computing gradient updates. The
                  challenge here is that the adjoint wavefield is
                  computed in reverse order during time stepping and
                  therefore requires storage or other type of
                  mitigation because storing the full time history of
                  the forward wavefield is too expensive in realistic
                  3D settings. To overcome this challenge, we propose
                  an approximate adjoint-state method where the
                  wavefields are subsampled randomly, which
                  drastically the amount of storage needed. By using
                  techniques from stochastic optimization, we control
                  the errors induced by the subsampling. Examples of
                  the proposed technique on a synthetic but realistic
                  2D model show that the subsampling-related artifacts
                  can be reduced significantly by changing the
                  sampling for each source after each model
                  update. Combination of this gradient approximation
                  with a quasi-Newton method shows virtually artifact
                  free inversion results requiring only 5% of storage
                  compared to saving the history at Nyquist. In
                  addition, we avoid having to recompute the
                  wavefields as is required by checkpointing.},
  keywords = {SEG, Full-waveform inversion, Acoustic, Subsampling, Time-domain, Inversion, Stochastic optimization, private},
  note = {(submitted to the SEG conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2015/louboutin2015SEGtcs/louboutin2015SEGtcs.html}
}


@UNPUBLISHED{peters2015SEGrwi,
  author = {Bas Peters and Zhilong Fang and Brendan Smithyman and Felix J. Herrmann},
  title = {Regularizing waveform inversion by projections onto convex sets --- application to the {2D} {Chevron} 2014 synthetic blind-test dataset},
  year = {2015},
  abstract = {A framework is proposed for regularizing the waveform
                  inversion problem by projections onto intersections
                  of convex sets. Multiple pieces of prior information
                  about the geology are represented by multiple convex
                  sets, for example limits on the velocity or minimum
                  smoothness conditions on the model.  The data-misfit
                  is then minimized, such that the estimated model is
                  always in the intersection of the convex
                  sets. Therefore, it is clear what properties the
                  estimated model will have at each iteration. This
                  approach does not require any quadratic penalties to
                  be used and thus avoids the known problems and
                  limitations of those types of penalties. It is shown
                  that by formulating waveform inversion as a
                  constrained problem, regularization ideas such as
                  Tikhonov regularization and gradient filtering can
                  be incorporated into one framework. The algorithm is
                  generally applicable, in the sense that it works
                  with any (differentiable) objective function and
                  does not require significant additional
                  computation. The method is demonstrated on the
                  inversion of the 2D marine isotropic elastic
                  synthetic seismic benchmark by Chevron using an
                  acoustic modeling code. To highlight the effect of
                  the projections, we apply no data pre-processing.},
  keywords = {SEG, Waveform inversion, regularization, projection, blind-test,Wavefield Reconstruction Inversion, private},
  note = {(submitted to the SEG conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2015/peters2015SEGrwi/peters2015SEGrwi.html}
}
 

@UNPUBLISHED{tu2015GJIsem,
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Source estimation with multiples—fast ambiguity-resolved seismic imaging},
  year = {2015},
  abstract = {We propose a fast "wavelet-free" least-squares imaging
                  procedure that produces high-accuracy seismic images
                  without the knowledge of the source
                  wavelet. Conventional reverse-time migration
                  requires the knowledge of the source wavelet, which
                  is either unavailable or very difficult to
                  accurately determine; inaccurate estimates of the
                  source wavelet can result in seriously degraded
                  reverse-time migrated images, and therefore wrong
                  geological interpretations. To solve this problem,
                  we present a "wavelet-free" imaging procedure that
                  simultaneously inverts for the source wavelet and
                  the seismic image, by tightly integrating source
                  estimation into a fast least-squares imaging
                  framework, namely compressive imaging, given a
                  reasonably accurate background velocity
                  model. However, this joint inversion problem is
                  difficult to solve as it is plagued with local
                  minima and the ambiguity with respect to amplitude
                  scalings, because of the multiplicative, and
                  therefore nonlinear, appearance of the source
                  wavelet in the otherwise linear formalism. We have
                  found a way to solve this nonlinear joint-inversion
                  problem using a technique called variable
                  projection, and a way to overcome the scaling
                  ambiguity by including surface-related multiples in
                  our imaging procedure following recent developments
                  in surface-related multiple prediction by sparse
                  inversion. As a result, we obtain highly accurate
                  estimates of the source wavelet and high-resolution
                  seismic images, comparable in quality to images
                  obtained assuming the true source wavelet is
                  known. By leveraging the computationally efficient
                  compressive-imaging methodology, these results are
                  obtained at affordable computational costs compared
                  with conventional processing work flows that include
                  surface-related multiple removal and reverse-time
                  migration.},
  keywords = {inverse theory, time series analysis, computational seismology, wave propagation, free surface, multiples, seismic imaging, private},
  note = {Submitted to Geophysical Journal International on May 14.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/tu2015GJIsem/tu2015GJIsem.html}
}


@UNPUBLISHED{vanleeuwen2015GPWEMVA,
  author = {Tristan van Leeuwen and Rajiv Kumar and Felix J. Herrmann},
  title = {Affordable omnidirectional subsurface extended image volumes},
  year = {2015},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for the inference of rock properties
                  and velocity analysis in areas of complex
                  geology. Traditionally, these gathers are thought of
                  as multidimensional correlations of the source and
                  receiver wavefields. The bottleneck in computing
                  these gathers lies in the fact that one needs to
                  store, compute, and correlate these wavefields in
                  order to obtain the desired image
                  gathers. Therefore, the image gathers are typically
                  only computed for a limited number of subsurface
                  points and for a limited range of subsurface
                  offsets. In this paper, we offer a new perspective
                  on such gathers by organizing the extended image, as
                  a function of all subsurface offsets and all
                  subsurface points, into a matrix whose (i,j)th entry
                  captures the interaction between gridpoints i and
                  j. Of course, it is infeasible to form and store
                  this matrix explicitly. Instead, we propose an
                  efficient algorithm to glean information from the
                  image volume via matrix-vector products, which can
                  be computed efficiently. The probing techniques have
                  two main advantages; i) by using all subsurface
                  offsets, we can handle complex geological structures
                  with steep dips, ii) we can probe the whole model
                  simultaneously and do not have to pick points at
                  which to compute the image gathers. We illustrate
                  these advantages using two important applications of
                  image gathers: amplitude-versus-angle reflectivity
                  analysis and migration velocity analysis.},
  keywords = {migration velocity analysis, AVA, stochastic optimization, private},
  note = {Submitted to Geophysical Prospecting on May 6.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/vanleeuwen2015GPWEMVA/vanleeuwen2015GPWEMVA.pdf}
}


@UNPUBLISHED{vanleeuwen2015IPpmp,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for {PDE}-constrained optimization in inverse problems},
  year = {2015},
  abstract = {Many inverse and parameter estimation problems can be
                  written as PDE-constrained optimization
                  problems. The goal, then, is to infer the
                  parameters, typically coefficients of the PDE, from
                  partial measurements of the solutions of the PDE for
                  several right-hand-sides. Such PDE-constrained
                  problems can be solved by finding a stationary point
                  of the Lagrangian, which entails simultaneously
                  updating the parameters and the (adjoint) state
                  variables. For large-scale problems, such an
                  all-at-once approach is not feasible as it requires
                  storing all the state variables. In this case one
                  usually resorts to a reduced approach where the
                  constraints are explicitly eliminated (at each
                  iteration) by solving the PDEs. These two
                  approaches, and variations thereof, are the main
                  workhorses for solving PDE-constrained optimization
                  problems arising from inverse problems. In this
                  paper, we present an alternative method that aims to
                  combine the advantages of both approaches. Our
                  method is based on a quadratic penalty formulation
                  of the constrained optimization problem. By
                  eliminating the state variable, we develop an
                  efficient algorithm that has roughly the same
                  computational complexity as the conventional reduced
                  approach while exploiting a larger search
                  space. Numerical results show that this method
                  indeed reduces some of the non-linearity of the
                  problem and is less sensitive the initial iterate.},
  keywords = {penalty method, PDE, optimization, inverse problems, private},
  note = {Submitted to Inverse Problems on April 10.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/vanleeuwen2015IPpmp/vanleeuwen2015IPpmp.pdf}
}


%-----2014-----%

@UNPUBLISHED{dasilva2014htuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Optimization on the {Hierarchical} {Tucker} manifold - applications to tensor completion},
  year = {2014},
  month = {04},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker (HT) tensors, an efficient
                  structured tensor format based on recursive subspace
                  factorizations. By exploiting the smooth manifold
                  structure of these tensors, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient for completing tensors from
                  missing entries. Our algorithmic framework is fast
                  and scalable to large problem sizes as we do not
                  require SVDs on the ambient tensor space, as
                  required by other methods. Moreover, we exploit the
                  structure of the Gramian matrices associated with
                  the HT format to regularize our problem, reducing
                  overfitting for high subsampling ratios. We also
                  find that the organization of the tensor can have a
                  major impact on completion from realistic seismic
                  acquisition geometries. These samplings are far from
                  idealized randomized samplings that are usually
                  considered in the literature but are realizable in
                  practical scenarios. Using these algorithms, we
                  successfully interpolate large-scale seismic data
                  sets and demonstrate the competitive computational
                  scaling of our algorithms as the problem sizes
                  grow.},
  keywords = {hierarchical tucker, structured tensor, tensor interpolation, differential geometry, riemannian optimization, gauss newton, private},
  note = {Accepted to be published in the journal - Linear Algebra and its Applications, 2015},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2014/dasilva2014htuck/dasilva2014htuck.pdf}
}


@UNPUBLISHED{kumar2014GEOPemc,
  author = {Rajiv Kumar and Curt Da Silva and Okan Akalin and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Efficient matrix completion for seismic data reconstruction},
  year = {2014},
  month = {08},
  abstract = {Despite recent developments in improved acquisition,
                  seismic data often remains undersampled along source
                  and/or receiver coordinates, resulting in incomplete
                  data for key applications such as migration and
                  multiple prediction requiring densely sampled,
                  alias-free wide azimuth data. When seismic data is
                  organized in monochromatic frequency slices,
                  missing-trace interpolation can be cast into a
                  matrix completion problem, where the low-rank
                  structure of seismic data in the appropriate domain
                  can be exploited to recover densely sampled data
                  volumes from data with missing entries. Current
                  approaches that exploit low-rank structure are based
                  on repeated singular value decompositions, which
                  become prohibitively expensive for large-scale
                  problems unless the data is partitioned and
                  processed in small windows. While computationally
                  manageable, our theory and experiments show degraded
                  results when the windows sizes become too small. To
                  overcome this problem, we carry out our
                  interpolations for each frequency independently
                  while working with the complete data in the
                  midpoint-offset domain instead of windowing. For
                  lateral varying geologies that are not too complex,
                  working in the midpoint-offset domain leads to
                  favorable rank minimization recovery because the
                  singular values decay faster while sampling-related
                  artifacts remain full rank. This combination of fast
                  decay and full-rank artifacts agrees with the
                  principles of the compressive sensing paradigm,
                  which is based on exploiting (low-rank) structure, a
                  sampling process that breaks this structure, and a
                  rank-minimizing optimization that restores the
                  signal's structure and interpolates the subsampled
                  data. To make our proposed method computationally
                  viable and practical, we introduce a
                  factorization-based approach that avoids computing
                  the singular values, and that therefore scales to
                  large seismic data problems as long as the factors
                  can be stored in memory. Tests on realistic two- and
                  three-dimensional seismic data show that our method
                  compares favorably, both in terms of computational
                  speed and recovery quality, to existing
                  curvelet-based and tensor-based techniques.},
  keywords = {interpolation, low-rank, private},
  note = {Submitted to Geophysics on August 8, 2014.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2014/kumar2014GEOPemc/kumar2014GEOPemc.pdf}
}


@UNPUBLISHED{oghenekohwo2014GEOPfrt,
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Compressive 4D—economic time-lapse seismic with randomized subsampling and joint recovery},
  year = {2014},
  month = {10},
  abstract = {The current paradigm of time-lapse seismic relies on
                  dense sampling and repeatability amongst the
                  baseline and the monitor surveys. Recent results in
                  distributed compressive sensing allow us to come up
                  with a new economic sampling paradigm where the
                  vintages and time-lapse difference are recovered
                  from incomplete data. The combination of randomized
                  sampling, signal structure and correlations among
                  the vintages underlies this approach. In a somewhat
                  idealized setting where effects such as difference
                  in currents are ignored, and where we do not have
                  access to dense samplings of the baseline and/or
                  monitor surveys, we can get high quality recovery of
                  these vintages and time-lapse difference when there
                  is a small “overlap” in the surveys—i.e., where the
                  random samplings have partial statistical
                  dependence. Specifically, we find that the quality
                  of the vintages improves for decreasing overlap in
                  the surveys while the converse is true for the
                  time-lapse difference. Our setting differs from
                  conventional time-lapse acquisition because we do
                  not have access to dense samplings. Surveys with
                  partial overlapping randomized samplings lead to the
                  best trade-off between the recovery quality of the
                  vintages and the time-lapse signal. We confirm this
                  by a series of experiments.},
  keywords = {acquistion, time-lapse, marine, sampling, random, joint recovery method, private},
  note = {Submitted revision 1 to Geophysics on October 22, 2014},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2014/oghenekohwo2014GEOPfrt/oghenekohwo2014GEOPfrt.html}
}

