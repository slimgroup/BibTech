% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2014-----%

@UNPUBLISHED{kumar2014GEOPemc,
  author = {Rajiv Kumar and Curt Da Silva and Okan Akalin and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Efficient matrix completion for seismic data reconstruction},
  year = {2014},
  month = {08},
  institution = {UBC},
  abstract = {Despite recent developments in improved acquisition,
                  seismic data often remains undersampled along source
                  and/or receiver coordinates, resulting in incomplete
                  data for key applications such as migration and
                  multiple prediction requiring densely sampled,
                  alias-free wide azimuth data. When seismic data is
                  organized in monochromatic frequency slices,
                  missing-trace interpolation can be cast into a
                  matrix completion problem, where the low-rank
                  structure of seismic data in the appropriate domain
                  can be exploited to recover densely sampled data
                  volumes from data with missing entries. Current
                  approaches that exploit low-rank structure are based
                  on repeated singular value decompositions, which
                  become prohibitively expensive for large-scale
                  problems unless the data is partitioned and
                  processed in small windows. While computationally
                  manageable, our theory and experiments show degraded
                  results when the windows sizes become too small. To
                  overcome this problem, we carry out our
                  interpolations for each frequency independently
                  while working with the complete data in the
                  midpoint-offset domain instead of windowing. For
                  lateral varying geologies that are not too complex,
                  working in the midpoint-offset domain leads to
                  favorable rank minimization recovery because the
                  singular values decay faster while sampling-related
                  artifacts remain full rank. This combination of fast
                  decay and full-rank artifacts agrees with the
                  principles of the compressive sensing paradigm,
                  which is based on exploiting (low-rank) structure, a
                  sampling process that breaks this structure, and a
                  rank-minimizing optimization that restores the
                  signal's structure and interpolates the subsampled
                  data. To make our proposed method computationally
                  viable and practical, we introduce a
                  factorization-based approach that avoids computing
                  the singular values, and that therefore scales to
                  large seismic data problems as long as the factors
                  can be stored in memory. Tests on realistic two- and
                  three-dimensional seismic data show that our method
                  compares favorably, both in terms of computational
                  speed and recovery quality, to existing
                  curvelet-based and tensor-based techniques.},
  keywords = {interpolation, low-rank, private},
  note = {Submitted to Geophysics on August 8, 2014.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2014/kumar2014GEOPemc/kumar2014GEOPemc.pdf}
}


@UNPUBLISHED{oghenekohwo2014GEOPfrt,
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Compressive 4D—economic time-lapse seismic with randomized subsampling and joint recovery},
  year = {2014},
  month = {10},
  institution = {UBC},
  abstract = {The current paradigm of time-lapse seismic relies on dense sampling and repeatability amongst
				the baseline and the monitor surveys. Recent results in distributed compressive sensing allow
				us to come up with a new economic sampling paradigm where the vintages and time-lapse
				difference are recovered from incomplete data. The combination of randomized sampling,
				signal structure and correlations among the vintages underlies this approach. In a somewhat
				idealized setting where effects such as difference in currents are ignored, and where we do not
				have access to dense samplings of the baseline and/or monitor surveys, we can get high quality
				recovery of these vintages and time-lapse difference when there is a small “overlap” in the
				surveys—i.e., where the random samplings have partial statistical dependence. Specifically,
				we find that the quality of the vintages improves for decreasing overlap in the surveys while
				the converse is true for the time-lapse difference. Our setting differs from conventional
				time-lapse acquisition because we do not have access to dense samplings. Surveys with
				partial overlapping randomized samplings lead to the best trade-off between the recovery
				quality of the vintages and the time-lapse signal. We confirm this by a series of experiments.},
  keywords = {acquistion, time-lapse, marine, sampling, random, joint recovery method, private},
  note = {Submitted revision 1 to Geophysics on October 22, 2014},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2014/oghenekohwo2014GEOPfrt/oghenekohwo2014GEOPfrt.html}
}


@UNPUBLISHED{vanLeeuwen2014pmpde,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {A penalty method for {PDE}-constrained optimization ({CONFIDENTIAL})},
  year = {2014},
  month = {04},
  institution = {UBC},
  abstract = {The invention relates to a partial-differential-equation
                  (PDE) constrained optimization method and especially
                  to a partial-differential-equation (PDE) constrained
                  optimization method for geophysical prospecting.},
  keywords = {FWI, optimization, patent, private},
  note = {Patent filed on April 22, 2014. PCT International Application.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2014/vanLeeuwen2014pmpde/vanLeeuwen2014pmpde.pdf}
}


@UNPUBLISHED{tu2014fis,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Fast imaging with surface-related multiples by sparse inversion},
  year = {2014},
  month = {04},
  institution = {UBC},
  abstract = {In marine exploration seismology, surface-related
                  multiples are usually treated as noise mainly
                  because subsequent processing steps, such as
                  migration velocity analysis and imaging, require
                  multiple-free data. Failure to remove these
                  wavefield components from the data may lead to
                  erroneous estimates for migration velocity or result
                  in strong coherent artifacts that interfere with the
                  imaged reflectors. However, multiples can carry
                  complementary information compared to primaries, as
                  they interact with the free surface and are
                  therefore exposed more to the subsurface. Recent
                  work has shown that when processed correctly
                  multiples can improve seismic illumination. Given a
                  sufficiently accurate background velocity model and
                  an estimate for the source signature, we propose a
                  new and computationally efficient two-way
                  wave-equation based linearized inversion procedure
                  that produces accurate images of the subsurface from
                  the total upgoing wavefield including
                  surface-related multiples. Modelling of the
                  surface-related multiples in the proposed method
                  derives from the well-known surface-related multiple
                  elimination method. We incur a minimal overhead from
                  incorporating the multiples by having the
                  wave-equation solver carry out the multiple
                  predictions via the inclusion of an areal source
                  instead of expensive dense matrix-matrix
                  multiplications. By using subsampling techniques, we
                  obtain high-quality true-amplitude least-squares
                  migrated images at computational costs of roughly a
                  single reverse-time migration with all the
                  data. These images are virtually free of coherent
                  artifacts from multiples. Proper inversion of the
                  multiples would be computationally infeasible
                  without using these techniques that significantly
                  brings down the cost. By promoting sparsity in the
                  curvelet domain and using rerandomization, out
                  method gains improved robustness to errors in the
                  background velocity model, and errors incurred in
                  the linearization of the wave-equation with respect
                  to the model. We demonstrate the superior
                  performance of the proposed method compared to the
                  conventional reverse-time migration using realistic
                  synthetic examples.},
  keywords = {multiples, inversion, Kaczmarz, compressive sensing, curvelet, approximate message passing, private},
  note = {Submitted revision 1 to Geophysical Journal International on October 27, 2014},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2014/tu2014fis/tu2014fis.pdf}
}


@UNPUBLISHED{vanLeeuwen20143Dfds,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {{3D} frequency-domain seismic inversion with controlled sloppiness},
  year = {2014},
  month = {03},
  abstract = {Seismic waveform inversion aims at obtaining detailed
                  estimates of subsurface medium parameters, such as
                  the spatial distribution of soundspeed, from
                  multi-experiment seismic data. A formulation of this
                  inverse problem in the frequency-domain leads to an
                  optimization problem constrained by a Helmholtz
                  equation with many right-hand-sides. Application of
                  this technique to industry-scale problem faces
                  several challenges: Firstly, we need to solve the
                  Helmholtz equation for high wavenumbers over large
                  computational domains. Secondly, the data consists
                  of many independent experiments, leading to a large
                  number of PDE-solves. This results in high
                  computational complexity both in terms of memory and
                  CPU time as well as i/o costs. Finally, the inverse
                  problem is highly non-linear and a lot of art goes
                  into preprocessing and regularization. Ideally, an
                  inversion needs to be run several times with
                  different initial guesses and/or tuning
                  parameters. In this paper, we discuss the
                  requirements of the various components (PDE-solver,
                  optimization method, ...) when applied to
                  large-scale 3D seismic waveform inversion and
                  combine several existing approaches into a flexible
                  inversion scheme for seismic waveform inversion. The
                  scheme is based on the idea that in the early stages
                  of the inversion we do not need all the data or very
                  accurate PDE-solves. We base our method on an
                  existing preconditioned Krylov solver (CARP-CG) and
                  use ideas from stochastic optimization to formulate
                  a gradient-based (Quasi-Newton) optimization
                  algorithm that works with small subsets of the
                  right-hand-sides and uses inexact PDE solves for the
                  gradient calculations. We proposed novel heuristics
                  to adaptively control both the accuracy and the
                  number of right-hand-sides. We illustrate the
                  algorithms on synthetic benchmark models for which
                  significant computational gains can be made without
                  being sensitive to noise and without loosing
                  accuracy of the inverted model.},
  keywords = {waveform inversion, optimization},
  note = {to appear in the SIAM Journal on Scientific Computing (SISC)}, 
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Journals/SIAM_Journal_on_Scientific_Computing/2014/vanLeeuwen20143Dfds/vanLeeuwen20143Dfds.pdf}
}


%-----2013-----%

@UNPUBLISHED{ghadermarzy2013ncs,
  author = {Navid Ghadermarzy and Hassan Mansour and Ozgur Yilmaz},
  title = {Non-convex compressed sensing using partial support information},
  year = {2013},
  institution = {UBC},
  abstract = {In this paper we address the recovery conditions of
                  weighted $\ell_p$ minimization for signal reconstruction
                  from compressed sensing measurements when partial
                  support in- formation is available. We show that
                  weighted $\ell_p$ minimization with 0 < p < 1 is stable
                  and robust under weaker sufficient conditions
                  compared to weighted $\ell_1$ minimization. Moreover, the
                  sufficient recovery conditions of weighted $\ell_p$ are
                  weaker than those of regular $\ell_p$ minimization if at
                  least 50\% of the support estimate is accurate. We
                  also review some algorithms which exist to solve the
                  non-convex $\ell_p$ problem and illustrate our results
                  with numerical experiments.},
  keywords = {Compressed sensing, weighted $\ell_p$, nonconvex optimization, sparse reconstruction},
  month = {11},
  url = {http://arxiv.org/abs/1311.3773}
}

