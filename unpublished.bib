% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2020-----%

@UNPUBLISHED{louboutin2020SCsta,
  author = {Mathias Louboutin and Fabio Luporini and Philipp A. Witte and Rhodri
Nelson and George Bisbas and Jan Thorbecke and Felix J. Herrmann and Gerard
Gorman},
  title = {Scaling through abstractions – high-performance vectorial wave
simulations for seismic inversion with Devito},
  year = {2020},
  month = {4},
  abstract = {[Devito] is an open-source Python project based on domain-specific
language and compiler technology. Driven by the requirements of rapid HPC
applications development in exploration seismology, the language and compiler
have evolved significantly since inception. Sophisticated boundary
conditions, tensor contractions, sparse operations and features such as
staggered grids and sub-domains are all supported; operators of essentially
arbitrary complexity can be generated. To accommodate this flexibility whilst
ensuring performance, data dependency analysis is utilized to schedule loops
and detect computational-properties such as parallelism. In this article, the
generation and simulation of MPI-parallel propagators (along with their
adjoints) for the pseudo-acoustic wave-equation in tilted transverse
isotropic media and the elastic wave-equation are presented. Simulations are
carried out on industry scale synthetic models in a HPC Cloud system and
reach a performance of 28TFLOP/s, hence demonstrating Devito's suitability
for production-grade seismic inversion problems.},
  keywords = {HPC, Devito, finite-difference, large-scale, RTM, elastic, TTI},
  note = {Submitted to SC20},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/louboutin2020SCsta/louboutin2020SCsta.html}
}

@UNPUBLISHED{louboutin2020SEGtwri,
  author = {Mathias Louboutin and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Time-domain Wavefield Reconstruction Inversion in a TTI medium},
  year = {2020},
  month = {4},
  abstract = {We introduce a generalization of time-domain wavefield
reconstruction inversion to anisotropic acoustic modeling. Wavefield
reconstruction inversion has been extensively researched in recent years for
its ability to mitigate cycle skipping. The original method was formulated in
the frequency domain with acoustic isotropic physics. However,
frequency-domain modeling requires sophisticated iterative solvers that are
difficult to scale to industrial-size problems and more realistic physical
assumptions, such as tilted transverse isotropy, object of this study. The
work presented here is based on a recently proposed dual formulation of
wavefield reconstruction inversion, which allows time-domain propagator that
are suitable to both large scales and more accurate physics.},
  keywords = {FWI, WRI, anisotropy, TTI, SEG},
  note = {Submitted to SEG},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/louboutin2020SEGtwri/louboutin2020SEGtwri.html}
}

@UNPUBLISHED{rizzuti2020SEGuqavp,
  author = {Gabrio Rizzuti and Ali Siahkoohi and Philipp A. Witte and Felix J. Herrmann},
  title = {Parameterizing uncertainty by deep invertible networks, an application
to reservoir characterization},
  year = {2020},
  month = {4},
  abstract = {Uncertainty quantification for full-waveform inversion provides a
probabilistic characterization of the ill-conditioning of the problem,
comprising the sensitivity of the solution with respect to the starting model
and data noise. This analysis allows to assess the confidence in the
candidate solution and how it is reflected in the tasks that are typically
performed after imaging (e.g., stratigraphic segmentation following reservoir
characterization). Classically, uncertainty comes in the form of a
probability distribution formulated from Bayesian principles, from which we
seek to obtain samples. A popular solution involves Monte Carlo sampling.
Here, we propose instead an approach characterized by training a deep network
that "pushes forward" Gaussian random inputs into the model space
(representing, for example, density or velocity) as if they were sampled from
the actual posterior distribution. Such network is designed to solve a
variational optimization problem based on the Kullback-Leibler divergence
between the posterior and the network output distributions. This work is
fundamentally rooted in recent developments for invertible networks. Special
invertible architectures, besides being computational advantageous with
respect to traditional networks, do also enable analytic computation of the
output density function. Therefore, after training, these networks can be
readily used as a new prior for a related inversion problem. This stands in
stark contrast with Monte-Carlo methods, which only produce samples. We
validate these ideas with an application to angle-versus-ray parameter
analysis for reservoir characterization.},
  keywords = {Full-waveform inversion, Uncertainty quantification, SEG},
  note = {Submitted to SEG},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/rizzuti2020SEGuqavp/rizzuti2020SEGuqavp.html}
}

@UNPUBLISHED{siahkoohi2020SEGwdp,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Weak deep priors for seismic imaging},
  year = {2020},
  month = {4},
  abstract = {Incorporating prior knowledge on model unknowns of interest is
essential when dealing with ill-posed inverse problems due to the
nonuniqueness of the solution and data noise. Unfortunately, it is not
trivial to fully describe our priors in a convenient and analytical way.
Parameterizing the unknowns with a convolutional neural network (CNN), and
assuming an uninformative Gaussian prior on its weights, leads to a
variational prior on the output space that favors "natural" images and
excludes noisy artifacts, as long as overfitting is prevented. This is the
so-called deep-prior approach. In seismic imaging, however, evaluating the
forward operator is computationally expensive, and training a randomly
initialized CNN becomes infeasible. We propose, instead, a weak version of
deep priors, which consists of relaxing the requirement that reflectivity
models must lie in the network range, and letting the unknowns deviate from
the network output according to a Gaussian distribution. Finally, we jointly
solve for the reflectivity model and CNN weights. The chief advantage of this
approach is that the updates for the CNN weights do not involve the modeling
operator, and become relatively cheap. Our synthetic numerical experiments
demonstrate that the weak deep prior is more robust with respect to noise
than conventional least-squares imaging approaches, with roughly twice the
computational cost of reverse-time migration, which is the affordable
computational budget in large-scale imaging problems.},
  keywords = {machine learning, deep prior, seismic imaging, SEG},
  note = {Submitted to SEG},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/siahkoohi2020SEGwdp/siahkoohi2020SEGwdp.html}
}

@UNPUBLISHED{siahkoohi2020SEGuqi,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Uncertainty quantification in imaging and automatic horizon
tracking—a Bayesian deep-prior based approach},
  year = {2020},
  month = {3},
  abstract = {In inverse problems, uncertainty quantification (UQ) deals with a
probabilistic description of the solution nonuniqueness and data noise
sensitivity. Setting seismic imaging into a Bayesian framework allows for a
principled way of studying uncertainty by solving for the model posterior
distribution. Imaging, however, typically constitutes only the first stage of
a sequential workflow, and UQ becomes even more relevant when applied to
subsequent tasks that are highly sensitive to the inversion outcome. In this
paper, we focus on how UQ trickles down to horizon tracking for the
determination of stratigraphic models and investigate its sensitivity with
respect to the imaging result. As such, the main contribution of this work
consists in a data-guided approach to horizon tracking uncertainty analysis.
This work is fundamentally based on a special reparameterization of
reflectivity, known as "deep prior". Feasible models are restricted to the
output of a convolutional neural network with a fixed input, while weights
and biases are Gaussian random variables. Given a deep prior model, the
network parameters are sampled from the posterior distribution via a Markov
chain Monte Carlo method, from which the conditional mean and point-wise
standard deviation of the inferred reflectivities are approximated. For each
sample of the posterior distribution, a reflectivity is generated, and the
horizons are tracked automatically. In this way, uncertainty on model
parameters naturally translates to horizon tracking. As part of the
validation for the proposed approach, we verified that the estimated
confidence intervals for the horizon tracking coincide with geologically
complex regions, such as faults.},
  keywords = {machine learning, uncertainty quantification, imaging, horizon
picking, SEG},
  note = {Submitted to SEG},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/siahkoohi2020SEGuqi/siahkoohi2020SEGuqi.html}
}

@UNPUBLISHED{yin2020SEGesi,
  author = {Ziyi Yin and Rafael Orozco and Philipp A. Witte and Mathias Louboutin and Gabrio
Rizzuti and Felix J. Herrmann},
  title = {Extended source imaging –- a unifying framework for seismic &
medical imaging},
  year = {2020},
  month = {4},
  abstract = {We present three imaging modalities that live on the crossroads of
seismic and medical imaging. Through the lens of extended source imaging, we
can draw deep connections among the fields of wave-equation based seismic and
medical imaging, despite first appearances. From the seismic perspective, we
underline the importance to work with the correct physics and spatially
varying velocity fields. Medical imaging, on the other hand, opens the
possibility for new imaging modalities where outside stimuli, such as laser
or radar pulses, can not only be used to identify endogenous optical or
thermal contrasts but that these sources can also be used to insonify the
medium so that images of the whole specimen can in principle be created.},
  keywords = {seismic imaging, medical imaging, variable projection, SEG},
  note = {Submitted to SEG},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/yin2020SEGesi/yin2020SEGesi.html}
}

@UNPUBLISHED{zhang2020SEGtli,
  author = {Mi Zhang and Ali Siahkoohi and Felix J. Herrmann},
  title = {Transfer learning in large-scale ocean bottom seismic wavefield
reconstruction},
  year = {2020},
  month = {4},
  abstract = {Achieving desirable receiver sampling in ocean bottom acquisition
is often not possible because of cost considerations. Assuming adequate
source sampling is available, which is achievable by virtue of reciprocity
and the use of modern randomized (simultaneous-source) marine acquisition
technology, we are in a position to train  convolutional neural networks
(CNNs) to bring the receiver sampling to the same spatial grid as the dense
source sampling. To accomplish this task, we form training pairs consisting
of densely sampled data and artificially subsampled data using a reciprocity
argument and the assumption that the source-site sampling is dense. While
this approach has successfully been used on the recovery monochromatic
frequency slices, its application in practice calls for wavefield
reconstruction of time-domain data. Despite having the option to parallelize,
the overall costs of this approach can become prohibitive if we decide to
carry out the training and recovery independently for each frequency. Because
different frequency slices share information, we propose the use the method
of transfer training to make our approach computationally more efficient by
warm starting the training with CNN weights obtained from a neighboring
frequency slices. If the two neighboring frequency slices share information,
we would expect the training to improve and converge faster. Our aim is to
prove this principle by carrying a series of carefully selected experiments
on a relatively large-scale five-dimensional data synthetic data volume
associated with wide-azimuth 3D ocean bottom node acquisition. From these
experiments, we observe that by transfer training we are able t significantly
speedup in the training, specially at relatively higher frequencies where
consecutive frequency slices are more correlated.},
  keywords = {transfer learning, wavefield reconstruction, SEG},
  note = {Submitted to SEG},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/zhang2020SEGtli/zhang2020SEGtli.html}
}

@UNPUBLISHED{zhang2020SEGwrw,
  author = {Yijun Zhang and Shashin Sharan and Oscar Lopez and Felix J. Herrmann},
  title = {Wavefield recovery with limited-subspace weighted matrix
factorizations},
  year = {2020},
  month = {4},
  abstract = {Modern-day seismic imaging and monitoring technology increasingly
rely on dense full-azimuth sampling. Unfortunately, the costs of acquiring
densely sampled data rapidly become prohibitive and we need to look for ways
to sparsely collect data, e.g. from sparsely distributed ocean bottom nodes,
from which we then derive densely sampled surveys through the method of
wavefield reconstruction. Because of their relatively cheap and simple
calculations, wavefield reconstruction via matrix factorizations has proven
to be a viable and scalable alternative to the more generally used
transform-based methods. While this method is capable of processing all full
azimuth data frequency by frequency slice, its performance degrades at higher
frequencies because monochromatic data at these frequencies is not as well
approximated by low-rank factorizations. We address this problem by proposing
a recursive recovery technique, which involves weighted matrix factorizations
where recovered wavefields at the lower frequencies serve as prior
information for the recovery of the higher frequencies. To limit the adverse
effects of potential overfitting, we propose a limited-subspace recursively
weighted matrix factorization approach where the size of the row and column
subspaces to construct the weight matrices is constrained. We apply our
method to data collected from the Gulf of Suez, and our results show that our
limited-subspace weighted recovery method significantly improves the recovery
quality.},
  keywords = {algorithm, data reconstruction, frequency-domain, interpolation,
Processing, SEG},
  note = {Submitted to SEG},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/zhang2020SEGwrw/zhang2020SEGwrw.html}
}

@UNPUBLISHED{yang2020lrpo,
  author = {Mengmeng Yang and Marie Graff and Rajiv Kumar and Felix J. Herrmann},
  title = {Low-rank representation of omnidirectional subsurface extended image
volumes},
  year = {2020},
  month = {3},
  abstract = {Subsurface-offset gathers play an increasingly important role in seismic
imaging. These gathers are used during velocity model building and inversion
of rock properties from amplitude variations. While powerful, these gathers
come with high computational and storage demands to form and manipulate these
high dimensional objects. This explains why only limited numbers of image
gathers are computed over a limited offset range. We avoid these high costs
by working with highly compressed low-rank factorizations. We arrive at these
factorizations via a combination of probings with the double two-way wave
equation and randomized singular value decompositions. In turn, the resulting
factorizations give us access to all subsurface offsets without having to
form the full extended image volumes. The latter is computationally
prohibitive because extended image volumes are quadratic in image size. As a
result, we can easily handle situations where conventional horizontal offset
gathers are no longer focused. More importantly, the factorization also
provides a mechanism to use the invariance relation of extended image volumes
for velocity continuation. With this technique, extended image volumes for
one background velocity model can directly be mapped to those of another
background velocity model. Our low-rank factorization inherits this
invariance property so we incur factorization costs only once when examining
different imaging scenarios. Because all imaging experiments only involve the
factors, they are computationally cheap with costs that scale with the rank
of the factorization. We validate our methodology on 2D synthetics including
a challenging imaging example with salt. Our experiments show that our
low-rank factorization parameterizes extended image volumes naturally.
Instead of brute force explicit cross-correlations between shifted source and receiver
wavefields, our approach relies on the underlying linear-algebra structure
that enables us to work with these objects without incurring unfeasible
demands on computation and storage.},
  keywords = {extended image volumes, low rank, randomized linear algebra, power
schemes, invariance relationship},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/yang2020lrpo/yang2020lrpo.html}
}

@UNPUBLISHED{yang2020tdsp,
  author = {Mengmeng Yang and Zhilong Fang and Philipp A. Witte and Felix J. Herrmann},
  title = {Time-domain sparsity promoting least-squares reverse time migration
with source estimation},
  year = {2020},
  month = {2},
  abstract = {Least-squares reverse time migration is well-known for its capability to generate artifact-free true-amplitude subsurface images through fitting observed data in the least-squares sense. However, when applied to realistic imaging problems, this approach is faced with issues related to overfitting and excessive computational costs induced by many wave-equation solves. The fact that the source function is unknown complicates this situation even further. Motivated by recent results in stochastic optimization and transform-domain sparsity-promotion, we demonstrate that the computational costs of inversion can be reduced significantly while avoiding imaging artifacts and restoring amplitudes. While powerfull, these new approaches do require accurate information on the source-time function, which is often lacking. Without this information, the imaging quality deteriorates rapidly. We address this issue by presenting an approach where the source-time function is estimated on the fly through a technique known as variable projection. Aside from introducing negligible computational overhead, the proposed method is shown to perform well on imaging problems with noisy data and problems that involve complex settings such as salt. In either case, the presented method produces high resolution high-amplitude fidelity images including an estimates for the source-time function. In addition, due to its use of stochastic optimization, we arrive at these images at roughly one to two times the cost of conventional reverse time migration involving all data.},
  keywords = {sparsity inversion, source estimation, penalty},
  note = {Submitted to Geophysical Prospecting},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/yang2020tdsp/yang2020tdsp.html}
}


%-----2019-----%

@UNPUBLISHED{daskalakis2019SIAMJISasr,
  author = {Emmanouil Daskalakis and Felix J. Herrmann and Rachel Kuske},
  title = {ACCELERATING SPARSE RECOVERY BY REDUCING CHATTER},
  year = {2019},
  abstract = {Compressive Sensing has driven a resurgence of sparse recovery algorithms with
  l_1-norm minimization. While these minimizations are relatively well understood for small underdetermined, possibly inconsistent systems, their behavior for large over-determined and inconsistent systems has received much less attention. Specifically, we focus on large systems where computational restrictions call for algorithms that use randomized subsets of rows that are touched a limited number of times. In that regime, l_1-norm minimization algorithms exhibit unwanted fluctuations near the desired solution, and the Linear Bregman iterations are no exception. We explain this observed lack of performance in terms of chatter, a well-known phenomena observed in non-smooth dynamical systems, where intermediate solutions wander between different states stifling convergence. By identifying chatter as the culprit, we modify the Bregman iterations with chatter reducing adaptive element-wise step lengths in combination with potential support detection via threshold crossing. We demonstrate the performance of our algorithm on carefully selected stylized examples and a realistic seismic imaging problem involving millions of unknowns and matrix-free matrix-vector products that involve expensive wave-equation solves.},
  keywords = {sparsity promotion, inconsistent linear systems, Kacmarz, linearized Bregman
dynamical systems, non-smooth dynamics, chatter},
  note = {Submitted to SIAM Journal on Imaging Sciences},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2019/daskalakis2019SIAMJISasr/daskalakis2019SIAMJISasr.pdf}
}