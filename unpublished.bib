% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2017-----%

@UNPUBLISHED{alfaraj2017EAGEswr,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Shear wave reconstruction from low cost randomized acquisition},
  year = {2017},
  abstract = {Shear waves travel in the subsurface at a lower speed
                  compared with compressional waves. Therefore, much
                  finer spatial sampling is required to properly
                  record the shear waves. This leads to higher
                  acquisition costs which are typically avoided by
                  designing surveys geared towards only compressional
                  waves imaging. We propose using randomly
                  under-sampled ocean bottom acquisition for recording
                  both compressional and shear waves. The recorded
                  multicomponent data is then interpolated using an
                  SVD-free low rank interpolation scheme that is
                  feasible for large scale seismic data volumes to
                  obtain finely sampled data. Following that, we
                  perform elastic wavefield decomposition at the ocean
                  bottom to recover accurate up- and dow-going
                  S-waves. Synthetic data results indicate that using
                  randomized under-sampled acquisition, we can recover
                  accurate S-waves with an economical cost compared
                  with conventional acquisition designs.},
  date_submitted = {01/15/2017},
  keywords = {EAGE, shear waves, rank minimization, interpolation, randomized acquisition, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/alfaraj2017EAGEswr/alfaraj2017EAGEswr.pdf}
}


@UNPUBLISHED{dasilva2017uls,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {A unified {2D/3D} large scale software environment for nonlinear inverse problems},
  year = {2017},
  abstract = {Large scale parameter estimation problems are some of
                  the most computationally demanding problems. An
                  academic researcher's domain-specific knowledge
                  often precludes that of software design, which
                  results in software frameworks for inversion that
                  are technically correct, but not scalable to
                  realistically-sized problems. On the other hand, the
                  computational demands of the problem for realistic
                  problems result in industrial codebases that are
                  geared solely for performance, rather than
                  comprehensibility or flexibility. We propose a new
                  software design that bridges the gap between these
                  two seemingly disparate worlds. A hierarchical and
                  modular design allows a user to delve into as much
                  detail as she desires, while using high performance
                  primitives at the lower levels. Our code has the
                  added benefit of actually reflecting the underlying
                  mathematics of the problem, which lowers the
                  cognitive load on user using it and reduces the
                  initial startup period before a researcher can be
                  fully productive. We also introduce a new
                  preconditioner for the Helmholtz equation that is
                  suitable for fault-tolerant distributed
                  systems. Numerical experiments on a variety of 2D
                  and 3D test problems demonstrate the effectiveness
                  of this approach on scaling algorithms from small to
                  large scale problems with minimal code changes.},
  keywords = {optimization, PDE-constrained inversion, large scale, matlab, private},
  note = {Submitted to ACM Transactions on Mathematical Software on February 14, 2017.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2017/dasilva2017uls/dasilva2017uls.html}
}


@UNPUBLISHED{fang2017WAVESuqf,
  author = {Zhilong Fang and Curt Da Silva and Rachel Kuske and Felix J. Herrmann},
  title = {Uncertainty quantification for inverse problems with a weak wave-equation constraint},
  year = {2017},
  abstract = {In this work, we present a new posterior distribution to
                  quantify uncertainties in solutions of wave-equation
                  based inverse problems. By introducing an auxiliary
                  variable for the wavefields, we weaken the strict
                  wave-equation constraint used by conventional
                  Bayesian approaches. With this weak constraint, the
                  new posterior distribution is a bi-Gaussian
                  distribution with respect to both model parameters
                  and wavefields, which can be directly sampled by the
                  Gibbs sampling method.},
  keywords = {uncertainty, wave equation, constraint, Gibbs sampling, private},
  note = {Submitted to the WAVES 2017 Conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/WAVES/2017/fang2017WAVESuqf/fang2017WAVESuqf.html}
}


@UNPUBLISHED{kumar2017EAGEdha,
  author = {Rajiv Kumar and Nick Moldoveanu and Felix J. Herrmann},
  title = {Denoising high-amplitude cross-flow noise using curvelet-based stable principle component pursuit},
  year = {2017},
  abstract = {Removal of high-amplitude cross-flow noise in marine
                  towed-streamer acquisition is of great interest
                  because cross-flow noise hinders the success of
                  subsequent processing (e.g. EPSI) and
                  migration. However, the removal of cross-flow noise
                  is a challenging process because cross-flow noise
                  dominates steep angles and low-frequency components
                  of the signal. As a result, applying a simple
                  high-pass filter can result in a loss of coherent
                  diving waves and reflected energy. We propose a
                  stable curvelet-based principle-component pursuit
                  approach that does not suffer from this shortcoming
                  because it uses angle- and scale-adaptivity of the
                  curvelet transform in combination with the low-rank
                  property of cross-flow noise. As long as the
                  cross-flow noise exhibits low-rank in the curvelet
                  domain, our method successfully separates this
                  signal component from the diving waves and seismic
                  reflectivity, which is well-know to be sparse in the
                  curvelet domain. Experimental results on a
                  common-shot gather extracted from a coil shooting
                  survey in the Gulf of Mexico shows the potential of
                  our approach.},
  date_submitted = {01/15/2017},
  keywords = {EAGE, denoising, coil data, cross-flow noise, curvelet, SPCP, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/kumar2017EAGEdha/kumar2017EAGEdha.html}
}


@UNPUBLISHED{louboutin2017EAGEdns,
  author = {Mathias Louboutin and Llu\'{i}s Guasch and Felix J. Herrmann},
  title = {Data normalization strategies for seismic inversion},
  year = {2017},
  abstract = {Amplitude mismatch is an inherent problem in seismic
                  inversion. Most of the source estimation techniques
                  are associated with amplitude uncertainty due to
                  incomplete representation of the physics or
                  estimation method parameters. Rewriting the
                  inversion problem in an amplitude free formulation
                  allows to mitigate the amplitude ambiguity and help
                  the inversion process to converge. We present in
                  this work two different strategies to lessen
                  amplitude effects in seismic inversion, derive the
                  corresponding update directions and show how we
                  handle scaling error correctly in both the objective
                  function and the gradient.},
  date_submitted = {01/15/2017},
  keywords = {EAGE, FWI, normalization, inversion, nonlinear, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/louboutin2017EAGEdns/louboutin2017EAGEdns.html}
}


@UNPUBLISHED{louboutin2017EAGEess,
  author = {Mathias Louboutin and Felix J. Herrmann},
  title = {Extending the search space of time-domain adjoint-state {FWI} with randomized implicit time shifts},
  year = {2017},
  abstract = {Adjoint-state full-waveform inversion aims to obtain
                  subsurface properties such as velocity, density or
                  anisotropy parameters, from surface recorded
                  data. As with any (non-stochastic) gradient based
                  optimization procedure, the solution of this
                  inversion procedure is to a large extend determined
                  by the quality of the starting model. If this
                  starting model is too far from the true model, these
                  derivative-based optimizations will likely end up in
                  local minima and erroneous inversion results. In
                  certain cases, extension of the search space,
                  e.g. by making the wavefields or focused matched
                  sources additional unknowns, has removed some of
                  these non-uniqueness issues but these rely on
                  time-harmonic formulations. Here, we follow a
                  different approach by combining an implicit
                  extension of the velocity model, time compression
                  techniques and recent results on stochastic sampling
                  in non-smooth/non-convex optimization},
  date_submitted = {01/15/2017},
  keywords = {EAGE, time domain, FWI, cycle skipping, inversion, nonconvex, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/louboutin2017EAGEess/louboutin2017EAGEess.html}
}


@UNPUBLISHED{oghenekohwo2017EAGEitl,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Improved time-lapse data repeatability with randomized sampling and distributed compressive sensing},
  year = {2017},
  abstract = {Recently, new ideas on randomized sampling for
                  time-lapse seismic acquisition have been proposed to
                  address some of the challenges of replicating
                  time-lapse surveys. These ideas, which stem from
                  distributed compressed sensing (DCS) led to the
                  birth of a joint recovery model (JRM) for processing
                  time-lapse data (noise-free) acquired from
                  non-replicated acquisition geometries. However, when
                  the earth does not change—i.e. no time-lapse—the
                  recovered vintages from two non-replicated surveys
                  should show high repeatability measured in terms of
                  normalized RMS, which is a standard metric for
                  quantifying time-lapse data repeatability. Under
                  this assumption of no time-lapse change, we
                  demonstrate improved repeatability (with JRM) of the
                  recovered data from non-replicated random samplings,
                  first with noisy data and secondly in situations
                  where there are calibration errors i.e. where the
                  acquisition parameters such as source/receiver
                  coordinates are not precise.},
  date_submitted = {01/15/2017},
  keywords = {EAGE, repeatability, time lapse, compressive sensing, calibration, noise, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/oghenekohwo2017EAGEitl/oghenekohwo2017EAGEitl.html}
}


@UNPUBLISHED{witte2017EAGEspl,
  author = {Philipp A. Witte and Mengmeng Yang and Felix J. Herrmann},
  title = {Sparsity-promoting least-squares migration with the linearized inverse scattering imaging condition},
  year = {2017},
  abstract = {Reverse-time migration (RTM) with the conventional
                  cross-correlation imaging condition suffers from
                  low-frequency artifacts that result from
                  backscattered energy in the background velocity
                  models. This problem translates to least-squares
                  reverse-time migration (LS-RTM), where these
                  artifacts slow down the convergence, as many of the
                  initial iterations are spent on removing them. In
                  RTM, this problem has been successfully addressed by
                  the introduction of the so-called inverse scattering
                  imaging condition, which naturally removes these
                  artifacts. In this work, we derive the corresponding
                  linearized forward operator of the inverse
                  scattering imaging operator and incorporate this
                  forward/adjoint operator pair into a
                  sparsity-promoting (SPLS-RTM) workflow. We
                  demonstrate on a challenging salt model, that LS-RTM
                  with the inverse scattering imaging condition is far
                  less prone to low-frequency artifacts than the
                  conventional cross-correlation imaging condition,
                  improves the convergence and does not require any
                  type of additional image filters within the
                  inversion. Through source subsampling and sparsity
                  promotion, we reduce the computational cost in terms
                  of PDE solves to a number comparable to conventional
                  RTM, making our workflow applicable to large-scale
                  problems.},
  date_submitted = {01/15/2017},
  keywords = {EAGE, least-squares migration, imaging condition, linearized Bregman, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/witte2017EAGEspl/witte2017EAGEspl.html}
}


%-----2016-----%

@UNPUBLISHED{deaguiar2016SCdff,
  author = {Marcos de Aguiar and Gerard Gorman and Felix J. Herrmann and Navjot Kukreja and Michael Lange and Mathias Louboutin and Felippe Vieira Zacarias},
  title = {DeVito: fast finite difference computation},
  year = {2016},
  keywords = {finite differences, high performance computing, full-waveform inversion, private},
  note = {to be presented at the Super Computing Conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SC/2016/deaguiar2016SCdff/deaguiar2016SCdff_poster.pdf}
}


@UNPUBLISHED{esser2016tvr,
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Total-variation regularization strategies in full-waveform inversion},
  year = {2016},
  abstract = {We propose an extended full-waveform inversion
                  formulation that includes general convex constraints
                  on the model. Though the full problem is highly
                  nonconvex, the overarching optimization scheme
                  arrives at geologically plausible results by solving
                  a sequence of relaxed and warm-started constrained
                  convex subproblems. The combination of box,
                  total-variation, and successively relaxed asymmetric
                  total-variation constraints allows us to steer free
                  from parasitic local minima while keeping the
                  estimated physical parameters laterally continuous
                  and in a physically realistic range. For accurate
                  starting models, numerical experiments carried out
                  on the challenging 2004 BP velocity benchmark
                  demonstrate that bound and total-variation
                  constraints improve the inversion result
                  significantly by removing inversion artifacts,
                  related to source encoding, and by clearly improved
                  delineation of top, bottom, and flanks of a
                  high-velocity high-contrast salt inclusion. The
                  experiments also show that for poor starting models
                  these two constraints by themselves are insufficient
                  to detect the bottom of high-velocity inclusions
                  such as salt. Inclusion of the one-sided asymmetric
                  total-variation constraint overcomes this issue by
                  discouraging velocity lows to buildup during the
                  early stages of the inversion. To the author's
                  knowledge the presented algorithm is the first to
                  successfully remove the imprint of local minima
                  caused by poor starting models and band-width
                  limited finite aperture data.},
  keywords = {full-waveform inversion, constrained optimization, total variation, salt, hinge loss, private},
  note = {Submitted on July 13, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/esser2016tvr/esser2016tvr.pdf}
}


@UNPUBLISHED{kukreja2016WOLFHPCdaf,
  author = {Navjot Kukreja and Mathias Louboutin and Felippe Vieira Zacarias and Fabio Luporini and Michael Lange and Gerard Gorman},
  title = {Devito: automated fast finite difference computation},
  year = {2016},
  abstract = {Domain specific languages have successfully been used in
                  a variety of fields to cleanly express scientific
                  problems as well as to simplify implementation and
                  performance optimization on different computer
                  architectures. Although a large number of stencil
                  languages are available, finite difference domain
                  specific languages have proved challenging to design
                  because most practical use cases require additional
                  features that fall outside the finite difference
                  abstraction. Inspired by the complexity of
                  real-world seismic imaging problems, we introduce
                  Devito, a domain specific language in which high
                  level equations are expressed using symbolic
                  expressions from the SymPy package. Complex
                  equations are automatically manipulated, optimized,
                  and translated into highly optimized C code that
                  aims to perform comparably or better than hand-tuned
                  code. All this is transparent to users, who only see
                  concise symbolic mathematical expressions.},
  keywords = {finite differences, high performance computing, modelling, acoustic, compiler, stencil, private},
  note = {to be presented at the WOLFHPC 2016 Workshop (in conjunction with Super Computing Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SC/2016/WOLFHPC/kukreja2016WOLFHPCdaf/kukreja2016WOLFHPCdaf.pdf}
}


@UNPUBLISHED{kumar2016bls,
  author = {Rajiv Kumar and Oscar Lopez and Damek Davis and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Beating level-set methods for {3D} seismic data interpolation: a primal-dual alternating approach},
  year = {2016},
  abstract = {Acquisition cost is a crucial bottleneck for seismic
                  workflows, and low-rank formulations for data
                  interpolation allow practitioners to 'fill in' data
                  volumes from critically subsampled data acquired in
                  the field. Tremendous size of seismic data volumes
                  required for seismic processing remains a major
                  challenge for these techniques. We propose a new
                  approach to solve residual constrained formulations
                  for interpolation. We represent the data volume
                  using matrix factors, and build a block-coordinate
                  algorithm with constrained convex subproblems that
                  are solved with a primal-dual splitting scheme. The
                  new approach is competitive with state of the art
                  level-set algorithms that interchange the role of
                  objectives with constraints. We use the new
                  algorithm to successfully interpolate a large scale
                  5D seismic data volume, generated from the
                  geologically complex synthetic 3D Compass velocity
                  model, where 80\% of the data has been removed.},
  keywords = {matrix completion, nuclear-norm relaxation, seismic data, interpolation, alternating minimization, primal-dual splitting, private},
  note = {Submitted on July 18, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/kumar2016bls/kumar2016bls.pdf}
}


@UNPUBLISHED{louboutin2016ppf,
  author = {Mathias Louboutin and Michael Lange and Felix J. Herrmann and Navjot Kukreja and Gerard Gorman},
  title = {Performance prediction of finite-difference solvers for different computer architectures},
  year = {2016},
  abstract = {The life-cycle of a partial differential equation (PDE)
                  solver is often characterized by three development
                  phases: the development of a stable numerical
                  discretization; development of a correct (verified)
                  implementation; and the optimization of the
                  implementation for different computer
                  architectures. Often it is only after significant
                  time and effort has been invested that the
                  performance bottlenecks of a PDE solver are fully
                  understood, and the precise details varies between
                  different computer architectures. One way to
                  mitigate this issue is to establish a reliable
                  performance model that allows a numerical analyst to
                  make reliable predictions of how well a numerical
                  method would perform on a given computer
                  architecture, before embarking upon potentially long
                  and expensive implementation and optimization
                  phases. The availability of a reliable performance
                  model also saves developer effort as it both informs
                  the developer on what kind of optimisations are
                  beneficial, and when the maximum expected
                  performance has been reached and optimisation work
                  should stop. We show how discretization of a wave
                  equation can be theoretically studied to understand
                  the performance limitations of the method on modern
                  computer architectures. We focus on the roofline
                  model, now broadly used in the high-performance
                  computing community, which considers the achievable
                  performance in terms of the peak memory bandwidth
                  and peak floating point performance of a computer
                  with respect to algorithmic choices. A first
                  principles analysis of operational intensity for key
                  time-stepping finite-difference algorithms is
                  presented. With this information available at the
                  time of algorithm design, the expected performance
                  on target computer systems can be used as a driver
                  for algorithm design.},
  keywords = {finite differences, performance, modeling, HPC, private},
  note = {Submitted to the Computers \& Geosciences Journal on September 16, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/louboutin2016ppf/louboutin2016ppf.pdf}
}


@UNPUBLISHED{oghenekohwo2016GEOPctl,
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Low-cost time-lapse seismic with distributed {Compressive} {Sensing}---exploiting common information amongst the vintages},
  year = {2016},
  abstract = {Time-lapse seismic is a powerful technology for
                  monitoring a variety of subsurface changes due to
                  reservoir fluid flow. However, the practice can be
                  technically challenging when one seeks to acquire
                  colocated time-lapse surveys with high degrees of
                  replicability amongst the shot locations. We
                  demonstrate that under "ideal" circumstances, where
                  we ignore errors related to taking measurements off
                  the grid, high-quality prestack data can be obtained
                  from randomized subsampled measurements that are
                  observed from surveys where we choose not to revisit
                  the same randomly subsampled on-the-grid shot
                  locations. Our acquisition is low cost since our
                  measurements are subsampled. We find that the
                  recovered finely sampled prestack baseline and
                  monitor data actually improve significantly when the
                  same on-the-grid shot locations are not
                  revisited. We achieve this result by using the fact
                  that different time-lapse data share information and
                  that nonreplicated (on-the-grid) acquisitions can
                  add information when prestack data are recovered
                  jointly. Whenever the time-lapse data exhibit joint
                  structure---i.e., are compressible in some transform
                  domain and share information---sparsity-promoting
                  recovery of the "common component" and
                  "innovations", with respect to this common
                  component, outperforms independent recovery of both
                  the prestack baseline and monitor data. The
                  recovered time-lapse data are of high enough quality
                  to serve as input to extract poststack attributes
                  used to compute time-lapse differences. Without
                  joint recovery, artifacts---due to the randomized
                  subsampling---lead to deterioration of the degree of
                  repeatability of the time-lapse data. We support our
                  claims by carrying out experiments that collect
                  reliable statistics from thousands of repeated
                  experiments. We also confirm that high degrees of
                  repeatability are achievable for an ocean-bottom
                  cable survey acquired with time-jittered continuous
                  recording. This is part 1 of a two-paper series on
                  time-lapse seismic with compressed sensing. Part 2:
                  "Cheap time-lapse with distributed Compressive
                  Sensing---impact on repeatability".},
  keywords = {acquisition, time-lapse seismic, marine, random sampling, joint recovery method, private},
  note = {Accepted to be published in Geophysics.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/oghenekohwo2016GEOPctl/oghenekohwo2016GEOPctl.html}
}


@UNPUBLISHED{wason2016GEOPctl,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Low-cost time-lapse seismic with distributed Compressive Sensing---impact on repeatability},
  year = {2016},
  abstract = {Irregular or off-the-grid spatial sampling of sources
                  and receivers is inevitable in field seismic
                  acquisitions. Consequently, time-lapse surveys
                  become particularly expensive since current
                  practices aim to replicate densely sampled surveys
                  for monitoring changes occurring in the reservoir
                  due to hydrocarbon production. We demonstrate that
                  under certain circumstances, high-quality prestack
                  data can be obtained from cheap randomized
                  subsampled measurements that are observed from
                  nonreplicated surveys. We extend our time-jittered
                  marine acquisition to time-lapse surveys by
                  designing acquisition on irregular spatial grids
                  that render simultaneous, subsampled and irregular
                  measurements. Using the fact that different
                  time-lapse data share information and that
                  nonreplicated surveys add information when prestack
                  data are recovered jointly, we recover periodic
                  densely sampled and colocated prestack data by
                  adapting the recovery method to incorporate a
                  regularization operator that maps traces from an
                  irregular spatial grid to a regular periodic
                  grid. The recovery method is, therefore, a combined
                  operation of regularization, interpolation
                  (estimating missing fine-grid traces from subsampled
                  coarse-grid data), and source separation (unraveling
                  overlapping shot records). By relaxing the
                  insistence on replicability between surveys, we find
                  that recovery of the time-lapse difference shows
                  little variability for realistic field scenarios of
                  slightly nonreplicated surveys that suffer from
                  unavoidable natural deviations in spatial sampling
                  of shots (or receivers) and pragmatic
                  compressed-sensing based nonreplicated surveys when
                  compared to the "ideal" scenario of exact
                  replicability between surveys. Moreover, the
                  recovered densely sampled prestack baseline and
                  monitor data improve significantly when the
                  acquisitions are not replicated, and hence can serve
                  as input to extract poststack attributes used to
                  compute time-lapse differences. Our observations are
                  based on experiments conducted for an ocean-bottom
                  cable survey acquired with time-jittered continuous
                  recording assuming source equalization (or same
                  source signature) for the time-lapse surveys and no
                  changes in wave heights, water column velocities or
                  temperature and salinity profiles, etc.},
  keywords = {marine acquisition, time-lapse seismic, off-the-grid recovery, random sampling, joint recovery method, optimization, private},
  note = {Accepted to be published in Geophysics.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/wason2016GEOPctl/wason2016GEOPctl.html}
}
