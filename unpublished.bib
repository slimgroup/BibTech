% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2020-----%

@UNPUBLISHED{siahkoohi2020SEGuqi,
  author = {Ali Siahkoohi and Gabrio Rizzuti and Felix J. Herrmann},
  title = {Uncertainty quantification in imaging and automatic horizon
trackingâ€”a Bayesian deep-prior based approach},
  year = {2020},
  month = {3},
  abstract = {In inverse problems, uncertainty quantification (UQ) deals with a
probabilistic description of the solution nonuniqueness and data noise
sensitivity. Setting seismic imaging into a Bayesian framework allows for a
principled way of studying uncertainty by solving for the model posterior
distribution. Imaging, however, typically constitutes only the first stage of
a sequential workflow, and UQ becomes even more relevant when applied to
subsequent tasks that are highly sensitive to the inversion outcome. In this
paper, we focus on how UQ trickles down to horizon tracking for the
determination of stratigraphic models and investigate its sensitivity with
respect to the imaging result. As such, the main contribution of this work
consists in a data-guided approach to horizon tracking uncertainty analysis.
This work is fundamentally based on a special reparameterization of
reflectivity, known as "deep prior". Feasible models are restricted to the
output of a convolutional neural network with a fixed input, while weights
and biases are Gaussian random variables. Given a deep prior model, the
network parameters are sampled from the posterior distribution via a Markov
chain Monte Carlo method, from which the conditional mean and point-wise
standard deviation of the inferred reflectivities are approximated. For each
sample of the posterior distribution, a reflectivity is generated, and the
horizons are tracked automatically. In this way, uncertainty on model
parameters naturally translates to horizon tracking. As part of the
validation for the proposed approach, we verified that the estimated
confidence intervals for the horizon tracking coincide with geologically
complex regions, such as faults.},
  keywords = {machine learning, uncertainty quantification, imaging, horizon
picking, SEG},
  note = {Submitted to SEG},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/siahkoohi2020SEGuqi/siahkoohi2020SEGuqi.html}
}

@UNPUBLISHED{yang2020lrpo,
  author = {Mengmeng Yang and Marie Graff and Rajiv Kumar and Felix J. Herrmann},
  title = {Low-rank representation of omnidirectional subsurface extended image
volumes},
  year = {2020},
  month = {3},
  abstract = {Subsurface-offset gathers play an increasingly important role in seismic
imaging. These gathers are used during velocity model building and inversion
of rock properties from amplitude variations. While powerful, these gathers
come with high computational and storage demands to form and manipulate these
high dimensional objects. This explains why only limited numbers of image
gathers are computed over a limited offset range. We avoid these high costs
by working with highly compressed low-rank factorizations. We arrive at these
factorizations via a combination of probings with the double two-way wave
equation and randomized singular value decompositions. In turn, the resulting
factorizations give us access to all subsurface offsets without having to
form the full extended image volumes. The latter is computationally
prohibitive because extended image volumes are quadratic in image size. As a
result, we can easily handle situations where conventional horizontal offset
gathers are no longer focused. More importantly, the factorization also
provides a mechanism to use the invariance relation of extended image volumes
for velocity continuation. With this technique, extended image volumes for
one background velocity model can directly be mapped to those of another
background velocity model. Our low-rank factorization inherits this
invariance property so we incur factorization costs only once when examining
different imaging scenarios. Because all imaging experiments only involve the
factors, they are computationally cheap with costs that scale with the rank
of the factorization. We validate our methodology on 2D synthetics including
a challenging imaging example with salt. Our experiments show that our
low-rank factorization parameterizes extended image volumes naturally.
Instead of brute force explicit cross-correlations between shifted source and receiver
wavefields, our approach relies on the underlying linear-algebra structure
that enables us to work with these objects without incurring unfeasible
demands on computation and storage.},
  keywords = {extended image volumes, low rank, randomized linear algebra, power
schemes, invariance relationship},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/yang2020lrpo/yang2020lrpo.html}
}

@UNPUBLISHED{yang2020tdsp,
  author = {Mengmeng Yang and Zhilong Fang and Philipp A. Witte and Felix J. Herrmann},
  title = {Time-domain sparsity promoting least-squares reverse time migration
with source estimation},
  year = {2020},
  month = {2},
  abstract = {Least-squares reverse time migration is well-known for its capability to generate artifact-free true-amplitude subsurface images through fitting observed data in the least-squares sense. However, when applied to realistic imaging problems, this approach is faced with issues related to overfitting and excessive computational costs induced by many wave-equation solves. The fact that the source function is unknown complicates this situation even further. Motivated by recent results in stochastic optimization and transform-domain sparsity-promotion, we demonstrate that the computational costs of inversion can be reduced significantly while avoiding imaging artifacts and restoring amplitudes. While powerfull, these new approaches do require accurate information on the source-time function, which is often lacking. Without this information, the imaging quality deteriorates rapidly. We address this issue by presenting an approach where the source-time function is estimated on the fly through a technique known as variable projection. Aside from introducing negligible computational overhead, the proposed method is shown to perform well on imaging problems with noisy data and problems that involve complex settings such as salt. In either case, the presented method produces high resolution high-amplitude fidelity images including an estimates for the source-time function. In addition, due to its use of stochastic optimization, we arrive at these images at roughly one to two times the cost of conventional reverse time migration involving all data.},
  keywords = {sparsity inversion, source estimation, penalty},
  note = {Submitted to Geophysical Prospecting},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/yang2020tdsp/yang2020tdsp.html}
}


%-----2019-----%

@UNPUBLISHED{daskalakis2019SIAMJISasr,
  author = {Emmanouil Daskalakis and Felix J. Herrmann and Rachel Kuske},
  title = {ACCELERATING SPARSE RECOVERY BY REDUCING CHATTER},
  year = {2019},
  abstract = {Compressive Sensing has driven a resurgence of sparse recovery algorithms with
  l_1-norm minimization. While these minimizations are relatively well understood for small underdetermined, possibly inconsistent systems, their behavior for large over-determined and inconsistent systems has received much less attention. Specifically, we focus on large systems where computational restrictions call for algorithms that use randomized subsets of rows that are touched a limited number of times. In that regime, l_1-norm minimization algorithms exhibit unwanted fluctuations near the desired solution, and the Linear Bregman iterations are no exception. We explain this observed lack of performance in terms of chatter, a well-known phenomena observed in non-smooth dynamical systems, where intermediate solutions wander between different states stifling convergence. By identifying chatter as the culprit, we modify the Bregman iterations with chatter reducing adaptive element-wise step lengths in combination with potential support detection via threshold crossing. We demonstrate the performance of our algorithm on carefully selected stylized examples and a realistic seismic imaging problem involving millions of unknowns and matrix-free matrix-vector products that involve expensive wave-equation solves.},
  keywords = {sparsity promotion, inconsistent linear systems, Kacmarz, linearized Bregman
dynamical systems, non-smooth dynamics, chatter},
  note = {Submitted to SIAM Journal on Imaging Sciences},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2019/daskalakis2019SIAMJISasr/daskalakis2019SIAMJISasr.pdf}
}

@UNPUBLISHED{witte2019RHPCssi,
  author = {Philipp A. Witte and Mathias Louboutin and Charles Jones and Felix J. Herrmann},
  title = {Serverless seismic imaging in the cloud},
  year = {2019},
  abstract = {This talk presents a serverless approach to seismic imaging in the
cloud based on high-throughput containerized batch processing, event-driven
computations and a  domain-specific language compiler for solving the
underlying wave equations. A 3D case study on Azure demonstrates that this
approach allows reducing the operating cost of up to a factor of 6, making
the cloud a viable alternative to on-premise HPC clusters for seismic
imaging.},
  keywords = {cloud, hpc, reverse-time migration, serverless},
  note = {Submitted to Rice Oil and Gas High Performance Computing Conference 2020 on November 27, 2019},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2019/witte2019RHPCssi/witte2019RHPCssi.html}
}

@UNPUBLISHED{witte2019TPDedas,
  author = {Philipp A. Witte and Mathias Louboutin and Henryk Modzelewski and Charles Jones and James Selvage and Felix J. Herrmann},
  title = {An Event-Driven Approach to Serverless Seismic Imaging in the Cloud},
  year = {2019},
  abstract = {Adapting the cloud for high-performance computing (HPC) is a
challenging task, as software for HPC applications hinges on fast network
connections and is sensitive to hardware failures. Using cloud infrastructure
to recreate conventional HPC clusters is therefore in many cases an
infeasible solution for migrating HPC applications to the cloud. As an
alternative to the generic lift and shift approach, we consider the specific
application of seismic imaging and demonstrate a serverless and event-driven
pproach for running large-scale instances of this problem in the cloud.
Instead of permanently running compute instances, our workflow is based on a
serverless architecture with high throughput batch computing and event-driven
computations, in which computational resources are only running as long as
they are utilized. We demonstrate that this approach is very flexible and
allows for resilient and nested levels of parallelization, including domain
decomposition for solving the underlying partial differential equations.
While the event-driven approach introduces some overhead as computational
resources are repeatedly restarted, it inherently provides resilience to
instance shut-downs and allows a significant reduction of cost by avoiding
idle instances, thus making the cloud a viable alternative to on-premise
clusters for large-scale seismic imaging.},
  keywords = {cloud, imaging, serverless, event-driven, lsrtm},
  note = {Submitted to IEEE Transactions on Parallel and Distributed Systems on August 20, 2019},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2019/witte2019TPDedas/witte2019TPDedas.html}
}

@UNPUBLISHED{kukreja2019PASCccd,
  author = {Navjot Kukreja and Jan Huckelheim and Mathias Louboutin and Kaiyuan Hou and Paul Hovland and Gerard Gorman},
  title = {Combining checkpointing and data compression to accelerate adjoint-based optimization problems},
  year = {2019},
  abstract = {Seismic inversion and imaging are adjoint-based optimization problems that processes up to terabytes of data, regularly exceeding the memory capacity of available computers. Data compression is an effective strategy to reduce this memory requirement by a certain factor, particularly if some loss in accuracy is acceptable. A popular alternative is checkpointing, where data is stored at selected points in time, and values at other times are recomputed as needed from the last stored state. This allows arbitrarily large adjoint computations with limited memory, at the cost of additional recomputations. In this paper we combine compression and checkpointing for the first time to compute a realistic seismic inversion. The combination of checkpointing and compression allows larger adjoint computations compared to using only compression, and reduces the recomputation overhead significantly compared to using only checkpointing.},
  keywords = {Adjoint-state, FD, checkpointing, compression, HPC, inverse problems},
  note = {Submitted to PASC19 on January 16, 2019},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2019/kukreja2019PASCccd/kukreja2019PASCccd.pdf}
}



%-----2018-----%

@UNPUBLISHED{luporini2018aap,
  author = {FABIO LUPORINI and MICHAEL LANGE and MATHIAS LOUBOUTIN and NAVJOT KUKREJA and JAN HUCKELHEIM and CHARLES YOUNT and PHILIPP A. WITTE and PAUL H. J. KELLY and GERARD J. GORMAN and FELIX J. HERRMANN},
  title = {Architecture and performance of Devito, a system for automated stencil computation},
  year = {2018},
  abstract = {Stencil computations are a key part of many high-performance computing applications, such as image processing, convolutional neural networks, and finite-difference solvers for partial differential equations. Devito is a framework capable of generating highly-optimized code given symbolic equations expressed in Python, specialized in, but not limited to, affine (stencil) codes. The lowering process â€“ from mathematical equations down to C++ code â€“ is
performed by the Devito compiler through a series of intermediate representations. Several performance optimizations are introduced, including advanced common sub-expressions elimination, tiling and parallelization. Some of these are obtained through well-established stencil optimizers, integrated in the back-end of the Devito compiler. The architecture of the Devito compiler, as well as the performance optimizations that are applied when generating code, are presented. The effectiveness of such performance optimizations is demonstrated using operators drawn from seismic imaging applications.},
  keywords = {Stencil, finite difference method, symbolic processing, structured grid, compiler, performance optimization},
  note = {Submitted to SIAM Journal on Scientific Computing on July 9, 2018.},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2018/luporini2018aap/luporini2018aap.pdf}
}
