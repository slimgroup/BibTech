% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2016-----%

@UNPUBLISHED{deaguiar2016SCdff,
  author = {Marcos de Aguiar and Gerard Gorman and Felix J. Herrmann and Navjot Kukreja and Michael Lange and Mathias Louboutin and Felippe Vieira Zacarias},
  title = {DeVito: fast finite difference computation},
  year = {2016},
  keywords = {finite differences, high performance computing, full-waveform inversion, private},
  note = {Submitted to the Super Computing Conference on July 22, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SC/2016/deaguiar2016SCdff/deaguiar2016SCdff_poster.pdf}
}


@UNPUBLISHED{esser2016tvr,
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Total-variation regularization strategies in full-waveform inversion},
  year = {2016},
  abstract = {We propose an extended full-waveform inversion
                  formulation that includes general convex constraints
                  on the model. Though the full problem is highly
                  nonconvex, the overarching optimization scheme
                  arrives at geologically plausible results by solving
                  a sequence of relaxed and warm-started constrained
                  convex subproblems. The combination of box,
                  total-variation, and successively relaxed asymmetric
                  total-variation constraints allows us to steer free
                  from parasitic local minima while keeping the
                  estimated physical parameters laterally continuous
                  and in a physically realistic range. For accurate
                  starting models, numerical experiments carried out
                  on the challenging 2004 BP velocity benchmark
                  demonstrate that bound and total-variation
                  constraints improve the inversion result
                  significantly by removing inversion artifacts,
                  related to source encoding, and by clearly improved
                  delineation of top, bottom, and flanks of a
                  high-velocity high-contrast salt inclusion. The
                  experiments also show that for poor starting models
                  these two constraints by themselves are insufficient
                  to detect the bottom of high-velocity inclusions
                  such as salt. Inclusion of the one-sided asymmetric
                  total-variation constraint overcomes this issue by
                  discouraging velocity lows to buildup during the
                  early stages of the inversion. To the author's
                  knowledge the presented algorithm is the first to
                  successfully remove the imprint of local minima
                  caused by poor starting models and band-width
                  limited finite aperture data.},
  keywords = {full-waveform inversion, constrained optimization, total variation, salt, hinge loss, private},
  note = {Submitted on July 13, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/esser2016tvr/esser2016tvr.pdf}
}


@UNPUBLISHED{kumar2016bls,
  author = {Rajiv Kumar and Oscar Lopez and Damek Davis and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Beating level-set methods for {3D} seismic data interpolation: a primal-dual alternating approach},
  year = {2016},
  abstract = {Acquisition cost is a crucial bottleneck for seismic
                  workflows, and low-rank formulations for data
                  interpolation allow practitioners to 'fill in' data
                  volumes from critically subsampled data acquired in
                  the field. Tremendous size of seismic data volumes
                  required for seismic processing remains a major
                  challenge for these techniques. We propose a new
                  approach to solve residual constrained formulations
                  for interpolation. We represent the data volume
                  using matrix factors, and build a block-coordinate
                  algorithm with constrained convex subproblems that
                  are solved with a primal-dual splitting scheme. The
                  new approach is competitive with state of the art
                  level-set algorithms that interchange the role of
                  objectives with constraints. We use the new
                  algorithm to successfully interpolate a large scale
                  5D seismic data volume, generated from the
                  geologically complex synthetic 3D Compass velocity
                  model, where 80\% of the data has been removed.},
  keywords = {matrix completion, nuclear-norm relaxation, seismic data, interpolation, alternating minimization, primal-dual splitting, private},
  note = {Submitted on July 18, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/kumar2016bls/kumar2016bls.pdf}
}


@UNPUBLISHED{oghenekohwo2016GEOPctl,
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Low-cost time-lapse seismic with distributed {Compressive} {Sensing}---exploiting common information amongst the vintages},
  year = {2016},
  abstract = {Time-lapse seismic is a powerful technology for
                  monitoring a variety of subsurface changes due to
                  reservoir fluid flow. However, the practice can be
                  technically challenging when one seeks to acquire
                  colocated time-lapse surveys with high degrees of
                  replicability amongst the shot locations. We
                  demonstrate that under "ideal" circumstances, where
                  we ignore errors related to taking measurements off
                  the grid, high-quality prestack data can be obtained
                  from randomized subsampled measurements that are
                  observed from surveys where we choose not to revisit
                  the same randomly subsampled on-the-grid shot
                  locations. Our acquisition is low cost since our
                  measurements are subsampled. We find that the
                  recovered finely sampled prestack baseline and
                  monitor data actually improve significantly when the
                  same on-the-grid shot locations are not
                  revisited. We achieve this result by using the fact
                  that different time-lapse data share information and
                  that nonreplicated (on-the-grid) acquisitions can
                  add information when prestack data are recovered
                  jointly. Whenever the time-lapse data exhibit joint
                  structure---i.e., are compressible in some transform
                  domain and share information---sparsity-promoting
                  recovery of the "common component" and
                  "innovations", with respect to this common
                  component, outperforms independent recovery of both
                  the prestack baseline and monitor data. The
                  recovered time-lapse data are of high enough quality
                  to serve as input to extract poststack attributes
                  used to compute time-lapse differences. Without
                  joint recovery, artifacts---due to the randomized
                  subsampling---lead to deterioration of the degree of
                  repeatability of the time-lapse data. We support our
                  claims by carrying out experiments that collect
                  reliable statistics from thousands of repeated
                  experiments. We also confirm that high degrees of
                  repeatability are achievable for an ocean-bottom
                  cable survey acquired with time-jittered continuous
                  recording. This is part 1 of a two-paper series on
                  time-lapse seismic with compressed sensing. Part 2:
                  "Cheap time-lapse with distributed Compressive
                  Sensing---impact on repeatability".},
  keywords = {acquisition, time-lapse seismic, marine, random sampling, joint recovery method, private},
  note = {Revision 1 submitted on July 7, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/oghenekohwo2016GEOPctl/oghenekohwo2016GEOPctl.html}
}


@UNPUBLISHED{wason2016GEOPctl,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Cheap time lapse with distributed {Compressive Sensing}---impact on repeatability},
  year = {2016},
  abstract = {Irregular or off-the-grid spatial sampling of sources
                  and receivers is inevitable in field seismic
                  acquisitions. Consequently, time-lapse surveys
                  become particularly expensive since current
                  practices aim to replicate densely sampled surveys
                  for monitoring changes occurring in the reservoir
                  due to hydrocarbon production. We demonstrate that
                  under certain circumstances, high-quality prestack
                  data can be obtained from cheap randomized
                  subsampled measurements that are observed from
                  nonreplicated surveys. We extend our time-jittered
                  marine acquisition to time-lapse surveys by
                  designing acquisition on irregular spatial grids
                  that render simultaneous, subsampled and irregular
                  measurements. Using the fact that different
                  time-lapse data share information and that
                  nonreplicated surveys add information when prestack
                  data are recovered jointly, we recover periodic
                  densely sampled and colocated prestack data by
                  adapting the recovery method to incorporate a
                  regularization operator that maps traces from an
                  irregular spatial grid to a regular periodic
                  grid. The recovery method is, therefore, a combined
                  operation of regularization, interpolation
                  (estimating missing fine-grid traces from subsampled
                  coarse-grid data), and source separation (unraveling
                  overlapping shot records). By relaxing the
                  insistence on replicability between surveys, we find
                  that recovery of the time-lapse difference shows
                  little variability for realistic field scenarios of
                  slightly nonreplicated surveys that suffer from
                  unavoidable natural deviations in spatial sampling
                  of shots (or receivers) and pragmatic
                  compressed-sensing based nonreplicated surveys when
                  compared to the "ideal" scenario of exact
                  replicability between surveys. Moreover, the
                  recovered densely sampled prestack baseline and
                  monitor data improve significantly when the
                  acquisitions are not replicated, and hence can serve
                  as input to extract poststack attributes used to
                  compute time-lapse differences. Our observations are
                  based on numerous experiments conducted for an
                  ocean-bottom cable survey acquired with
                  time-jittered continuous recording assuming source
                  equalization (or same source signature) for the
                  time-lapse surveys and no changes in wave heights,
                  water column velocities or temperature and salinity
                  profiles, etc.},
  keywords = {marine acquisition, time-lapse seismic, off-the-grid recovery, random sampling, joint recovery method, optimization, private},
  note = {Submitted to Geophysics on May 13, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/wason2016GEOPctl/wason2016GEOPctl.html}
}


%-----2015-----%

@UNPUBLISHED{vanleeuwen2015GPWEMVA,
  author = {Tristan van Leeuwen and Rajiv Kumar and Felix J. Herrmann},
  title = {Enabling affordable omnidirectional subsurface extended image volumes via probing},
  year = {2015},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for the inference of rock properties
                  and velocity analysis in areas of complex
                  geology. Traditionally, these gathers are thought of
                  as multidimensional correlations of the source and
                  receiver wavefields. The bottleneck in computing
                  these gathers lies in the fact that one needs to
                  store, compute, and correlate these wavefields for
                  all shots in order to obtain the desired image
                  gathers. Therefore, the image gathers are typically
                  only computed for a limited number of subsurface
                  points and for a limited range of subsurface
                  offsets, which may cause problems in complex
                  geological areas with large geologic dips. We
                  overcome increasing computational and storage costs
                  of extended image volumes by introducing a
                  formulation that avoids explicit storage and removes
                  the customary and expensive loop over shots, found
                  in conventional extended imaging. As a result, we
                  end up with a matrix-vector formulation from which
                  different image gathers can be formed and with which
                  amplitude-versus-angle and wave-equation migration
                  velocity analyses can be performed without requiring
                  prior information on the geologic dips. Aside from
                  demonstrating the formation of two-way extended
                  image gathers for different purposes and at greatly
                  reduced costs, we also present a new approach to
                  conduct automatic wave-equation based
                  migration-velocity analysis. Instead of focussing in
                  particular offset directions and preselected subsets
                  of subsurface points, our method focuses every
                  subsurface point for all subsurface offset
                  directions using a randomized probing technique. As
                  a consequence, we obtain good velocity models at low
                  cost for complex models without the need to provide
                  information on the geologic dips.},
  keywords = {migration velocity analysis, AVA, stochastic optimization, private},
  note = {Submitted to Geophysical Prospecting on May 6. Revision 1 submitted on November 30.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/vanleeuwen2015GPWEMVA/vanleeuwen2015GPWEMVA.pdf}
}

