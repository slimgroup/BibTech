% This file was created with JabRef 2.9.
% Encoding: MacRoman

@UNPUBLISHED{kumar2013ICMLlr,
  author = {Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht
	and Felix J. Herrmann},
  title = {Matrix factorization using SPGL1},
  year = {2013},
  abstract = {Recent SVD-free matrix factorization formulations have enabled rank
	optimization for extremely large-scale systems (millions of rows
	and columns). In this paper, we consider rank-regularized formulations
	that only require a target data-fitting error level, and propose
	an algorithm for the corresponding problem. We illustrate the advantages
	of the new approach using the Netflix problem, and use it to obtain
	high quality results for seismic trace interpolation, a key application
	in exploration geophysics. We show that factor rank can be easily
	adjusted as the inversion proceeds, and propose a weighted extension
	that allows known subspace information to improve the results of
	matrix completion formulations. Using these methods, we obtain high-quality
	reconstructions for large scale seismic interpolation problems with
	real data.},
  keywords = {Interpolation, ICML, private},
  optmonth = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/kumar2013ICMLlr/kumar2013ICMLlr.pdf}
}

@UNPUBLISHED{bander2012dre,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  year = {2012},
  abstract = {Data-driven methods---such as the estimation of primaries by sparse
	inversion---suffer from the 'curse of dimensionality' that leads
	to disproportional growth in computational and storage demands when
	moving to realistic 3D field data. To remove this fundamental impediment,
	we propose a dimensionality-reduction technique where the 'data matrix'
	is approximated adaptively by a randomized low-rank factorization.
	Compared to conventional methods, which need for each iteration passage
	through all data possibly requiring on-the-fly interpolation, our
	randomized approach has the advantage that the total number of passes
	is reduced to only one to three. In addition, the low-rank matrix
	factorization leads to considerable reductions in storage and computational
	costs of the matrix multiplies required by the sparse inversion.
	Application of the proposed method to synthetic and real data shows
	that significant performance improvements in speed and memory use
	are achievable at a low computational up-front cost required by the
	low-rank factorization.},
  keywords = {multiples,Processing,Optimization},
  optmonth = {02/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/bander2012dre/bander2012dre.pdf}
}

@UNPUBLISHED{lin2012robustepsi,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust estimation of primaries by sparse inversion via one-norm minimization},
  year = {2012},
  abstract = {Even though contemporary methods for the removal of multiple events
	in seismic data due to a free-surface are built upon reciprocity
	relationships between wavefields, they are often still implemented
	as prediction-subtraction processes. The subtraction process does
	not always compensate for imperfect prediction of multiple events,
	and itself often leads to distortion of primary events. A recently
	proposed method called Estimation of Primaries by Sparse Inversion
	avoids the subtraction process altogether by directly prediction
	the primary impulse response as a collection of band-limited spikes
	under sparsity-regulated wavefield inversion approach. Although it
	can be shown that the correct primary impulse response is obtained
	through the sparsest possible solution, the Estimation of Primaries
	by Sparse Inversion algorithm was not designed to seek such a solution,
	instead depending on a predetermined degree of sparsity as an inversion
	parameter. This leads to imperfect multiple rejection when the sparsity
	is overestimated, and problems with recovering late primary events
	when it is underestimated. In this paper, we propose a new algorithm
	where we make obtaining the sparsest solution our explicit goal.
	Our approach remains a gradient-based approach like the original
	algorithm, but is in turn derived from a new optimization framework
	based on an extended basis pursuit denoising formulation. We show
	that the sparsity-minimizing objective of our formulation enables
	it to operate successfully on a wide variety of synthetic and field
	marine dataset without excessive tweaking of inversion parameters.
	We also demonstrate that Robust EPSI produces a more artifact-free
	impulse response compared to the original algorithm, which has interesting
	implications for broadband seismic applications. Finally we demonstrate
	through field data that recovering the primary impulse response under
	transform domains can significantly improve the recovery of weak
	primary late arrivals, without appreciable change to the underlying
	algorithm.},
  keywords = {multiples, optimization, sparsity, waveform inversion, pareto, biconvex,
	algorithm, EPSI},
  notes = {submitted to Geophysics, March 12, 2012},
  optmonth = {03/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/lin2012robustepsi/lin2012robustepsi.pdf}
}

@UNPUBLISHED{mansour12iwr,
  author = {Hassan Mansour and Felix J. Herrmann and Ozgur Yilmaz},
  title = {Improved wavefield reconstruction from randomized sampling via weighted
	one-norm minimization},
  year = {2012},
  abstract = {Missing-trace interpolation aims to recover the gaps caused by physical
	obstacles or deliberate subsampling to control acquisition costs
	in otherwise regularly-sampled seismic wavefields. While transform-domain
	sparsity promotion has proven to be an effective tool to solve this
	recovery problem, current recovery techniques make no use of a priori
	information on the locations of transform-domain coefficients. In
	this paper, we propose recovery by weighted one-norm minimization,
	which exploits correlations between the locations of significant
	coefficients of different partitions, e.g., shot records, common-offset
	gathers, or frequency slices of the acquired data. We use these correlations
	to define a sequence of 2D curvelet-based recovery problems that
	exploit 3D continuity exhibited by seismic wavefields without relying
	on the highly redundant 3D curvelet transform. To illustrate the
	performance of our weighted algorithm, we compare recoveries from
	different scenarios of partitioning for a seismic line from the Gulf
	of Suez. These examples demonstrate that our method is superior to
	standard $\ell_1$ minimization in terms of reconstruction quality
	and computational memory requirements.},
  keywords = {Trace interpolation, weighted one-norm minimization, compressed sensing,
	randomized sampling},
  optmonth = {10/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/mansour12iwr/mansour12iwr.pdf}
}

@UNPUBLISHED{vanderneut12irs,
  author = {Joost {van der Neut} and Felix J. Herrmann},
  title = {Interferometric redatuming by sparse inversion},
  year = {2012},
  abstract = {Assuming that exact transmission responses are known between the surface
	and a particular depth level in the subsurface, seismic sources can
	be eﬀectively mapped to that level by a process called interferometric
	redatuming. After redatuming, the obtained waveﬁelds can be used
	for imaging below this particular depth level. Interferometric redatuming
	consists of two steps, namely (i) the decomposition of the observed
	waveﬁelds into up- and down-going constituents and (ii) a multidimensional
	deconvolution of the up- and downgoing waveﬁelds. While this method
	works in theory, sensitivity to noise and artifacts due to incomplete
	acquisition call for a diﬀerent formulation. In this letter, we
	demonstrate the beneﬁts of formulating the two steps that undergird
	interferometric redatuming in terms of a transform-domain sparsity-promoting
	program. By exploiting compressibility of seismic waveﬁelds in
	the curvelet domain, we not only become robust with respect to noise
	but we are also able to remove certain artifacts while preserving
	the frequency content. These improvements lead to a better image
	of the target from the redatumed data.},
  keywords = {Controlled source seismology, Interferometry, Inverse theory},
  optmonth = {10/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/vanderneut12irs/vanderneut12irs.pdf}
}

@UNPUBLISHED{vanleeuwen2012smii,
  author = {Tristan {van Leeuwen}},
  title = {A parallel matrix-free framework for frequency-domain seismic modelling,
	imaging and inversion in Matlab},
  year = {2012},
  abstract = {I present a parallel matrix-free framework for frequency-domain seismic
	modeling, imaging and inversion. The framework provides basic building
	blocks for designing and testing optimization-based formulations
	of both linear and non-linear seismic in- verse problems. By overloading
	standard linear-algebra operations, such as matrix- vector multiplications,
	standard optimization packages can be used to work with the code
	without any modification. This leads to a scalable testbed on which
	new methods can be rapidly prototyped and tested on medium-sized
	2D problems. I present some numerical examples on both linear and
	non-linear seismic inverse problems.},
  keywords = {Seismic imaging,optimization,Matlab,object-oriented programming},
  optmonth = {07/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/vanleeuwen2012smii/vanleeuwen2012smii.pdf}
}

