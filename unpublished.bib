% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2017-----%

@UNPUBLISHED{alfaraj2017EAGEswr,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Shear wave reconstruction from low cost randomized acquisition},
  year = {2017},
  abstract = {Shear waves travel in the subsurface at a lower speed
                  compared with compressional waves. Therefore, much
                  finer spatial sampling is required to properly
                  record the shear waves. This leads to higher
                  acquisition costs which are typically avoided by
                  designing surveys geared towards only compressional
                  waves imaging. We propose using randomly
                  under-sampled ocean bottom acquisition for recording
                  both compressional and shear waves. The recorded
                  multicomponent data is then interpolated using an
                  SVD-free low rank interpolation scheme that is
                  feasible for large scale seismic data volumes to
                  obtain finely sampled data. Following that, we
                  perform elastic wavefield decomposition at the ocean
                  bottom to recover accurate up- and dow-going
                  S-waves. Synthetic data results indicate that using
                  randomized under-sampled acquisition, we can recover
                  accurate S-waves with an economical cost compared
                  with conventional acquisition designs.},
  keywords = {EAGE, shear waves, rank minimization, interpolation, randomized acquisition, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/alfaraj2017EAGEswr/alfaraj2017EAGEswr.pdf}
}


@UNPUBLISHED{alfaraj2017SEGrsw,
  author = {Ali M. Alfaraj and Rajiv Kumar and Felix J. Herrmann},
  title = {Reconstruction of {S-waves} from low-cost randomized and simultaneous acquisition by joint sparse inversion},
  year = {2017},
  abstract = {Spatial Nyquist sampling rate, which is proportional to
                  the apparent subsurface velocity, is the key
                  deciding cost factor for conventional seismic
                  acquisition. Due to the lower shear wave velocity
                  compared with compressional waves, finer spatial
                  sampling is required to properly record the earlier,
                  which increases the acquisition costs. This is one
                  of the reasons why shear waves are usually not
                  considered in practice. To avoid having the Nyquist
                  criterion as the deciding cost factor and to utilize
                  multicomponent data to its available full extent, we
                  propose acquiring randomly undersampled ocean bottom
                  seismic data. Each component can then be
                  interpolated separately, followed by elastic
                  decomposition to recover up- and down-going
                  S-waves. Instead, we jointly interpolate and
                  decompose the recorded multicomponent data by
                  solving one sparsity promoting optimization
                  problem. This way we ensure that the relative
                  amplitudes of the multicomponent data is
                  preserved. We compare two sparsifying transforms:
                  the curvelet transform and the frequency-wavenumber
                  transform. Another key cost deciding factor for
                  seismic acquisition is the efficiency of acquiring
                  data. This calls for simultaneous acquisition, which
                  requires a source separation step. Similarly,
                  instead of taking a two-step approach, we perform a
                  sparsity-promoting joint source separation
                  decomposition. Results on economically and
                  efficiently acquired synthetic data of both joint
                  methods show their ability of reconstructing
                  accurate up- and down-going S-waves.},
  keywords = {SEG, elastic, shear wave, randomized acquisition, interpolation, source separation, decomposition, sparse, private},
  date_submitted = {04/01/2017},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/alfaraj2017SEGrsw/alfaraj2017SEGrsw.html}
}


@UNPUBLISHED{dasilva2017uls,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {A unified {2D/3D} large scale software environment for nonlinear inverse problems},
  year = {2017},
  abstract = {Large scale parameter estimation problems are some of
                  the most computationally demanding problems. An
                  academic researcher's domain-specific knowledge
                  often precludes that of software design, which
                  results in software frameworks for inversion that
                  are technically correct, but not scalable to
                  realistically-sized problems. On the other hand, the
                  computational demands of the problem for realistic
                  problems result in industrial codebases that are
                  geared solely for performance, rather than
                  comprehensibility or flexibility. We propose a new
                  software design that bridges the gap between these
                  two seemingly disparate worlds. A hierarchical and
                  modular design allows a user to delve into as much
                  detail as she desires, while using high performance
                  primitives at the lower levels. Our code has the
                  added benefit of actually reflecting the underlying
                  mathematics of the problem, which lowers the
                  cognitive load on user using it and reduces the
                  initial startup period before a researcher can be
                  fully productive. We also introduce a new
                  preconditioner for the Helmholtz equation that is
                  suitable for fault-tolerant distributed
                  systems. Numerical experiments on a variety of 2D
                  and 3D test problems demonstrate the effectiveness
                  of this approach on scaling algorithms from small to
                  large scale problems with minimal code changes.},
  keywords = {optimization, PDE-constrained inversion, large scale, matlab, private},
  note = {Submitted to ACM Transactions on Mathematical Software on February 14, 2017.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2017/dasilva2017uls/dasilva2017uls.html}
}


@UNPUBLISHED{daskalakis2017SEGdds,
  author = {Emmanouil Daskalakis and Rachel Kuske and Felix J. Herrmann},
  title = {Developments in the direction of solving extremly large problems in {Geophysics}},
  year = {2017},
  abstract = {Often in exploration Geophysics we are forced to work
                  with extremely large problems. Acquisitions via
                  dense grids of receivers translate into very large
                  mathematical systems. Usually, depending on the
                  acquisition, the size of the matrix of the system to
                  be solved, can be measured in the millions. The
                  proper way to address this problem is by
                  subsampling. Even though subsampling can reduce the
                  computational efforts required, it can not address
                  stability problems caused by preconditioning and/or
                  instrumental response errors. In this abstract, we
                  introduce a modification of the linearized Bregman
                  solver for these large problems that resolves
                  stability issues.},
  keywords = {SEG, linearized Bregman, cycling, weighted increment, least-squares migration, private},
  date_submitted = {04/01/2017},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/daskalakis2017SEGdds/daskalakis2017SEGdds.html}
}


@UNPUBLISHED{fang2017WAVESuqf,
  author = {Zhilong Fang and Curt Da Silva and Rachel Kuske and Felix J. Herrmann},
  title = {Uncertainty quantification for inverse problems with a weak wave-equation constraint},
  year = {2017},
  abstract = {In this work, we present a new posterior distribution to
                  quantify uncertainties in solutions of wave-equation
                  based inverse problems. By introducing an auxiliary
                  variable for the wavefields, we weaken the strict
                  wave-equation constraint used by conventional
                  Bayesian approaches. With this weak constraint, the
                  new posterior distribution is a bi-Gaussian
                  distribution with respect to both model parameters
                  and wavefields, which can be directly sampled by the
                  Gibbs sampling method.},
  keywords = {uncertainty, wave equation, constraint, Gibbs sampling, private},
  note = {Submitted to the WAVES 2017 Conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/WAVES/2017/fang2017WAVESuqf/fang2017WAVESuqf.html}
}


@UNPUBLISHED{kumar2017EAGEdha,
  author = {Rajiv Kumar and Nick Moldoveanu and Felix J. Herrmann},
  title = {Denoising high-amplitude cross-flow noise using curvelet-based stable principle component pursuit},
  year = {2017},
  abstract = {Removal of high-amplitude cross-flow noise in marine
                  towed-streamer acquisition is of great interest
                  because cross-flow noise hinders the success of
                  subsequent processing (e.g. EPSI) and
                  migration. However, the removal of cross-flow noise
                  is a challenging process because cross-flow noise
                  dominates steep angles and low-frequency components
                  of the signal. As a result, applying a simple
                  high-pass filter can result in a loss of coherent
                  diving waves and reflected energy. We propose a
                  stable curvelet-based principle-component pursuit
                  approach that does not suffer from this shortcoming
                  because it uses angle- and scale-adaptivity of the
                  curvelet transform in combination with the low-rank
                  property of cross-flow noise. As long as the
                  cross-flow noise exhibits low-rank in the curvelet
                  domain, our method successfully separates this
                  signal component from the diving waves and seismic
                  reflectivity, which is well-know to be sparse in the
                  curvelet domain. Experimental results on a
                  common-shot gather extracted from a coil shooting
                  survey in the Gulf of Mexico shows the potential of
                  our approach.},
  keywords = {EAGE, denoising, coil data, cross-flow noise, curvelet, SPCP, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/kumar2017EAGEdha/kumar2017EAGEdha.html}
}


@UNPUBLISHED{kumar2017hrc,
  author = {Rajiv Kumar and Haneet Wason and Shashin Sharan and Felix J. Herrmann},
  title = {Highly repeatable {3D} compressive full-azimuth towed-streamer time-lapse acquisition --- a numerical feasibility study at scale},
  year = {2017},
  month = {05},
  abstract = {Most conventional 3D time-lapse (or 4D) acquisitions are
                  ocean-bottom cable (OBC) or ocean-bottom node (OBN)
                  surveys since these surveys are relatively easy to
                  replicate compared to towed-streamer surveys. To
                  attain high degrees of repeatability, survey
                  replicability and dense periodic sampling has become
                  the norm for 4D surveys that renders this technology
                  expensive. Conventional towed-streamer acquisitions
                  suffer from limited illumination of subsurface due
                  to narrow azimuth. Although, acquisition techniques
                  such as multi-azimuth, wide-azimuth, rich-azimuth
                  acquisition, etc., have been developed to illuminate
                  the subsurface from all possible angles, these
                  techniques can be prohibitively expensive for
                  densely sampled surveys. This leads to uneven
                  sampling, i.e., dense receiver and coarse source
                  sampling or vice-versa, in order to make these
                  acquisitions more affordable. Motivated by the
                  design principles of Compressive Sensing (CS), we
                  acquire economic, randomly subsampled (or
                  compressive) and simultaneous towed-streamer
                  time-lapse data without the need of replicating the
                  surveys. We recover densely sampled time-lapse data
                  on one and the same periodic grid by using a
                  joint-recovery model (JRM) that exploits shared
                  information among different time-lapse recordings,
                  coupled with a computationally cheap and scalable
                  rank-minimization technique. The acquisition is low
                  cost since we have subsampled measurements (about
                  70\% subsampled), simulated with a simultaneous
                  long-offset acquisition configuration of two source
                  vessels travelling across a survey area at random
                  azimuths. We analyze the performance of our proposed
                  compressive acquisition and subsequent recovery
                  strategy by conducting a synthetic, at scale,
                  seismic experiment on a 3D time-lapse model
                  containing geological features such as channel
                  systems, dipping and faulted beds, unconformities
                  and a gas cloud. Our findings indicate that the
                  insistence on replicability between surveys and the
                  need for OBC/OBN 4D surveys can, perhaps, be
                  relaxed. Moreover, this is a natural next step
                  beyond the successful CS acquisition examples
                  discussed in this special issue.},
  keywords = {time-lapse seismic, marine, 3D, simultaneous long offset, CS, rank minimization, private},
  note = {Submitted to The Leading Edge},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2017/kumar2017hrc/kumar2017hrc.html}
}


@UNPUBLISHED{louboutin2017EAGEdns,
  author = {Mathias Louboutin and Llu\'{i}s Guasch and Felix J. Herrmann},
  title = {Data normalization strategies for seismic inversion},
  year = {2017},
  abstract = {Amplitude mismatch is an inherent problem in seismic
                  inversion. Most of the source estimation techniques
                  are associated with amplitude uncertainty due to
                  incomplete representation of the physics or
                  estimation method parameters. Rewriting the
                  inversion problem in an amplitude free formulation
                  allows to mitigate the amplitude ambiguity and help
                  the inversion process to converge. We present in
                  this work two different strategies to lessen
                  amplitude effects in seismic inversion, derive the
                  corresponding update directions and show how we
                  handle scaling error correctly in both the objective
                  function and the gradient.},
  keywords = {EAGE, FWI, normalization, inversion, nonlinear, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/louboutin2017EAGEdns/louboutin2017EAGEdns.html}
}


@UNPUBLISHED{louboutin2017EAGEess,
  author = {Mathias Louboutin and Felix J. Herrmann},
  title = {Extending the search space of time-domain adjoint-state {FWI} with randomized implicit time shifts},
  year = {2017},
  abstract = {Adjoint-state full-waveform inversion aims to obtain
                  subsurface properties such as velocity, density or
                  anisotropy parameters, from surface recorded
                  data. As with any (non-stochastic) gradient based
                  optimization procedure, the solution of this
                  inversion procedure is to a large extend determined
                  by the quality of the starting model. If this
                  starting model is too far from the true model, these
                  derivative-based optimizations will likely end up in
                  local minima and erroneous inversion results. In
                  certain cases, extension of the search space,
                  e.g. by making the wavefields or focused matched
                  sources additional unknowns, has removed some of
                  these non-uniqueness issues but these rely on
                  time-harmonic formulations. Here, we follow a
                  different approach by combining an implicit
                  extension of the velocity model, time compression
                  techniques and recent results on stochastic sampling
                  in non-smooth/non-convex optimization},
  keywords = {EAGE, time domain, FWI, cycle skipping, inversion, nonconvex, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/louboutin2017EAGEess/louboutin2017EAGEess.html}
}


@UNPUBLISHED{oghenekohwo2017EAGEitl,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Improved time-lapse data repeatability with randomized sampling and distributed compressive sensing},
  year = {2017},
  abstract = {Recently, new ideas on randomized sampling for
                  time-lapse seismic acquisition have been proposed to
                  address some of the challenges of replicating
                  time-lapse surveys. These ideas, which stem from
                  distributed compressed sensing (DCS) led to the
                  birth of a joint recovery model (JRM) for processing
                  time-lapse data (noise-free) acquired from
                  non-replicated acquisition geometries. However, when
                  the earth does not change—i.e. no time-lapse—the
                  recovered vintages from two non-replicated surveys
                  should show high repeatability measured in terms of
                  normalized RMS, which is a standard metric for
                  quantifying time-lapse data repeatability. Under
                  this assumption of no time-lapse change, we
                  demonstrate improved repeatability (with JRM) of the
                  recovered data from non-replicated random samplings,
                  first with noisy data and secondly in situations
                  where there are calibration errors i.e. where the
                  acquisition parameters such as source/receiver
                  coordinates are not precise.},
  keywords = {EAGE, repeatability, time lapse, compressive sensing, calibration, noise, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/oghenekohwo2017EAGEitl/oghenekohwo2017EAGEitl.html}
}


@UNPUBLISHED{oghenekohwo2017hrt,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Highly repeatable time-lapse seismic with distributed {Compressive} {Sensing}---mitigating effects of calibration errors},
  year = {2017},
  month = {05},
  abstract = {Recently, we demonstrated that combining joint recovery
                  with low-cost non-replicated randomized sampling
                  tailored to time-lapse seismic can give us access to
                  high fidelity, highly repeatable, dense prestack
                  vintages, and high-grade time-lapse. To arrive at
                  this result, we assumed well-calibrated
                  surveys---i.e., we presumed accurate post-plot
                  source/receiver positions. Unfortunately, in
                  practice seismic surveys are prone to calibration
                  errors, which are unknown deviations between actual
                  and post-plot acquisition geometry. By means of
                  synthetic experiments, we analyze the possible
                  impact of these errors on vintages and on time-lapse
                  data obtained with our joint recovery model from
                  compressively sampled surveys. Supported by these
                  experiments, we demonstrate that highly repeatable
                  time-lapse vintages are attainable despite the
                  presence of unknown calibration errors in the
                  positions of the shots. We assess the repeatability
                  quantitatively for two scenarios by studying the
                  impact of calibration errors on conventional dense
                  but irregularly sampled surveys and on low-cost
                  compressed surveys. To separate time-lapse effects
                  from calibration issues, we consider the idealized
                  case where the subsurface remains unchanged and the
                  practical situation where time-lapse changes are
                  restricted to a subset of the data. In both cases,
                  the quality of the recovered vintages and time-lapse
                  decreases gracefully for low-cost compressed surveys
                  with increasing calibration errors. Conversely, the
                  quality of vintages from expensive densely
                  periodically sampled surveys decreases more rapidly
                  as unknown and difficult to control calibration
                  errors increase.},
  keywords = {time-lapse seismic, marine, random sampling, calibration errors, joint-recovery method, private},
  note = {Submitted to The Leading Edge},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2017/oghenekohwo2017hrt/oghenekohwo2017hrt.html}
}


@UNPUBLISHED{sharan2017SEGhrf,
  author = {Shashin Sharan and Rongrong Wang and Felix J. Herrmann},
  title = {High resolution fast microseismic source collocation and source time function estimation},
  year = {2017},
  abstract = {Sparsity promotion based joint microseismic source
                  collocation and source time function estimation,
                  using Linearized Bregman algorithm, although simple
                  to implement, suffers from slow convergence. This is
                  due to the fact that Linearized Bregman algorithm
                  has only first order of convergence. In this work,
                  we propose to accelerate the existing Linearized
                  Bregman algorithm using the L-BFGS
                  algorithm. Without any initial guess for the source
                  location or source time function, our method is able
                  to estimate the source location and source time
                  function for kinematically correct smooth velocity
                  model. We demonstrate the effectiveness of our
                  method for multiple sources spaced within half a
                  wavelength. We also compare our results with
                  Linearized Bregman based method in ``2.5``D instead
                  of ``2``D.},
  keywords = {SEG, algorithm, full-waveform inversion, microseismic, passive imaging, wave equation, private},
  date_submitted = {04/01/2017},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/sharan2017SEGhrf/sharan2017SEGhrf.html}
}


@UNPUBLISHED{wang2017SEGdff,
  author = {Rongrong Wang and Felix J. Herrmann},
  title = {A denoising formulation of full-waveform inversion},
  year = {2017},
  abstract = {We propose a wave-equation-based subsurface inversion
                  method that in many cases is more robust than the
                  conventional Full-Waveform Inversion. The new
                  formulation is written in a denoising form that
                  allows the synthetic data to match the observed ones
                  up to a small error. Compared to the Full-Waveform
                  Inversion, our method treats the noise arising from
                  the data measuring/recording process and that from
                  the synthetic modelling process
                  separately. Comparing to the Wavefields
                  Reconstruction Inversion, the new formulation
                  mitigates the difficulty of choosing the penalty
                  parameter $\lambda$. To solve the proposed
                  optimization problem, we develop an efficient
                  frequency domain algorithm that alternatively
                  updates the model and the data. Numerical
                  experiments confirm strong stability of the proposed
                  method by comparisons between the results of our
                  algorithm with that from both plain FWI and a
                  weighted formulation of the FWI.},
  keywords = {SEG, FWI, denoising, private},
  date_submitted = {04/01/2017},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/wang2017SEGdff/wang2017SEGdff.pdf}
}


@UNPUBLISHED{witte2017EAGEspl,
  author = {Philipp A. Witte and Mengmeng Yang and Felix J. Herrmann},
  title = {Sparsity-promoting least-squares migration with the linearized inverse scattering imaging condition},
  year = {2017},
  abstract = {Reverse-time migration (RTM) with the conventional
                  cross-correlation imaging condition suffers from
                  low-frequency artifacts that result from
                  backscattered energy in the background velocity
                  models. This problem translates to least-squares
                  reverse-time migration (LS-RTM), where these
                  artifacts slow down the convergence, as many of the
                  initial iterations are spent on removing them. In
                  RTM, this problem has been successfully addressed by
                  the introduction of the so-called inverse scattering
                  imaging condition, which naturally removes these
                  artifacts. In this work, we derive the corresponding
                  linearized forward operator of the inverse
                  scattering imaging operator and incorporate this
                  forward/adjoint operator pair into a
                  sparsity-promoting (SPLS-RTM) workflow. We
                  demonstrate on a challenging salt model, that LS-RTM
                  with the inverse scattering imaging condition is far
                  less prone to low-frequency artifacts than the
                  conventional cross-correlation imaging condition,
                  improves the convergence and does not require any
                  type of additional image filters within the
                  inversion. Through source subsampling and sparsity
                  promotion, we reduce the computational cost in terms
                  of PDE solves to a number comparable to conventional
                  RTM, making our workflow applicable to large-scale
                  problems.},
  keywords = {EAGE, least-squares migration, imaging condition, linearized Bregman, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2017/witte2017EAGEspl/witte2017EAGEspl.html}
}


@UNPUBLISHED{yang2017SEGfsp,
  author = {Mengmeng Yang and Emmanouil Daskalakis and Felix J. Herrmann},
  title = {Fast sparsity-promoting least-squares migration with multiples in time domain},
  year = {2017},
  abstract = {Based on the latest developments of research in
                  inversion technology with optimization, researchers
                  have made significant progress in the implementation
                  of least-squares reverse-time migration (LS-RTM) of
                  primaries. In Marine data however, these
                  applications rely on the success of a pre-imaging
                  separation of primaries and multiples, which can be
                  modeled as a multi-dimensional convolution between
                  the vertical derivative of the surface-free Green’s
                  function and the down-going receiver
                  wavefield. Instead of imaging the primaries and
                  multiples separately, we implement the LS-RTM of the
                  total down-going wavefield by combining areal source
                  injection and linearized Born modelling, where
                  strong surface related multiples are generated from
                  a strong density variation at the ocean bottom. The
                  advantage including surface related multiples in
                  LS-RTM is the extra illumination we obtain from
                  these multiples without incurring additional
                  computational costs related to carrying out
                  multi-dimensional convolutions part of conventional
                  multiple prediction procedures. Even though we are
                  able to avert these computational costs, our
                  approach shares the large costs of LSRTM. We reduce
                  these costs by combining randomized source
                  subsampling with our sparsity-promoting imaging
                  technology, which produces artifact-free,
                  high-resolution images, with the surface-related
                  multiples migrated properly.},
  keywords = {SEG, sparsity, least-squares RTM, multiples, time domain, private},
  date_submitted = {04/01/2017},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/yang2017SEGfsp/yang2017SEGfsp.html}
}


@UNPUBLISHED{zhang2017SEGmsd,
  author = {Yiming Zhang and Curt Da Silva and Rajiv Kumar and Felix J. Herrmann},
  title = {Massive {3D} seismic data compression and inversion with hierarchical {Tucker}},
  year = {2017},
  abstract = {Modern-day oil and gas exploration, especially in areas
                  of complex geology such as fault belts and sub-salt
                  areas, is an increasingly expensive and risky
                  endeavour. Typically long-offset and dense sampling
                  seismic data are required for subsequent shot based
                  processing procedures, e.g. wave-equation based
                  inversion (WEI) and surface-related multiple
                  elimination (SRME). However, these strict
                  requirements result in an exponential growth in data
                  volume size and prohibitive demands on computational
                  resources, given the multidimensional nature of the
                  data volumes. Moreover the physical constraints and
                  cost limitations impose restrictions on acquiring
                  fully sampled data. In this work, we propose to
                  invert our large-scale data from a set of subsampled
                  measurements, resulting in an estimate of the true
                  volume in a compressed low-rank tensor
                  format. Rather than expanding the data to its
                  fully-sampled form for later downstream processes,
                  we demonstrate how to use this compressed data
                  directly via on-the-fly common shot or receiver
                  gathers extraction. The combination of massive
                  compression and fast on demand data reconstruction
                  of 3D shot or receiver gathers leads to a
                  substantial reduction in memory costs but with
                  minimal effects on results in the subsequent
                  processing procedures. We demonstrate the effective
                  implementation of our proposed framework on
                  full-waveform inversion on a 3D seismic synthetic
                  data set generated from a Overthrust model.},
  keywords = {SEG, 3D, tensor algebra, FWI, private},
  date_submitted = {04/01/2017},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2017/zhang2017SEGmsd/zhang2017SEGmsd.html}
}


%-----2016-----%

@UNPUBLISHED{esser2016tvr,
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Total-variation regularization strategies in full-waveform inversion},
  year = {2016},
  abstract = {We propose an extended full-waveform inversion
                  formulation that includes general convex constraints
                  on the model. Though the full problem is highly
                  nonconvex, the overarching optimization scheme
                  arrives at geologically plausible results by solving
                  a sequence of relaxed and warm-started constrained
                  convex subproblems. The combination of box,
                  total-variation, and successively relaxed asymmetric
                  total-variation constraints allows us to steer free
                  from parasitic local minima while keeping the
                  estimated physical parameters laterally continuous
                  and in a physically realistic range. For accurate
                  starting models, numerical experiments carried out
                  on the challenging 2004 BP velocity benchmark
                  demonstrate that bound and total-variation
                  constraints improve the inversion result
                  significantly by removing inversion artifacts,
                  related to source encoding, and by clearly improved
                  delineation of top, bottom, and flanks of a
                  high-velocity high-contrast salt inclusion. The
                  experiments also show that for poor starting models
                  these two constraints by themselves are insufficient
                  to detect the bottom of high-velocity inclusions
                  such as salt. Inclusion of the one-sided asymmetric
                  total-variation constraint overcomes this issue by
                  discouraging velocity lows to buildup during the
                  early stages of the inversion. To the author's
                  knowledge the presented algorithm is the first to
                  successfully remove the imprint of local minima
                  caused by poor starting models and band-width
                  limited finite aperture data.},
  keywords = {full-waveform inversion, constrained optimization, total variation, salt, hinge loss, private},
  note = {Submitted on July 13, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/esser2016tvr/esser2016tvr.pdf}
}


@UNPUBLISHED{louboutin2016ppf,
  author = {Mathias Louboutin and Michael Lange and Felix J. Herrmann and Navjot Kukreja and Gerard Gorman},
  title = {Performance prediction of finite-difference solvers for different computer architectures},
  year = {2016},
  abstract = {The life-cycle of a partial differential equation (PDE)
                  solver is often characterized by three development
                  phases: the development of a stable numerical
                  discretization; development of a correct (verified)
                  implementation; and the optimization of the
                  implementation for different computer
                  architectures. Often it is only after significant
                  time and effort has been invested that the
                  performance bottlenecks of a PDE solver are fully
                  understood, and the precise details varies between
                  different computer architectures. One way to
                  mitigate this issue is to establish a reliable
                  performance model that allows a numerical analyst to
                  make reliable predictions of how well a numerical
                  method would perform on a given computer
                  architecture, before embarking upon potentially long
                  and expensive implementation and optimization
                  phases. The availability of a reliable performance
                  model also saves developer effort as it both informs
                  the developer on what kind of optimisations are
                  beneficial, and when the maximum expected
                  performance has been reached and optimisation work
                  should stop. We show how discretization of a wave
                  equation can be theoretically studied to understand
                  the performance limitations of the method on modern
                  computer architectures. We focus on the roofline
                  model, now broadly used in the high-performance
                  computing community, which considers the achievable
                  performance in terms of the peak memory bandwidth
                  and peak floating point performance of a computer
                  with respect to algorithmic choices. A first
                  principles analysis of operational intensity for key
                  time-stepping finite-difference algorithms is
                  presented. With this information available at the
                  time of algorithm design, the expected performance
                  on target computer systems can be used as a driver
                  for algorithm design.},
  keywords = {finite differences, performance, modeling, HPC, private},
  note = {Submitted to the Computers \& Geosciences Journal on September 16, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/louboutin2016ppf/louboutin2016ppf.pdf}
}

