% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2016-----%

@UNPUBLISHED{bougher2016SEGava,
  author = {Ben B. Bougher and Felix J. Herrmann},
  title = {AVA classification as an unsupervised machine-learning problem},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {Much of AVA analysis relies on characterizing background
                  trends and anomalies in pre-stack seismic
                  data. Analysts reduce a seismic section into a small
                  number of these trends and anomalies, suggesting
                  that a low-dimensional structure can be inferred
                  from the data. We describe AVA-attribute
                  characterization as an unsupervised-learning
                  problem, where AVA classes are learned directly from
                  the data without any prior assumptions on physics
                  and geological settings. The method is demonstrated
                  on the Marmousi II elastic model, where a gas
                  reservoir was successfully delineated from a
                  background trend in a depth migrated image.},
  keywords = {AVA, machine learning, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/bougher2016SEGava/bougher2016SEGava.html}
}


@UNPUBLISHED{chai2016SEGlbm,
  author = {Xintao Chai and Mengmeng Yang and Philipp A. Witte and Rongrong Wang and Zhilong Fang and Felix J. Herrmann},
  title = {A linearized {Bregman} method for compressive waveform inversion},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {We present an implementation of a recently developed and
                  relatively simple linearized Bregman method to solve
                  the large-scale $\ell_1$-norm sparsity-promoting
                  Gauss-Newton (GN) full-waveform inversion (FWI)
                  problem. Numerical experiments demonstrate that: the
                  much simpler linearized Bregman method does a
                  comparable and even superior job compared to a
                  state-of-the-art $\ell_1$-norm solver SPGL1, which
                  is previously used in the modified Gauss-Newton FWI;
                  the linearized Bregman method is also more efficient
                  (faster) than SPGL1; the FWI result with the
                  linearized Bregman method solving $\ell_1$-norm
                  sparsity-promoting problems to get the model updates
                  is better than that obtained by solving
                  $\ell_2$-norm least-squares problems to get the
                  model updates.},
  keywords = {Bregman method, waveform inversion, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/chai2016SEGlbm/chai2016SEGlbm.pdf}
}


@UNPUBLISHED{dasilva2016SEGuse,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {A unified {2D/3D} software environment for large scale time-harmonic full waveform inversion},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {Full-Waveform Inversion is a costly and complex
                  procedure for realistically sized 3D seismic
                  data. The performance-critical nature of this
                  problem often results in software environments that
                  are written entirely in low-level languages, making
                  them hard to understand, maintain, improve, and
                  extend. The unreadability of such codes can stymie
                  research developments, where the translation from
                  higher level mathematical ideas to high performance
                  codes can be lost, inhibiting the uptake of new
                  ideas in production-level codebases. We propose a
                  new software organization paradigm for Full-Waveform
                  Inversion and other PDE-constrained optimization
                  problems that is flexible, efficient, scalable, and
                  demonstrably correct. We decompose the various
                  structural components of FWI in to its constituent
                  components, from parallel computation to assembling
                  objective functions and gradients to lower level
                  matrix-vector multiplications. This decomposition
                  allows us to create a framework where individual
                  components can be easily swapped out to suit a
                  particular user's existing software environment. The
                  ease of applying high-level algorithms to the FWI
                  problem allows us to easily implement stochastic FWI
                  and demonstrate its effectiveness on a large scale
                  3D problem.},
  keywords = {software, full-waveform inversion, large scale, numerical analysis, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/dasilva2016SEGuse/dasilva2016SEGuse.html}
}


@UNPUBLISHED{fang2016SEGuqw,
  author = {Zhilong Fang and Chia Ying Lee and Curt Da Silva and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Uncertainty quantification for {Wavefield} {Reconstruction} {Inversion} using a {PDE} free semidefinite {Hessian} and randomize-then-optimize method},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {The data of full-waveform inversion often contains
                  noise, which induces uncertainties in the inversion
                  results. Ideally, one would like to run a number of
                  independent inversions with different realizations
                  of the noise and assess model-side uncertainties
                  from the resulting models, however this is not
                  feasible because we collect the data only once. To
                  circumvent this restriction, various sampling
                  schemes have been devised to generate an ensemble of
                  models that fit the data to within the noise
                  level. Such sampling schemes typically involve
                  running multiple inversions or evaluating the
                  Hessian of the cost function, both of which are
                  computationally expensive. In this work, we propose
                  a new method to quantify uncertainties based on a
                  novel formulation of the full-waveform inversion
                  problem – wavefield reconstruction inversion. Based
                  on this formulation, we formulate a semidefinite
                  approximation of the corresponding Hessian
                  matrix. By precomputing certain quantities, we are
                  able to apply this Hessian to given input vectors
                  without additional solutions of the underlying
                  partial differential equations. To generate a
                  sample, we solve an auxiliary stochastic
                  optimization problem involving this Hessian. The
                  result is a computationally feasible method that,
                  with little overhead, can generate as many samples
                  as required at small additional cost. We test our
                  method on the synthetic BG Compass model and compare
                  the results to a direct-sampling approach. The
                  results show the feasibility of applying our method
                  to computing statistical quantities such as the mean
                  and standard deviation in the context of wavefield
                  reconstruction inversion.},
  keywords = {WRI, uncertainty quantification, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/fang2016SEGuqw/fang2016SEGuqw.html}
}


@UNPUBLISHED{kumar2016SEGtjm,
  author = {Rajiv Kumar and Shashin Sharan and Haneet Wason and Felix J. Herrmann},
  title = {Time-jittered marine acquisition---a rank-minimization approach for {5D} source separation},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {Simultaneous source marine acquisition has been
                  recognized as an economic way of improving spatial
                  sampling and speedup acquisition time, where a
                  single- (or multiple-) source vessel fires at
                  jittered source locations and time
                  instances. Consequently, the acquired simultaneous
                  data volume is processed to separate the overlapping
                  shot records resulting in densely sampled data
                  volume. It has been shown in the past that the
                  simultaneous source acquisition design and source
                  separation process can be setup as a compressed
                  sensing problem, where conventional seismic data is
                  reconstructed from simultaneous data via a
                  sparsity-promoting optimization formulation. While
                  the recovery quality of separated data is reasonably
                  well, the recovery process can be computationally
                  expensive due to transform-domain redundancy. In
                  this paper, we present a computationally tractable
                  rank-minimization algorithm to separate simultaneous
                  data volumes. The proposed algorithm is suitable for
                  large-scale seismic data, since it avoids
                  singular-value decompositions and uses a low-rank
                  based factorized formulation instead. Results are
                  illustrated for simulations of simultaneous
                  time-jittered continuous recording for a 3D
                  ocean-bottom cable survey.},
  keywords = {5D, marine, time-jittered acquisition, source separation, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/kumar2016SEGtjm/kumar2016SEGtjm.html}
}


@UNPUBLISHED{louboutin2016SEGocp,
  author = {Mathias Louboutin and Gerard Gorman and Felix J. Herrmann},
  title = {Optimizing the computational performance and maintainability of time-domain modelling---leveraging multiple right-hand-sides},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {Numerical solvers for the wave equation are a key
                  component of Full-Waveform Inversion (FWI) and
                  Reverse-Time Migration (RTM). The main computational
                  cost of a wave-equation solver stems from the
                  computation of the Laplacian at each time step. When
                  using a finite difference discretization this can be
                  characterized as a structured grid computation
                  within Colella's Seven Dwarfs. Independent of the
                  degree of parallelization the performance will be
                  limited by the relatively low operational intensity
                  (number of operations divided by memory traffic) of
                  finite-difference stencils, that is so say that the
                  method is memory bandwidth bound. For this reason
                  many developers have focused on porting their code
                  to platforms that have higher memory bandwidth, such
                  as GPU's, or put significant effort into highly
                  intrusive optimisations. However, these
                  optimisations rarely strike the right performance vs
                  productivity balance as the software becomes less
                  maintainable and extensible. By solving the wave
                  equation for multiple sources/right-hand-sides
                  (RHSs) at once, we overcome this problem arriving at
                  a time-stepping solver with higher operational
                  intensity. In essence, we arrive at this result by
                  turning the usual matrix-vector products into a
                  matrix-matrix products where the first matrix
                  implements the discretized wave equation and each
                  column of the second matrix contain separate
                  wavefields for each given source. By making this
                  relatively minor change to the solver we readily
                  achieved a $\times{2}$ speedup. While we limit
                  ourselves to acoustic modeling, our approach can
                  easily be extended to the anisotropic or elastic
                  cases.},
  keywords = {modelling, 3D, time-domain, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/louboutin2016SEGocp/louboutin2016SEGocp.html}
}


@UNPUBLISHED{oghenekohwo2016GEOPctl,
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Cheap time lapse with distributed Compressive Sensing---exploiting common information among the vintages},
  year = {2016},
  abstract = {Time-lapse seismic has been a powerful, but technically
                  challenging, practice for monitoring changes due to
                  production. Aside from physical changes amongst
                  time-lapse surveys---such as noise and variations in
                  water column velocities, over which we have limited
                  control---executing colocated densely sampled
                  baseline and monitor surveys with high degrees of
                  replicability are expensive propositions. We
                  demonstrate that under “ideal” circumstances, where
                  the surveys are in principle replicable,
                  high-quality pre-stack data can be obtained from
                  cheap randomized subsampled measurements that are
                  observed from nonreplicated surveys. Aside from the
                  obvious economic advantage of removing reliance on
                  replication during acquisition, we find that the
                  recovered densely sampled pre-stack baseline and
                  monitor data actually improve significantly when the
                  acquisitions are not replicated. We achieve this
                  result by using the fact that different time-lapse
                  data share information and that nonreplicated
                  surveys can add information when pre-stack data are
                  recovered jointly. Whenever the time-lapse data
                  exhibit joint structure---i.e., are compressible in
                  some transform domain and share
                  information---sparsity-promoting recovery of the
                  “common component” and “innovations”, with respect
                  to this common component, outperforms independent
                  recovery of both the pre-stack baseline and monitor
                  data. The recovered time-lapse data are of high
                  enough quality to serve as input to extract
                  post-stack attributes used to compute time-lapse
                  differences. Without joint recovery, artifacts---due
                  to the randomized subsampling---lead to
                  deterioration of the degree of repeatability of the
                  time-lapse data. We support our claims by carrying
                  out experiments that collect reliable statistics
                  from thousands of repeated experiments. We also
                  confirm that high degrees of repeatability are
                  achievable for an ocean-bottom cable survey acquired
                  with time-jittered continuous recording.},
  keywords = {acquisition, time-lapse seismic, marine, random sampling, joint recovery method, private},
  note = {Submitted to Geophysics on February 5, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/oghenekohwo2016GEOPctl/oghenekohwo2016GEOPctl.html}
}


@UNPUBLISHED{peters2016SEGprs,
  author = {Bas Peters and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Parallel reformulation of the sequential adjoint-state method},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {This abstract is about reducing the total computation
                  time to solve full-waveform inversion problems. A
                  common problem formulation is a nonlinear
                  least-squares problem where the gradient is computed
                  using the adjoint-state algorithm. This formulation
                  offers parallelism for the computation of gradients
                  for different sources and frequencies. The
                  adjoint-state algorithm itself is sequential however
                  and this is a limiting factor when a lot of compute
                  nodes are available and only a few wavefields need
                  to be computed. This situation occurs when
                  stochastic optimization strategies are used to
                  minimize the objective function. We present a
                  parallel reformulation of the sequential
                  adjoint-state algorithm, which allows the forward-
                  and adjoint wavefields to be computed in
                  parallel. Both algorithms are mathematically
                  equivalent but the parallel version is twice as fast
                  in run time. An important characteristic of the
                  proposed algorithm is that one wavefield needs to be
                  computed per source and one per receiver. These
                  fields can be used to apply the (inverse)
                  Gauss-Newton Hessian to a vector without recomputing
                  wavefields. A 2D example shows that good
                  full-waveform inversion results are obtained, even
                  when a small number of sources and receivers is
                  used.},
  keywords = {numerical linear algebra, PDE-constrained optimization, FWI, parallel computing, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/peters2016SEGprs/peters2016SEGprs.html}
}


@UNPUBLISHED{sharan2016SEGspj,
  author = {Shashin Sharan and Rongrong Wang and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Sparsity-promoting joint microseismic source collocation and source-time function estimation},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {In this work, we propose a new method to simultaneously
                  locate microseismic events (e.g., induced by
                  hydraulic fracturing) and estimate the source
                  signature of these events. We use the method of
                  linearized Bregman. This algorithm focuses unknown
                  sources at their true locations by promoting
                  sparsity along space and at the same time keeping
                  the energy along time in check. We are particularly
                  interested in situations where the microseismic data
                  is noisy, sources have different signatures and we
                  only have access to the smooth background-velocity
                  model. We perform numerical experiments to
                  demonstrate the usability of the proposed method. We
                  also compare our results with full-waveform
                  inversion based microseismic event collocation
                  methods. Our method gives flexibility to
                  simultaneously get a more accurate source image
                  along with an estimate of the source-time function,
                  which carries important information on the rupturing
                  process and source mechanism.},
  keywords = {microseismic, sparse, sources, wave-equation, noise, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/sharan2016SEGspj/sharan2016SEGspj.html}
}


@UNPUBLISHED{wang2016SEGfde,
  author = {Rongrong Wang and Felix J. Herrmann},
  title = {Frequency down-extrapolation with {TV} norm minimization},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {In this work, we present a total-variation (TV)-norm
                  minimization method to perform model-free
                  low-frequency data extrapolation for the purpose of
                  assisting full-waveform inversion. Low-frequency
                  extrapolation is the process of extending reliable
                  frequency bands of the raw data towards the lower
                  end of the spectrum. To this end, we propose to
                  solve a TV-norm based convex optimization problem
                  that has a global minimum and is equipped with a
                  fast solver. The approach takes into account both
                  the sparsity of the reflectivity series associated
                  with a single trace, as well as the inter-trace
                  correlations. A favorable byproduct of this approach
                  is that it allows one to work with coarsely sampled
                  trace along time (as coarse as 0.02s per sample),
                  hence substantially reduces the size of the proposed
                  optimization problem. Last, we show the
                  effectiveness of the method for frequency-domain FWI
                  on the Marmousi model.},
  keywords = {extrapolation, sparse, FWI, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/wang2016SEGfde/wang2016SEGfde.html}
}


@UNPUBLISHED{wason2016GEOPctl,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Cheap time lapse with distributed {Compressive Sensing}---impact on repeatability},
  year = {2016},
  abstract = {Irregular or off-the-grid spatial sampling of sources
                  and receivers is inevitable in field seismic
                  acquisitions. Consequently, time-lapse surveys
                  become particularly expensive since current
                  practices aim to replicate densely sampled surveys
                  for monitoring changes occurring in the reservoir
                  due to hydrocarbon production. We demonstrate that
                  under certain circumstances, high-quality prestack
                  data can be obtained from cheap randomized
                  subsampled measurements that are observed from
                  nonreplicated surveys. We extend our time-jittered
                  marine acquisition to time-lapse surveys by
                  designing acquisition on irregular spatial grids
                  that render simultaneous, subsampled and irregular
                  measurements. Using the fact that different
                  time-lapse data share information and that
                  nonreplicated surveys add information when prestack
                  data are recovered jointly, we recover periodic
                  densely sampled and colocated prestack data by
                  adapting the recovery method to incorporate a
                  regularization operator that maps traces from an
                  irregular spatial grid to a regular periodic
                  grid. The recovery method is, therefore, a combined
                  operation of regularization, interpolation
                  (estimating missing fine-grid traces from subsampled
                  coarse-grid data), and source separation (unraveling
                  overlapping shot records). By relaxing the
                  insistence on replicability between surveys, we find
                  that recovery of the time-lapse difference shows
                  little variability for realistic field scenarios of
                  slightly nonreplicated surveys that suffer from
                  unavoidable natural deviations in spatial sampling
                  of shots (or receivers) and pragmatic
                  compressed-sensing based nonreplicated surveys when
                  compared to the "ideal" scenario of exact
                  replicability between surveys. Moreover, the
                  recovered densely sampled prestack baseline and
                  monitor data improve significantly when the
                  acquisitions are not replicated, and hence can serve
                  as input to extract poststack attributes used to
                  compute time-lapse differences. Our observations are
                  based on numerous experiments conducted for an
                  ocean-bottom cable survey acquired with
                  time-jittered continuous recording assuming source
                  equalization (or same source signature) for the
                  time-lapse surveys and no changes in wave heights,
                  water column velocities or temperature and salinity
                  profiles, etc.},
  keywords = {marine acquisition, time-lapse seismic, off-the-grid recovery, random sampling, joint recovery method, optimization, private},
  note = {Submitted to Geophysics on May 13, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/wason2016GEOPctl/wason2016GEOPctl.html}
}


@UNPUBLISHED{witte2016SEGpve,
  author = {Philipp A. Witte and Christiaan C. Stolk and Felix J. Herrmann},
  title = {Phase velocity error minimizing scheme for the anisotropic pure {P-wave} equation},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {Pure P-wave equations for acoustic modeling in
                  transverse isotropic media are derived by
                  approximating the exact pure Pwave dispersion
                  relation. In this work, we present an alternative
                  approach to the approximate dispersion relation of
                  Etgen and Brandsberg-Dahl, in which we approximate
                  the exact dispersion relation through a polynomial
                  expansion and determine its coefficients by solving
                  a linear least squares problem that minimizes the
                  phase velocity error over the entire range of phase
                  angles. The coefficients are also optimized over a
                  pre-defined range of Thomsen parameters, so that the
                  phase error is small for models with spatially
                  varying anisotropy. Phase velocity error analysis
                  shows that the optimized pure P-wave equation is up
                  to one order of magnitude more accurate than other
                  popular pure P-wave equations, even for highly
                  non-elliptic anisotropy. The optimized equation can
                  be easily turned into a time-domain forward modeling
                  scheme and comparisons of the modeled waveforms with
                  analytical travel times once more illustrate its
                  high accuracy. We also provide an efficient
                  implementation of our approach for 3D tilted TI
                  media that limits the count of fast Fourier
                  transforms per time step to a number that is
                  comparable to other pure P-wave equations.},
  keywords = {anisotropy, modelling, TTI, least squares, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/witte2016SEGpve/witte2016SEGpve.html}
}


@UNPUBLISHED{yang2016SEGtds,
  author = {Mengmeng Yang and Philipp A. Witte and Zhilong Fang and Felix J. Herrmann},
  title = {Time-domain sparsity-promoting least-squares migration with source estimation},
  year = {2016},
  date_submitted = {04/01/2016},
  abstract = {Traditional reverse-time migration (RTM) gives images
                  with wrong amplitudes and low
                  resolution. Least-squares RTM (LS-RTM) on the other
                  hand, is capable of obtaining true-amplitude images
                  as solutions of $\ell_2$-norm norm minimization
                  problems by fitting the synthetic and observed
                  reflection data. The shortcoming of this approach is
                  that solutions of these $\ell_2$ problems are
                  typically smoothed, tend to be overfitted, and
                  computationally too expensive because it requires
                  compared to standard RTM too many iterations. By
                  working with randomized subsets of data only, the
                  computational costs of LS-RTM can be brought down to
                  an acceptable level while producing artifact-free
                  high-resolution images without overfitting the
                  data. While initial results of these "compressive
                  imaging" methods were encouraging various open
                  issues remain including guaranteed convergence,
                  algorithmic complexity of the solver, and lack of
                  on-the-fly source estimation for LS-RTMs with
                  wave-equation solvers based on time-stepping. By
                  including on-the-fly source-time function estimation
                  into the method of Linearized Bregman (LB), on which
                  we reported before, we tackle all these issues
                  resulting in a easy-to-implement algorithm that
                  offers flexibility in the trade-off between the
                  number of iterations and the number of wave-equation
                  solves per iteration for a fixed total number of
                  wave-equation solves. Application of our algorithm
                  on a 2D synthetic shows that we are able to obtain
                  high-resolution images, including accurate estimates
                  of the wavelet, for a single pass through the
                  data. The produced image, which is by virtue of the
                  inversion deconvolved with respect to the wavelet,
                  is roughly of the same quality as the image obtained
                  given the correct source function.},
  keywords = {time domain, least squares, RTM, source estimation, SEG, private},
  note = {submitted to the SEG conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2016/yang2016SEGtds/yang2016SEGtds.html}
}


%-----2015-----%

@UNPUBLISHED{vanleeuwen2015GPWEMVA,
  author = {Tristan van Leeuwen and Rajiv Kumar and Felix J. Herrmann},
  title = {Enabling affordable omnidirectional subsurface extended image volumes via probing},
  year = {2015},
  abstract = {Image gathers as a function of subsurface offset are an
                  important tool for the inference of rock properties
                  and velocity analysis in areas of complex
                  geology. Traditionally, these gathers are thought of
                  as multidimensional correlations of the source and
                  receiver wavefields. The bottleneck in computing
                  these gathers lies in the fact that one needs to
                  store, compute, and correlate these wavefields for
                  all shots in order to obtain the desired image
                  gathers. Therefore, the image gathers are typically
                  only computed for a limited number of subsurface
                  points and for a limited range of subsurface
                  offsets, which may cause problems in complex
                  geological areas with large geologic dips. We
                  overcome increasing computational and storage costs
                  of extended image volumes by introducing a
                  formulation that avoids explicit storage and removes
                  the customary and expensive loop over shots, found
                  in conventional extended imaging. As a result, we
                  end up with a matrix-vector formulation from which
                  different image gathers can be formed and with which
                  amplitude-versus-angle and wave-equation migration
                  velocity analyses can be performed without requiring
                  prior information on the geologic dips. Aside from
                  demonstrating the formation of two-way extended
                  image gathers for different purposes and at greatly
                  reduced costs, we also present a new approach to
                  conduct automatic wave-equation based
                  migration-velocity analysis. Instead of focussing in
                  particular offset directions and preselected subsets
                  of subsurface points, our method focuses every
                  subsurface point for all subsurface offset
                  directions using a randomized probing technique. As
                  a consequence, we obtain good velocity models at low
                  cost for complex models without the need to provide
                  information on the geologic dips.},
  keywords = {migration velocity analysis, AVA, stochastic optimization, private},
  note = {Submitted to Geophysical Prospecting on May 6. Revision 1 submitted on November 30.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2015/vanleeuwen2015GPWEMVA/vanleeuwen2015GPWEMVA.pdf}
}

