% This file was created with JabRef 2.9.
% Encoding: MacRoman

@UNPUBLISHED{miao2013CSEGaospsa,
  author = {Lina Miao and Felix J. Herrmann},
  title = {Acceleration on Sparse Promoting Seismic Applications},
  booktitle = {CSEG},
  year = {2013},
  abstract = {Sparse promoting oriented problems are never new in
                  seismic applications. Back in 1970s, geophysicists
                  had well exploited the robustness of sparse
                  solutions. Moreover, with the emerging usage of
                  compressed sensing in recent years, sparse recovery
                  have been favored in dealing with 'curse of
                  dimensionality' in various seismic field
                  acquisition, data processing, and imaging
                  applications. Although sparsity has provided a
                  promising approach, solving for it presents a big
                  challenge. How to work efficiently with the
                  extremely large-scale seismic problem, and how to
                  improve the convergence rate reducing computation
                  time are most frequently asked questions in this
                  content. In this abstract, the author proposed a new
                  algorithm -- PQN$\ell_1$, trying to address those
                  questions. One example on seismic data processing is
                  included.},
  keywords = {CSEG,sparsity-promotion,SPG$\ell_1$,Projected Quasi Newton,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/CSEG/2013/miao2013CSEGaospsa/miao2013CSEGaospsa.pdf}
}


@UNPUBLISHED{petrenko2013HPCSsaoc,
  date_submitted = {04/03/2013},
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of CARP, an iterative linear solver
                  and preconditioner},
  booktitle = {HPCS},
  year = {2013},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {HPCS,Helmholtz equation, Kaczmarz, software, wave
                  propagation, frequency-domain, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/HPCS/2013/petrenko2013HPCSsaoc/petrenko2013HPCSsaoc.pdf}
}


@UNPUBLISHED{petrenko2013SEGsaoc,
  date_submitted = {04/03/2013},
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of CARP, an iterative linear solver
                  and preconditioner},
  booktitle = {SEG},
  year = {2013},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {SEG,Helmholtz equation, Kaczmarz, software, wave
                  propagation, frequency-domain, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/petrenko2013SEGsaoc/petrenko2013SEGsaoc.pdf}
}


@UNPUBLISHED{dasilva2013SEGhtuck,
  date_submitted = {04/03/2013},
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Structured tensor missing-trace interpolation in the Hierarchical Tucker format},
  booktitle = {SEG},
  year = {2013},
  abstract = {Owing to the large scale and dimensionality of a 3D
                  seismic experiment, acquiring fully-sampled data
                  according to the Nyquist criterion is an exceedingly
                  arduous and cost-prohibitive task. In this paper, we
                  develop tools to interpolate 5D seismic volumes with
                  randomly missing sources or receivers using a
                  relatively novel tensor format known as the
                  Hierarchical Tucker (HT) format. By exploiting the
                  underlying smooth structure of HT tensors,
                  specifically its smooth manifold structure, we
                  develop solvers which are fast, immediately
                  parallelizable, and SVD-free, making these solvers
                  amenable to large-scale problems where SVD-based
                  projection methods are far too costly. We also build
                  on intuition of multidimen- sional sampling from the
                  perspective of matrix-completion and demonstrate the
                  ability of our algorithms to recover frequency
                  slices even amidst very high levels of source
                  subsampling on a synthetic large-scale 3D North Sea
                  dataset.},
  keywords = {SEG, hierarchical tucker, structured tensor, tensor
                  interpolation, differential geometry, riemannian
                  optimization, gauss newton, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/dasilva2013SEGhtuck/dasilva2013SEGhtuck.pdf}
}


@UNPUBLISHED{kumar2013SEGHSS,
  date_submitted = {04/03/2013},
  author = {Rajiv Kumar and Hassan Mansour and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Reconstruction of seismic wavefields via low-rank matrix
                  factorization in the hierarchical-separable matrix
                  representation},
  booktitle = {SEG},
  year = {2013},
  abstract = {Recent developments in matrix rank optimization have
                  allowed for new computational approaches in the
                  field of seismic data interpolation. In this paper,
                  we propose an approach for seismic data
                  interpolation which incorporates the Hierarchical
                  Semi-Separable Structure (HSS) inside
                  rank-regularized least-squares formulations for the
                  missing-trace interpolation problem. The proposed
                  approach is suitable for large scale problems, since
                  it avoids SVD computations and uses a low-rank
                  factorized formulation instead. We illustrate the
                  advantages of the new HSS approach by interpolating
                  a seismic line from the Gulf of Suez and compare the
                  reconstruction with conventional rank minimization.},
  keywords = {SEG,interpolation,HSS, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/kumar2013SEGHSS/kumar2013SEGHSS.pdf} 
}


@UNPUBLISHED{kumar2013SEGAVA,
  date_submitted = {04/03/2013},
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann},
  title = {AVA analysis and geological dip estimation via two-way
                  wave-equation based extended images},
  booktitle = {SEG},
  year = {2013},
  abstract = {In this paper, we present an efficient way to compute
                  extended images for \emph{all} subsurface offsets
                  without explicitly calculating the source and
                  receiver wavefields for \emph{all} the
                  sources. Because the extended images contain
                  \emph{all} possible subsurface offsets, we compute
                  the angle-domain image gathers by selecting the
                  subsurface offset that is aligned with the local
                  dip. We also propose a method to compute the local
                  dip information directly from common-image-point
                  gathers. To assess the quality of the angle-domain
                  common-image-points gathers we compute the
                  angle-dependent reflectivity coefficients and
                  compare them with theoretical reflectivity
                  coefficients yielded by the (linearized) Zoeppritz
                  equations for a few synthetic models.},
  keywords = {SEG,AVA,dip,wave-equation,extended images, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/kumar2013SEGAVA/kumar2013SEGAVA.pdf} 
}


@UNPUBLISHED{li2013SEGodmvdaiedwawe,
  date_submitted = {04/03/2013},
  author = {Xiang Li and Anais Tamalet and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Optimization driven model-space versus data-space
                  approaches to invert elastic data with the acoustic
                  wave equation},
  booktitle = {SEG},
  year = {2013},
  abstract = {Inverting data with elastic phases using an acoustic
                  wave equation can lead to erroneous results,
                  especially when the number of iterations is too
                  high, which may lead to over fitting the
                  data. Several approaches have been proposed to
                  address this issue. Most commonly, people apply
                  "data-independent" filtering operations that are
                  aimed to deemphasize the elastic phases in the data
                  in favor of the acoustic phases. Examples of this
                  approach are nested loops over offset range and
                  Laplace parameters. In this paper, we discuss two
                  complementary optimization-driven methods where the
                  minimization process decides adaptively which of the
                  data or model components are consistent with the
                  objective. Specifically, we compare the Student's t
                  misfit function as the data-space alternative and
                  curvelet-domain sparsity promotion as the
                  model-space alternative. Application of these two
                  methods to a realistic synthetic lead to comparable
                  results that we believe can be improved by combining
                  these two methods.},
  keywords = {SEG,full-waveform inversion,elastic,least-squares,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/li2013SEGodmvdaiedwawe/li2013SEGodmvdaiedwawe.pdf} 
}


@UNPUBLISHED{oghenekohwo2013SEGtlswrs,
  date_submitted = {04/03/2013},
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Time-lapse seismics with randomized sampling},
  booktitle = {SEG},
  year = {2013},
  abstract = {In time-lapse or 4D seismics, repeatability of the
                  acquisition is a very crucial step, as we do not
                  want spurious events that are not there. In this
                  paper, we propose an approach which avoids any
                  requirement to repeat the surveys, by using
                  randomized sampling technique which allows us to be
                  more efficient in the acquisition. Our method
                  applies to sampling data using ocean bottom nodes
                  (OBN) as receivers. We test the efficacy of our
                  proposed randomized acquisition geometry for
                  time-lapse survey on two different models. In the
                  first example, model properties does not change with
                  time, while in the second example, model exhibit a
                  time-lapse effect which may be caused by the
                  migration of fluid within the reservoir. We perform
                  two types of randomized sampling - uniform
                  randomized sampling and jittered sampling to
                  visualize the effects of non-repeatability in
                  time-lapse survey. We observe that jittered
                  randomized sampling is a more efficient method
                  compared to randomized sampling, due to it's
                  requirement to control the maximum spacing between
                  the receivers. The results are presented, in the
                  image space, as a least-squares migration of the
                  model perturbation and they are shown for a subset
                  of a synthetic model - the Marmousi model},
  keywords = {SEG,acquisition,time-lapse,migration,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/oghenekohwo2013SEGtlswrs/oghenekohwo2013SEGtlswrs.pdf} 
}


@UNPUBLISHED{tu2013SEGldi,
  date_submitted = {04/03/2013},
  author = {Ning Tu and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Limitations of the deconvolutional imaging condition for
                  two-way propagators},
  booktitle = {SEG},
  year = {2013},
  abstract = {The deconvolutional imaging condition has gained wide
                  attention in recent years, as it is often used to
                  image surface-related multiples. However, we noticed
                  on close inspection that this condition was derived
                  from one-way propagation principles. Now that
                  two-way wave-equation based simulations have become
                  more affordable , we revisit the deconvolutional
                  imaging condition and reveal its limitations for
                  two-way propagators. First, it can distort the image
                  due to receiver-side propagation effects. Second,
                  when used to image surface-related multiples, it is
                  not capable of removing all interfering phantom
                  reflectors.},
  keywords = {SEG,migration,inversion,multiples,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/tu2013SEGldi/tu2013SEGldi.pdf}
}


@UNPUBLISHED{tu2013SEGcle,
  date_submitted = {04/03/2013},
  author = {Ning Tu and Xiang Li and Felix J. Herrmann},
  title = {Controlling linearization errors in $\ell_1$ regularized
                  inversion by rerandomization},
  booktitle = {SEG},
  year = {2013},
  abstract = {Linearized inversion is a data fitting procedure that
                  tries to match the observed seismic data with data
                  predicted by linearized modelling. In practice, the
                  observed data is not necessarily in the column space
                  of the linearized modelling operator. This can be
                  caused by lack of an accurate background velocity
                  model or by coherent noises not explained by
                  linearized modelling. Through carefully designed
                  experiments, we ob- serve that a moderate data
                  mismatch does not pose an issue if we can use all
                  the data in the inversion. However, artifacts do
                  arise from the mismatch when randomized
                  dimensionality reduction techniques are adopted to
                  speed up the inversion. To stabilize the inversion
                  for dimensionality reduction with randomized source
                  aggregates, we propose to rerandomize by drawing
                  independent simultaneous sources occasionally during
                  the inversion. The effect of this rerandomization is
                  remarkable because it results in virtually
                  artifact-free images at a cost comparable to a
                  single reverse-time migration. Implications of our
                  method are profound because we are now able to
                  resolve fine-scale steep subsalt features in a
                  computationally feasible manner.},
  keywords = {SEG,sparsity,inversion,rerandomization,message passing,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/tu2013SEGcle/tu2013SEGcle.pdf}
}


@UNPUBLISHED{wason2013SEGtjo,
  date_submitted = {04/03/2013},
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Time-jittered ocean bottom seismic acquisition},
  booktitle = {SEG},
  year = {2013},
  abstract = {Leveraging ideas from the field of compressed sensing,
                  we show how simultaneous or blended acquisition can
                  be setup as a -- compressed sensing problem. This
                  helps us to design a pragmatic \emph{time-jittered}
                  marine acquisition scheme where multiple source
                  vessels sail across an ocean-bottom array firing
                  airguns at -- jittered source locations and
                  instances in time, resulting in better spatial
                  sampling, and speedup acquisition. Furthermore, we
                  can significantly impact the reconstruction quality
                  of conventional seismic data (from jittered data)
                  and demonstrate successful recovery by sparsity
                  promotion. In contrast to random (under)sampling,
                  acquisition via jittered (under)sampling helps in
                  controlling the maximum gap size, which is a
                  practical requirement of wavefield reconstruction
                  with localized sparsifying transforms. Results are
                  illustrated with simulations of \emph{time-jittered}
                  marine acquisition, which translates to jittered
                  source locations for a given speed of the source
                  vessel, for two source vessels.}, 
  keywords = {SEG,acquisition, marine, OBC, jittered sampling,
                  blending, deblending, interpolation, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/wason2013SEGtjo/wason2013SEGtjo.pdf} 
}


@UNPUBLISHED{dasilva2013SAMPTAhtuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical Tucker Tensor Optimization - Applications to Tensor Completion},
  booktitle = {SAMPTA},
  year = {2013},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker tensors, an efficient structured
                  tensor format based on recursive subspace
                  factorizations. Using the differential geometric
                  tools presented here, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient, for interpolating tensors in HT
                  format. We also empirically examine the importance
                  of one's choice of data organization in the success
                  of tensor recovery by drawing upon insights from the
                  Matrix Completion literature. Using these
                  algorithms, we recover various seismic data sets
                  with randomly missing source pairs.},
  keywords = {SAMPTA, hierarchical tucker, structured tensor, tensor
                  interpolation, differential geometry, riemannian
                  optimization},
  month = {02/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2013/dasilva2013SAMPTAhtuck/dasilva2013SAMPTAhtuck.pdf}
}


@UNPUBLISHED{dasilva2013EAGEhtucktensor,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical Tucker Tensor Optimization - Applications to 4D Seismic Data Interpolation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {In this work, we develop optimization algorithms on the
                  manifold of Hierarchical Tucker (HT) tensors, an
                  extremely efficient format for representing
                  high-dimensional tensors exhibiting particular
                  low-rank structure. With some minor alterations to
                  existing theoretical developments, we develop an
                  optimization framework based on the geometric
                  understanding of HT tensors as a smooth manifold, a
                  generalization of smooth curves/surfaces. Building
                  on the existing research of solving optimization
                  problems on smooth manifolds, we develop Steepest
                  Descent and Conjugate Gradient methods for HT
                  tensors. The resulting algorithms converge quickly,
                  are immediately parallelizable, and do not require
                  the computation of SVDs. We also extend ideas about
                  favourable sampling conditions for missing-data
                  recovery from the field of Matrix Completion to
                  Tensor Completion and demonstrate how the
                  organization of data can affect the success of
                  recovery. As a result, if one has data with randomly
                  missing source pairs, using these ideas, coupled
                  with an efficient solver, one can interpolate
                  large-scale seismic data volumes with missing
                  sources and/or receivers by exploiting the
                  multidimensional dependencies in the data. We are
                  able to recover data volumes amidst extremely high
                  subsampling ratios (in some cases, > 75%) using this
                  approach.},
  keywords = {EAGE, structured tensor, 3D data interpolation, riemannian optimization},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/dasilva2013EAGEhtucktensor/dasilva2013EAGEhtucktensor.pdf}
}


@UNPUBLISHED{kumar2013EAGEsind,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Seismic data interpolation and denoising using SVD-free low-rank matrix factorization},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Recent developments in rank optimization have allowed
                  new approaches for seismic data interpolation and
                  denoising. In this paper, we propose an approach for
                  simultaneous seismic data interpolation and
                  denoising using robust rank-regularized
                  formulations. The proposed approach is suitable for
                  large scale problems, since it avoids SVD
                  computations by using factorized formulations. We
                  illustrate the advantages of the new approach using
                  a seismic line from Gulf of Suez and 5D synthetic
                  seismic data to obtain high quality results for
                  interpolation and denoising, a key application in
                  exploration geophysics.},
  keywords = {EAGE, interpolation, denoising},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/kumar2013EAGEsind/kumar2013EAGEsind.pdf}
}


@UNPUBLISHED{lin2013EAGEcsd,
  author = {Tim T. Y. Lin and Felix J. Herrmann},
  title = {Cosparse seismic data interpolation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Many modern seismic data interpolation and redatuming
                  algorithms rely on the promotion of transform-domain
                  sparsity for high-quality results. Amongst the large
                  diversity of methods and different ways of realizing
                  sparse reconstruction lies a central question that
                  often goes unaddressed: is it better for the
                  transform-domain sparsity to be achieved through
                  explicit construction of sparse representations
                  (e.g., by thresholding of small transform-domain
                  coefficients), or by demanding that the algorithm
                  return physical signals which produces sparse
                  coefficients when hit with the forward transform?
                  Recent results show that the two approaches give
                  rise to different solutions when the transform is
                  redundant, and that the latter approach imposes a
                  whole new class of constraints related to where the
                  forward transform produces zero coefficients. From
                  this framework, a new reconstruction algorithm is
                  proposed which may allow better reconstruction from
                  subsampled signaled than what the sparsity
                  assumption alone would predict. In this work we
                  apply the new framework and algorithm to the case of
                  seismic data interpolation under the curvelet
                  domain, and show that it admits better
                  reconstruction than some existing L1 sparsity-based
                  methods derived from compressive sensing for a range
                  of subsampling factors.},
  keywords = {EAGE, cosparsity, interpolation, curvelet, algorithm, optimization},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/lin2013EAGEcsd/lin2013EAGEcsd.pdf}
}


@UNPUBLISHED{tu2013EAGElsm,
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast least-squares migration with multiples and source estimation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {The advent of modern computing has made it possible to
                  do seismic imaging using least-squares reverse-time
                  migration. We obtain superior images by solving an
                  optimization problem that recovers the
                  true-amplitude images. However, its success hinges
                  on overcoming several issues, including overwhelming
                  problem size, unknown source wavelet, and
                  interfering coherent events like multiples. In this
                  abstract, we reduce the problem size by using ideas
                  from compressive sensing, and estimate source
                  wavelet by generalized variable projection. We also
                  demonstrate how to invert for subsurface information
                  encoded in surface-related multiples by
                  incorporating the free-surface operator as an areal
                  source in reverse-time migration. Our synthetic
                  examples show that multiples help to improve the
                  resolution of the image, as well as remove the
                  amplitude ambiguity in wavelet estimation.},
  keywords = {EAGE, imaging, sparse, source estimation, multiples},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/tu2013EAGElsm/tu2013EAGElsm.pdf}
}


@UNPUBLISHED{vanleeuwen2013EAGErobustFWI,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Henri Calandra and Felix J. Herrmann},
  title = {In which domain should we measure the misfit for robust full waveform inversion?},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Full-waveform inversion relies on minimizing the
                  difference between observed and modeled data, as
                  measured by some penalty function. A popular choice,
                  of course, is the least-squares penalty. However,
                  when outliers are present in the data, the use of
                  robust penalties such as the Huber or Student's t
                  may significantly improve the results since they put
                  relatively less weight on large residuals. In order
                  for robust penalties to be effective, the outliers
                  must be somehow localized and distinguishable from
                  the good data. We propose to first transform the
                  residual into a domain where the outliers are
                  localized before measuring the misfit with a robust
                  penalty. This is exactly how one would normally
                  devise filters to remove the noise before applying
                  conventional FWI. We propose to merge the two steps
                  and let the inversion process implicitly filter out
                  the noise. Results on a synthetic dataset show the
                  effectiveness of the approach.},
  keywords = {EAGE, full waveform inversion},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/vanleeuwen2013EAGErobustFWI/vanleeuwen2013EAGErobustFWI.pdf}
}


@UNPUBLISHED{wason2013EAGEobs,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Ocean bottom seismic acquisition via jittered sampling},
  booktitle = {EAGE},
  year = {2013},
  abstract = {We present a pragmatic marine acquisition scheme where
                  multiple source vessels sail across an ocean-bottom
                  array firing at airgunsjittered source locations and
                  instances in time. Following the principles of
                  compressive sensing, we can significantly impact the
                  reconstruction quality of conventional seismic data
                  (from jittered data) and demonstrate successful
                  recovery by sparsity promotion. In contrast to
                  random (under)sampling, acquisition via jittered
                  (under)sampling helps in controlling the maximum gap
                  size, which is a practical requirement of wavefield
                  reconstruction with localized sparsifying
                  transforms. Results are illustrated with simulations
                  of time-jittered marine acquisition, which
                  translates to jittered source locations for a given
                  speed of the source vessel, for two source vessels.},
  keywords = {EAGE, acquisition, blended, marine, deblending, interpolation},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/wason2013EAGEobs/wason2013EAGEobs.pdf}
}


@UNPUBLISHED{aravkin2013ICASSPssi,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Ning Tu},
  title = {Sparse seismic imaging using variable projection},
  booktitle = {ICASSP},
  year = {2012},
  abstract = {We consider an important class of signal processing
                  problems where the signal of interest is known to be
                  sparse, and can be recovered from data given
                  auxiliary information about how this data was
                  generated. For example, a sparse green's function
                  may be recovered from seismic experimental data
                  using sparsity optimization when the source
                  signature is known. Unfortunately, in practice this
                  information is often missing, and must be recovered
                  from data along with the signal using deconvolution
                  techniques. In this paper, we present a novel
                  methodology to simulta- neously solve for the sparse
                  signal and auxiliary parameters using a recently
                  proposed variable projection technique. Our main
                  contribution is to combine variable projection with
                  spar- sity promoting optimization, obtaining an
                  efficient algorithm for large-scale sparse
                  deconvolution problems. We demon- strate the
                  algorithm on a seismic imaging example.},
  keywords = {imaging, sparsity, optimization, variable projection},
  month = {12/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2013/aravkin2013ICASSPssi/aravkin2013ICASSPssi.pdf}
}


@UNPUBLISHED{bander2012dre,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  year = {2012},
  abstract = {Data-driven methods---such as the estimation of primaries by sparse
	inversion---suffer from the 'curse of dimensionality' that leads
	to disproportional growth in computational and storage demands when
	moving to realistic 3D field data. To remove this fundamental impediment,
	we propose a dimensionality-reduction technique where the 'data matrix'
	is approximated adaptively by a randomized low-rank factorization.
	Compared to conventional methods, which need for each iteration passage
	through all data possibly requiring on-the-fly interpolation, our
	randomized approach has the advantage that the total number of passes
	is reduced to only one to three. In addition, the low-rank matrix
	factorization leads to considerable reductions in storage and computational
	costs of the matrix multiplies required by the sparse inversion.
	Application of the proposed method to synthetic and real data shows
	that significant performance improvements in speed and memory use
	are achievable at a low computational up-front cost required by the
	low-rank factorization.},
  keywords = {multiples,Processing,Optimization},
  month = {02/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/bander2012dre/bander2012dre.pdf}
}

@UNPUBLISHED{lin2012robustepsi,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust estimation of primaries by sparse inversion via one-norm minimization},
  year = {2012},
  abstract = {Even though contemporary methods for the removal of multiple events
	in seismic data due to a free-surface are built upon reciprocity
	relationships between wavefields, they are often still implemented
	as prediction-subtraction processes. The subtraction process does
	not always compensate for imperfect prediction of multiple events,
	and itself often leads to distortion of primary events. A recently
	proposed method called Estimation of Primaries by Sparse Inversion
	avoids the subtraction process altogether by directly prediction
	the primary impulse response as a collection of band-limited spikes
	under sparsity-regulated wavefield inversion approach. Although it
	can be shown that the correct primary impulse response is obtained
	through the sparsest possible solution, the Estimation of Primaries
	by Sparse Inversion algorithm was not designed to seek such a solution,
	instead depending on a predetermined degree of sparsity as an inversion
	parameter. This leads to imperfect multiple rejection when the sparsity
	is overestimated, and problems with recovering late primary events
	when it is underestimated. In this paper, we propose a new algorithm
	where we make obtaining the sparsest solution our explicit goal.
	Our approach remains a gradient-based approach like the original
	algorithm, but is in turn derived from a new optimization framework
	based on an extended basis pursuit denoising formulation. We show
	that the sparsity-minimizing objective of our formulation enables
	it to operate successfully on a wide variety of synthetic and field
	marine dataset without excessive tweaking of inversion parameters.
	We also demonstrate that Robust EPSI produces a more artifact-free
	impulse response compared to the original algorithm, which has interesting
	implications for broadband seismic applications. Finally we demonstrate
	through field data that recovering the primary impulse response under
	transform domains can significantly improve the recovery of weak
	primary late arrivals, without appreciable change to the underlying
	algorithm.},
  keywords = {multiples, optimization, sparsity, waveform inversion, pareto, biconvex,
	algorithm, EPSI},
  notes = {submitted to Geophysics, March 12, 2012},
  month = {03/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/lin2012robustepsi/lin2012robustepsi.pdf}
}

@UNPUBLISHED{mansour12iwr,
  author = {Hassan Mansour and Felix J. Herrmann and Ozgur Yilmaz},
  title = {Improved wavefield reconstruction from randomized sampling via weighted
	one-norm minimization},
  year = {2012},
  abstract = {Missing-trace interpolation aims to recover the gaps caused by physical
	obstacles or deliberate subsampling to control acquisition costs
	in otherwise regularly-sampled seismic wavefields. While transform-domain
	sparsity promotion has proven to be an effective tool to solve this
	recovery problem, current recovery techniques make no use of a priori
	information on the locations of transform-domain coefficients. In
	this paper, we propose recovery by weighted one-norm minimization,
	which exploits correlations between the locations of significant
	coefficients of different partitions, e.g., shot records, common-offset
	gathers, or frequency slices of the acquired data. We use these correlations
	to define a sequence of 2D curvelet-based recovery problems that
	exploit 3D continuity exhibited by seismic wavefields without relying
	on the highly redundant 3D curvelet transform. To illustrate the
	performance of our weighted algorithm, we compare recoveries from
	different scenarios of partitioning for a seismic line from the Gulf
	of Suez. These examples demonstrate that our method is superior to
	standard $\ell_1$ minimization in terms of reconstruction quality
	and computational memory requirements.},
  keywords = {Trace interpolation, weighted one-norm minimization, compressed sensing,
	randomized sampling},
  month = {10/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/mansour12iwr/mansour12iwr.pdf}
}

@UNPUBLISHED{vanderneut12irs,
  author = {Joost {van der Neut} and Felix J. Herrmann},
  title = {Interferometric redatuming by sparse inversion},
  year = {2012},
  abstract = {Assuming that exact transmission responses are known between the surface
	and a particular depth level in the subsurface, seismic sources can
	be eﬀectively mapped to that level by a process called interferometric
	redatuming. After redatuming, the obtained waveﬁelds can be used
	for imaging below this particular depth level. Interferometric redatuming
	consists of two steps, namely (i) the decomposition of the observed
	waveﬁelds into up- and down-going constituents and (ii) a multidimensional
	deconvolution of the up- and downgoing waveﬁelds. While this method
	works in theory, sensitivity to noise and artifacts due to incomplete
	acquisition call for a diﬀerent formulation. In this letter, we
	demonstrate the beneﬁts of formulating the two steps that undergird
	interferometric redatuming in terms of a transform-domain sparsity-promoting
	program. By exploiting compressibility of seismic waveﬁelds in
	the curvelet domain, we not only become robust with respect to noise
	but we are also able to remove certain artifacts while preserving
	the frequency content. These improvements lead to a better image
	of the target from the redatumed data.},
  keywords = {Controlled source seismology, Interferometry, Inverse theory},
  month = {10/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/vanderneut12irs/vanderneut12irs.pdf}
}

@UNPUBLISHED{vanleeuwen2012smii,
  author = {Tristan {van Leeuwen}},
  title = {A parallel matrix-free framework for frequency-domain seismic modelling,
	imaging and inversion in Matlab},
  year = {2012},
  abstract = {I present a parallel matrix-free framework for frequency-domain seismic
	modeling, imaging and inversion. The framework provides basic building
	blocks for designing and testing optimization-based formulations
	of both linear and non-linear seismic in- verse problems. By overloading
	standard linear-algebra operations, such as matrix- vector multiplications,
	standard optimization packages can be used to work with the code
	without any modification. This leads to a scalable testbed on which
	new methods can be rapidly prototyped and tested on medium-sized
	2D problems. I present some numerical examples on both linear and
	non-linear seismic inverse problems.},
  keywords = {Seismic imaging,optimization,Matlab,object-oriented programming},
  month = {07/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/vanleeuwen2012smii/vanleeuwen2012smii.pdf}
}

