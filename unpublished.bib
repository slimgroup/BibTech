% This file was created with JabRef 2.9.
% Encoding: MacRoman

@UNPUBLISHED{herrmann2013ffwi,
  author   = {Felix J. Herrmann and Andrew J. Calvert and Ian Hanlon
                  and Mostafa Javanmehri and Rajiv Kumar and Tristan
                  van Leeuwen and Xiang Li and Brendan Smithyman and
                  Eric Takam Takougang and Haneet Wason},
  title    = {Frugal full-waveform inversion: from theory to a practical algorithm},
  year     = {2013},
  institution = {UBC},
  abstract = {As conventional oil and gas fields are maturing, our
                  profession is challenged to come up with the
                  next-generation of more and more sophisticated
                  exploration tools. In exploration seismology this
                  trend has led to the emergence of wave-equation
                  based inversion technologies such as reverse-time
                  migration and full-waveform inversion. While
                  significant progress has been made in wave-equation
                  based inversion, major challenges remain in the
                  development of robust and computationally feasible
                  workflows that give reliable results in
                  geophysically challenging areas that may include
                  ultra-low shear velocity zones or high-velocity
                  salt. Moreover, sub-salt production carries risks
                  that needs mitigation, which raises the bar from
                  creating sub-salt images to inverting for sub-salt
                  overpressure. Amongst the many challenges that
                  wave-equation based inversion faces, we focus in
                  this contribution on reducing the excessive
                  computational costs of full-waveform inversion
                  (FWI). We accomplish these cost reductions by using
                  modern techniques from machine learning and
                  compressive sensing. Contrary to many
                  implementations of wave-equation based inversion, we
                  propose a methodology where we do not insist on
                  using all data—i.e., looping over all sources, to
                  calculate the velocity model updates. Instead, we
                  rely on a formulation that only calls for more data
                  and more accuracy in the wave simulations as needed
                  by the inversion. This leads to major savings in
                  particular in the beginning when we are far from the
                  solution. Since this approach reduces the
                  computational costs significantly, we open the way
                  to test different scenarios or to include more
                  sophisticated regularization. Without this cost
                  reduction, these would both be computationally
                  infeasible. The paper is organized as
                  follows. First, we introduce the basics of
                  full-waveform inversion, followed by stochastic
                  sampling techniques to reduce the computational
                  costs. Next, we propose an adaptive scheme that
                  selects the required sample size–i.e, the number of
                  source experiments that partake in the inversion,
                  and forward modeling accuracy. Recent results on the
                  Chevron Gulf of Mexico dataset are discussed next
                  followed by a discussion on challenges of FWI and
                  possible roads ahead.},
  keywords = {waveform inversion, optimization, private},
  month    = {05/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2013/herrmann2013ffwi/herrmann2013ffwi.html}
}


@UNPUBLISHED{vanLeeuwen2013Penalty1,
  author   = {Tristan van Leeuwen and Felix J. Herrmann},
  title    = {Mitigating local minima in full-waveform inversion by expanding the search space},
  year     = {2013},
  institution = {UBC},
  abstract = {Wave-equation based inversions, such as full-waveform
                  inversion, are challenging because of their
                  computational costs, memory requirements, and
                  reliance on accurate initial models. To confront
                  these issues, we propose a novel formulation of
                  full-waveform inversion based on a penalty
                  method. In this formulation, the objective function
                  consists of a data-misfit term and a penalty term
                  which measures how accurately the wavefields satisfy
                  the wave-equation. Because we carry out the
                  inversion over a larger search space, including both
                  the model and synthetic wavefields, our approach
                  suffers less from local minima. Our main
                  contribution is the development of an efficient
                  optimization scheme that avoids having to store and
                  update the wavefields by explicit
                  elimination. Compared to existing optimization
                  strategies for full-waveform inversion, our method
                  differers in two main aspects; i. The wavefields are
                  solved from an augmented wave-equation, where the
                  solution is forced to solve the wave-equation and
                  fit the observed data, ii. no adjoint wavefields are
                  required to update the model, which leads to
                  significant computational savings. We demonstrate
                  the validity of our approach by carefully selected
                  examples and discuss possible extensions and future
                  research.},
  keywords = {waveform inversion, optimization, private},
  month    = {04/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2013/vanLeeuwen2013Penalty1/vanLeeuwen2013Penalty1.pdf}
}


@UNPUBLISHED{vanLeeuwen2013Penalty2,
  author   = {Tristan van Leeuwen and Felix J. Herrmann},
  title    = {A penalty method for PDE-constrained optimization},
  year     = {2013},
  institution = {UBC},
  abstract = {We present a method for solving PDE constrained
                  optimization problems based on a penalty
                  formulation. This method aims to combine advantages
                  of both full-space and reduced methods by exploiting
                  a large search-space (consisting of both control and
                  state variables) while allowing for an efficient
                  implementation that avoids storing and updating the
                  state-variables. This leads to a method that has
                  roughly the same per-iteration complexity as
                  conventional reduced approaches while dening an
                  objective that is less non-linear in the control
                  variable by implicitly relaxing the constraint. We
                  apply the method to a seismic inverse problem where
                  it leads to a particularly ecient implementation
                  when compared to a conventional reduced approach as
                  it avoids the use of adjoint
                  state-variables. Numerical examples illustrate the
                  approach and suggest that the proposed formulation
                  can indeed mitigate some of the well-known problems
                  with local minima in the seismic inverse problem.},
  keywords = {waveform inversion, optimization, private},
  month    = {04/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2013/vanLeeuwen2013Penalty2/vanLeeuwen2013Penalty2.pdf}
}


@UNPUBLISHED{vanLeeuwen20133DFWI,
  author   = {Tristan van Leeuwen and Felix J. Herrmann},
  title    = {3D frequency-domain seismic inversion using a row-projected helmholtz solver},
  year     = {2013},
  institution = {UBC},
  abstract = {Seismic waveform inversion aims at obtaining detailed
                  estimates of subsurface medium parameters, such as
                  soundspeed, from seismic data. A formulation in the
                  frequency-domain leads to an optimization problem
                  constrained by a Helmholtz equation with many
                  right-hand-sides. Application of this technique in
                  3D precludes the use of factorization techniques to
                  solve the Helmholtz equation due the large number of
                  gridpoints and the bandwidth of the matrix. While
                  more attractive from the perspective of memory use,
                  the computations costs of iterative methods scale
                  linearly with the number of right-hand-sides. Many
                  sophisticated preconditioned iterative techniques
                  have been developed for the Helmholtz equation,
                  however, they often include model-specific tuning
                  parameters and are thus not very attractive for
                  inversion since the medium parameters change from
                  one iteration to the next. In this paper, we propose
                  a method for 3D seismic waveform inversion that
                  addresses both the need to efficiently solve the
                  Helmholtz equation as well as the computational cost
                  induced by the many right-hand-sides. To solve the
                  Helmholtz equation, we consider a simple generic
                  preconditioned iterative method (CARP-CG) that is
                  well-suited for inversion because of its
                  robustness. We extend this method to a
                  block-iterative method that can efficiently handle
                  multiple right-hand sides. To reduce the
                  computational cost of of the overall optimization
                  procedure, we use recently proposed techniques from
                  stochastic optimization that allow us to work with
                  approximate gradient information. These
                  approximations are obtained by evaluating only a
                  small portion of the right-hand sides and/or by
                  solving the PDE approximately. We propose heuristics
                  to adaptively determine the required accuracy of the
                  PDE solves and the sample-size and illustrate the
                  algorithms on synthetic benchmark models.},
  keywords = {waveform inversion, optimization, private},
  month    = {04/2013},
  url      = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2013/vanLeeuwen20133DFWI/vanLeeuwen20133DFWI.pdf}
}


@UNPUBLISHED{dasilva2013SEGhtuck,
  date_submitted = {04/03/2013},
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Structured tensor missing-trace interpolation in the Hierarchical Tucker format},
  booktitle = {SEG},
  year = {2013},
  abstract = {Owing to the large scale and dimensionality of a 3D
                  seismic experiment, acquiring fully-sampled data
                  according to the Nyquist criterion is an exceedingly
                  arduous and cost-prohibitive task. In this paper, we
                  develop tools to interpolate 5D seismic volumes with
                  randomly missing sources or receivers using a
                  relatively novel tensor format known as the
                  Hierarchical Tucker (HT) format. By exploiting the
                  underlying smooth structure of HT tensors,
                  specifically its smooth manifold structure, we
                  develop solvers which are fast, immediately
                  parallelizable, and SVD-free, making these solvers
                  amenable to large-scale problems where SVD-based
                  projection methods are far too costly. We also build
                  on intuition of multidimen- sional sampling from the
                  perspective of matrix-completion and demonstrate the
                  ability of our algorithms to recover frequency
                  slices even amidst very high levels of source
                  subsampling on a synthetic large-scale 3D North Sea
                  dataset.},
  keywords = {SEG, hierarchical tucker, structured tensor, tensor
                  interpolation, differential geometry, riemannian
                  optimization, gauss newton, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/dasilva2013SEGhtuck/dasilva2013SEGhtuck.pdf}
}


@UNPUBLISHED{dasilva2013SAMPTAhtuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical Tucker Tensor Optimization - Applications to Tensor Completion},
  organization = {SAMPTA},
  year = {2013},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker tensors, an efficient structured
                  tensor format based on recursive subspace
                  factorizations. Using the differential geometric
                  tools presented here, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient, for interpolating tensors in HT
                  format. We also empirically examine the importance
                  of one's choice of data organization in the success
                  of tensor recovery by drawing upon insights from the
                  Matrix Completion literature. Using these
                  algorithms, we recover various seismic data sets
                  with randomly missing source pairs.},
  keywords = {SAMPTA, hierarchical tucker, structured tensor, tensor interpolation, differential geometry, riemannian optimization},
  month = {02/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2013/dasilva2013SAMPTAhtuck/dasilva2013SAMPTAhtuck.pdf}
}


@UNPUBLISHED{kumar2013SEGHSS,
  date_submitted = {04/03/2013},
  author = {Rajiv Kumar and Hassan Mansour and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Reconstruction of seismic wavefields via low-rank matrix
                  factorization in the hierarchical-separable matrix
                  representation},
  booktitle = {SEG},
  year = {2013},
  abstract = {Recent developments in matrix rank optimization have
                  allowed for new computational approaches in the
                  field of seismic data interpolation. In this paper,
                  we propose an approach for seismic data
                  interpolation which incorporates the Hierarchical
                  Semi-Separable Structure (HSS) inside
                  rank-regularized least-squares formulations for the
                  missing-trace interpolation problem. The proposed
                  approach is suitable for large scale problems, since
                  it avoids SVD computations and uses a low-rank
                  factorized formulation instead. We illustrate the
                  advantages of the new HSS approach by interpolating
                  a seismic line from the Gulf of Suez and compare the
                  reconstruction with conventional rank minimization.},
  keywords = {SEG,interpolation,HSS, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/kumar2013SEGHSS/kumar2013SEGHSS.pdf} 
}


@UNPUBLISHED{kumar2013SEGAVA,
  date_submitted = {04/03/2013},
  author = {Rajiv Kumar and Tristan van Leeuwen and Felix J. Herrmann},
  title = {AVA analysis and geological dip estimation via two-way
                  wave-equation based extended images},
  booktitle = {SEG},
  year = {2013},
  abstract = {In this paper, we present an efficient way to compute
                  extended images for all subsurface offsets without
                  explicitly calculating the source and receiver
                  wavefields for all the sources. Because the extended
                  images contain all possible subsurface offsets, we
                  compute the angle-domain image gathers by selecting
                  the subsurface offset that is aligned with the local
                  dip. We also propose a method to compute the local
                  dip information directly from common-image-point
                  gathers. To assess the quality of the angle-domain
                  common-image-points gathers we compute the
                  angle-dependent reflectivity coefficients and
                  compare them with theoretical reflectivity
                  coefficients yielded by the (linearized) Zoeppritz
                  equations for a few synthetic models.},
  keywords = {SEG,AVA,dip,wave-equation,extended images, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/kumar2013SEGAVA/kumar2013SEGAVA.pdf} 
}


@UNPUBLISHED{aravkin2013SISCLR,
  date_submitted = {04/30/2013},
  author = {Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {A robust SVD-free approach to matrix completion, with applications to interpolation of large scale data},
  year = {2013},
  abstract = {Recent SVD-free matrix factorization formulations have
                  enabled rank minimization for systems with millions
                  of rows and columns, paving the way for matrix
                  completion in extremely large-scale applications,
                  such as seismic data interpolation. In this paper,
                  we consider matrix completion formulations designed
                  to hit a target data-fitting error level provided by
                  the user, and propose an algorithm that is able to
                  exploit factorized formulations to solve the
                  corresponding optimization problem. Since
                  practitioners typically have strong prior knowledge
                  about target error level, this innovation makes it
                  easy to apply the algorithm in practice, leaving
                  only the factor rank to be determined. We explore
                  the role that rank of the factors plays in our
                  formulation, and show that rank can be easily
                  adjusted as the inversion proceeds. Within the
                  established framework, we then propose two
                  extensions that are highly relevant to solving
                  practical challenges of data interpolation. First,
                  we propose a weighted extension that allows known
                  subspace information to improve the results of
                  matrix completion formulations. We show how this
                  weighting can be used in the context of frequency
                  continuation, an essential aspect to seismic data
                  interpolation. Second, we extend matrix completion
                  formulations to be extremely robust to large
                  measurement errors in the available data. We
                  illustrate the advantages of the basic approach on
                  the Netflix Prize problem using the Movielens (1M)
                  dataset. Then, we use the new method, along with its
                  robust and subspace re-weighted extensions, to
                  obtain high-quality reconstructions for large scale
                  seismic interpolation problems with real data, even
                  in the presence of extreme data contamination.},
  keywords = {interpolation, denoising, robust, SVD-free, private},
  url = {http://arxiv.org/abs/1302.4886} 
}


@UNPUBLISHED{li2013SEGodmvdaiedwawe,
  date_submitted = {04/03/2013},
  author = {Xiang Li and Anais Tamalet and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Optimization driven model-space versus data-space
                  approaches to invert elastic data with the acoustic
                  wave equation},
  booktitle = {SEG},
  year = {2013},
  abstract = {Inverting data with elastic phases using an acoustic
                  wave equation can lead to erroneous results,
                  especially when the number of iterations is too
                  high, which may lead to over fitting the
                  data. Several approaches have been proposed to
                  address this issue. Most commonly, people apply
                  "data-independent" filtering operations that are
                  aimed to deemphasize the elastic phases in the data
                  in favor of the acoustic phases. Examples of this
                  approach are nested loops over offset range and
                  Laplace parameters. In this paper, we discuss two
                  complementary optimization-driven methods where the
                  minimization process decides adaptively which of the
                  data or model components are consistent with the
                  objective. Specifically, we compare the Student's t
                  misfit function as the data-space alternative and
                  curvelet-domain sparsity promotion as the
                  model-space alternative. Application of these two
                  methods to a realistic synthetic lead to comparable
                  results that we believe can be improved by combining
                  these two methods.},
  keywords = {SEG,full-waveform inversion,elastic,least-squares,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/li2013SEGodmvdaiedwawe/li2013SEGodmvdaiedwawe.pdf} 
}


@UNPUBLISHED{oghenekohwo2013SEGtlswrs,
  date_submitted = {04/03/2013},
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Time-lapse seismics with randomized sampling},
  booktitle = {SEG},
  year = {2013},
  abstract = {In time-lapse or 4D seismics, repeatability of the
                  acquisition is a very crucial step, as we do not
                  want spurious events that are not there. In this
                  paper, we propose an approach which avoids any
                  requirement to repeat the surveys, by using
                  randomized sampling technique which allows us to be
                  more efficient in the acquisition. Our method
                  applies to sampling data using ocean bottom nodes
                  (OBN) as receivers. We test the efficacy of our
                  proposed randomized acquisition geometry for
                  time-lapse survey on two different models. In the
                  first example, model properties does not change with
                  time, while in the second example, model exhibit a
                  time-lapse effect which may be caused by the
                  migration of fluid within the reservoir. We perform
                  two types of randomized sampling - uniform
                  randomized sampling and jittered sampling to
                  visualize the effects of non-repeatability in
                  time-lapse survey. We observe that jittered
                  randomized sampling is a more efficient method
                  compared to randomized sampling, due to it's
                  requirement to control the maximum spacing between
                  the receivers. The results are presented, in the
                  image space, as a least-squares migration of the
                  model perturbation and they are shown for a subset
                  of a synthetic model - the Marmousi model},
  keywords = {SEG,acquisition,time-lapse,migration,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/oghenekohwo2013SEGtlswrs/oghenekohwo2013SEGtlswrs.pdf} 
}


@UNPUBLISHED{petrenko2013HPCSsaoc,
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of CARP, an iterative linear solver
                  and preconditioner},
  organization = {HPCS},
  year = {2013},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {HPCS,Helmholtz equation, Kaczmarz, software, wave
                  propagation, frequency-domain},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/HPCS/2013/petrenko2013HPCSsaoc/petrenko2013HPCSsaoc.pdf}
}


@UNPUBLISHED{petrenko2013SEGsaoc,
  date_submitted = {04/03/2013},
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of CARP, an iterative linear solver
                  and preconditioner},
  booktitle = {SEG},
  year = {2013},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {SEG,Helmholtz equation, Kaczmarz, software, wave
                  propagation, frequency-domain, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/petrenko2013SEGsaoc/petrenko2013SEGsaoc.pdf}
}


@UNPUBLISHED{tu2013SEGldi,
  date_submitted = {04/03/2013},
  author = {Ning Tu and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Limitations of the deconvolutional imaging condition for
                  two-way propagators},
  booktitle = {SEG},
  year = {2013},
  abstract = {The deconvolutional imaging condition has gained wide
                  attention in recent years, as it is often used to
                  image surface-related multiples. However, we noticed
                  on close inspection that this condition was derived
                  from one-way propagation principles. Now that
                  two-way wave-equation based simulations have become
                  more affordable , we revisit the deconvolutional
                  imaging condition and reveal its limitations for
                  two-way propagators. First, it can distort the image
                  due to receiver-side propagation effects. Second,
                  when used to image surface-related multiples, it is
                  not capable of removing all interfering phantom
                  reflectors.},
  keywords = {SEG,migration,inversion,multiples,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/tu2013SEGldi/tu2013SEGldi.pdf}
}


@UNPUBLISHED{tu2013SEGcle,
  date_submitted = {04/03/2013},
  author = {Ning Tu and Xiang Li and Felix J. Herrmann},
  title = {Controlling linearization errors in $\ell_1$ regularized
                  inversion by rerandomization},
  booktitle = {SEG},
  year = {2013},
  abstract = {Linearized inversion is a data fitting procedure that
                  tries to match the observed seismic data with data
                  predicted by linearized modelling. In practice, the
                  observed data is not necessarily in the column space
                  of the linearized modelling operator. This can be
                  caused by lack of an accurate background velocity
                  model or by coherent noises not explained by
                  linearized modelling. Through carefully designed
                  experiments, we ob- serve that a moderate data
                  mismatch does not pose an issue if we can use all
                  the data in the inversion. However, artifacts do
                  arise from the mismatch when randomized
                  dimensionality reduction techniques are adopted to
                  speed up the inversion. To stabilize the inversion
                  for dimensionality reduction with randomized source
                  aggregates, we propose to rerandomize by drawing
                  independent simultaneous sources occasionally during
                  the inversion. The effect of this rerandomization is
                  remarkable because it results in virtually
                  artifact-free images at a cost comparable to a
                  single reverse-time migration. Implications of our
                  method are profound because we are now able to
                  resolve fine-scale steep subsalt features in a
                  computationally feasible manner.},
  keywords = {SEG,sparsity,inversion,rerandomization,message passing,private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/tu2013SEGcle/tu2013SEGcle.pdf}
}


@UNPUBLISHED{wason2013SEGtjo,
  date_submitted = {04/03/2013},
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Time-jittered ocean bottom seismic acquisition},
  booktitle = {SEG},
  year = {2013},
  abstract = {Leveraging ideas from the field of compressed sensing,
                  we show how simultaneous or blended acquisition can
                  be setup as a -- compressed sensing problem. This
                  helps us to design a pragmatic time-jittered
                  marine acquisition scheme where multiple source
                  vessels sail across an ocean-bottom array firing
                  airguns at -- jittered source locations and
                  instances in time, resulting in better spatial
                  sampling, and speedup acquisition. Furthermore, we
                  can significantly impact the reconstruction quality
                  of conventional seismic data (from jittered data)
                  and demonstrate successful recovery by sparsity
                  promotion. In contrast to random (under)sampling,
                  acquisition via jittered (under)sampling helps in
                  controlling the maximum gap size, which is a
                  practical requirement of wavefield reconstruction
                  with localized sparsifying transforms. Results are
                  illustrated with simulations of time-jittered
                  marine acquisition, which translates to jittered
                  source locations for a given speed of the source
                  vessel, for two source vessels.}, 
  keywords = {SEG,acquisition, marine, OBC, jittered sampling,
                  blending, deblending, interpolation, private},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2013/wason2013SEGtjo/wason2013SEGtjo.pdf} 
}




@UNPUBLISHED{dasilva2013EAGEhtucktensor,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical Tucker Tensor Optimization - Applications to 4D Seismic Data Interpolation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {In this work, we develop optimization algorithms on the
                  manifold of Hierarchical Tucker (HT) tensors, an
                  extremely efficient format for representing
                  high-dimensional tensors exhibiting particular
                  low-rank structure. With some minor alterations to
                  existing theoretical developments, we develop an
                  optimization framework based on the geometric
                  understanding of HT tensors as a smooth manifold, a
                  generalization of smooth curves/surfaces. Building
                  on the existing research of solving optimization
                  problems on smooth manifolds, we develop Steepest
                  Descent and Conjugate Gradient methods for HT
                  tensors. The resulting algorithms converge quickly,
                  are immediately parallelizable, and do not require
                  the computation of SVDs. We also extend ideas about
                  favourable sampling conditions for missing-data
                  recovery from the field of Matrix Completion to
                  Tensor Completion and demonstrate how the
                  organization of data can affect the success of
                  recovery. As a result, if one has data with randomly
                  missing source pairs, using these ideas, coupled
                  with an efficient solver, one can interpolate
                  large-scale seismic data volumes with missing
                  sources and/or receivers by exploiting the
                  multidimensional dependencies in the data. We are
                  able to recover data volumes amidst extremely high
                  subsampling ratios (in some cases, > 75%) using this
                  approach.},
  keywords = {EAGE, structured tensor, 3D data interpolation, riemannian optimization},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/dasilva2013EAGEhtucktensor/dasilva2013EAGEhtucktensor.pdf}
}


@UNPUBLISHED{kumar2013EAGEsind,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Seismic data interpolation and denoising using SVD-free low-rank matrix factorization},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Recent developments in rank optimization have allowed
                  new approaches for seismic data interpolation and
                  denoising. In this paper, we propose an approach for
                  simultaneous seismic data interpolation and
                  denoising using robust rank-regularized
                  formulations. The proposed approach is suitable for
                  large scale problems, since it avoids SVD
                  computations by using factorized formulations. We
                  illustrate the advantages of the new approach using
                  a seismic line from Gulf of Suez and 5D synthetic
                  seismic data to obtain high quality results for
                  interpolation and denoising, a key application in
                  exploration geophysics.},
  keywords = {EAGE, interpolation, denoising},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/kumar2013EAGEsind/kumar2013EAGEsind.pdf}
}


@UNPUBLISHED{lin2013EAGEcsd,
  author = {Tim T. Y. Lin and Felix J. Herrmann},
  title = {Cosparse seismic data interpolation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Many modern seismic data interpolation and redatuming
                  algorithms rely on the promotion of transform-domain
                  sparsity for high-quality results. Amongst the large
                  diversity of methods and different ways of realizing
                  sparse reconstruction lies a central question that
                  often goes unaddressed: is it better for the
                  transform-domain sparsity to be achieved through
                  explicit construction of sparse representations
                  (e.g., by thresholding of small transform-domain
                  coefficients), or by demanding that the algorithm
                  return physical signals which produces sparse
                  coefficients when hit with the forward transform?
                  Recent results show that the two approaches give
                  rise to different solutions when the transform is
                  redundant, and that the latter approach imposes a
                  whole new class of constraints related to where the
                  forward transform produces zero coefficients. From
                  this framework, a new reconstruction algorithm is
                  proposed which may allow better reconstruction from
                  subsampled signaled than what the sparsity
                  assumption alone would predict. In this work we
                  apply the new framework and algorithm to the case of
                  seismic data interpolation under the curvelet
                  domain, and show that it admits better
                  reconstruction than some existing L1 sparsity-based
                  methods derived from compressive sensing for a range
                  of subsampling factors.},
  keywords = {EAGE, cosparsity, interpolation, curvelet, algorithm, optimization},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/lin2013EAGEcsd/lin2013EAGEcsd.pdf}
}


@UNPUBLISHED{tu2013EAGElsm,
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast least-squares migration with multiples and source estimation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {The advent of modern computing has made it possible to
                  do seismic imaging using least-squares reverse-time
                  migration. We obtain superior images by solving an
                  optimization problem that recovers the
                  true-amplitude images. However, its success hinges
                  on overcoming several issues, including overwhelming
                  problem size, unknown source wavelet, and
                  interfering coherent events like multiples. In this
                  abstract, we reduce the problem size by using ideas
                  from compressive sensing, and estimate source
                  wavelet by generalized variable projection. We also
                  demonstrate how to invert for subsurface information
                  encoded in surface-related multiples by
                  incorporating the free-surface operator as an areal
                  source in reverse-time migration. Our synthetic
                  examples show that multiples help to improve the
                  resolution of the image, as well as remove the
                  amplitude ambiguity in wavelet estimation.},
  keywords = {EAGE, imaging, sparse, source estimation, multiples},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/tu2013EAGElsm/tu2013EAGElsm.pdf}
}


@UNPUBLISHED{vanleeuwen2013EAGErobustFWI,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Henri Calandra and Felix J. Herrmann},
  title = {In which domain should we measure the misfit for robust full waveform inversion?},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Full-waveform inversion relies on minimizing the
                  difference between observed and modeled data, as
                  measured by some penalty function. A popular choice,
                  of course, is the least-squares penalty. However,
                  when outliers are present in the data, the use of
                  robust penalties such as the Huber or Student's t
                  may significantly improve the results since they put
                  relatively less weight on large residuals. In order
                  for robust penalties to be effective, the outliers
                  must be somehow localized and distinguishable from
                  the good data. We propose to first transform the
                  residual into a domain where the outliers are
                  localized before measuring the misfit with a robust
                  penalty. This is exactly how one would normally
                  devise filters to remove the noise before applying
                  conventional FWI. We propose to merge the two steps
                  and let the inversion process implicitly filter out
                  the noise. Results on a synthetic dataset show the
                  effectiveness of the approach.},
  keywords = {EAGE, full waveform inversion},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/vanleeuwen2013EAGErobustFWI/vanleeuwen2013EAGErobustFWI.pdf}
}


@UNPUBLISHED{wason2013EAGEobs,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Ocean bottom seismic acquisition via jittered sampling},
  booktitle = {EAGE},
  year = {2013},
  abstract = {We present a pragmatic marine acquisition scheme where
                  multiple source vessels sail across an ocean-bottom
                  array firing at airgunsjittered source locations and
                  instances in time. Following the principles of
                  compressive sensing, we can significantly impact the
                  reconstruction quality of conventional seismic data
                  (from jittered data) and demonstrate successful
                  recovery by sparsity promotion. In contrast to
                  random (under)sampling, acquisition via jittered
                  (under)sampling helps in controlling the maximum gap
                  size, which is a practical requirement of wavefield
                  reconstruction with localized sparsifying
                  transforms. Results are illustrated with simulations
                  of time-jittered marine acquisition, which
                  translates to jittered source locations for a given
                  speed of the source vessel, for two source vessels.},
  keywords = {EAGE, acquisition, blended, marine, deblending, interpolation},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/wason2013EAGEobs/wason2013EAGEobs.pdf}
}


@UNPUBLISHED{aravkin2013ICASSPssi,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Ning Tu},
  title = {Sparse seismic imaging using variable projection},
  booktitle = {ICASSP},
  year = {2013},
  abstract = {We consider an important class of signal processing
                  problems where the signal of interest is known to be
                  sparse, and can be recovered from data given
                  auxiliary information about how this data was
                  generated. For example, a sparse green's function
                  may be recovered from seismic experimental data
                  using sparsity optimization when the source
                  signature is known. Unfortunately, in practice this
                  information is often missing, and must be recovered
                  from data along with the signal using deconvolution
                  techniques. In this paper, we present a novel
                  methodology to simulta- neously solve for the sparse
                  signal and auxiliary parameters using a recently
                  proposed variable projection technique. Our main
                  contribution is to combine variable projection with
                  spar- sity promoting optimization, obtaining an
                  efficient algorithm for large-scale sparse
                  deconvolution problems. We demon- strate the
                  algorithm on a seismic imaging example.},
  keywords = {imaging, sparsity, optimization, variable projection},
  month = {05/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2013/aravkin2013ICASSPssi/aravkin2013ICASSPssi.pdf}
}


@UNPUBLISHED{bander2012dre,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  year = {2012},
  abstract = {Data-driven methods---such as the estimation of primaries by sparse
	inversion---suffer from the 'curse of dimensionality' that leads
	to disproportional growth in computational and storage demands when
	moving to realistic 3D field data. To remove this fundamental impediment,
	we propose a dimensionality-reduction technique where the 'data matrix'
	is approximated adaptively by a randomized low-rank factorization.
	Compared to conventional methods, which need for each iteration passage
	through all data possibly requiring on-the-fly interpolation, our
	randomized approach has the advantage that the total number of passes
	is reduced to only one to three. In addition, the low-rank matrix
	factorization leads to considerable reductions in storage and computational
	costs of the matrix multiplies required by the sparse inversion.
	Application of the proposed method to synthetic and real data shows
	that significant performance improvements in speed and memory use
	are achievable at a low computational up-front cost required by the
	low-rank factorization.},
  keywords = {multiples,Processing,Optimization},
  month = {02/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/bander2012dre/bander2012dre.pdf}
}

@UNPUBLISHED{lin2012robustepsi,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust estimation of primaries by sparse inversion via one-norm minimization},
  year = {2012},
  abstract = {Even though contemporary methods for the removal of multiple events
	in seismic data due to a free-surface are built upon reciprocity
	relationships between wavefields, they are often still implemented
	as prediction-subtraction processes. The subtraction process does
	not always compensate for imperfect prediction of multiple events,
	and itself often leads to distortion of primary events. A recently
	proposed method called Estimation of Primaries by Sparse Inversion
	avoids the subtraction process altogether by directly prediction
	the primary impulse response as a collection of band-limited spikes
	under sparsity-regulated wavefield inversion approach. Although it
	can be shown that the correct primary impulse response is obtained
	through the sparsest possible solution, the Estimation of Primaries
	by Sparse Inversion algorithm was not designed to seek such a solution,
	instead depending on a predetermined degree of sparsity as an inversion
	parameter. This leads to imperfect multiple rejection when the sparsity
	is overestimated, and problems with recovering late primary events
	when it is underestimated. In this paper, we propose a new algorithm
	where we make obtaining the sparsest solution our explicit goal.
	Our approach remains a gradient-based approach like the original
	algorithm, but is in turn derived from a new optimization framework
	based on an extended basis pursuit denoising formulation. We show
	that the sparsity-minimizing objective of our formulation enables
	it to operate successfully on a wide variety of synthetic and field
	marine dataset without excessive tweaking of inversion parameters.
	We also demonstrate that Robust EPSI produces a more artifact-free
	impulse response compared to the original algorithm, which has interesting
	implications for broadband seismic applications. Finally we demonstrate
	through field data that recovering the primary impulse response under
	transform domains can significantly improve the recovery of weak
	primary late arrivals, without appreciable change to the underlying
	algorithm.},
  keywords = {multiples, optimization, sparsity, waveform inversion, pareto, biconvex,
	algorithm, EPSI},
  notes = {submitted to Geophysics, March 12, 2012},
  month = {03/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/lin2012robustepsi/lin2012robustepsi.pdf}
}

@UNPUBLISHED{mansour12iwr,
  author = {Hassan Mansour and Felix J. Herrmann and Ozgur Yilmaz},
  title = {Improved wavefield reconstruction from randomized sampling via weighted
	one-norm minimization},
  year = {2012},
  abstract = {Missing-trace interpolation aims to recover the gaps caused by physical
	obstacles or deliberate subsampling to control acquisition costs
	in otherwise regularly-sampled seismic wavefields. While transform-domain
	sparsity promotion has proven to be an effective tool to solve this
	recovery problem, current recovery techniques make no use of a priori
	information on the locations of transform-domain coefficients. In
	this paper, we propose recovery by weighted one-norm minimization,
	which exploits correlations between the locations of significant
	coefficients of different partitions, e.g., shot records, common-offset
	gathers, or frequency slices of the acquired data. We use these correlations
	to define a sequence of 2D curvelet-based recovery problems that
	exploit 3D continuity exhibited by seismic wavefields without relying
	on the highly redundant 3D curvelet transform. To illustrate the
	performance of our weighted algorithm, we compare recoveries from
	different scenarios of partitioning for a seismic line from the Gulf
	of Suez. These examples demonstrate that our method is superior to
	standard $\ell_1$ minimization in terms of reconstruction quality
	and computational memory requirements.},
  keywords = {Trace interpolation, weighted one-norm minimization, compressed sensing,
	randomized sampling},
  month = {10/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/mansour2012iwr/mansour2012iwr.pdf}
}


@UNPUBLISHED{vanleeuwen2012smii,
  author = {Tristan {van Leeuwen}},
  title = {A parallel matrix-free framework for frequency-domain seismic modelling,
	imaging and inversion in Matlab},
  year = {2012},
  abstract = {I present a parallel matrix-free framework for frequency-domain seismic
	modeling, imaging and inversion. The framework provides basic building
	blocks for designing and testing optimization-based formulations
	of both linear and non-linear seismic in- verse problems. By overloading
	standard linear-algebra operations, such as matrix- vector multiplications,
	standard optimization packages can be used to work with the code
	without any modification. This leads to a scalable testbed on which
	new methods can be rapidly prototyped and tested on medium-sized
	2D problems. I present some numerical examples on both linear and
	non-linear seismic inverse problems.},
  keywords = {Seismic imaging,optimization,Matlab,object-oriented programming},
  month = {07/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/vanleeuwen2012smii/vanleeuwen2012smii.pdf}
}

