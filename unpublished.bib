% This file was created with JabRef 2.9.
% Encoding: MacRoman

@UNPUBLISHED{dasilva2013SAMPTAhtuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical Tucker Tensor Optimization - Applications to Tensor Completion},
  booktitle = {SAMPTA},
  year = {2013},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker tensors, an efficient structured
                  tensor format based on recursive subspace
                  factorizations. Using the differential geometric
                  tools presented here, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient, for interpolating tensors in HT
                  format. We also empirically examine the importance
                  of one's choice of data organization in the success
                  of tensor recovery by drawing upon insights from the
                  Matrix Completion literature. Using these
                  algorithms, we recover various seismic data sets
                  with randomly missing source pairs.},
  keywords = {hierarchical tucker, structured tensor, tensor interpolation, differential geometry, riemannian optimization, private},
  month = {02/2013},
  date_submitted = {02/07/2013}
}


@UNPUBLISHED{dasilva2013EAGEhtucktensor,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical Tucker Tensor Optimization - Applications to 4D Seismic Data Interpolation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {In this work, we develop optimization algorithms on the
                  manifold of Hierarchical Tucker (HT) tensors, an
                  extremely efficient format for representing
                  high-dimensional tensors exhibiting particular
                  low-rank structure. With some minor alterations to
                  existing theoretical developments, we develop an
                  optimization framework based on the geometric
                  understanding of HT tensors as a smooth manifold, a
                  generalization of smooth curves/surfaces. Building
                  on the existing research of solving optimization
                  problems on smooth manifolds, we develop Steepest
                  Descent and Conjugate Gradient methods for HT
                  tensors. The resulting algorithms converge quickly,
                  are immediately parallelizable, and do not require
                  the computation of SVDs. We also extend ideas about
                  favourable sampling conditions for missing-data
                  recovery from the field of Matrix Completion to
                  Tensor Completion and demonstrate how the
                  organization of data can affect the success of
                  recovery. As a result, if one has data with randomly
                  missing source pairs, using these ideas, coupled
                  with an efficient solver, one can interpolate
                  large-scale seismic data volumes with missing
                  sources and/or receivers by exploiting the
                  multidimensional dependencies in the data. We are
                  able to recover data volumes amidst extremely high
                  subsampling ratios (in some cases, > 75%) using this
                  approach.},
  keywords = {structured tensor, 3D data interpolation, riemannian optimization},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/dasilva2013EAGEhtucktensor/dasilva2013EAGEhtucktensor.pdf}
}


@UNPUBLISHED{kumar2013EAGEsind,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Seismic data interpolation and denoising using SVD-free low-rank matrix factorization},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Recent developments in rank optimization have allowed
                  new approaches for seismic data interpolation and
                  denoising. In this paper, we propose an approach for
                  simultaneous seismic data interpolation and
                  denoising using robust rank-regularized
                  formulations. The proposed approach is suitable for
                  large scale problems, since it avoids SVD
                  computations by using factorized formulations. We
                  illustrate the advantages of the new approach using
                  a seismic line from Gulf of Suez and 5D synthetic
                  seismic data to obtain high quality results for
                  interpolation and denoising, a key application in
                  exploration geophysics.},
  keywords = {interpolation, denoising},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/kumar2013EAGEsind/kumar2013EAGEsind.pdf}
}


@UNPUBLISHED{lin2013EAGEcsd,
  author = {Tim T. Y. Lin and Felix J. Herrmann},
  title = {Cosparse seismic data interpolation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Many modern seismic data interpolation and redatuming
                  algorithms rely on the promotion of transform-domain
                  sparsity for high-quality results. Amongst the large
                  diversity of methods and different ways of realizing
                  sparse reconstruction lies a central question that
                  often goes unaddressed: is it better for the
                  transform-domain sparsity to be achieved through
                  explicit construction of sparse representations
                  (e.g., by thresholding of small transform-domain
                  coefficients), or by demanding that the algorithm
                  return physical signals which produces sparse
                  coefficients when hit with the forward transform?
                  Recent results show that the two approaches give
                  rise to different solutions when the transform is
                  redundant, and that the latter approach imposes a
                  whole new class of constraints related to where the
                  forward transform produces zero coefficients. From
                  this framework, a new reconstruction algorithm is
                  proposed which may allow better reconstruction from
                  subsampled signaled than what the sparsity
                  assumption alone would predict. In this work we
                  apply the new framework and algorithm to the case of
                  seismic data interpolation under the curvelet
                  domain, and show that it admits better
                  reconstruction than some existing L1 sparsity-based
                  methods derived from compressive sensing for a range
                  of subsampling factors.},
  keywords = {cosparsity, interpolation, curvelet, algorithm, optimization},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/lin2013EAGEcsd/lin2013EAGEcsd.pdf}
}


@UNPUBLISHED{tu2013EAGElsm,
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast least-squares migration with multiples and source estimation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {The advent of modern computing has made it possible to
                  do seismic imaging using least-squares reverse-time
                  migration. We obtain superior images by solving an
                  optimization problem that recovers the
                  true-amplitude images. However, its success hinges
                  on overcoming several issues, including overwhelming
                  problem size, unknown source wavelet, and
                  interfering coherent events like multiples. In this
                  abstract, we reduce the problem size by using ideas
                  from compressive sensing, and estimate source
                  wavelet by generalized variable projection. We also
                  demonstrate how to invert for subsurface information
                  encoded in surface-related multiples by
                  incorporating the free-surface operator as an areal
                  source in reverse-time migration. Our synthetic
                  examples show that multiples help to improve the
                  resolution of the image, as well as remove the
                  amplitude ambiguity in wavelet estimation.},
  keywords = {imaging, sparse, source estimation, multiples},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/tu2013EAGElsm/tu2013EAGElsm.pdf}
}


@UNPUBLISHED{vanleeuwen2013EAGErobustFWI,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Henri Calandra and Felix J. Herrmann},
  title = {In which domain should we measure the misfit for robust full waveform inversion?},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Full-waveform inversion relies on minimizing the
                  difference between observed and modeled data, as
                  measured by some penalty function. A popular choice,
                  of course, is the least-squares penalty. However,
                  when outliers are present in the data, the use of
                  robust penalties such as the Huber or Student's t
                  may significantly improve the results since they put
                  relatively less weight on large residuals. In order
                  for robust penalties to be effective, the outliers
                  must be somehow localized and distinguishable from
                  the good data. We propose to first transform the
                  residual into a domain where the outliers are
                  localized before measuring the misfit with a robust
                  penalty. This is exactly how one would normally
                  devise filters to remove the noise before applying
                  conventional FWI. We propose to merge the two steps
                  and let the inversion process implicitly filter out
                  the noise. Results on a synthetic dataset show the
                  effectiveness of the approach.},
  keywords = {full waveform inversion},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/vanleeuwen2013EAGErobustFWI/vanleeuwen2013EAGErobustFWI.pdf}
}


@UNPUBLISHED{wason2013EAGEobs,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Ocean bottom seismic acquisition via jittered sampling},
  booktitle = {EAGE},
  year = {2013},
  abstract = {We present a pragmatic marine acquisition scheme where
                  multiple source vessels sail across an ocean-bottom
                  array firing at airgunsjittered source locations and
                  instances in time. Following the principles of
                  compressive sensing, we can significantly impact the
                  reconstruction quality of conventional seismic data
                  (from jittered data) and demonstrate successful
                  recovery by sparsity promotion. In contrast to
                  random (under)sampling, acquisition via jittered
                  (under)sampling helps in controlling the maximum gap
                  size, which is a practical requirement of wavefield
                  reconstruction with localized sparsifying
                  transforms. Results are illustrated with simulations
                  of time-jittered marine acquisition, which
                  translates to jittered source locations for a given
                  speed of the source vessel, for two source vessels.},
  keywords = {EAGE, acquisition, blended, marine, deblending, interpolation},
  month = {01/2013},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/wason2013EAGEobs/wason2013EAGEobs.pdf}
}


@UNPUBLISHED{aravkin2013ICASSPssi,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Ning Tu},
  title = {Sparse seismic imaging using variable projection},
  booktitle = {ICASSP},
  year = {2012},
  abstract = {We consider an important class of signal processing
                  problems where the signal of interest is known to be
                  sparse, and can be recovered from data given
                  auxiliary information about how this data was
                  generated. For example, a sparse green's function
                  may be recovered from seismic experimental data
                  using sparsity optimization when the source
                  signature is known. Unfortunately, in practice this
                  information is often missing, and must be recovered
                  from data along with the signal using deconvolution
                  techniques. In this paper, we present a novel
                  methodology to simulta- neously solve for the sparse
                  signal and auxiliary parameters using a recently
                  proposed variable projection technique. Our main
                  contribution is to combine variable projection with
                  spar- sity promoting optimization, obtaining an
                  efficient algorithm for large-scale sparse
                  deconvolution problems. We demon- strate the
                  algorithm on a seismic imaging example.},
  keywords = {imaging, sparsity, optimization, variable projection},
  month = {12/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Conferences/ICASSP/2013/aravkin2013ICASSPssi/aravkin2013ICASSPssi.pdf}
}


@UNPUBLISHED{bander2012dre,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  year = {2012},
  abstract = {Data-driven methods---such as the estimation of primaries by sparse
	inversion---suffer from the 'curse of dimensionality' that leads
	to disproportional growth in computational and storage demands when
	moving to realistic 3D field data. To remove this fundamental impediment,
	we propose a dimensionality-reduction technique where the 'data matrix'
	is approximated adaptively by a randomized low-rank factorization.
	Compared to conventional methods, which need for each iteration passage
	through all data possibly requiring on-the-fly interpolation, our
	randomized approach has the advantage that the total number of passes
	is reduced to only one to three. In addition, the low-rank matrix
	factorization leads to considerable reductions in storage and computational
	costs of the matrix multiplies required by the sparse inversion.
	Application of the proposed method to synthetic and real data shows
	that significant performance improvements in speed and memory use
	are achievable at a low computational up-front cost required by the
	low-rank factorization.},
  keywords = {multiples,Processing,Optimization},
  month = {02/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/bander2012dre/bander2012dre.pdf}
}

@UNPUBLISHED{lin2012robustepsi,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust estimation of primaries by sparse inversion via one-norm minimization},
  year = {2012},
  abstract = {Even though contemporary methods for the removal of multiple events
	in seismic data due to a free-surface are built upon reciprocity
	relationships between wavefields, they are often still implemented
	as prediction-subtraction processes. The subtraction process does
	not always compensate for imperfect prediction of multiple events,
	and itself often leads to distortion of primary events. A recently
	proposed method called Estimation of Primaries by Sparse Inversion
	avoids the subtraction process altogether by directly prediction
	the primary impulse response as a collection of band-limited spikes
	under sparsity-regulated wavefield inversion approach. Although it
	can be shown that the correct primary impulse response is obtained
	through the sparsest possible solution, the Estimation of Primaries
	by Sparse Inversion algorithm was not designed to seek such a solution,
	instead depending on a predetermined degree of sparsity as an inversion
	parameter. This leads to imperfect multiple rejection when the sparsity
	is overestimated, and problems with recovering late primary events
	when it is underestimated. In this paper, we propose a new algorithm
	where we make obtaining the sparsest solution our explicit goal.
	Our approach remains a gradient-based approach like the original
	algorithm, but is in turn derived from a new optimization framework
	based on an extended basis pursuit denoising formulation. We show
	that the sparsity-minimizing objective of our formulation enables
	it to operate successfully on a wide variety of synthetic and field
	marine dataset without excessive tweaking of inversion parameters.
	We also demonstrate that Robust EPSI produces a more artifact-free
	impulse response compared to the original algorithm, which has interesting
	implications for broadband seismic applications. Finally we demonstrate
	through field data that recovering the primary impulse response under
	transform domains can significantly improve the recovery of weak
	primary late arrivals, without appreciable change to the underlying
	algorithm.},
  keywords = {multiples, optimization, sparsity, waveform inversion, pareto, biconvex,
	algorithm, EPSI},
  notes = {submitted to Geophysics, March 12, 2012},
  month = {03/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/lin2012robustepsi/lin2012robustepsi.pdf}
}

@UNPUBLISHED{mansour12iwr,
  author = {Hassan Mansour and Felix J. Herrmann and Ozgur Yilmaz},
  title = {Improved wavefield reconstruction from randomized sampling via weighted
	one-norm minimization},
  year = {2012},
  abstract = {Missing-trace interpolation aims to recover the gaps caused by physical
	obstacles or deliberate subsampling to control acquisition costs
	in otherwise regularly-sampled seismic wavefields. While transform-domain
	sparsity promotion has proven to be an effective tool to solve this
	recovery problem, current recovery techniques make no use of a priori
	information on the locations of transform-domain coefficients. In
	this paper, we propose recovery by weighted one-norm minimization,
	which exploits correlations between the locations of significant
	coefficients of different partitions, e.g., shot records, common-offset
	gathers, or frequency slices of the acquired data. We use these correlations
	to define a sequence of 2D curvelet-based recovery problems that
	exploit 3D continuity exhibited by seismic wavefields without relying
	on the highly redundant 3D curvelet transform. To illustrate the
	performance of our weighted algorithm, we compare recoveries from
	different scenarios of partitioning for a seismic line from the Gulf
	of Suez. These examples demonstrate that our method is superior to
	standard $\ell_1$ minimization in terms of reconstruction quality
	and computational memory requirements.},
  keywords = {Trace interpolation, weighted one-norm minimization, compressed sensing,
	randomized sampling},
  month = {10/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/mansour12iwr/mansour12iwr.pdf}
}

@UNPUBLISHED{vanderneut12irs,
  author = {Joost {van der Neut} and Felix J. Herrmann},
  title = {Interferometric redatuming by sparse inversion},
  year = {2012},
  abstract = {Assuming that exact transmission responses are known between the surface
	and a particular depth level in the subsurface, seismic sources can
	be eﬀectively mapped to that level by a process called interferometric
	redatuming. After redatuming, the obtained waveﬁelds can be used
	for imaging below this particular depth level. Interferometric redatuming
	consists of two steps, namely (i) the decomposition of the observed
	waveﬁelds into up- and down-going constituents and (ii) a multidimensional
	deconvolution of the up- and downgoing waveﬁelds. While this method
	works in theory, sensitivity to noise and artifacts due to incomplete
	acquisition call for a diﬀerent formulation. In this letter, we
	demonstrate the beneﬁts of formulating the two steps that undergird
	interferometric redatuming in terms of a transform-domain sparsity-promoting
	program. By exploiting compressibility of seismic waveﬁelds in
	the curvelet domain, we not only become robust with respect to noise
	but we are also able to remove certain artifacts while preserving
	the frequency content. These improvements lead to a better image
	of the target from the redatumed data.},
  keywords = {Controlled source seismology, Interferometry, Inverse theory},
  month = {10/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/vanderneut12irs/vanderneut12irs.pdf}
}

@UNPUBLISHED{vanleeuwen2012smii,
  author = {Tristan {van Leeuwen}},
  title = {A parallel matrix-free framework for frequency-domain seismic modelling,
	imaging and inversion in Matlab},
  year = {2012},
  abstract = {I present a parallel matrix-free framework for frequency-domain seismic
	modeling, imaging and inversion. The framework provides basic building
	blocks for designing and testing optimization-based formulations
	of both linear and non-linear seismic in- verse problems. By overloading
	standard linear-algebra operations, such as matrix- vector multiplications,
	standard optimization packages can be used to work with the code
	without any modification. This leads to a scalable testbed on which
	new methods can be rapidly prototyped and tested on medium-sized
	2D problems. I present some numerical examples on both linear and
	non-linear seismic inverse problems.},
  keywords = {Seismic imaging,optimization,Matlab,object-oriented programming},
  month = {07/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/vanleeuwen2012smii/vanleeuwen2012smii.pdf}
}

