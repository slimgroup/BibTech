% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2020-----%


@UNPUBLISHED{bisbas2020IPDPStbf,
  author = {George Bisbas and Fabio Luporini and Mathias Louboutin and Rhodri
Nelson and Gerard Gorman and Paul H. J. Kelly},
  title = {Temporal blocking of finite-difference stencil operators with sparse
"off-the-grid" sources},
  year = {2020},
  month = {10},
  abstract = {Stencil kernels dominate a range of scientific applications
including seismic and medical imaging, image processing, and neural networks.
Temporal blocking is a performance optimisation that aims to reduce the
required memory bandwidth of stencil computations by re-using data from the
cache for multiple time steps. It has already been shown to be beneficial for
this class of algorithms. However, optimising stencils for practical
applications remains challenging. These computations often include sparsely
located operators, not aligned with the computational grid ("off-the-grid").
For example, our work is motivated by sources that inject a wavefield and
measurements interpolating grid values. The resulting data dependencies make
the adoption of temporal blocking much more challenging. We propose a
methodology to inspect these data dependencies and reorder the computation,
leading to performance gains in stencil codes where temporal blocking has not
been applicable. We implement this novel scheme in the Devito domain-specific
compiler toolchain. Devito implements a domain-specific language embedded in
Python to generate optimised partial differential equation solvers using the
finite-difference method from high-level symbolic problem definitions. We
evaluate our scheme using isotropic acoustic, anisotropic acoustic and
isotropic elastic wave propagators of industrial significance. Performance
evaluation, after auto-tuning, shows that this enables substantial
performance improvement through temporal blocking, over highly-optimised
vectorized spatially-blocked code of up to 1.6x.},
  keywords = {Finite-difference, wave-equation, HPC, performance},
  note = {Submitted to IPDPS},
  url = {https://arxiv.org/pdf/2010.10248.pdf}
}

@UNPUBLISHED{sharan2020lsh,
  author = {Shashin Sharan and Yijun Zhang and Oscar Lopez and Felix J. Herrmann},
  title = {Large scale high-frequency wavefield reconstruction with recursively weighted matrix factorizations},
  year = {2020},
  month = {10},
  abstract = {Acquiring seismic data on a regular periodic fine grid is challenging. By exploiting the low-rank approximation property of fully sampled seismic data in some transform domain, low-rank matrix completion offers a scalable way to reconstruct seismic data on a regular periodic fine grid from coarsely randomly sampled data acquired in the field. While wavefield reconstruction have been applied successfully at the lower end of the spectrum, its performance deteriorates at the higher frequencies where the low-rank assumption no longer holds rendering this type of wavefield reconstruction ineffective in situations where high resolution images are desired. We overcome this shortcoming by exploiting similarities between adjacent frequency slices explicitly. During low-rank matrix factorization, these similarities translate to alignment of subspaces of the factors, a notion we propose to employ as we reconstruct monochromatic frequency slices recursively starting at the low frequencies. While this idea is relatively simple in its core, to turn this recent insight into a successful scalable wavefield reconstruction scheme for 3D seismic requires a number of important steps. First, we need to move the weighting matrices, which encapsulate the prior information from adjacent frequency slices, from the objective to the data misfit constraint. This move considerably improves the performance of the weighted low-rank matrix factorization on which our wavefield reconstructions is based. Secondly, we introduce approximations that allow us to decouple computations on a row-by-row and column-by-column basis, which in turn allow to parallelize the alternating optimization on which our low-rank factorization relies. The combination of weighting and decoupling leads to a computationally feasible full-azimuth wavefield reconstruction scheme that scales to industry-scale problem sizes. We demonstrate the performance of the proposed parallel algorithm on a 2D field data and on a 3D synthetic dataset. In both cases our approach produces high-fidelity broadband wavefield reconstructions from severely subsampled data.},
  keywords = {5D reconstruction, compressed sensing, frequency-domain, parallel, signal processing},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/sharan2020lsh/sharan2020lsh.html}
}

@UNPUBLISHED{rizzuti2020dfw,
  author = {Gabrio Rizzuti and Mathias Louboutin and Rongrong Wang and Felix J. Herrmann},
  title = {A dual formulation of wavefield reconstruction inversion for
large-scale seismic inversion},
  year = {2020},
  month = {10},
  abstract = {Most of the seismic inversion techniques currently proposed focus
on robustness with respect to the background model choice or inaccurate
physical modeling assumptions, but are not apt to large-scale 3D
applications. On the other hand, methods that are computationally feasible
for industrial problems, such as full waveform inversion, are notoriously
bogged down by local minima and require adequate starting models. We propose
a novel solution that is both scalable and less sensitive to starting model
or inaccurate physics when compared to full waveform inversion. The method is
based on a dual (Lagrangian) reformulation of the classical wavefield
reconstruction inversion, whose robustness with respect to local minima is
well documented in the literature. However, it is not suited to 3D, as it
leverages expensive frequency-domain solvers for the wave equation. The
proposed reformulation allows the deployment of state-of-the-art time-domain
finite-difference methods, and is computationally mature for industrial scale
problems.},
  keywords = {3D, Full-waveform inversion, Wave equation, Finite-difference},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/rizzuti2020dfw/rizzuti2020dfw.html}
}

@UNPUBLISHED{kukreja2020lcc,
  author = {Navjot Kukreja and Jan Hueckelheim and Mathias Louboutin and John Washbourne and Paul H. J. Kelly and Gerard J. Gorman},
  title = {Lossy Checkpoint Compression in Full Waveform Inversion},
  year = {2020},
  month = {9},
  abstract = {This paper proposes a new method that combines check-pointing
methods with error-controlled lossy compression for large-scale
high-performance Full-Waveform Inversion (FWI), an inverse problem commonly
used in geophysical exploration. This combination can significantly reduce
data movement, allowing a reduction in run time as well as peak memory. In
the Exascale computing era, frequent data transfer (e.g., memory bandwidth,
PCIe bandwidth for GPUs, or network) is the performance bottleneck rather
than the peak FLOPS of the processing unit. Like many other adjoint-based
optimization problems, FWI is costly in terms of the number of floating-point
operations, large memory footprint during backpropagation, and data transfer
overheads. Past work for adjoint methods has developed checkpointing methods
that reduce the peak memory requirements during backpropagation at the cost
of additional floating-point computations. Combining this traditional
checkpointing with error-controlled lossy compression, we explore the
three-way tradeoff between memory, precision, and time to solution. We
investigate how approximation errors introduced by lossy compression of the
forward solution impact the objective function gradient and final inverted
solution. Empirical results from these numerical experiments indicate that
high lossy-compression rates (compression factors ranging up to 100) have a
relatively minor impact on convergence rates and the quality of the final
solution.},
  keywords = {Lossy compression, Full waveform inversion, checkpointing, memory},
  note = {Submitted to GMD},
  url = {https://arxiv.org/pdf/2009.12623.pdf}
}

@UNPUBLISHED{yang2020lrpo,
  author = {Mengmeng Yang and Marie Graff and Rajiv Kumar and Felix J. Herrmann},
  title = {Low-rank representation of omnidirectional subsurface extended image
volumes},
  year = {2020},
  month = {3},
  abstract = {Subsurface-offset gathers play an increasingly important role in seismic
imaging. These gathers are used during velocity model building and inversion
of rock properties from amplitude variations. While powerful, these gathers
come with high computational and storage demands to form and manipulate these
high dimensional objects. This explains why only limited numbers of image
gathers are computed over a limited offset range. We avoid these high costs
by working with highly compressed low-rank factorizations. We arrive at these
factorizations via a combination of probings with the double two-way wave
equation and randomized singular value decompositions. In turn, the resulting
factorizations give us access to all subsurface offsets without having to
form the full extended image volumes. The latter is computationally
prohibitive because extended image volumes are quadratic in image size. As a
result, we can easily handle situations where conventional horizontal offset
gathers are no longer focused. More importantly, the factorization also
provides a mechanism to use the invariance relation of extended image volumes
for velocity continuation. With this technique, extended image volumes for
one background velocity model can directly be mapped to those of another
background velocity model. Our low-rank factorization inherits this
invariance property so we incur factorization costs only once when examining
different imaging scenarios. Because all imaging experiments only involve the
factors, they are computationally cheap with costs that scale with the rank
of the factorization. We validate our methodology on 2D synthetics including
a challenging imaging example with salt. Our experiments show that our
low-rank factorization parameterizes extended image volumes naturally.
Instead of brute force explicit cross-correlations between shifted source and receiver
wavefields, our approach relies on the underlying linear-algebra structure
that enables us to work with these objects without incurring unfeasible
demands on computation and storage.},
  keywords = {extended image volumes, low rank, randomized linear algebra, power
schemes, invariance relationship},
  note = {Submitted to Geophysics},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/yang2020lrpo/yang2020lrpo.html}
}

@UNPUBLISHED{yang2020tdsp,
  author = {Mengmeng Yang and Zhilong Fang and Philipp A. Witte and Felix J. Herrmann},
  title = {Time-domain sparsity promoting least-squares reverse time migration
with source estimation},
  year = {2020},
  month = {2},
  abstract = {Least-squares reverse time migration is well-known for its capability to generate artifact-free true-amplitude subsurface images through fitting observed data in the least-squares sense. However, when applied to realistic imaging problems, this approach is faced with issues related to overfitting and excessive computational costs induced by many wave-equation solves. The fact that the source function is unknown complicates this situation even further. Motivated by recent results in stochastic optimization and transform-domain sparsity-promotion, we demonstrate that the computational costs of inversion can be reduced significantly while avoiding imaging artifacts and restoring amplitudes. While powerfull, these new approaches do require accurate information on the source-time function, which is often lacking. Without this information, the imaging quality deteriorates rapidly. We address this issue by presenting an approach where the source-time function is estimated on the fly through a technique known as variable projection. Aside from introducing negligible computational overhead, the proposed method is shown to perform well on imaging problems with noisy data and problems that involve complex settings such as salt. In either case, the presented method produces high resolution high-amplitude fidelity images including an estimates for the source-time function. In addition, due to its use of stochastic optimization, we arrive at these images at roughly one to two times the cost of conventional reverse time migration involving all data.},
  keywords = {sparsity inversion, source estimation, penalty},
  note = {Submitted to Geophysical Prospecting},
  url = {https://slim.gatech.edu/Publications/Public/Submitted/2020/yang2020tdsp/yang2020tdsp.html}
}
