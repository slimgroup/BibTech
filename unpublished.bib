% This file was created with JabRef 2.9.
% Encoding: MacRoman

%-----2017-----%

@UNPUBLISHED{fang2017WAVESuqf,
  author = {Zhilong Fang and Curt Da Silva and Rachel Kuske and Felix J. Herrmann},
  title = {Uncertainty quantification for inverse problems with a weak wave-equation constraint},
  year = {2017},
  abstract = {In this work, we present a new posterior distribution to
                  quantify uncertainties in solutions of wave-equation
                  based inverse problems. By introducing an auxiliary
                  variable for the wavefields, we weaken the strict
                  wave-equation constraint used by conventional
                  Bayesian approaches. With this weak constraint, the
                  new posterior distribution is a bi-Gaussian
                  distribution with respect to both model parameters
                  and wavefields, which can be directly sampled by the
                  Gibbs sampling method.},
  date_submitted = {12/13/2016},
  keywords = {uncertainty, wave equation, constraint, Gibbs sampling, private},
  note = {Submitted to the WAVES 2017 Conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/WAVES/2017/fang2017WAVESuqf/fang2017WAVESuqf.html}
}


%-----2016-----%

@UNPUBLISHED{deaguiar2016SCdff,
  author = {Marcos de Aguiar and Gerard Gorman and Felix J. Herrmann and Navjot Kukreja and Michael Lange and Mathias Louboutin and Felippe Vieira Zacarias},
  title = {DeVito: fast finite difference computation},
  year = {2016},
  keywords = {finite differences, high performance computing, full-waveform inversion, private},
  note = {to be presented at the Super Computing Conference},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SC/2016/deaguiar2016SCdff/deaguiar2016SCdff_poster.pdf}
}


@UNPUBLISHED{esser2016tvr,
  author = {Ernie Esser and Llu\'{i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Total-variation regularization strategies in full-waveform inversion},
  year = {2016},
  abstract = {We propose an extended full-waveform inversion
                  formulation that includes general convex constraints
                  on the model. Though the full problem is highly
                  nonconvex, the overarching optimization scheme
                  arrives at geologically plausible results by solving
                  a sequence of relaxed and warm-started constrained
                  convex subproblems. The combination of box,
                  total-variation, and successively relaxed asymmetric
                  total-variation constraints allows us to steer free
                  from parasitic local minima while keeping the
                  estimated physical parameters laterally continuous
                  and in a physically realistic range. For accurate
                  starting models, numerical experiments carried out
                  on the challenging 2004 BP velocity benchmark
                  demonstrate that bound and total-variation
                  constraints improve the inversion result
                  significantly by removing inversion artifacts,
                  related to source encoding, and by clearly improved
                  delineation of top, bottom, and flanks of a
                  high-velocity high-contrast salt inclusion. The
                  experiments also show that for poor starting models
                  these two constraints by themselves are insufficient
                  to detect the bottom of high-velocity inclusions
                  such as salt. Inclusion of the one-sided asymmetric
                  total-variation constraint overcomes this issue by
                  discouraging velocity lows to buildup during the
                  early stages of the inversion. To the author's
                  knowledge the presented algorithm is the first to
                  successfully remove the imprint of local minima
                  caused by poor starting models and band-width
                  limited finite aperture data.},
  keywords = {full-waveform inversion, constrained optimization, total variation, salt, hinge loss, private},
  note = {Submitted on July 13, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/esser2016tvr/esser2016tvr.pdf}
}


@UNPUBLISHED{kukreja2016WOLFHPCdaf,
  author = {Navjot Kukreja and Mathias Louboutin and Felippe Vieira Zacarias and Fabio Luporini and Michael Lange and Gerard Gorman},
  title = {Devito: automated fast finite difference computation},
  year = {2016},
  abstract = {Domain specific languages have successfully been used in
                  a variety of fields to cleanly express scientific
                  problems as well as to simplify implementation and
                  performance optimization on different computer
                  architectures. Although a large number of stencil
                  languages are available, finite difference domain
                  specific languages have proved challenging to design
                  because most practical use cases require additional
                  features that fall outside the finite difference
                  abstraction. Inspired by the complexity of
                  real-world seismic imaging problems, we introduce
                  Devito, a domain specific language in which high
                  level equations are expressed using symbolic
                  expressions from the SymPy package. Complex
                  equations are automatically manipulated, optimized,
                  and translated into highly optimized C code that
                  aims to perform comparably or better than hand-tuned
                  code. All this is transparent to users, who only see
                  concise symbolic mathematical expressions.},
  keywords = {finite differences, high performance computing, modelling, acoustic, compiler, stencil, private},
  note = {to be presented at the WOLFHPC 2016 Workshop (in conjunction with Super Computing Conference)},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SC/2016/WOLFHPC/kukreja2016WOLFHPCdaf/kukreja2016WOLFHPCdaf.pdf}
}


@UNPUBLISHED{kumar2016bls,
  author = {Rajiv Kumar and Oscar Lopez and Damek Davis and Aleksandr Y. Aravkin and Felix J. Herrmann},
  title = {Beating level-set methods for {3D} seismic data interpolation: a primal-dual alternating approach},
  year = {2016},
  abstract = {Acquisition cost is a crucial bottleneck for seismic
                  workflows, and low-rank formulations for data
                  interpolation allow practitioners to 'fill in' data
                  volumes from critically subsampled data acquired in
                  the field. Tremendous size of seismic data volumes
                  required for seismic processing remains a major
                  challenge for these techniques. We propose a new
                  approach to solve residual constrained formulations
                  for interpolation. We represent the data volume
                  using matrix factors, and build a block-coordinate
                  algorithm with constrained convex subproblems that
                  are solved with a primal-dual splitting scheme. The
                  new approach is competitive with state of the art
                  level-set algorithms that interchange the role of
                  objectives with constraints. We use the new
                  algorithm to successfully interpolate a large scale
                  5D seismic data volume, generated from the
                  geologically complex synthetic 3D Compass velocity
                  model, where 80\% of the data has been removed.},
  keywords = {matrix completion, nuclear-norm relaxation, seismic data, interpolation, alternating minimization, primal-dual splitting, private},
  note = {Submitted on July 18, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/kumar2016bls/kumar2016bls.pdf}
}


@UNPUBLISHED{louboutin2016ppf,
  author = {Mathias Louboutin and Michael Lange and Felix J. Herrmann and Navjot Kukreja and Gerard Gorman},
  title = {Performance prediction of finite-difference solvers for different computer architectures},
  year = {2016},
  abstract = {The life-cycle of a partial differential equation (PDE)
                  solver is often characterized by three development
                  phases: the development of a stable numerical
                  discretization; development of a correct (verified)
                  implementation; and the optimization of the
                  implementation for different computer
                  architectures. Often it is only after significant
                  time and effort has been invested that the
                  performance bottlenecks of a PDE solver are fully
                  understood, and the precise details varies between
                  different computer architectures. One way to
                  mitigate this issue is to establish a reliable
                  performance model that allows a numerical analyst to
                  make reliable predictions of how well a numerical
                  method would perform on a given computer
                  architecture, before embarking upon potentially long
                  and expensive implementation and optimization
                  phases. The availability of a reliable performance
                  model also saves developer effort as it both informs
                  the developer on what kind of optimisations are
                  beneficial, and when the maximum expected
                  performance has been reached and optimisation work
                  should stop. We show how discretization of a wave
                  equation can be theoretically studied to understand
                  the performance limitations of the method on modern
                  computer architectures. We focus on the roofline
                  model, now broadly used in the high-performance
                  computing community, which considers the achievable
                  performance in terms of the peak memory bandwidth
                  and peak floating point performance of a computer
                  with respect to algorithmic choices. A first
                  principles analysis of operational intensity for key
                  time-stepping finite-difference algorithms is
                  presented. With this information available at the
                  time of algorithm design, the expected performance
                  on target computer systems can be used as a driver
                  for algorithm design.},
  keywords = {finite differences, performance, modeling, HPC, private},
  note = {Submitted to the Computers \& Geosciences Journal on September 16, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/louboutin2016ppf/louboutin2016ppf.pdf}
}


@UNPUBLISHED{oghenekohwo2016GEOPctl,
  author = {Felix Oghenekohwo and Haneet Wason and Ernie Esser and Felix J. Herrmann},
  title = {Low-cost time-lapse seismic with distributed {Compressive} {Sensing}---exploiting common information amongst the vintages},
  year = {2016},
  abstract = {Time-lapse seismic is a powerful technology for
                  monitoring a variety of subsurface changes due to
                  reservoir fluid flow. However, the practice can be
                  technically challenging when one seeks to acquire
                  colocated time-lapse surveys with high degrees of
                  replicability amongst the shot locations. We
                  demonstrate that under "ideal" circumstances, where
                  we ignore errors related to taking measurements off
                  the grid, high-quality prestack data can be obtained
                  from randomized subsampled measurements that are
                  observed from surveys where we choose not to revisit
                  the same randomly subsampled on-the-grid shot
                  locations. Our acquisition is low cost since our
                  measurements are subsampled. We find that the
                  recovered finely sampled prestack baseline and
                  monitor data actually improve significantly when the
                  same on-the-grid shot locations are not
                  revisited. We achieve this result by using the fact
                  that different time-lapse data share information and
                  that nonreplicated (on-the-grid) acquisitions can
                  add information when prestack data are recovered
                  jointly. Whenever the time-lapse data exhibit joint
                  structure---i.e., are compressible in some transform
                  domain and share information---sparsity-promoting
                  recovery of the "common component" and
                  "innovations", with respect to this common
                  component, outperforms independent recovery of both
                  the prestack baseline and monitor data. The
                  recovered time-lapse data are of high enough quality
                  to serve as input to extract poststack attributes
                  used to compute time-lapse differences. Without
                  joint recovery, artifacts---due to the randomized
                  subsampling---lead to deterioration of the degree of
                  repeatability of the time-lapse data. We support our
                  claims by carrying out experiments that collect
                  reliable statistics from thousands of repeated
                  experiments. We also confirm that high degrees of
                  repeatability are achievable for an ocean-bottom
                  cable survey acquired with time-jittered continuous
                  recording. This is part 1 of a two-paper series on
                  time-lapse seismic with compressed sensing. Part 2:
                  "Cheap time-lapse with distributed Compressive
                  Sensing---impact on repeatability".},
  keywords = {acquisition, time-lapse seismic, marine, random sampling, joint recovery method, private},
  note = {Revision 1 submitted on July 7, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/oghenekohwo2016GEOPctl/oghenekohwo2016GEOPctl.html}
}


@UNPUBLISHED{peters2016cvp,
  author = {Bas Peters and Felix J. Herrmann},
  title = {Constraints versus penalties for edge-preserving full-waveform inversion},
  year = {2016},
  abstract = {Full-waveform inversion is challenging in complex
                  geological areas. Even when provided with an
                  accurate starting model, the inversion algorithms
                  often struggle to update the velocity
                  model. Contrary to other areas in applied
                  geophysics, including prior information in
                  full-waveform inversion is still relatively in its
                  infancy. In part this is due to the fact that
                  incorporating prior information that relates to
                  geological settings where strong discontinuities in
                  the velocity model dominate is difficult, because
                  these settings call for non-smooth
                  regularizations. We tackle this problem by including
                  constraints on the spatial variations and value
                  ranges of the inverted velocities, as opposed to
                  adding penalties to the objective as is more
                  customary in main stream geophysical inversion. By
                  demonstrating the lack of predictability of
                  edge-preserving inversion when the regularization is
                  in the form of an added penalty term, we advocate
                  the inclusion of constraints instead. Our examples
                  show that the latter lead to more predictable
                  results and to significant improvements in the
                  delineation of Salt bodies.},
  keywords = {full-waveform inversion, constrained optimization, total variation, penalty methods, private},
  note = {Revision 1 submitted to The Leading Edge on November 13, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/peters2016cvp/peters2016cvp.html}
}


@UNPUBLISHED{wason2016GEOPctl,
  author = {Haneet Wason and Felix Oghenekohwo and Felix J. Herrmann},
  title = {Cheap time lapse with distributed {Compressive Sensing}---impact on repeatability},
  year = {2016},
  abstract = {Irregular or off-the-grid spatial sampling of sources
                  and receivers is inevitable in field seismic
                  acquisitions. Consequently, time-lapse surveys
                  become particularly expensive since current
                  practices aim to replicate densely sampled surveys
                  for monitoring changes occurring in the reservoir
                  due to hydrocarbon production. We demonstrate that
                  under certain circumstances, high-quality prestack
                  data can be obtained from cheap randomized
                  subsampled measurements that are observed from
                  nonreplicated surveys. We extend our time-jittered
                  marine acquisition to time-lapse surveys by
                  designing acquisition on irregular spatial grids
                  that render simultaneous, subsampled and irregular
                  measurements. Using the fact that different
                  time-lapse data share information and that
                  nonreplicated surveys add information when prestack
                  data are recovered jointly, we recover periodic
                  densely sampled and colocated prestack data by
                  adapting the recovery method to incorporate a
                  regularization operator that maps traces from an
                  irregular spatial grid to a regular periodic
                  grid. The recovery method is, therefore, a combined
                  operation of regularization, interpolation
                  (estimating missing fine-grid traces from subsampled
                  coarse-grid data), and source separation (unraveling
                  overlapping shot records). By relaxing the
                  insistence on replicability between surveys, we find
                  that recovery of the time-lapse difference shows
                  little variability for realistic field scenarios of
                  slightly nonreplicated surveys that suffer from
                  unavoidable natural deviations in spatial sampling
                  of shots (or receivers) and pragmatic
                  compressed-sensing based nonreplicated surveys when
                  compared to the "ideal" scenario of exact
                  replicability between surveys. Moreover, the
                  recovered densely sampled prestack baseline and
                  monitor data improve significantly when the
                  acquisitions are not replicated, and hence can serve
                  as input to extract poststack attributes used to
                  compute time-lapse differences. Our observations are
                  based on numerous experiments conducted for an
                  ocean-bottom cable survey acquired with
                  time-jittered continuous recording assuming source
                  equalization (or same source signature) for the
                  time-lapse surveys and no changes in wave heights,
                  water column velocities or temperature and salinity
                  profiles, etc.},
  keywords = {marine acquisition, time-lapse seismic, off-the-grid recovery, random sampling, joint recovery method, optimization, private},
  note = {Submitted to Geophysics on May 13, 2016.},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/2016/wason2016GEOPctl/wason2016GEOPctl.html}
}

