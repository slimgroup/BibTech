%-------------------------------------------------2012-------------------------------------------------

@conference{peters2012SINBADfde, 
        title = {Frequency domain 3D elastic wave propagation in general anisotropic media},
        booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
        abstract = {Elastic wave propagation in 3 spatial dimensions is modeled using a wave equation containing the full stiffness tensor consisting of 21 independent components. This allows modeling in general anisotropic media. The wave equation is discretized on several Cartesian and rotated Cartesian staggered finite-difference grids (using a 2nd order approximation). The grids are linearly combined and, in combination with a antilumped mass strategy, minimize numerical dispersion while requiring a low number of grid points per wavelength. In case not all 21 components need to be modeled, an approximation of the stiffness tensor can be used (e.g., orthorhombic anisotropy, TTI, ...). This results in a linear system of equations, which is solved using an iterative method. The modeling of all 21 components of the stiffness tensor (or an approximation) enables the development of new waveform inversion functionalities.},
        keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Bas Peters}
}
        
@conference {Akalin2012SINBADlss,
	title = {Large scale seismic data interpolation with matrix completion},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Seismic surveys amass large and incomplete data sets, and designing algorithms to interpolate the missing data at very large scales poses a daunting and critical challenge.  We study how to apply scalable matrix completion methods to such interpolation problems.  Recent studies in matrix completion have shown that a matrix that has low rank can be exactly completed when only a small number of observations are available. However, there are two challenges to applying matrix completion to seismic data. Matrix completion is typically applied to two dimensional or dyadic data whereas seismic data is often tensorial. Also successful matrix completion requires a low-rank matrix structure. We address these problems by organizing the seismic data on a matrix grid which exhibits a low-rank structure. This encoding allows us to apply the Jellyfish algorithm, developed at the University of Wisconsin, which achieves state-of-the-art performance for large-scale matrix completion. The proposed framework makes it possible to complete high-SNR interpolations of gigabytes of 4-D seismic data in minutes on standard multicore workstations. Our preliminary experimental results suggest that matrix completion provides a promising new approach to the seismic data interpolation problem.
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Okan Akalin}
}

@conference {xiang2012SINBADfgn,
	title = {Fast Gauss-Newton full-waveform inversion with sparsity regularization},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Full-waveform inversion (FWI) can be considered as a controlled data fitting process, in which we approximately fit observed data by iteratively updating the initial velocity model, we expect the final model can reveal subsurface structure till the wavefield misfit can converge to designed tolerance.  The conventional FWI approach is expensive since it requires the inversion of a linear system, which involves extremely large multi-experiment data volumes. To overcome this issue we percent a curvetlet based sparsity-promoting Gauss-Newton inversion method. In this presentation we invert for the model updates by replacing the normal Gauss-Newton linearized subproblem for subsampled FWI with a sparsity promoting FWI formulation. We speed up the algorithm and avoid over fitting the data by solving the problem approximately. Aside from this, we control wavefield dispersion by gradually increasing grid size as we move to higher frequencies. Our approach is successful because it reduces the size of seismic data volumes without loss of information. With this reduction, we can compute a Newton-like update with the reduced data volume at the cost of roughly one gradient update for the fully sampled wavefield.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Xiang Li}
}

@conference {krislock2012SINBADwsn,
	title = {Wireless Sensor Network Localization},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Locating the position of sensors connected together in a wireless network given only the position of a small number of the sensors and estimates of some of the distances between the sensors is a difficult problem with many modern applications. Within the last few years research in wireless sensor network localization has greatly increased due to the many new applications using wireless sensors, from lightweight sensors used to monitor the environment to ocean-bottom sensors used in geophysical applications. A second reason for this increased interest is our recent ability to efficiently solve these problems using modern semidefinite optimization solvers. We will discuss how semidefinite optimization can be used to solve such problems and possible directions for future work.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Nathan Krislock}
}

@conference {miao2012SINBADtjs,
	title = {Towards joint sparsity in seismic imaging},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {As has been widely explored in l1 migration scheme, seismic image is sparse under proper transform bases, for example, curvelet. Accordingly, as suggested by seismic imaging condition, similar sparse pattern is expected among ray parameters in prestack image gather, providing a joint sparsity scheme for prestack l1 constrained migration. By moving from poststack sparse migraion to prestack joint sparse migration, we show here the recovery result are more informative. In terms of
wave extrapolation, a spectral projector is introduced to eliminate evanescent wave mode. The spectral projector is designed as a recursive matrix polynomial, which is accelerated by exploiting hieratically semi-separable matrix approximation. This hss acceleration, by itself has more broad application potential.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Lina Miao}
}

@conference {miao2012SINBADasp,
	title = {Accelerating on sparse promoting recovery and its benefits in seismic application},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Sparse promoting recovery problem arises more and more frequently with the broad application of compressed sensing tool in exploration seismology. Because of the curse of dimensionality, the prohibitive computation burden on iteratively evaluating objective functions is one of the key issues that constrain high performance l1 solver. In this paper, we try to further improve the convergence performance of SPGl1, one of the state-of-the-art large scale sparse recovery solver, and as a result limit the number of objective function evaluations by introducing a projected quasi Newton method. Examples showing acceleration on seismic data collection, data processing as well as inversion are included.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Lina Miao}
}

@conference {friedlander2012SINBADrsh,
	title = {Randomized sampling: How confident are you?},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {At last year's consortium meeting, I described an inexact gradient method and sampling scheme for data fitting. The randomization method has good convergence properties, at least as measured by the distance to the solution----in expectation. But as one insightful critic rightly pointed out, we don't usually observe the expectation, at least not in a single run. In this talk I will characterize the convergence of the method in terms of bounds on the probability of being too far away from the solution.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Michael P. Friedlander}
}

@conference {tamalet2012SINBADvpe,
	title = { Variance parameters estimation - Application to Full Waveform Inversion},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Many inverse problems include nuisance parameters. While not of direct interest, these parameters are required to recover primary parameters. In order to estimate these nuisance parameters as well as the primary parameters in large-scale inverse problems, a method based on variable projection, which consists in projecting out a subset over the variables, has been developed. We present here the application of this method to the problem of variance parameters estimation in multiple datasets, which is an important problem in many areas including geophysics. More precisely, we apply the method to Full Waveform Inversion and demonstrate the improvement in recovery of the model parameters in the case where the variance of the noise increases with the frequency.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Anais Tamalet}
}

@conference {yilmaz2012SINBADwms,
	title = {Weighted methods in sparse recovery},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {In the recent years we have successfully employed "weighted" algorithms to recover sparse signals from few linear, non-adaptive measurements. The general principle here is to use prior knowledge about the signal to be recovered, e.g., approximate locations of large-in-magnitude transform coefficients, if such information is available. An example for this is the use of weighted 1-norm minimization to improve wavefield reconstruction from randomized (sub)sampling. We will review these results and outline some new directions we have explored during the last year, such as weighted non-convex sparse recovery (see Ghadermarzy's talk), weighted analysis-based recovery (see Hargreaves's talk), and a weighted randomized Kaczmarz algorithm for solving large overdetermined systems of equations that are known to admit a (nearly) sparse solution. Various examples in seismic will be shown.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Ozgur Yilmaz}
}

@conference {Lin2012SINBADrdr,
	title = {Recent developments on the robust estimation of primaries by sparse inversion},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Robust estimation of primaries by sparse inversion is a next-generation surface multiple removal technique with an objective to truly invert an operator that models the free-surface. Key to the success of this approach is the imposition of a sparsity constraint on the primary impulse response in the time domain. This is accomplished by carefully applying large-scale convex optimization techniques on an extended L1 minimization problem. One of the benefits of our approach is that many extensions to the algorithm can be devised under this optimization framework to improve the quality of the solution given fixed computational costs and mitigating various shortcomings in field data. This talk will first review the basic technique of Robust EPSI and follow with some highlights on recent further developments of the algorithm, including a discussion on the role of regularization by reciprocity and the interpolation of near-offset data, as well as investigations into optimality and robustness to data outliers.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Tim T.Y. Lin}
}

@conference {lin2012SINBADics,
	title = {An introduction to cosparse signal reconstruction},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Undersampling techniques in exploration seismology usually relies on the assumption that seismic records and images permit sparse approximations under certain representations, such as Curvelet coefficients. Recent findings have suggested that for redundant representations (of which Curvelet is an example), the analysis operator that maps the physical signal to coefficients may also play a crucial role in recovering data from incomplete observations. In particular, the number of zero-valued coefficients given by the analysis operator acting on the signal, referred to as its "cosparsity", have an analogous role to the sparsity of the signal in terms of the coefficients. The cosparsity of the signal permits recovery guarantees that are completely separate from sparsity-based models, and gives rise to distinct sets of reconstruction algorithms and performances compared to sparsity-based approaches. We present in this talk some initial findings on the viability of cosparse reconstruction for a variety of seismic applications that previously relied on sparse signal reconstruction, such as data interpolation and source separation.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Tim T.Y. Lin}
}

@conference {ning2012SINBADfim,
	title = {Fast imaging with multiples},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {If correctly used, multiple energy can be mapped to the correct subsurface locations. However, simply applying the cross-correlation imaging condition will introduce non-causal artifacts into the final image. Here we propose an inversion approach to image primaries and multiples simultaneously that yields an artifact-free image. To address the high computational cost associated with inversion, we propose to: i) have the wave-equation solver carry out the multi-dimensional convolutions implicitly, and ii) reduce the number of PDE solves by randomized subsampling. We then propose to improve the overall performance of this algorithm by a process called rerandomization, which helps to cancel the correlation built between model iterate and the subsampling operator. We show the merits of our approach on a number of examples.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Ning Tu}
}

@conference {hargreaves2012SINBADavs,
	title = {Analysis versus synthesis in weighted sparse recovery},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
        abstract = {The synthesis model for compressive sensing has been the model of choice for many years and various weighting schemes have been shown to improve it's performance (see Yilmaz, Mansour, and Ghadermarzy talks). However, there is a counterpart model to synthesis, namely the analysis model, which has been less popular but recently attracted more attention (see Lin's talk). In this talk, weighting in the analysis model is discussed and applied to the seismic trace interpolation problem.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Brock Hargreaves}
}


@conference {wason2012SINBADobn,
	title = {Ocean-bottom node acquisition via jittered sampling},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {We present a pragmatic marine acquisition scheme where a single (or multiple) vessel sails across an ocean-bottom array firing airguns at — optimally jittered source locations and instances in time. Following the principles of compressive sensing, we can significantly impact the reconstruction quality of conventional seismic data (from jittered data) and demonstrate successful recovery by sparsity promotion. In contrast to random (under)sampling, acquisition via jittered (under)sampling helps in controlling the maximum gap size, which is a practical requirement of wavefield reconstruction with localized sparsifying transforms. Results are illustrated with simulations of optimally jittered marine acquisition, and periodic time-dithering marine acquisition.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Haneet Wason}
}


@conference {petrenko2012SINBADcarp,
	title = {CARP-CG: A computational study},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Forward modelling of the wave equation is a key ingredient in seismic full waveform inversion (FWI). Simulation in the time domain and solution of the wave equation in the frequency domain are two competing approaches to modelling. Frequency domain approaches can further be categorized as using either direct or iterative solvers. For 3D FWI, iterative solvers in the frequency domain are attractive, partly because they require less memory than the other methods. This is due to the fact that there is no need to store the wavefield at each time step, or compute a factorization of the Helmholtz operator that will not be as sparse as the original matrix.
One iterative solver that has been applied to the Helmholtz equation is CARP-CG. CARP-CG uses Kaczmarz row projections for each block of a domain decomposition scheme to precondition the Helmholtz system into being symmetric and positive semidefinite. The method of conjugate gradients is then used to solve the preconditioned system.

We present a comparison of the performance of CARP-CG implemented in several languages (MATLAB, C, FORTRAN, julia) and in two different hardware environments: the LIMA HPC cluster hosted at UBC, and the Checkers cluster which is part of the Westgrid consortium. Parallelization of the algorithm via domain decomposition implemented with MPI (distributed memory) and OMP (shared memory) is also examined.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Art Petrenko}
}

@conference {mansour2012SINBADsti,
	title = {Seismic trace interpolation via sparsity promoting reweighted algorithms},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Missing-trace interpolation aims to reconstruct regularly sampled wavefields from periodically sampled data with gaps caused by physical constraints. While transform-domain sparsity promotion has proven to be an effective tool to solve this recovery problem, current recovery techniques make no use of a priori information on the transform-domain coefficients. To overcome these vulnerabilities in solving the recovery problem for large-scale problems, we propose recovery by weighted one-norm minimization, which exploits correlations between locations of significant coefficients of different partitions, e.g., shot records, common-offset gathers, or frequency slices, of the acquired data. Moreover, in situations where no prior support estimate is available, we propose the WSPGL1 algorithm that outperforms standard $\ell_1$ minimization in finding sparse solutions to underdetermined linear systems of equations. Our algorithm is a modification of the SPGL1 algorithm and enjoys better sparse recovery performance at no additional computational cost. We illustrate the improved recovery using WSPGL1 for randomly subsampled seismic traces.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Hassan Mansour}
}

@conference {vanleeuwen2012SINBADyap,
	title = {Yet another perspective on image volumes},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {An extended image is defined as the multi-dimensional cross-correlation of the source and receiver wavefields used for imaging.  This extended image will reveal velocity errors by de-focusing and can thus be used for velocity analysis. However, for optimal sensitivity to velocity errors, the subsurface offset has to be aligned with the local dip. As this dip is not known a priori, we consider forming the extended image for subsurface offsets in all directions. However, computing and storing such a large image volume is not computationally feasible.  We organize the image volume in a matrix and use matrix-probing techniques to glean information form the matrix without explicitly forming it.  A matrix-vector multiply with the image-volume matrix can be performed at the cost of two wave-equation solves and does not require any explicit cross-correlations of the wavefields.  Such techniques can also be used to evaluate focusing penalties without forming the whole image volume.  Finally, the matrix-viewpoint allows us to derive a 2-way equivalent of the DSR equation in a straightforward manner and provides a possible avenue for developing new velocity-continuation techniques.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Tristan van Leeuwen}
}

@conference {vanleeuwen2012SINBAD3dfd,
	title = {3D Frequency-domain waveform inversion using a row-projected Helmholtz solver},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {3D frequency-domain full waveform inversion relies on being able to efficiently solve the 3D Helmholtz equation.  Iterative methods require sophisticated preconditioners because the Helmholtz matrix is typically indefinite. In the first part of the talk I review a preconditioning technique that is based on row-projections. Notable advantages of this preconditioner over existing ones are that it has low algorithmic complexity, is easily parallelizable and extendable to time-harmonic vector equations. In the second part of the talk I discuss how the row-projected solver can be used in the context of waveform inversion. Key aspects are: the use of block-iterative methods for multiple sources and adapting the accuracy of the solver.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Tristan van Leeuwen}
}

@conference {oghenekohwo2012SINBADcs,
	title = {Compressed sensing:  a tool for eliminating repeatability in acquisition of 4D (time-lapse) seismic data},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {In 4D (time-lapse) seismic data acquisition, a very significant step is the repeatability of the acquisition process.  In other words, the geophones must be placed at the exact location as they were, during baseline survey and acquisition. This condition is required to be able to produce an image of the same location over time and this enhances a proper reservoir characterization. The cost of repeating the seismic acquisition is very expensive, as geophones (receivers) have to be left at the same location over the period for which the data will be acquired. In this talk, we attempt to highlight the effort of Compressed Sensing, to eliminate this condition of repeatability of the acquisition . We show that a random sampling of the shots or random placement of the geophones is able to reproduce the same image over time, hence eliminating any acquisition imprints on the final seismic image. },
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Felix Oghenekohwo}
}


@conference {kumar2012SINBADsdi,
	title = {Seismic data interpolation and denoising using SVD Free Pareto Curve based Low Rank Optimization},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Seismic data acquisition is curse by missing data and noise caused by physical and/or budget constraints. Aim of Interpolation and denoising technique is to spatially transform irregularly acquired data to regularly sampled data and reduce the random and/or coherent noise while maintaining the events coherency. While transform-domain sparsity promotion has proven to be an effective tool to solve this recovery problem, recent developments in Rank penalizing techniques opens new horizon to improved recovery by exploiting low-rank structure. A major downside of current state of the art techniques is their reliance on the SVD of seismic data structures, which can be prohibitively expensive. Fortunately, recent work allows us to circumvent this problem by working with matrix factorizations. We review a novel approach to rank penalization, and successfully apply it to the seismic interpolation problem by exploiting the low-rank structure of seismic data in the midpoint-offset domain for 2D acquisition. We also show how to incorporate important extensions such as weighted rank minimization into the new framework. Experiments for the recovery of 2D and 3D acquisition support the feasibility and potential of the new approach.
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Rajiv Kumar}
}


@conference {au-yeung2012SINBADcs,
	title = {Compressed sensing, random Fourier matrix and Jitter sampling},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = { Compressed sensing is an emerging signal processing technique that allows signals to be sampled well below the Nyquist rate, when the signal has a sparse representation in an orthonormal basis. By using a random Fourier matrix or a Gaussian matrix as our measurement matrix, we can reconstruct a signal from far fewer measurements than required by Shannon sampling theorem. In this talk, we will discuss the role of uniform versus jitter sampling, both in a theoretical and practical viewpoint.
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Enrico Au-Yeung and Hassan Mansour and Ozgur Yilmaz}
}

@conference {dasilva2012SINBADhtt,
	title = {Hierarchical Tucker Tensor Optimization - Applications to 4D Seismic Data Interpolation},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {There has been a swell of research in the scientific computing community in the last couple of years which tries to extend notions of linear algebra (rank, the SVD, linear systems, etc.) to higher dimensional arrays, or tensors. Much work has been proposed to try to overcome the so called "curse of dimensionality", the O(N^d) storage required for a d-dimensional array, where N is the size of each dimension. The hierarchical Tucker format is one such tensor representation which manages to decompose a hierarchy of dimensions into parameter matrices of very manageable size, requiring at most dNK + (d - 2)K^3 + K^2 parameters, where K is an internal rank parameter. In this work, we extend ideas of matrix completion to the tensor case, where we only know a small number of randomly distributed entries from various 4D frequency slices, and try to recover the fully sampled tensor based on the knowledge that it has low hierarchical tucker rank in a particular arrangement of dimensions. Using this approach, we exploit the multi-dimensional dependencies within the full data in order to achieve very promising interpolation results even from heavily subsampled data. 
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Curt Da Silva}
}

@conference {aravkin2012SINBADenp,
	title = {Estimating Nuisance Parameters in Inverse Problems},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Many inverse problems include nuisance parameters which, while not of direct interest, are required to recover primary parameters. In this talk, we present the idea of "projecting out" these variables, and how this idea allows us to design methods for solving a broad class of problems with nuisance parameters, such as variance or degrees of freedom. We then discuss several geophysical applications, including including estimation of unknown variance parameters in the Gaussian model for full waveform inversion, degree of freedom (d.o.f.) parameter estimation in the context of robust imaging problems, and robust source estimation.
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Aleksandr Y. Aravkin}
}

@conference {aravkin2012SINBADgsf,
	title = {Generalized SPGL1: from theory to applications},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {The SPGL1 solver has been effectively used for many geophysical applications, including curvelet data interpolation, imaging, and as a subroutine in full waveform inversion. In this talk, we present an overview of the theoretical foundation of the solver, along with a broad generalization of this foundation. We then introduce several applications, including robust & sparse imaging, sparse deconvolution, and data interpolation by matrix completion.  
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Aleksandr Y. Aravkin}
}

@conference {ghadermarzy2012SINBADncc,
	title = {Non-convex compressed sensing using partial support information},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {In this talk, we will address the recovery conditions of weighted $\ell_p$ minimization for signal reconstruction from compressed sensing measurements when (possibly inaccurate) partial support information is available. First we will motivate the use of (weighted) $\ell_p$ minimization with $p<1$ and point out its advantages over weighted $\ell_1$ minimization when there is prior information on the support of the signal that is possibly partial and inaccurate. Then we will provide theoretical guarantees of sufficient recovery conditions for weighted $\ell_p$ minimization, which are better than those for (unweighted) $\ell_p$ minimization as well as those for weighted $\ell_1$. In the last part of the talk, we will illustrate our results with some numerical experiments stylized applications.
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Navid Ghadermarzy}
}

@conference{herrmann2012UW,
author = {Felix J. Herrmann},
year = {2012},
booktitle = {Talk at University of Wisconsin},
title = {Compressive sensing and sparse recovery in exploration seismology},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Seminar/2012/herrmann2012UW/herrmann2012UW_pres.pdf}
}

@conference{min2012CSEGrgfe,
author = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
booktitle = {CSEG technical program},
title = {Re-establishment of gradient in frequency-domain elastic waveform inversion},
abstract = {To obtain solutions close to global minimum in waveform 
inversion, the gradients computed at each frequency need to be 
weighted to appropriately describe the residuals between modeled and 
field data. While the low-frequency components of the gradients should 
be weighted to recover the long- wavelength structures, the high-frequency 
components of the gradients need to be weighted when the short-wavelength 
structures are restored. However, the conventional elastic waveform inversion 
algorithms cannot properly weight the gradients computed at each frequency. 
When gradients are scaled using the pseudo-Hessian matrix inside the 
frequency loop, gradients obtained at high frequencies are over-emphasized. 
When the gradients are scaled outside the frequency loop, gradients are 
weighted by the source spectra. In this study, we propose applying weighting 
factors to the gradients obtained at each frequency so that gradients can 
properly reflect the differences between the true and assumed models satisfying 
the general inverse theory. The weighting factors are composed by the 
backpropagated residuals. Numerical examples for the simple rectangular-shaped 
model and the modified version of the Marmousi-2 model show that the 
weighting method enhances gradient images and inversion results compared 
to the conventional inversion algorithms.},
keywords = {elastic, waveform inversion, frequency-domain, weighting factors},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2012/min2012CSEGrgfe/min2012CSEGrgfe.pdf}
}

@conference{wason2012CSEGode,
author = {Haneet Wason and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
booktitle = {CSEG technical program},
title = {Only dither: efficient simultaneous marine acquisition},
abstract = {Simultaneous-source acquisition is an emerging technology
that is stimulating both geophysical research and commercial efforts. 
Simultaneous marine acquisition calls for the development of a new set
of design principles and post-processing tools. The focus here is on 
simultaneous-source marine acquisition design and sparsity-promoting 
sequential-source data recovery. We propose a pragmatic simultaneous-source, 
randomized marine acquisition scheme where multiple vessels sail across 
an ocean-bottom array firing airguns at — sequential locations and randomly 
time-dithered instances. By leveraging established findings from the field of 
compressive sensing, where the choice of the sparsifying transform needs 
to be incoherent with the compressive sampling matrix, we can significantly 
impact the reconstruction quality, and demonstrate that the compressive 
sampling matrix resulting from the proposed sampling scheme is sufficiently 
incoherent with the curvelet transform to yield successful recovery by sparsity 
promotion. Results are illustrated with simulations of “purely” random marine 
acquisition, which requires an airgun to be located at each source location, 
and random time-dithering marine acquisition with one and two source vessels. 
Size of the collected data volumes in all cases is the same. Compared to the 
recovery from the former acquisition scheme (SNR = 10.5dB), we get good 
results by dithering with only one source vessel (SNR = 8.06dB) in the latter 
scheme, which improve at the cost of having an additional source vessel 
(SNR = 9.85dB).},
keywords = {CSEG, acquisition, marine, simultaneous},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2012/wason2012CSEGode/wason2012CSEGode.pdf}
}

@conference{li2011CSEGefimag,
author = {Xiang Li and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
booktitle = {CSEG technical program},
title = {Efficient full-waveform inversion with marine acquisition geometry},
abstract = {Full-waveform inversion (FWI) is a nonlinear data fitting procedure 
based on seismic data to derive a accurate velocity model. With the increasing 
demand for high resolution images in complex geological settings, the 
importance of improvements in acquisition and inversion become more and 
more critical. However, these improvements will be obtained at high 
computational cost, as a typical marine survey contains thousands of shot 
and receiver positions, and FWI needs several passes through massive seismic 
data. Computational cost of FWI will grow exponentially as the size of seismic 
data and desired resolution increase. In this paper we present a modified 
Gauss-Newton (GN) method that borrows ideas from compressive sensing, 
where we compute the GN updates from a few randomly selected sequential 
shots. Each subproblem is solved by using a sparsity promoting algorithm. 
With this approach, we dramatically reduce the size and hence the 
computational costs of the problem, whilst we control information loss by 
redrawing a different set of sequential shots for each subproblem.},
keywords = {CSEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2012/li2011CSEGefimag/li2011CSEGefimag.pdf}
}


@conference{herrmann2012SEGfwi,
author = {Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv Kumar and Tristan van Leeuwen and Xiang Li and Brendan Smithyman and Eric Takam Takougang and Haneet Wason and Felix J. Herrmann},
title = {FWI from the West Coasts: lessons learned from "Gulf of Mexico Imaging Challenges: What Can Full Waveform Inversion Achieve?"},
year = {2012},
booktitle = {SEG Technical Program Expanded Abstracts},
organization = {SEG},
keywords = {workshop, fwi, SEG},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/herrmann2012SEGfwi/herrmann2012SEGfwi_pres.pdf}
}


@conference{herrmann2012SEGals,
author = {Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {Accelerated large-scale inversion with message passing},
abstract = {To meet current-day challenges, exploration seismology increasingly 
relies on more and more sophisticated algorithms that require 
multiple paths through all data. This requirement leads to problems 
because the size of seismic data volumes is increasing exponentially, 
exposing bottlenecks in IO and computational capability. To overcome 
these bottlenecks, we follow recent trends in machine learning and 
compressive sensing by proposing a sparsity-promoting inversion 
technique that works on small randomized subsets of data only. We 
boost the performance of this algorithm significantly by modifying a 
state-of-the-art l1-norm solver to benefit from message 
passing, which breaks the build up of correlations between model 
iterates and the randomized linear forward model. We demonstrate the 
performance of this algorithm on a toy sparse-recovery problem and 
on a realistic reverse-time-migration example with random source 
encoding. The improvements in speed, memory use, and output quality 
are truly remarkable.},
keywords = {Imaging, Optimization, Compressive Sensing, SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals.pdf},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals_pres.pdf}
}

@conference{vanderneut2012SEGirs,
author = {Joost {van der Neut} and Felix J. Herrmann and Kees Wapenaar},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {Interferometric redatuming with simultaneous and missing 
sources using sparsity promotion in the curvelet domain},
abstract = {Interferometric redatuming is a velocity-independent 
method to turn downhole receivers into virtual sources. Accurate 
redatuming involves solving an inverse problem, which can be 
highly ill-posed, especially in the presence of noise, incomplete 
data and limited aperture. We address these issues by combining 
interferometric redatuming with transform-domain sparsity 
promotion, leading to a formulation that deals with data imperfections. 
We show that sparsity promotion improves the retrieval of virtual 
shot records under a salt flank. To reduce acquisition costs, it can 
be beneficial to reduce the number of sources or shoot them 
simultaneously. It is shown that sparse inversion can still provide 
a stable solution in such cases.},
keywords = {Processing,Imaging,Optimization,Interferometry, SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/vanderneut2012SEGirs/vanderneut2012SEGirs.pdf}
}

@conference{li2012SEGspmamp,
author = {Xiang Li and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {Sparsity-promoting migration accelerated by message passing},
abstract = {Seismic imaging via linearized inversion requires multiple 
iterations to minimize the least-squares misfit as a function of the 
medium perturbation. Unfortunately, the cost for these iterations 
are prohibitive because each iteration requires many wave-equation 
simulations, which without direct solvers require an expensive 
separate solve for each source. To overcome this problem, we use 
dimensionality-reduction to decrease the size of seismic imaging 
problem by turning the large number of sequential shots into a much 
small number of simultaneous shots. In our approach, we take 
advantage of sparsifying transforms to remove source crosstalk 
resulting from randomly weighting and stacking sequential shots into 
a few super shots. We also take advantage of the fact that the 
convergence of large-scale sparsity-promoting solvers can be 
improved significantly by borrowing ideas from message passing, 
which are designed to break correlation built up between the linear 
system and the model iterate. In this way, we arrive at a 
formulation where we run the sparsity-promoting solver for a 
relatively large number of very iterations. Aside from leading to a 
significant speed up, our approach had the advantage of greatly 
reducing the memory imprint and IO requirements. We demonstrate this 
feature by solving a sparsity-promoting imaging problem with 
operators of reverse-time migration, which is computationally 
infeasible without the dimensionality reduction.},
keywords = {SEG, Imaging, Inversion},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp.pdf},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp_pres.pdf}
}


@conference{aravkin2012SEGST,
author = {Aleksandr Y. Aravkin and Tristan {van Leeuwen} and Kenneth Bube and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {On Non-Uniqueness of the Student's t-formulation for Linear Inverse Problems},
abstract = {We review the statistical interpretation of inverse problem 
formulations, and the motivations for selecting non-convex penalties 
for robust behaviour with respect to measurement outliers or artifacts 
in the data. An important downside of using non-convex formulations 
such as the Student's t is the potential for non-uniqueness, and we 
present a simple example where the Student's t penalty can be made 
to have many local minima by appropriately selecting the degrees of 
freedom parameter. 
On the other hand, the non-convexity of the Student's t is precisely 
what gives it the ability to ignore artifacts in the data. We explain this 
idea, and present a stylized imaging experiment, where the Student's t 
is able to recover a velocity perturbation from data contaminated by 
a very peculiar artifact --- data from a different velocity perturbation. 
The performance of Student's t inversion is investigated empirically 
for different values of the degrees of freedom parameter, and 
different initial conditions.},
keywords = {student's t, robust, non-convex, uniqueness, SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/aravkin2012SEGST/aravkin2012SEGST.pdf}
}

@conference{tu2012SEGima,
author = {Ning Tu and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {Imaging with multiples accelerated by message passing},
abstract = {With the growing realization that multiples can provide 
valuable information, there is a paradigm shift from removing them 
to using them. For instance, primary estimation by sparse inversion 
has demonstrated its superiority over surface-related multiple removal 
in many aspects. Inspired by this shift, we propose a method to 
image directly from the total up-going wavefield, including surface-related 
multiples, by sparse inversion. To address the high computational cost 
associated with this method, we propose to speed up the inversion by 
having the wave-equation solver carry out the multi-dimensional 
convolutions implicitly and cheaply by randomized subsampling. We 
improve the overall performance of this algorithm by selecting new 
independent copies of the randomized modeling operator, which 
leads to a cancellation of correlations that hamper the speed of 
convergence of the solver. We show the merits of our approach on 
a number of examples.},
keywords = {SEG, Imaging, Multiples},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima.pdf},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima_pres.pdf}
}

@conference{mansour2012SSPwspgl1,
author = {Hassan Mansour},
year = {2012},
optmonth = {03/2012},
title = {Beyond $\ell_1$ norm minimization for sparse signal recovery},
booktitle={2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
address={Ann Arbor, Michigan, USA},
organization={IEEE},
abstract = {Sparse signal recovery has been dominated by the basis 
pursuit denoise (BPDN) problem formulation for over a decade. In this 
paper, we propose an algorithm that outperforms BPDN in finding 
sparse solutions to underdetermined linear systems of equations at no 
additional computational cost. Our algorithm, called WSPGL1, is a 
modification of the spectral projected gradient for $\ell_1$ minimization 
(SPGL1) algorithm in which the sequence of LASSO subproblems are 
replaced by a sequence of weighted LASSO subproblems with constant 
weights applied to a support estimate. The support estimate is derived 
from the data and is updated at every iteration. The algorithm also 
modifies the Pareto curve at every iteration to reflect the new weighted 
$\ell_1$ minimization problem that is being solved. We demonstrate through 
extensive simulations that the sparse recovery performance of our 
algorithm is superior to that of $\ell_1$ minimization and approaches the 
recovery performance of iterative re-weighted $\ell_1$ (IRWL1) minimization 
of Cand{\`e}s, Wakin, and Boyd. Moreover, our algorithm has the 
computational cost of a single BPDN problem.},
keywords = {Sparse recovery, compressed sensing, iterative algorithms, weighted $\ell_1$ minimization, partial support recovery},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1_pres.pdf},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1.pdf}
}

@conference{herrmann2012SSPamp,
author = {Felix J. Herrmann},
year = {2012},
optmonth = {03/2012},
title = {Approximate message passing meets exploration seismology},
booktitle={2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
address={Ann Arbor, Michigan, USA},
organization={IEEE},
abstract = {Data collection, data processing, and imaging in exploration 
seismology increasingly hinge on large-scale sparsity promoting solvers 
to remove artifacts caused by efforts to reduce costs. We show how the 
inclusion of a 'message term' in the calculation of the residuals improves 
the convergence of these iterative solvers by breaking correlations that 
develop between the model iterate and the linear system that needs to 
be inverted. We compare this message-passing scheme to state-of-the-art 
solvers for problems in missing-trace interpo- lation and in 
dimensionality-reduced imaging with phase en- coding.},
keywords = {Exploration seismology, compressive sensing, transform-domain sparsity promotion, seismic imaging},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp_pres.pdf},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp.pdf}
}

@CONFERENCE{herrmann2012EAGEcsm,
  author       = {Felix J. Herrmann and Haneet Wason},
  title        = {Compressive sensing in marine acquisition and beyond},
  year         = {2012},
  optmonth     = {02/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {workshop, acquisition, marine},
  abstract     = {Simultaneous-source marine acquisition is an example
                  of compressive sensing where acquisition with a
                  single vessel is replaced by simultaneous
                  acquisition by multiple vessels with sources that
                  fire at randomly dithered times. By identifying
                  simultaneous acquisition as compressive sensing, we
                  are able to design acquisitions that favour recovery
                  by sparsity promotion. Compared to conventional
                  processing that yields estimates for sequential
                  data, sparse recovery leads to significantly
                  improved results for simultaneous data volumes that
                  are collected in shorter times. These improvements
                  are the result of proper design of the acquisition,
                  selection of the appropriate transform domain, and
                  solution of the recovery problem by sparsity
                  promotion. During this talk, we will show how these
                  design principles can be applied to marine
                  acquisition and to other problems in exploration
                  seismology that can benefit from compressive
                  sensing.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm.pdf}
}


@CONFERENCE{min2012EAGEefwi,
  author       = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
  title        = {Frequency-domain elastic waveform inversion using weighting factors related to source-deconvolved residuals},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {elastic, waveform inversion, frequency-domain, weighting factors},
  abstract     = {One of the limitations in seismic waveform inversion
                  is that inversion results are very sensitive to
                  initial guesses, which may be because the gradients
                  computed at each frequency are not properly weighted
                  depending on given models.Analyzingthe conventional
                  waveform inversion algorithms using the
                  pseudo-Hessian matrix as a pre-conditioner shows
                  that the gradientsdo not properly describe the
                  feature of given models or high- and low-end
                  frequencies do not contribute the model parameter
                  updates due to banded spectra of source wavelet. For
                  a better waveform inversion algorithm, we propose
                  applying weighting factors to gradients computed at
                  each frequency. The weighting factors are designed
                  using the source-deconvolved back-propagated
                  wavefields.  Numerical results for the SEG/EAGE salt
                  model show that the weighting method improves
                  gradient images and its inversion results are
                  compatible with true velocities even with poorly
                  estimated initial guesses.},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/min2012EAGEefwi/min2012EAGEefwi.pdf}
}


@CONFERENCE{dasilva2012EAGEprobingprecond,
  author       = {Curt {Da Silva} and Felix J. Herrmann},
  title        = {Matrix Probing and Simultaneous Sources: A New Approach for Preconditioning the Hessian},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, matrix probing, pseudo-differential operator},
  abstract     = {Recent advances based on the mathematical
                  understanding of the Hessian as, under certain
                  conditions, a pseudo-differential operator have
                  resulted in a new preconditioner by L. Demanet et
                  al. Basing their approach on a suitable basis
                  expansion for the Hessian, by suitably 'probing' the
                  Hessian, i.e. applying the Hessian to a small number
                  of randomized model perturbations, one can obtain an
                  approximation to the inverse Hessian in an efficient
                  manner.  Building upon this approach, we consider
                  this preconditioner in the context of least-squares
                  migration and Full Waveform Inversion and
                  specifically dimensionality reduction techniques in
                  these domains.  By utilizing previous work in
                  simultaneous sources, we are able to develop an
                  efficient least-squares migration scheme which
                  recovers higher quality images and hence higher
                  quality search directions in the context of a
                  Gauss-Newton method for Full Waveform Inversion
                  while simultaneously avoiding inordinate amounts of
                  additional work.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond.pdf}
}


@CONFERENCE{wason2012EAGEode,
  author       = {Haneet Wason and Felix J. Herrmann},
  title        = {Only dither: efficient simultaneous marine acquisition},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, acquisition, marine, simultaneous},
  abstract     = {Simultaneous-source acquisition is an emerging
                  technology that is stimulating both geophysical
                  research and commercial efforts. The focus here is
                  on simultaneous-source marine acquisition design and
                  sparsity-promoting sequential-source data
                  recovery. We propose a pragmatic
                  simultaneous-source, randomized marine acquisition
                  scheme where multiple vessels sail across an
                  ocean-bottom array firing airguns at—sequential
                  locations and randomly time-dithered
                  instances. Within the context of compressive
                  sensing, where the choice of the sparsifying
                  transform needs to be incoherent with the
                  compressive sampling matrix, we can significantly
                  impact the reconstruction quality, and demonstrate
                  that the compressive sampling matrix resulting from
                  the proposed sampling scheme is sufficiently
                  incoherent with the curvelet transform to yield
                  successful recovery by sparsity promotion. Results
                  are illustrated with simulations of “purely” random
                  marine acquisition, which requires an airgun to be
                  located at each source location, and random
                  time-dithering marine acquisition with one and two
                  source vessels. Size of the collected data volumes
                  in all cases is the same. Compared to the recovery
                  from the former acquisition scheme (SNR = 10.5dB),
                  we get good results by dithering with only one
                  source vessel (SNR = 8.06dB) in the latter scheme,
                  which improve at the cost of having an additional
                  source vessel (SNR = 9.44dB).},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode.pdf}
}


@CONFERENCE{tu2011EAGElmf,
  author       = {Ning Tu and Felix J. Herrmann},
  title        = {Least-squares migration of full wavefield with source encoding},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, depth migration, surface-related multiples},
  abstract     = {Multiples can provide valuable information that is
                  missing in primaries, and there is a growing
                  interest in using them for seismic imaging.  In our
                  earlier work, we proposed to combine primary
                  estimation and migration to image from the total
                  up-going wavefield. The method proves to be
                  effective but computationally expensive. In this
                  abstract, we propose to reduce the computational
                  cost by removing the multi-dimensional convolution
                  required by primary estimation, and reducing the
                  number of PDE solves in migration by introducing
                  simultaneous sources with source renewal. We gain
                  great performance boost without compromising the
                  quality of the image.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/tu2011EAGElmf/tu2011EAGElmf_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/tu2011EAGElmf/tu2011EAGElmf.pdf}
}


@CONFERENCE{aravkin2012EAGErobust,
  author       = {Aleksandr Y. Aravkin and Tristan {van Leeuwen} and Henri Calandra and Felix J. Herrmann},
  title        = {Source estimation for frequency-domain FWI with robust penalties},
  year         = {2012},
  optmonth     = {01/2012},
  keywords     = {EAGE},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  abstract     = {Source estimation is an essential component of full
                  waveform inversion. In the standard frequency domain
                  formulation, there is closed form solution for the
                  the optimal source weights, which can thus be
                  cheaply estimated on the fly. A growing body of work
                  underscores the importance of robust modeling for
                  data with large outliers or artifacts that are not
                  captured by the forward model. Effectively, the
                  least-squares penalty on the residual is replaced by
                  a robust penalty, such as Huber, Hybrid `1-`2 or
                  Student’s t. As we will demonstrate, it is essential
                  to use the same robust penalty for source
                  estimation. In this abstract, we present a general
                  approach to robust waveform inversion with robust
                  source estimation. In this general formulation,
                  there is no closed form solution for the optimal
                  source weights so we need to solve a scalar
                  optimization problem to obtain these weights. We can
                  efficiently solve this optimization problem with a
                  Newton-like method in a few iterations. The
                  computational cost involved is of the same order as
                  the usual least-squares source estimation
                  procedure. We show numerical examples illustrating
                  robust source estimation and robust waveform
                  inversion on synthetic data with outliers.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust.pdf}
}


@CONFERENCE{vanleeuwen2012EAGEext,
  author       = {Tristan {van Leeuwen} and Felix J. Herrmann},
  title        = {Wave-equation extended images: computation and velocity continuation},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, extended image, velocity continuation},
  abstract     = {An extended image is a multi-dimensional correlation
                  of source and receiver wavefields. For a
                  kinematically correct velocity, most of the energy
                  will be concentrated at zero offset. Because of the
                  computational cost involved in correlating the
                  wavefields for all offsets, such exteded images are
                  computed for a subsurface offset that is aligned
                  with the local dip. In this paper, we present an
                  efficient way to compute extended images for all
                  subsurface offsets without explicitly calculating
                  the receiver wavefields, thus making it
                  computationally feasible to compute such extended
                  images.  We show how more conventional image
                  gathers, where the offset is aligned with the dip,
                  can be extracted from this extended image. We also
                  present a velocity continuation procedure that
                  allows us to compute the extended image for a given
                  velocity without recomputing all the source
                  wavefields.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext.pdf}
}


@CONFERENCE{vanleeuwen2012EAGEcarpcg,
  author       = {Tristan {van Leeuwen} and Dan Gordon and Rachel Gordon and Felix J. Herrmann},
  title        = {Preconditioning the Helmholtz equation via row-projections},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, Helmholtz equation, precondition},
  abstract     = {3D frequency-domain full waveform inversion relies
                  on being able to efficiently solve the 3D Helmholtz
                  equation. Iterative methods require sophisticated
                  preconditioners because the Helmholtz matrix is
                  typically indefinite. We review a preconditioning
                  technique that is based on row-projections.  Notable
                  advantages of this preconditioner over existing ones
                  are that it has low algorithmic complexity, is
                  easily parallelizable and extendable to
                  time-harmonic vector equations.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg.pdf}
}


@CONFERENCE{vanderneut2012EAGEdecomp,
  author       = {Joost {van der Neut} and Felix J. Herrmann},
  title        = {Up / down wavefield decomposition by sparse inversion},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE,wavefield decomposition, sparse inversion},
  abstract     = {Expressions have been derived for the decomposition
                  of multi-component seismic recordings into up- and
                  down-going constituents. However, these expressions
                  contain singularities at critical angles and can be
                  sensitive for noise. By interpreting wavefield
                  decomposition as an inverse problem and imposing
                  constraints on the sparseness of the solution, we
                  arrive at a robust formalism that can be applied to
                  noisy data. The method is demonstrated on synthetic
                  data with multi-component receivers in a horizontal
                  borehole, but can also be applied for different
                  configurations, including OBC and dual-sensor
                  streamers.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp.pdf}
}


@CONFERENCE{herrmann2012EAGEpmr,
  author       = {Felix J. Herrmann},
  title        = {Pass on the message: recent insights in large-scale sparse recovery},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, message passing, sparse inversion},
  abstract     = {Data collection, data processing, and imaging in
                  exploration seismology increasingly hinge on
                  large-scale sparsity promoting solvers to remove
                  artifacts caused by efforts to reduce costs. We show
                  how the inclusion of a “message term“ in the
                  calculation of the residuals improves the
                  convergence of these iterative solvers by breaking
                  correlations that develop between the model iterate
                  and the linear system that needs to be inverted. We
                  compare this message-passing scheme to
                  state-of-the-art solvers for problems in
                  missing-trace interpolation and in
                  dimensionality-reduced imaging with phase encoding.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr.pdf}
}


@CONFERENCE{aravkin2012ICASSProbustb,
  author       = {Aleksandr Y. Aravkin and Michael P. Friedlander and Tristan van Leeuwen },
  booktitle    = {ICASSP},
  Keywords     = {ICASSP},
  Organization = {ICASSP},
  Title        = {Robust inversion via semistochastic dimensionality reduction},
  Year         = {2012},
  pages        = {5245-5248},
  optdoi       = {10.1109/ICASSP.2012.6289103},
  Abstract     = {We consider a class of inverse problems where it is
                  possible to aggregate the results of multiple
                  experiments. This class includes problems where the
                  forward model is the solution operator to linear
                  ODEs or PDEs. The tremendous size of such problems
                  motivates the use dimensionality reduction (DR)
                  techniques based on randomly mixing
                  experiments. These techniques break down, however,
                  when robust data-fitting formulations are used,
                  which are essential in cases of missing data,
                  unusually large errors, and systematic features in
                  the data unexplained by the forward model. We survey
                  robust methods within a statistical framework, and
                  propose a sampling optimization approach that allows
                  DR. The efficacy of the methods are demonstrated for
                  a large-scale seismic inverse problem using the
                  robust Student's t-distribution, where a useful
                  synthetic velocity model is recovered in the extreme
                  scenario of 60% corrupted data. The sampling
                  approach achieves this recovery using 20% of the
                  effort required by a direct robust approach.},
  keywords     = {SLIM,Optimization,Full-waveform inversion},
  URL          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/AravkinFriedlanderLeeuwen/AravkinFriedlanderLeeuwen.pdf }
}


@CONFERENCE{aravkin2012ICASSPfastseis,
  Author       = {Aleksandr Y. Aravkin and Xiang Li and Felix J. Herrmann },
  Title        = {Fast seismic imaging for marine data},
  booktitle    = {ICASSP},
  Abstract     = {Seismic imaging can be formulated as a linear
                  inverse problem where a medium perturbation is
                  obtained via minimization of a least-squares misfit
                  functional. The demand for higher resolution images
                  in more geophysically complex areas drives the need
                  to develop techniques that handle problems of
                  tremendous size with limited computational
                  resources. While seismic imaging is amenable to
                  dimensionality reduction techniques that collapse
                  the data volume into a smaller set of “super-shots”,
                  these techniques break down for complex acquisition
                  geometries such as marine acquisition, where sources
                  and receivers move during acquisition. To meet these
                  challenges, we propose a novel method that combines
                  sparsity-promoting (SP) solvers with random sub- set
                  selection of sequential shots, yielding a SP
                  algorithm that only ever sees a small portion of the
                  full data, enabling its application to very
                  large-scale problems. Application of this technique
                  yields excellent results for a complicated
                  synthetic, which underscores the robustness of
                  sparsity promotion and its suitability for seismic
                  imaging.},
  Keywords     = {ICASSP},
  Organization = {ICASSP},
  Year         = {2012},
  keywords     = {SLIM,Imaging,Optimization,Compressive Sensing},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/AravkinLiHerrmann/AravkinLiHerrmann.pdf}
}


@CONFERENCE{mansour2012ICASSadapt,
  Author       = {Hassan Mansour and Ozgur Yilmaz},
  Title        = {Adaptive compressed sensing for video acquisition},  
  booktitle    = {ICASSP},
  Abstract     = {In this paper, we propose an adaptive compressed
                  sensing scheme that utilizes a support estimate to
                  focus the measurements on the large valued
                  coefficients of a compressible signal. We embed a
                  “sparse-filtering” stage into the measure- ment
                  matrix by weighting down the contribution of signal
                  coefficients that are outside the support
                  estimate. We present an application which can
                  benefit from the proposed sampling scheme, namely,
                  video compressive acquisition. We demonstrate that
                  our proposed adaptive CS scheme results in a
                  significant improvement in reconstruction quality
                  compared with standard CS as well as adaptive
                  recovery using weighted $\ell$1 minimization.},
  Keywords     = {ICASSP},
  Organization = {ICASSP},
  Year         = {2012},
  keywords     = {Compressive Sensing},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPaCS/MansourYilmazICASSPaCS.pdf}
}


@CONFERENCE{mansour2012ICASSsupport,
  Author       = {Hassan Mansour and Ozgur Yilmaz},
  Title        = {Support driven reweighted $\ell_1$ minimization},
  booktitle    = {ICASSP},
  Abstract     = {In this paper, we propose a support driven
                  reweighted $\ell$_1 minimization algorithm (SDRL1)
                  that solves a sequence of weighted $\ell$_1
                  problems and relies on the support estimate accu-
                  racy. Our SDRL1 algorithm is related to the IRL1
                  algorithm proposed by Cande`s, Wakin, and Boyd. We
                  demonstrate that it is sufficient to find support
                  estimates with good accuracy and apply constant
                  weights instead of using the inverse coefficient
                  magnitudes to achieve gains similar to those of
                  IRL1. We then prove that given a support estimate
                  with sufficient accuracy, if the signal decays
                  according to a specific rate, the solution to the
                  weighted $\ell$_1 minimization problem results in a
                  support estimate with higher accuracy than the
                  initial estimate. We also show that under certain
                  conditions, it is possible to achieve higher
                  estimate accuracy when the inter- section of support
                  estimates is considered. We demonstrate the
                  performance of SDRL1 through numerical simulations
                  and compare it with that of IRL1 and standard
                  $\ell$_1 minimization.},
  Keywords     = {ICASSP},
  Organization = {ICASSP},
  Year         = {2012},
  keywords     = {Compressive Sensing},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPwL1/MansourYilmazICASSPwL1.pdf}
}



%-------------------------------------------------2011-------------------------------------------------

@CONFERENCE{herrmann2011SEGffw,
  author       = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen and Xiang Li},
  title        = {Fast full-waveform inversion with compressive sensing},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  keywords     = {SEG,SLIM,Presentation,Full-waveform inversion},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/HerrmannSEG2011fws/HerrmannSEG2011fws.pdf}
}


@CONFERENCE{herrmann2011SPIEmsp,
  author       = {Felix J. Herrmann and Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen},
  title        = {A modified, sparsity promoting, {G}auss-{N}ewton algorithm for seismic waveform inversion},
  booktitle    = {Proc. SPIE},
  year         = {2011},
  number       = {81380V},
  notes        = {TR-2011-05},
  optmonth     = {08/2011},
  issn         = {1},
  url          = {http://slim.eos.ubc.ca/Publications/public/Journals/SPIEreport.pdf},
  optdoi       = {doi:10.1117/12.893861},
  keywords     = {SLIM,Compressive Sensing,Optimization,Full-waveform inversion},
  abstract     = {Images obtained from seismic data are used by the
                  oil and gas industry for geophysical
                  exploration. Cutting-edge methods for transforming
                  the data into interpretable images are moving away
                  from linear approximations and high-frequency
                  asymptotics towards Full Waveform Inversion (FWI), a
                  nonlinear data-fitting procedure based on full data
                  modeling using the wave-equation. The size of the
                  problem, the nonlinearity of the for- ward model,
                  and ill-posedness of the formulation all contribute
                  to a pressing need for fast algorithms and novel
                  regularization techniques to speed up and improve
                  inversion results. In this paper, we design a
                  modified Gauss-Newton algorithm to solve the PDE-
                  constrained optimization problem using ideas from
                  stochastic optimization and com- pressive
                  sensing. More specifically, we replace the
                  Gauss-Newton subproblems by ran- domly subsampled,
                  -$\ell_1$ regularized subproblems. This allows us us
                  significantly reduce the computational cost of
                  calculating the updates and exploit the
                  compressibility of wavefields in Curvelets. We
                  explain the relationships and connections between
                  the new method and stochastic optimization and
                  compressive sensing (CS), and demonstrate the
                  efficacy of the new method on a large-scale
                  synthetic seismic example.}
}


@CONFERENCE{aravkin2011SIAMfwi,
  author       = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen and James V. Burke and Xiang Li},
  title        = {Full Waveform Inversion with Compressive Updates},
  booktitle    = {SIAM},
  year         = {2011},
  organization = {SIAM CS\&E 2011},
  abstract     = {Full-waveform inversion relies on large
                  multi-experiment data volumes.  While improvements
                  in acquisition and inversion have been extremely
                  successful, the current push for higher quality
                  models reveals fundamental shortcomings handling
                  increasing problem sizes numerically. To address
                  this fundamental issue, we propose a randomized
                  dimensionality-reduction strategy motivated by
                  recent developments in stochastic optimization and
                  compressive sensing. In this formulation
                  conventional Gauss-Newton iterations are replaced by
                  dimensionality-reduced sparse recovery problems with
                  source encodings.},
  keywords     = {SLIM,Presentation,Full-waveform inversion}, 
  presentation = {http://slim/Publications/Public/Presentations/2011/Aravkin2.28.2011.pdf}
}


@CONFERENCE{aravkin2011ICIAMspf,
  author       = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and Felix J. Herrmann},
  title        = {Sparsity promoting formulations and algorithms for
                  FWI. Presented at AMP Medical and Seismic Imaging,
                  2011, Vancouver BC.},
  booktitle    = {ICIAM},
  year         = {2011},
  organization = {ICIAM 2011},
  abstract     = {Full Waveform Inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. FWI is typically formulated as a nonlinear
                  least squares optimization problem, and various
                  regularization techniques are used to guide the
                  optimization because the problem is ill-posed. We
                  propose a novel sparse regularization which exploits
                  the ability of curvelets to efficiently represent
                  geophysical images.  We then formulate a
                  corresponding sparsity promoting constrained
                  optimization problem, which we solve using an open
                  source algorithm.  The techniques are applicable to
                  any inverse problem where sparsity modeling is
                  appropriate.  We demonstrate the efficacy of the
                  formulation on a toy example (stylized cross-well
                  experiment) and on a realistic Seismic example
                  (partial Marmoussi model). We also discuss the
                  tradeoff between model fit and sparsity promotion,
                  with a view to extend existing techniques for linear
                  inverse problems to the case where the forward model
                  is nonlinear. },
  optmonth     = {07/2011},
  date-added   = {2011-07-15},
  keywords     = {SLIM,Presentation,Full-waveform inversion,Optimization}, 
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/aravkin2011AMP.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/aravkin2011AMP.pdf}
}


@CONFERENCE{aravkin2011ICIAMrfwiu,
  author       = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Robust FWI using Student's t-distribution},
  booktitle    = {ICIAM},
  year         = {2011},
  organization = {ICIAM 2011},
  abstract     = {Iterative inversion algorithms require repeated
                  simulation of 3D time-dependent acoustic, elastic,
                  or electromagnetic wave fields, extending hundreds
                  of wavelengths and hundreds of periods. Also,
                  seismic data is rich in information at every
                  representable scale. Thus simulation-driven
                  optimization approaches to inversion impose great
                  demands on simulator efficiency and accuracy. While
                  computer hardware advances have been of critical
                  importance in bringing inversion closer to practical
                  application, algorithmic advances in simulator
                  methodology have been equally important. Speakers in
                  this two-part session will address a variety of
                  numerical issues arising in the wave simulation, and
                  in its application to inversion. },
  date-added   = {2011-07-20},
  optmonth     = {07/2011},
  keywords     = {SLIM,Presentation,ICIAM,Full-waveform inversion,Optimization}, 
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/aravkin2011WAVES.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/aravkin2011WAVES.pdf}
}


@CONFERENCE{aravkin2011EAGEspfwi,
  author       = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and Felix J. Herrmann},
  title        = {Sparsity Promoting Formulations and Algorithms for FWI},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/Aravkin2011EAGEspfwi/Aravkin2011EAGEspfwi.pdf },
  keywords     = {EAGE,Presentation, Full-waveform inversion}
}


@CONFERENCE{aravkin2011EAGEnspf,
  author       = {Aleksandr Y. Aravkin and James V. Burke and Felix J. Herrmann and Tristan van Leeuwen},
  title        = {A Nonlinear Sparsity Promoting Formulation and Algorithm for Full Waveform Inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Full Waveform Inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. FWI is typically formulated as a nonlinear
                  least squares optimization problem, and various
                  regularization techniques are used to guide the
                  optimization because the problem is illposed. In
                  this paper, we propose a novel sparse regularization
                  which exploits the ability of curvelets to
                  efficiently represent geophysical images. We then
                  formulate a corresponding sparsity promoting
                  constrained optimization problem, which we call
                  Nonlinear Basis Pursuit Denoise (NBPDN) and present
                  an algorithm to solve this problem to recover medium
                  parameters. The utility of the NBPDN formulation and
                  efficacy of the algorithm are demonstrated on a
                  stylized cross-well exper- iment, where a sparse
                  velocity perturbation is recovered with higher
                  quality than the standard FWI formulation (solved
                  with LBFGS).  The NBPDN formulation and algorithm
                  can recover the sparse perturbation even when the
                  data volume is compressed to 5 percent of the
                  original size using random superposition.},
  keywords     = {EAGE,Presentation,Full-waveform inversion,Optimization},
  optmonth     = {01/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/aravkin2011eage.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/aravkin11EAGEnspf/aravkin11EAGEnspf.pdf }
}


@CONFERENCE{aravkin2011SEGrobust,
  author       = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Robust full-waveform inversion using the Student's t-distribution},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {2669-2673},
  organization = {SEG},
  abstract     = {Full-waveform inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. Robust meth- ods for FWI are needed to
                  overcome sensitivity to noise and in cases where
                  modeling is particularly poor or far from the real
                  data generating process.  We survey previous robust
                  methods from a statistical perspective, and use this
                  perspective to derive a new robust method by
                  assuming the random errors in our model arise from
                  the Student's t-distribution.  We show that in
                  contrast to previous robust methods, the new method
                  progres- sively down-weighs large outliers,
                  effectively ignoring them once they are large
                  enough. This suggests that the new method is more
                  robust and suitable for situations with very poor
                  data quality or modeling. Experiments show that the
                  new method recovers as well or better than previous
                  robust methods, and can recover models with quality
                  comparable to standard meth- ods on noise-free data
                  when some of the data is completely corrupted, and
                  even when a marine acquisition mask is entirely
                  ignored in the modeling. The ability to ignore a
                  marine acqui- sition mask via robust FWI methods
                  offers an opportunity for stochastic optimization
                  methods in marine acquisition.},
  keywords     = {SEG,Full-waveform inversion,Optimization},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3627747},
  timestamp    = {2011-04-06 15:00:00 -0700},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/aravkin11SEGrobust/aravkin11SEGrobust.pdf }
}


@CONFERENCE{herrmann2011ICIAMconvexcompfwi,
  author       = {Felix J. Herrmann and Aleksandr Y. Aravkin and Tristan van Leeuwen and Xiang Li},
  title        = {FWI with sparse recovery: a convex-composite approach},
  booktitle    = {ICIAM},
  year         = {2011},
  organization = {ICIAM 2011},
  abstract     = {Iterative inversion algorithms require repeated
                  simulation of 3D time-dependent acoustic, elastic,
                  or electromagnetic wave fields, extending hundreds
                  of wavelengths and hundreds of periods. Also,
                  seismic data is rich in information at every
                  representable scale. Thus simulation-driven
                  optimization approaches to inversion impose great
                  demands on simulator efficiency and accuracy. While
                  computer hardware advances have been of critical
                  importance in bringing inversion closer to practical
                  application, algorithmic advances in simulator
                  methodology have been equally important. Speakers in
                  this two-part session will address a variety of
                  numerical issues arising in the wave simulation, and
                  in its application to inversion. },
  date-added   = {2011-07-20},
  optmonth     = {07/2011},
  keywords     = {ICIAM,Presentation,Full-waveform inversion,Optimization},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/herrmann2011ICIAM.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/herrmann2011ICIAM.pdf}
}


@CONFERENCE{herrmann2011SLRAfwi,
  author       = {Felix J. Herrmann and Aleksandr Y. Aravkin and Xiang Li and Tristan van Leeuwen},
  title        = {Full Waveform Inversion with Compressive Updates},
  booktitle    = {SLRA},
  year         = {2011},
  organization = {Sparse and Low Rank Approximation 2011},
  abstract     = {Full-waveform inversion relies on large
                  multi-experiment data volumes.  While improvements
                  in acquisition and inversion have been extremely
                  successful, the current push for higher quality
                  models reveals fundamental shortcomings handling
                  increasing problem sizes numerically. To address
                  this fundamental issue, we propose a randomized
                  dimensionality-reduction strategy motivated by
                  recent developments in stochastic optimization and
                  compressive sensing. In this formulation
                  conventional Gauss-Newton iterations are replaced by
                  dimensionality-reduced sparse recovery problems with
                  source encodings.},
  keywords     = {Presentation,Full-waveform inversion},
  presentation = {http://slim/Publications/Public/Presentations/2011/Herrmann2011css.pdf}
}


@CONFERENCE{herrmann2011EAGEefmsp,
  author       = {Felix J. Herrmann and Xiang Li},
  title        = {Efficient least-squares migration with sparsity promotion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Seismic imaging relies on the collection of
                  multi-experimental data volumes in combination with
                  a sophisticated back-end to create high-fidelity
                  inversion results. While significant improve- ments
                  have been made in linearized inversion, the current
                  trend of incessantly pushing for higher quality
                  models in increasingly complicated regions reveals
                  fundamental shortcomings in handling increasing
                  problem sizes numerically.  The so-called Òcurse of
                  dimensionalityÓ is the main culprit because it leads
                  to an exponential growth in the number of sources
                  and the corresponding number of wavefield
                  simulations required by Ôwave-equationÕ
                  migration. We address this issue by reducing the
                  number of sources by a randomized dimensionality
                  reduction technique that combines recent
                  developments in stochastic optimization and
                  compressive sensing.  As a result, we replace the
                  cur- rent formulations of imaging that rely on all
                  data by a sequence of smaller imaging problems that
                  use the output of the previous inversion as input
                  for the next. Empirically, we find speedups of at
                  least one order-of-magnitude when each reduced
                  experiment is considered theoretically as a separate
                  compressive-sensing experiment.},
  keywords     = {Presentation,EAGE,Imaging},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/herrmann2011eage.pdf},
  optmonth     = {01/2011},
  timestamp    = {2011-01-14},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/herrmann11EAGEefmsp/herrmann11EAGEefmsp.pdf }
}


@CONFERENCE{herrmann2011SLIMsummer1,
  author       = {Felix J. Herrmann},
  title        = {Gene Golub SIAM Summer School July 4 - 15, 2011},
  booktitle    = {SLIM},
  year         = {2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/HerrmannLecture1.pdf},
  timestamp    = {2011.08.05},
  keywords     = {Presentation},
  optmonth     = {08/2011}
}


@CONFERENCE{herrmann11SLIMsummer2,
  author       = {Felix J. Herrmann},
  title        = {Lecture 2. Gene Golub SIAM Summer School July 4 - 15, 2011},
  booktitle    = {SLIM},
  year         = {2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/HerrmannLecture2.pdf},
  timestamp    = {2011.08.05},
  keywords     = {Presentation},
  optmonth     = {08/2011}
}


@CONFERENCE{jumah2011SEGdrepsi,
  author       = {Bander Jumah and Felix J. Herrmann},
  title        = {Dimensionality-reduced estimation of primaries by sparse inversion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {3520-3525},
  organization = {SEG},
  abstract     = {Data-driven methods---such as the estimation of
                  primaries by sparse inversion---suffer from the "curse
                  of dimensionality", which leads to disproportional
                  growth in computational and storage demands when
                  moving to realistic 3-D field data. To re- move this
                  fundamental impediment, we propose a dimensional-
                  ity reduction technique where the "data matrix" is
                  approximated adaptively by a randomized low-rank
                  approximation. Com- pared to conventional methods,
                  our approach has the advantage that the cost of the
                  low-rank approximation is reduced significantly,
                  which may lead to considerable reductions in storage
                  and computational costs of the sparse
                  inversion. Application of the proposed formalism to
                  synthetic data shows that significant improvements
                  are achievable at low computational overhead
                  required to compute the low-rank approximations.},
  date-added   = {2011-04-06 15:00:00 -0700},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3627931},
  keywords     = {Presentation,SEG,Processing},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi.pdf}
}


@CONFERENCE{li2011EAGEfwirr,
  author       = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Full-waveform inversion with randomized L1 recovery for the model updates},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Full-waveform inversion (FWI) is a data fitting
                  procedure that relies on the collection of seis- mic
                  data volumes and sophisticated computing to create
                  high-resolution results. With the advent of FWI, the
                  improvements in acquisition and inversion have been
                  substantial, but these improvements come at a high
                  cost because FWI involves extremely large
                  multi-experiment data volumes. The main obstacle is
                  the Ôcurse of dimensionalityÕ exemplified by
                  NyquistÕs sampling criterion, which puts a
                  disproportionate strain on current acquisition and
                  processing systems as the size and desired
                  resolution increases. In this paper, we address the
                  Ôcurse of dimensionalityÕ by randomized dimen-
                  sionality reduction of the FWI problem adapted from
                  the field of CS. We invert for model updates by
                  replacing the Gauss-Newton linearized subproblem for
                  subsampled FWI with a sparsity promoting
                  formulation, and solve this formulation using the
                  SPGl1 algorithm. We speed up the algorithm and avoid
                  overfitting the data by solving for the linearized
                  updates only approximately.  Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we can compute a Newton-like update with
                  the reduced data volume at the cost of roughly one
                  gradient update for the fully sampled wavefield.},
  keywords     = {Presentation,EAGE,Full-waveform inversion},
  optmonth     = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr.pdf}
}


@CONFERENCE{li2011SBGFmgnsu,
  author       = {Xiang Li and Felix J. Herrmann and Tristan van Leeuwen and Aleksandr Y. Aravkin},
  title        = {Modified Gauss-Newton with Sparse Updates},
  booktitle    = {SBGF},
  year         = {2011},
  organization = {SBGF},
  abstract     = {Full-waveform inversion (FWI) is a data fitting
                  procedure that relies on the collection of seismic
                  data volumes and sophisticated computing to create
                  high-resolution models.With the advent of FWI, the
                  improvements in acquisition and inversion have been
                  substantial, but these improvements come at a high
                  cost because FWI involves extremely large
                  multi-experiment data volumes. The main obstacle is
                  the {\textquoteleft}curse of
                  dimensionality{\textquoteright} exemplified by
                  Nyquist{\textquoteright}s sampling criterion, which
                  puts a disproportionate strain on current
                  acquisition and processing systems as the size and
                  desired resolution increases. In this paper, we
                  address the {\textquoteleft}curse of
                  dimensionality{\textquoteright} by using randomized
                  dimensionality reduction of the FWI problem, coupled
                  with a modified Gauss-Newton (GN) method designed to
                  promote curvelet-domain sparsity of model
                  updates. We solve for these updates using the
                  spectral projected gradient method, implemented in
                  the SPG￿1 software package. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we can compute Gauss-Newton updates with
                  the reduced data volume at the cost of roughly one
                  gradient update for the fully sampled wavefield},
  keywords     = {SBGF,Full-waveform inversion},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SBGF/2011/li11SBGFmgnsu/li11SBGFmgnsu.pdf}
}


@CONFERENCE{lin2011EAGEepsic,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Estimating primaries by sparse inversion in a curvelet-like representation domain},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {We present an uplift in the fidelity and wavefront
                  continuity of results obtained from the Estimation
                  of Primaries by Sparse Inversion (EPSI) program by
                  reconstructing the primary events in a hybrid
                  wavelet-curvelet representation domain. EPSI is a
                  multiple removal technique that belongs to the class
                  of wavefield inversion methods, as an alternative to
                  the traditional adaptive-subtraction process. The
                  main assumption is that the correct primary events
                  should be as sparsely-populated in time as
                  possible. A convex reformulation of the original
                  EPSI algorithm allows its convergence property to be
                  preserved even when the solution wavefield is not
                  formed in the physical domain. Since wavefronts and
                  edge-type singularities are sparsely represented in
                  the curvelet domain, sparse solutions formed in this
                  domain will exhibit vastly improved continuity when
                  compared to those formed in the physical domain,
                  especially for the low-energy events at later
                  arrival times. Further- more, a wavelet-type
                  representation domain will preserve sparsity in the
                  reflected events even if they originate from
                  non-zero-order discontinuities in the subsurface,
                  providing an additional level of robustness. This
                  method does not require any changes in the
                  underlying computational algorithm and does not
                  explicitly impose continuity constraints on each
                  update.},
  keywords     = {Presentation,EAGE,Processing},
  optmonth     = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic.pdf}
}


@CONFERENCE{lin2011SEGrssde,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Robust source signature deconvolution and the estimation of primaries by sparse inversion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {4354-4359},
  abstract     = {The past few years had seen some concentrated
                  interest on a particular wavefield-inversion
                  approach to the popular SRME multiple removal
                  technique called Estimation of Primaries by Sparse
                  Inversion (EPSI). EPSI promises greatly improved
                  tol- erance to noise, missing data, edge effect, and
                  other physi- cal phenomenon generally not described
                  by the SRME relation (van Groenestijn and Verschuur,
                  2009a,b). It is based on the premise that it is
                  possible to stably invert for both the primary
                  impulse response and the source signature despite
                  beforehand having no (or very limited) explicit
                  knowledge of latter. The key to successful
                  applications of EPSI, as shown in very recent works
                  (Savels et al., 2010), is a robust way to
                  reconstruct very sparse primary impulse response
                  events as part of the inver- sion process. Based on
                  the various successful demonstrations in literature,
                  there is a very strong sense that EPSI will also
                  play an important role in future developments of
                  source sig- nature deconvolution and the general
                  recovering of wavefield spectrum. },
  organization = {Dept. of Earth and Ocean Sciences, University of British Columbia},
  keywords     = {Presentation,deconvolution, SEG, sparse inversion,Processing},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde.pdf},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3628116}
}


@CONFERENCE{mansour2011SBGFcspsma,
  author       = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title        = {A compressive sensing perspective on simultaneous marine acquisition},
  booktitle    = {SBGF},
  year         = {2011},
  organization = {SBGF},
  abstract     = {The high cost of acquiring seismic data in Marine
                  environments compels the adoption of simultaneous-
                  source acquisition - an emerging technology that is
                  stimulating both geophysical research and commercial
                  efforts.  In this paper, we discuss the properties
                  of randomized simultaneous acquisition matrices and
                  demonstrate that sparsity-promoting recovery
                  improves the quality of the reconstructed seismic
                  data volumes. Simultaneous Marine acquisition calls
                  for the development of a new set of design
                  principles and post-processing tools. Leveraging
                  established findings from the field of compressed
                  sensing, the recovery from simultaneous sources
                  depends on a sparsifying transform that compresses
                  seismic data, is fast, and reasonably incoherent
                  with the compressive sampling matrix. To achieve
                  this incoherence, we use random time dithering where
                  sequential acquisition with a single airgun is
                  replaced by continuous acquisition with multiple
                  airguns firing at random times and at random
                  locations. We demonstrate our results with
                  simulations of simultaneous Marine acquisition using
                  periodic and randomized time dithering.},
  keywords     = {Presentation,SBGF,Acquisition,Compressive Sensing},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SBGF/2011/Mansour11SBGFcspsma/Mansour11SBGFcspsma.pdf}
}


@CONFERENCE{vanleeuwen2011SIAMGEOmawt,
  author       = {Tristan van Leeuwen and Wim Mulder},
  title        = {Multiscale aspects of waveform tomography},
  booktitle    = {SIAMGEO},
  year         = {2011},
  organization = {SIAM GeoSciences 2011},
  abstract     = {We consider the inference of medium velocity from
                  transmitted acoustic waves. Typically, the
                  measurements are done in a narrow frequency band. As
                  a result the sensitivity of the data with respect to
                  velocity perturbations varies dramatically with the
                  scale of the perturbation.
                  {\textquoteleft}Smooth{\textquoteright}
                  perturbations will cause a phase shift, whereas
                  perturbations that vary on the wavelength-scale
                  cause amplitude variations. We investigate how to
                  incorporate this scale dependent behavior in the
                  formulation of the inverse problem.},
  keywords     = {Presentation},
  presentation = {http://slim/Publications/Public/Presentations/2011/SIAMGS11_MS61_Leeuwen.pdf}
}


@CONFERENCE{tu2011SEGmult,
  author       = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Migration with surface-related multiples from incomplete seismic data},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {3222-3227},
  organization = {SEG},
  abstract     = {Seismic acquisition is confined by limited aperture
                  that leads to finite illumination, which, together
                  with other factors, hin- ders imaging of subsurface
                  objects in complex geological set- tings such as
                  salt structures. Conventional processing, includ-
                  ing surface-related multiple elimination, further
                  reduces the amount of information we can get from
                  seismic data. With the growing consensus that
                  multiples carry valuable informa- tion that is
                  missing from primaries, we are motivated to exploit
                  the extra illumination provided by multiples to
                  image the sub- surface. In earlier research, we
                  proposed such a method by combining primary
                  estimation and sparsity-promoting migra- tion to
                  invert for model perturbations directly from the
                  total up-going wavefield. In this abstract, we focus
                  on a particular case. By exploiting the extra
                  illumination from surface-related multiples, we
                  mitigate the effects caused by migrating from in-
                  complete data with missing sources and missing
                  near-offsets.},
  keywords     = {Presentation,SEG,Imaging,Processing},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3627865},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult.pdf}
}


@CONFERENCE{tu2011EAGEspmsrm,
  author       = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Sparsity-promoting migration with surface-related multiples},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Multiples, especially the surface-related multiples,
                  form a significant part of the total up-going wave-
                  field. If not properly dealt with, they can lead to
                  false reflectors in the final image. So
                  conventionally practitioners remove them prior to
                  migration. Recently research has revealed that
                  multiples can actually provide extra illumination so
                  different methods are proposed to address the issue
                  that how to use multiples in seismic imaging, but
                  with various kinds of limitations.  In this
                  abstract, we combine primary estimation and
                  sparsity-promoting migration into one
                  convex-optimization process to include information
                  from multiples. Synthetic examples show that
                  multiples do make active contributions to seismic
                  migration. Also by this combination, we can benefit
                  from better recoveries of the GreenÕs function by
                  using sparsity-promoting algorithms since
                  reflectivity is sparser than the GreenÕs function.},
  keywords     = {Presentation,EAGE,Imaging,Processing},
  optmonth     = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm.pdf}
}


@CONFERENCE{vanleeuwen2011AMPhsdmwi,
  author       = {Tristan van Leeuwen and Mark Schmidt and Michael P. Friedlander and Felix J. Herrmann},
  title        = {A hybrid stocahstic-deterministic method for
                  waveform inversion. Presented at AMP Medical and
                  Seismic Imaging, 2011, Vancouver BC.},
  booktitle    = {AMP},
  year         = {2011},
  organization = {WAVES 2011},
  abstract     = {A lot of seismic and medical imaging problems can be
                  written as a least-squares data- fitting problem. In
                  particular, we consider the case of multi-experiment
                  data, where the data consists of a large number of
                  ÔindependentÕ measurements. Solving the inverse
                  prob- lem then involves repeatedly forward modeling
                  the data for each of these experiments. In case the
                  number of experiments is large and the modeling
                  kernel expensive to apply, such an approach may be
                  prohibitively expensive. We review techniques from
                  stochastic opti- mization which aim at dramatically
                  reducing the number of experiments that need to be
                  modeled at each iteration. This reduction is
                  typically achieved by randomly subsampling the
                  data. Special care needs to be taken in the
                  optimization to deal with the stochasticity that is
                  introduced in this way. },
  date-added   = {2011-07-15},
  optmonth     = {07/2011},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/AMPleeuwen2011b.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/AMPleeuwen2011b.pdf}
}


@CONFERENCE{vanleeuwen2011EAGEhsdomwi,
  author       = {Tristan van Leeuwen and Felix J. Herrmann and Mark Schmidt and Michael P. Friedlander},
  title        = {A hybrid stochastic-deterministic optimization method for waveform inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Present-day high quality 3D acquisition can give us
                  lower frequencies and longer offsets with which to
                  invert. However, the computational costs involved in
                  handling this data explosion are
                  tremendous. Therefore, recent developments in
                  full-waveform inversion have been geared towards
                  reducing the computational costs involved. A key
                  aspect of several approaches that have been proposed
                  is a dramatic reduction in the number of sources
                  used in each iteration. A reduction in the number of
                  sources directly translates to less PDE-solves and
                  hence a lower computational cost. Re- cent attention
                  has been drawn towards reducing the sources by
                  randomly combining the sources in to a few
                  supershots, but other strategies are also
                  possible. In all cases, the full data misfit, which
                  involves all the sequential sources, is replaced by
                  a reduced misfit that is much cheaper to evaluate
                  because it involves only a small number of sources
                  (batchsize). The batchsize controls the accuracy
                  with which the reduced misfit approximates the full
                  misfit. The optimization of such an inaccurate, or
                  noisy, misfit is the topic of stochastic
                  optimization. In this paper, we propose an
                  optimization strategy that borrows ideas from the
                  field of stochastic optimization. The main idea is
                  that in the early stage of the optimization, far
                  from the true model, we do not need a very accurate
                  misfit. The strategy consists of gradually
                  increasing the batchsize as the iterations
                  proceed. We test the proposed strategy on a
                  synthetic dataset. We achieve a very reasonable
                  inversion result at the cost of roughly 13
                  evaluations of the full misfit. We observe a
                  speed-up of roughly a factor 20.},
  keywords     = {Presentation,EAGE,Full-waveform inversion,Optimization},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi_pres.pdf},
  optmonth     = {01/2011},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi.pdf}
}


@CONFERENCE{vanleeuwen2011WAVESpeiv,
  author       = {Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Probing the extended image volume for seismic velocity inversion},
  booktitle    = {WAVES},
  year         = {2011},
  organization = {Waves 2011},
  abstract     = {In seismic velocity inversion one aims to
                  reconstruct a kinematically correct subsurface
                  velocity model that can be used as input for further
                  processing and inversion of the data. An important
                  tool in velocity inversion is the prestack image
                  volume. This image volume can be defined as a cross-
                  correlation of the source and receivers wavefields
                  for non-zero space and time lags. If the background
                  velocity is kinematically acceptable, this image
                  volume will have its main contributions at zero lag,
                  even for complex models. Thus, it is an ideal tool
                  for wave-equation migration velocity analysis in the
                  presence of strong lateral heterogeneity. In
                  particular, it allows us to pose migration velocity
                  analysis as a PDE- constrained optimization problem,
                  where the goal is to minimize the energy in the
                  image volume at non-zero lag subject to fitting the
                  data approximately. However, it is computationally
                  infeasible to explicitly form the whole image
                  volume. In this paper, we discuss several ways to
                  reduce the computational costs involved in computing
                  the image volume and evaluating the focusing
                  criterion.  We reduce the costs for calculating the
                  data by randomized source synthesis. We also present
                  an efficient way to subsample the image
                  volume. Finally, we propose an alternative
                  optimization criterion and suggest a multiscale
                  inversion strategy for wave-equation MVA.  },
  date-added   = {2011-07-29},
  optmonth     = {07/2011},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/waves11leeuwen.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/waves11leeuwen.pdf}
}


@CONFERENCE{vanleeuwen2011SEGext,
  author       = {Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Probing the extended image volume},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {4045-4050},
  organization = {SEG},
  abstract     = {The prestack image volume can be defined as a cross-
                  correlation of the source and receivers wavefields
                  for non-zero space and time lags.  If the background
                  velocity is kinemati- cally acceptable, this image
                  volume will have its main contri- butions at zero
                  lag, even for complex models. Thus, it is an ideal
                  tool for wave-equation migration velocity analysis
                  in the presence of strong lateral heterogeneity. In
                  particular, it allows us to pose migration velocity
                  analysis as a PDE-constrained optimization problem,
                  where the goal is to minimize the en- ergy in the
                  image volume at non-zero lag subject to fitting the
                  data approximately.  However, it is computationally
                  infeasi- ble to explicitly form the whole image
                  volume. In this paper, we discuss several ways to
                  reduce the computational costs in- volved in
                  computing the image volume and evaluating the fo-
                  cusing criterion. We reduce the costs for
                  calculating the data by randomized source
                  synthesis. We also present an efficient way to
                  subsample the image volume. Finally, we propose an
                  alternative optimization criterion and suggest a
                  multiscale in- version strategy for wave-equation
                  MVA.},
  keywords     = {SEG,Imaging},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3628051},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/vanleeuwen11SEGext/vanleeuwen11SEGext.pdf}
}


@CONFERENCE{vanleeuwen2011ICIAMcbmcwe,
  author       = {Tristan van Leeuwen},
  title        = {A correlation-based misfit criterion for
                  wave-equation traveltime tomography. Presented at
                  ICIAM 2011, Vancouver BC.},
  booktitle    = {ICIAM},
  year         = {2011},
  organization = {ICIAM 2011},
  abstract     = {The inference of subsurface medium parameters from
                  seismic data can be posed as a PDE-constrained
                  data-fitting procedure. This approach is successful
                  in reconstructing medium perturbations that are in
                  the order of the wavelength. In practice, the data
                  lack low frequency content and this means that one
                  needs a good initial guess of the slowly varying
                  component of the medium. For a wrong starting model
                  an iterative reconstruction procedure is likely to
                  end up in a local minimum. We propose to use a
                  different measure of the misfit that makes the
                  optimization problem well-posed in terms of the
                  slowly varying velocity structures. This procedure
                  can be seen as a generalization of ray-based
                  traveltime tomography. We discuss the theoretical
                  underpinnings of the method and give some numerical
                  examples.},
  date-added   = {2011-07-19},
  optmonth     = {07/2011},
  keywords     = {Presentation,ICIAM,Imaging},
  file         = {http://slim/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf:PDF},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf}
}


@CONFERENCE{wason2011SEGsprsd,
  author       = {Haneet Wason and Felix J. Herrmann and Tim T.Y. Lin},
  title        = {Sparsity-promoting recovery from simultaneous data:
                  a compressive sensing approach},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {6-10},
  organization = {SEG},
  abstract     = {Seismic data acquisition forms one of the main
                  bottlenecks in seismic imaging and inversion. The
                  high cost of acquisition work and collection of
                  massive data volumes compel the adoption of
                  simultaneous-source seismic data acquisition - an
                  emerging technology that is developing rapidly,
                  stimulating both geophysical research and commercial
                  efforts.  Aimed at improving the performance of
                  marine- and land-acquisition crews, simultaneous
                  acquisition calls for development of a new set of
                  design principles and post-processing
                  tools. Leveraging developments from the field of
                  compressive sensing the focus here is on
                  simultaneous-acquisition design and
                  sequential-source data recovery. Apart from proper
                  compressive sensing sampling schemes, the recovery
                  from simultaneous simulations depends on a
                  sparsifying transform that compresses seismic data,
                  is fast, and reasonably incoherent with the
                  compressive-sampling matrix. Using the curvelet
                  transform, in which seismic data can be represented
                  parsimoniously, the recovery of the
                  sequential-source data volumes is achieved using the
                  sparsity-promoting program {\textemdash} SPGL1, a
                  solver based on projected spectral gradients. The
                  main outcome of this approach is a new technology
                  where acquisition related costs are no longer
                  determined by the stringent Nyquist sampling
                  criteria.},
  keywords     = {Presentation,SEG,Acquisition,Compressive Sensing},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3628174},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd.pdf}
}


%-------------------------------------------------2010-------------------------------------------------

@CONFERENCE{herrmann2010SEGerc, 
  author       = {Felix J. Herrmann},
  title        = {Empirical recovery conditions for seismic sampling},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2010},
  abstract     = {In this paper, we offer an alternative sampling
                  method leveraging recent insights from compressive
                  sensing towards seismic acquisition and processing
                  for data that are traditionally considered to be
                  undersampled.  The main outcome of this approach is
                  a new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria, such as Nyquist. At the
                  heart of our approach lies randomized incoherent
                  sampling that breaks subsampling related
                  interferences by turning them into harmless noise,
                  which we subsequently remove by promoting
                  transform-domain sparsity. Now, costs no longer grow
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold.  First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic acquisition.
                  Second, we show that accurate recovery can be
                  accomplished for compressively sampled data volumes
                  sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.  We illustrate this
                  principle by means of number of case studies.},
  keywords     = {SEG},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/herrmann10erc.pdf}
}


@CONFERENCE{frijlink2010EAGEcos,
  author       = {M.O. Frijlink and Reza Shahidi and Felix J. Herrmann and R.G. van Borselen},
  title        = {Comparison of Standard Adaptive Subtraction and
                  Primary-multiple Separation in the Curvelet Domain},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {In recent years, data-driven multiple prediction
                  methods and wavefield extrapolation methods have
                  proven to be powerful methods to attenuate multiples
                  from data acquired in complex 3-D geologic
                  environments.  These methods make use of a two-stage
                  approach, where first the multiples (surface-related
                  and / or internal) multiples are predicted before
                  they are subtracted from the original input data in
                  an adaptively.  The quality of these predicted
                  multiples often raises high expectations for the
                  adaptive subtraction techniques, but for various
                  reasons these expectations are not always met in
                  practice. Standard adaptive subtraction methods use
                  the well-known minimum energy criterion, stating
                  that the total energy after optimal multiple
                  attenuation should be minimal. When primaries and
                  multiples interfere , the minimum energy criterion
                  is no longer appropriate. Also, when multiples of
                  different orders interfere, adaptive energy
                  minimization will lead to a compromise between
                  different amplitudes corrections for the different
                  orders of multiples. This paper investigates the
                  performance of two multiple subtraction schemes for
                  a real data set that exhibits both interference
                  problems. Results from an adaptive subtraction in
                  the real curvelet domain, separating primaries and
                  multiples, are compared to those obtained using a
                  more conventional adaptive subtraction method in the
                  spatial domain.},
  keywords     = {EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/frijlink10EAGEcos/frijlink10EAGEcos.pdf}
}


@CONFERENCE{herrmann2010MATHIAScssr,
  author       = {Felix J. Herrmann},
  title        = {Compressive Sensing and Sparse Recovery in
                  Exploration Seismology. Presented at MATHIAS 2010
                  organized by Total SA. Paris.},
  booktitle    = {MATHIAS},
  year         = {2010},
  abstract     = {During this presentation, I will talk about how
                  recent results from compressive sensing and sparse
                  recovery can be used to solve problems in
                  exploration seismology where incomplete sampling is
                  ubiquitous.  I will also talk about how these ideas
                  apply to dimensionality reduction of full-waveform
                  inversion by randomly phase encoded sources.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2010/herrmann2010total.pdf}
}


@CONFERENCE{herrmann2010EAGErds,
  author       = {Felix J. Herrmann and Xiang Li},
  title        = {Randomized dimensionality reduction for full-waveform inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth continues to reveal
                  fundamental shortcomings in our ability to handle
                  the ever increasing problem size numerically.  Two
                  causes can be identified as the main culprits
                  responsible for this barrier. First, there is the
                  so-called {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase.  Secondly, there is the
                  recent {\textquoteleft}{\textquoteleft}departure
                  from Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this. In this paper, we address this situation by
                  randomized dimensionality reduction, which we adapt
                  from the field of compressive sensing.  In this
                  approach, we combine deliberate randomized
                  subsampling with structure-exploiting
                  transform-domain sparsity promotion. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we compute Newton-like updates at the
                  cost of roughly one gradient update for the
                  fully-sampled wavefield.},
  keywords     = {Presentation,EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds_pres.pdf},  
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds.pdf/}
}


@CONFERENCE{herrmann2010EAGErss,
  author       = {Felix J. Herrmann},
  title        = {Randomized sampling strategies},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {Seismic exploration relies on the collection of
                  massive data volumes that are subsequently mined for
                  information during seismic processing.  While this
                  approach has been extremely successful in the past,
                  the current trend towards higher quality images in
                  increasingly complicated regions continues to reveal
                  fundamental shortcomings in our workflows for
                  high-dimensional data volumes. Two causes can be
                  identified..  First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase.  Secondly, there is the
                  recent {\textquoteleft}{\textquoteleft}departure
                  from Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  randomized subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. As such we end up with a new
                  technology where the costs of acquisition and
                  processing are no longer dictated by the size of the
                  acquisition but by the transform-domain sparsity of
                  the end-product.},
  keywords     = {Presentation,EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss_pres.pdf},  
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss.pdf}
}


@CONFERENCE{herrmann2010IRISsns,
  author       = {Felix J. Herrmann},
  title        = {Sub-Nyquist sampling and sparsity: getting more
                  information from fewer samples. Presented at the
                  IRIS Workshop},
  booktitle    = {IRIS},
  year         = {2010},
  abstract     = {Many seismic exploration techniques rely on the
                  collection of massive data volumes. While this
                  approach has been extremely successful in the past,
                  current efforts toward higher resolution images in
                  increasingly complicated regions of the Earth
                  continue to reveal fundamental shortcomings in our
                  workflows. Chiefly amongst these is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. In this presentation, we offer an
                  alternative sampling method leveraging recent
                  insights from compressive sensing towards seismic
                  acquisition and processing of severely under-sampled
                  data. The main outcome of this approach is a new
                  technology where acquisition and processing related
                  costs are no longer determined by overly stringent
                  sampling criteria, such as Nyquist. At the heart of
                  our approach lies randomized incoherent sampling
                  that breaks subsampling related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting transform-domain
                  sparsity. Now, costs no longer grow significantly
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  exploration. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity. Many seismic exploration
                  techniques rely on the collection of massive data
                  volumes. While this approach has been extremely
                  successful in the past, current efforts toward
                  higher resolution images in increasingly complicated
                  regions of the Earth continue to reveal fundamental
                  shortcomings in our workflows. Chiefly amongst these
                  is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. In this presentation, we offer an
                  alternative sampling method leveraging recent
                  insights from compressive sensing towards seismic
                  acquisition and processing of severely under-sampled
                  data. The main outcome of this approach is a new
                  technology where acquisition and processing related
                  costs are no longer determined by overly stringent
                  sampling criteria, such as Nyquist. At the heart of
                  our approach lies randomized incoherent sampling
                  that breaks subsampling related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting transform-domain
                  sparsity. Now, costs no longer grow significantly
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  exploration. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2010/herrmann2010iris.pdf}
}


@CONFERENCE{johnson2010EAGEeop,
  author       = {James Johnson and Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Estimation of primaries via sparse inversion with reciprocity},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {Accurate removal of surface related multiples is a
                  key step in seismic data processing. The industry
                  standard for removing multiples is SRME, which
                  involves convolving the data with itself to predict
                  the multiples, followed by an adaptive subtraction
                  procedure to recover the primaries (Verschuur and
                  Berkhout, 1997). Other methods involve
                  multidimensional division of the up-going and
                  down-going wavefields (Amundsen, 2001). However,
                  this approach may suffer from stability
                  problems. With the introduction of the
                  {\textquoteleft}{\textquoteleft}estimation of
                  primaries by sparse
                  inversion{\textquoteright}{\textquoteright}(EPSI),
                  van Groenestijn and Verschuur (2009) recentely
                  reformulated SRME to jointly estimate the
                  surface-free impulse response and the source
                  signature directly from the data. The advantage of
                  EPSI is that it recovers the primary response
                  directly, and does not require a second processing
                  step for the subtraction of estimated multiples from
                  the original data. However, because it estimates
                  both the primary impulse response and source
                  signature from the data EPSI must be regularized.
                  Motivated by recent successful application of the
                  curvelet transform in seismic data processing
                  (Herrmann et al., 2007), we formulate EPSI as a
                  bi-convex optimization problem that seeks sparsity
                  on the surface-free Green{\textquoteright}s function
                  and Fourier-domain smoothness on the source
                  wavelet. Our main contribution compared to previous
                  work (Lin and Herrmann, 2009), and the contribution
                  of that author to the proceedings of this
                  meeting(Lin and Herrmann, 2010), is that we employ
                  the physical principle of as source-receiver
                  reciprocity to improve the inversion.},
  keywords     = {EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/johnson10EAGEeop/johnson10EAGEeop.pdf}
}


@CONFERENCE{li2010SEGfwi,
  author       = {Xiang Li and Felix J. Herrmann},
  title        = {Full-waveform inversion from compressively recovered model updates},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2010},
  volume       = {29},
  pages        = {1029-1033},
  organization = {SEG},
  abstract     = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth reveals fundamental
                  shortcomings in our ability to handle increasing
                  problem size numerically. Two main culprits can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution increases. Secondly,
                  there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this. In this paper, we address this situation by
                  randomized dimensionality reduction, which we adapt
                  from the field of compressive sensing.  In this
                  approach, we combine deliberate randomized
                  subsampling with structure-exploiting
                  transform-domain sparsity promotion. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we compute Newton-like updates at the
                  cost of roughly one gradient update for the
                  fully-sampled wavefield.},
  keywords     = {Presentation,SEG,Full-waveform inversion},
  optdoi       = {10.1190/1.3513022},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi.pdf}
}


@CONFERENCE{lin2010EAGEseo,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Stabilization of estimation of primaries via sparse inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {Recent works on surface-related multiple removal
                  include a direct estimation method proposed by van
                  Groenestijn and Verschuur (2009), where under a
                  sparsity assumption the primary impulse response is
                  determined directly from a data-driven wavefield
                  inversion process called Estimation of Primaries by
                  Sparse Inversion (EPSI). The authors have shown that
                  this approach is superior to traditional estimation
                  subtraction processes such as SRME on shallow bottom
                  marine data, where by expanding the model to
                  simultaneously invert for the near-offset traces,
                  which are not directly available in most situation
                  but are observable in the data multiples, a large
                  improvement over Radon interpolation is
                  demonstrated. One of the major roadblocks to the
                  widespread adoption of EPSI is that one must have
                  precise knowledge of a time-window that contains
                  multiple-free primaries during each update. There is
                  some anecdotal evidence that the inversion result is
                  unstable under errors in the time-window length, a
                  behavior that runs contrary to the strengths of EPSI
                  and diminishes its effectiveness for shallow-bottom
                  marine data where multiples are closely spaced.
                  Moreover, due to the nuances involved in
                  regularizing the model impulse response in the
                  inverse problem, the EPSI approach has an additional
                  number of inversion parameters to choose and often
                  also does not often lead to a stable solution under
                  perturbations to these parameters.  We show that the
                  specific sparsity constraint on the EPSI updates
                  lead to an inherently intractable problem, and that
                  the time-window and other inversion variables arise
                  as additional regularizations on the unknown towards
                  a meaningful solution. We furthermore suggest a way
                  to remove almost all of these parameters via a L0 to
                  L1 convexification, which stabilizes the inversion
                  while preserving the crucial sparsity assumption in
                  the primary impulse response model.},
  keywords     = {Presentation,EAGE,Processing},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo_pres.pdf},  
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo.pdf}
}


@CONFERENCE{lin10SEGspm,
  author       = {Tim T.Y. Lin and Ning Tu and Felix J. Herrmann},
  title        = {Sparsity-promoting migration from surface-related multiples},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2010},
  volume       = {29},
  pages        = {3333-3337},
  organization = {SEG},
  abstract     = {Seismic imaging typically begins with the removal of
                  multiple energy in the data, out of fear that it may
                  introduce erroneous structure.  However, seismic
                  multiples have effectively seen more of the
                  earth{\textquoteright}s structure, and if treated
                  correctly can potential supply more information to a
                  seismic image compared to primaries. Past approaches
                  to accomplish this leave ample room for improvement;
                  they either require extensive modification to
                  standard migration techniques, rely too much on
                  prior information, require extensive pre-processing,
                  or resort to full-waveform inversion. We take some
                  valuable lessons from these efforts and present a
                  new approach balanced in terms of ease of
                  implementation, robustness, efficiency and
                  well-posedness, involving a sparsity-promoting
                  inversion procedure using standard Born migration
                  and a data-driven multiple modeling approach based
                  on the focal transform.},
  keywords     = {Presentation,SEG,Processing},
  optdoi       = {10.1190/1.3513540},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm.pdf}
}


@CONFERENCE{moghaddam2010SEGrfw,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann},
  title        = {Randomized full-waveform inversion: a dimenstionality-reduction approach},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2010},
  volume       = {29},
  pages        = {977-982},
  organization = {SEG},
  abstract     = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth reveals fundamental
                  shortcomings in our ability to handle increasing
                  problem sizes numerically. Two main culprits can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution increases. Secondly,
                  there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to develop algorithms that are amenable to
                  parallelization.  In this paper, we discuss
                  different strategies that address these issues via
                  randomized dimensionality reduction.},
  keywords     = {Presentation,SEG,Full-waveform inversion,Optimization},
  optdoi       = {10.1190/1.3513940},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw.pdf}
}



%-------------------------------------------------2009-------------------------------------------------

@CONFERENCE{erlangga2009SEGfwi,
  author       = {Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Full-waveform Inversion with Gauss-Newton-Krylov Method},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  abstract     = {This abstract discusses an implicit implementation
                  of the Gauss-Newton method, used for the
                  frequency-domain full-waveform inversion, where the
                  inverse of the Hessian for the update is never
                  formed explicitly.  Instead, the inverse of the
                  Hessian is computed approximately by a conjugate
                  gradient (CG) method, which only requires the action
                  of the Hessian on the CG search direction. This
                  procedure avoids an excessive computer storage,
                  usually needed for storing the Hessian, at the
                  expense of extra computational work in CG. An
                  effective preconditioner for the Hessian is
                  important to improve the convergence of CG, and
                  hence to reduce the overall computational work.},
  keywords     = {Presentation, SEG, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/erlangga09segfwi.pdf}
}


@CONFERENCE{erlangga2009EAGEmwi,
  author       = {Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Migration with implicit solvers for the time-harmonic Helmholtz equation},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2009},
  organization = {EAGE},
  abstract     = {From the measured seismic data, the location and the
                  amplitude of reflectors can be determined via a
                  migration algorithm. Classically, following
                  Claerbout{\textquoteright}s imaging principle [2], a
                  reflector is located at the position where the
                  source{\textquoteright}s forward-propagated
                  wavefield correlates with the backward-propagated
                  wavefield of the receiver data. Lailly and Tarantola
                  later showed that this imaging principle is an
                  instance of inverse problems, with the associated
                  migration operator formulated via a least-squares
                  functional; see [6, 12, 13]. Furthermore, they
                  showed that the migrated image is associated with
                  the gradient of this functional with respect to the
                  image. If the solution of the least-squares
                  functional is done iteratively, the
                  correlation-based image coincides up to a constant
                  with the first iteration of a gradient method. In
                  practice, this migration is done either in the time
                  domain or in the frequency domain. In the
                  frequency-domain migration, the main bottleneck thus
                  far, which renders its full implementation to large
                  scale problems, is the lack of efficient solvers for
                  computing wavefields. Robust direct methods easily
                  run into excessive memory requirements as the size
                  of the problem increases. On the other hand,
                  iterative methods, which are less demanding in terms
                  of memory, suffered from lack of convergence. During
                  the past years, however, progress has been made in
                  the development of an efficient iterative method [4,
                  3] for the frequency-domain wavefield
                  computations. In this paper, we will show the
                  significance of this method (called MKMG) in the
                  context of the frequency-domain migration, where
                  multi-shot-frequency wavefields (of order of 10,000
                  related wavefields) need to be computed.},
  keywords     = {Presentation,EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/erlanggaEAGE2009.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Erlangga09EAGEmwi/Erlangga09EAGEmwi.pdf}
}


@CONFERENCE{erlangga2009SEGswi,
  author       = {Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Seismic waveform inversion with Gauss-Newton-Krylov method},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {2357-2361},
  organization = {SEG},
  abstract     = {This abstract discusses an implicit implementation
                  of the Gauss-Newton method, used for the
                  frequency-domain full-waveform inversion, where the
                  inverse of the Hessian for the update is never
                  formed explicitly.  Instead, the inverse of the
                  Hessian is computed approximately by a conjugate
                  gradient (CG) method, which only requires the action
                  of the Hessian on the CG search direction. This
                  procedure avoids an excessive computer storage,
                  usually needed for storing the Hessian, at the
                  expense of extra computational work in CG. An
                  effective preconditioner for the Hessian is
                  important to improve the convergence of CG, and
                  hence to reduce the overall computational work.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255332},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi.pdf}
}


@CONFERENCE{friedlander2009VIETcsgpa,
  author       = {Michael P. Friedlander},
  title        = {Computing sparse and group-sparse approximations},
  booktitle    = {VIET},
  organization = {2009 High Performance Scientific Computing Conference},
  year         = {2009},
  address      = {Hanoi, Vietnam},
  keywords     = {minimization, SLIM}
}


@CONFERENCE{friedlander2009NUalssr,
  author       = {Michael P. Friedlander},
  title        = {Algorithms for large-scale sparse reconstruction},
  booktitle    = {IEMS},
  organization = {IEMS Colloquim Speaker},
  year         = {2009},
  address      = {Northwestern University},
  keywords     = {minimization, SLIM}
}


@CONFERENCE{friedlander2009SCAIMspot,
  author       = {Ewout {van den Berg} and Michael P. Friedlander},
  title        = {Spot: A linear-operator toolbox for Matlab},
  booktitle    = {SCAIM},
  organization = {SCAIM Seminar},
  year         = {2009},
  address      = {University of British Columbia},
  keywords     = {minimization, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Private/Presentations/sinbad/2010Fall/Thu-13-50-Friedlander.pdf }
}


@CONFERENCE{herrmann2009SEGcib,
  author       = {Felix J. Herrmann},
  title        = {Compressive imaging by wavefield inversion with group sparsity},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {2337-2341},
  organization = {SEG},
  abstract     = {Migration relies on multi-dimensional correlations
                  between source- and residual wavefields. These
                  multi-dimensional correlations are computationally
                  expensive because they involve operations with
                  explicit and full matrices that contain both
                  wavefields. By leveraging recent insights from
                  compressive sampling, we present an alternative
                  method where linear correlation-based imaging is
                  replaced by imaging via multidimensional
                  deconvolutions of compressibly sampled wavefields.
                  Even though this approach goes at the expense of
                  having to solve a sparsity-promotion recovery
                  program for the image, our wavefield inversion
                  approach has the advantage of reducing the system
                  size in accordance to transform-domain sparsity of
                  the image. Because seismic images also exhibit a
                  focusing of the energy towards zero offset, the
                  compressive-wavefield inversion itself is carried
                  out using a recent extension of one-norm solver
                  technology towards matrix-valued problems. These
                  so-called hybrid $(1,\,2)$-norm solvers allow us to
                  penalize pre-stack energy away from zero offset
                  while exploiting joint sparsity amongst near-offset
                  images. Contrary to earlier work to reduce modeling
                  and imaging costs through random phase-encoded
                  sources, our method compressively samples wavefields
                  in model space.  This approach has several
                  advantages amongst which improved system-size
                  reduction, and more flexibility during subsequent
                  inversions for subsurface properties.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255328},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib.pdf}
}


@CONFERENCE{herrmann2009EAGEcsa,
  author       = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title        = {Compressive sensing applied to full-waveform inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2009},
  organization = {EAGE},
  abstract     = {With the recent resurgence of full-waveform
                  inversion, the computational cost of solving forward
                  modeling problems has become{\textendash}-aside from
                  issues with non-uniqueness{\textendash}-one of the
                  major impediments withstanding successful
                  application of this technology to industry-size data
                  volumes. To overcome this impediment, we argue that
                  further improvements in this area will depend on a
                  problem formulation with a computational complexity
                  that is no longer strictly determined by the size of
                  the discretization but by transform-domain sparsity
                  of its solution. In this new paradigm, we bring
                  computational costs in par with our ability to
                  compress seismic data and images. This premise is
                  related to two recent developments. First, there is
                  the new field of compressive sensing (CS in short
                  throughout the paper, Cand{\textquoteleft}es et al.,
                  2006; Donoho, 2006){\textendash}-where the argument
                  is made, and rigorously proven, that compressible
                  signals can be recovered from severely sub-Nyquist
                  sampling by solving a sparsity promoting
                  program. Second, there is in the seismic community
                  the recent resurgence of simultaneous-source
                  acquisition (Beasley, 2008; Krohn and Neelamani,
                  2008; Herrmann et al., 2009; Berkhout, 2008;
                  Neelamani et al., 2008), and continuing efforts to
                  reduce the cost of seismic modeling, imaging, and
                  inversion through phase encoding of simultaneous
                  sources (Morton and Ober, 1998; Romero et al., 2000;
                  Krohn and Neelamani, 2008; Herrmann et al., 2009),
                  removal of subsets of angular frequencies (Sirgue
                  and Pratt, 2004; Mulder and Plessix, 2004; Lin et
                  al., 2008) or plane waves (Vigh and Starr, 2008). By
                  using CS principles, we remove sub-sampling
                  interferences asocciated with these approaches
                  through a combination of exploiting transform-domain
                  sparsity, properties of certain sub-sampling
                  schemes, and the existence of sparsity promoting
                  solvers.},
  keywords     = {Presentation,EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09eagecs.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Herrmann09EAGEcsa/Herrmann09EAGEcsa.pdf}
}


@CONFERENCE{herrmann2009SAMPTAcws,
  author       = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title        = {Compressive-wavefield simulations},
  booktitle    = {SAMPTA},
  year         = {2009},
  organization = {SAMPTA},
  abstract     = {Full-waveform inversion{\textquoteright}s high
                  demand on computational resources forms, along with
                  the non-uniqueness problem, the major impediment
                  withstanding its widespread use on industrial-size
                  datasets.  Turning modeling and inversion into a
                  compressive sensing problem{\textendash}-where
                  simulated data are recovered from a relatively small
                  number of independent simultaneous
                  sources{\textendash}-can effectively mitigate this
                  high-cost impediment. The key is in showing that we
                  can design a sub-sampling operator that commutes
                  with the time-harmonic Helmholtz system. As in
                  compressive sensing, this leads to a reduction in
                  simulation cost.  Moreover, this reduction is
                  commensurate with the transform-domain sparsity of
                  the solution, implying that computational costs are
                  no longer determined by the size of the
                  discretization but by transform-domain sparsity of
                  the solution of the CS problem which forms our data.
                  The combination of this sub-sampling strategy with
                  our recent work on implicit solvers for the
                  Helmholtz equation provides a viable alternative to
                  full-waveform inversion schemes based on explicit
                  finite-difference methods.},
  keywords     = {SAMPTA},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2009/Herrmann09SAMPTAcws/Herrmann09SAMPTAcws.pdf}
}


@CONFERENCE{herrmann09EAGEbnrs,
  author       = {Felix J. Herrmann and Gang Tang and Reza Shahidi and Gilles Hennenfent and Tim T.Y. Lin},
  title        = {Beating Nyquist by randomized sampling. Presented at the EAGE (workshop), Amsterdam},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2009},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09eagews.pdf}
}


@CONFERENCE{herrmann2009IAPcsisa,
  author       = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title        = {Compressive seismic imaging with simultaneous
                  acquisition presented at the IAP meeting, Vienna,},
  booktitle    = {IAP},
  year         = {2009},
  abstract     = {The shear size of seismic data volumes forms one of
                  the major impediments for the inversion of seismic
                  data. Turning forward modeling and inversion into a
                  compressive sensing (CS) problem - where simulated
                  data are recovered from a relatively small number of
                  independent sources - can effectively mitigate this
                  high-cost impediment. Our key contribution lies in
                  the design of a sub-sampling operator that commutes
                  with the time-harmonic Helmholtz system. As in
                  compressive sensing, this leads to a reduction of
                  simulation cost. This reduction is commensurate with
                  the transform-domain sparsity of the solution.,
                  implying that computational costs are no longer
                  determined by the size of the discretization but by
                  transform-domain sparsity of the solution of the CS
                  problem that recovers the data. The combination of
                  this sub-sampling strategy with our recent work on
                  preconditioned implicit solvers for the
                  time-harmonic Helmholtz equation provides a viable
                  alternative to full-waveform inversion schemes based
                  on explicit time-domain finite-difference methods.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09AIP1.pdf}
}


@CONFERENCE{herrmann2009SEGsns,
  author       = {Felix J. Herrmann},
  title        = {Sub-Nyquist sampling and sparsity: How to get more information from fewer samples},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3410-3415},
  organization = {SEG},
  abstract     = {Seismic exploration relies on the collection of
                  massive data volumes that are subsequently mined for
                  information during seismic processing.  While this
                  approach has been extremely successful in the past,
                  the current trend of incessantly pushing for higher
                  quality images in increasingly complicated regions
                  of the Earth continues to reveal fundamental
                  shortcomings in our workflows to handle massive
                  high-dimensional data volumes. Two causes can be
                  identified as the main culprits responsible for this
                  barrier. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase.  Secondly, there is the
                  recent {\textquoteleft}{\textquoteleft}departure
                  from Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  \emph{randomized} subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. Because of this size reduction
                  both impediments are removed and we end up with a
                  new technology where the costs of acquisition and
                  processing are no longer dictated by the \emph{size
                  of the acquisition} but by the transform-domain
                  \emph{sparsity} of the end-product after
                  processing.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255570},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns.pdf}
}


@CONFERENCE{herrmann2009PIMScssr3,
  author       = {Felix J. Herrmann},
  title        = {Compressed sensing and sparse recovery in
                  exploration seismology. Lecture III presented at the
                  PIMS Summer School on Seismic Imaging. Seattle.},
  booktitle    = {PIMS},
  year         = {2009},
  abstract     = {In this course, I will present how recent results
                  from compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well.  The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS3.pdf}
}


@CONFERENCE{herrmann2009PIMScssr2,
  author       = {Felix J. Herrmann},
  title        = {Compressed sensing and sparse recovery in
                  exploration seismology. Lecture II presented at the
                  PIMS Summer School on Seismic Imaging. Seattle.},
  booktitle    = {PIMS},
  year         = {2009},
  abstract     = {In this course, I will present how recent results
                  from compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well.  The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS2.pdf}
}


@CONFERENCE{herrmann2009PIMScssr1,
  author       = {Felix J. Herrmann},
  title        = {Compressed sensing and sparse recovery in
                  exploration seismology. Lecture I presented at the
                  PIMS Summer School on Seismic Imaging. Seattle.},
  booktitle    = {PIMS},
  year         = {2009},
  abstract     = {In this course, I will present how recent results
                  from compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well.  The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS1.pdf}
}


@CONFERENCE{herrmann2009SEGrpl,
  author       = {Felix J. Herrmann},
  title        = {Reflector-preserved lithological upscaling},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3466-3470},
  organization = {SEG},
  abstract     = {By combining Percolation models with lithological
                  smoothing, we arrive at method for upscaling rock
                  elastic constants that preserves reflections.  In
                  this approach, the Percolation model predicts sharp
                  onsets in the elastic moduli of sand-shale mixtures
                  when the shales reach a critical volume fraction. At
                  that point, the shale inclusions form a connected
                  cluster, and the macroscopic rock properties change
                  with the power-law growth of the cluster. This
                  switch-like nonlinearity preserves singularities,
                  and hence reflections, even if no sharp transition
                  exists in the lithology or if they are smoothed out
                  using standard upscaling procedures.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255582},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl.pdf}
}


@CONFERENCE{kumar2009SEGins,
  author       = {Vishal Kumar and Felix J. Herrmann},
  title        = {Incoherent noise suppression with curvelet-domain sparsity},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3356-3360},
  organization = {SEG},
  abstract     = {The separation of signal and noise is a key issue in
                  seismic data processing. By noise we refer to the
                  incoherent noise that is present in the data. We use
                  the recently introduced multiscale and
                  multidirectional curvelet transform for suppression
                  of random noise. The curvelet transform decomposes
                  data into directional plane waves that are local in
                  nature. The coherent features of the data occupy the
                  large coefficients in the curvelet domain, whereas
                  the incoherent noise lives in the small
                  coefficients. In other words, signal and noise have
                  minimal overlap in the curvelet domain. This gives
                  us a chance to use curvelets to suppress noise
                  present in data.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255557},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins.pdf}
}


@CONFERENCE{lin2009SEGcsf,
  author       = {Tim T.Y. Lin and Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Compressive simultaneous full-waveform simulation},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {2577-2581},
  organization = {SEG},
  abstract     = {The fact that the computational complexity of
                  wavefield simulation is proportional to the size of
                  the discretized model and acquisition geometry, and
                  not to the complexity of the simulated wavefield, is
                  a major impediment within seismic imaging. By
                  turning simulation into a compressive sensing
                  problem{\textendash}where simulated data is
                  recovered from a relatively small number of
                  independent simultaneous sources{\textendash}we
                  remove this impediment by showing that compressively
                  sampling a simulation is equivalent to compressively
                  sampling the sources, followed by solving a reduced
                  system. As in compressive sensing, this allows for a
                  reduction in sampling rate and hence in simulation
                  costs. We demonstrate this principle for the
                  time-harmonic Helmholtz solver. The solution is
                  computed by inverting the reduced system, followed
                  by a recovery of the full wavefield with a sparsity
                  promoting program. Depending on the
                  wavefield{\textquoteright}s sparsity, this approach
                  can lead to a significant cost reduction, in
                  particular when combined with the implicit
                  preconditioned Helmholtz solver, which is known to
                  converge even for decreasing mesh sizes and
                  increasing angular frequencies. These properties
                  make our scheme a viable alternative to explicit
                  time-domain finite-difference.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255381},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf.pdf}
}


@CONFERENCE{lin2009EAGEdsa,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Designing simultaneous acquisitions with compressive sensing},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2009},
  organization = {EAGE},
  abstract     = {The goal of this paper is in designing a functional
                  simultaneous acquisition scheme by applying the
                  principles of compressive sensing. By framing the
                  acquisition in a compressive sensing setting we
                  immediately gain insight into not only how to choose
                  the source signature and shot patterns, but also in
                  how well we can hope to demultiplex the data when
                  given a set amount of reduction in the number of
                  sweeps. The principles of compressive sensing
                  dictates that the quality of the demultiplexed data
                  is closely related to the transform-domain sparsity
                  of the solution. This means that, given an estimate
                  in the complexity of the expectant data wavefield,
                  it is possible to controllably reduce the number of
                  shots that needs to be recorded in the field. We
                  show a proof of concept by introducing an
                  acquisition compatible with compressive sensing
                  based on randomly phase-encoded vibroseis sweeps.},
  keywords     = {Presentation,EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/lin2009eagedsa.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/lin09EAGEdsa/lin09EAGEdsa.pdf}
}


@CONFERENCE{lin2009SEGucs,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Unified compressive sensing framework for
                  simultaneous acquisition with primary estimation},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3113-3117},
  organization = {SEG},
  abstract     = {The central promise of simultaneous acquisition is a
                  vastly improved crew efficiency during acquisition
                  at the cost of additional post-processing to obtain
                  conventional source-separated data volumes. Using
                  recent theories from the field of compressive
                  sensing, we present a way to systematically model
                  the effects of simultaneous acquisition.  Our
                  formulation form a new framework in the study of
                  acquisition design and naturally leads to an
                  inversion-based approach for the separation of shot
                  records. Furthermore, we show how other
                  inversion-based methods, such as a recently proposed
                  method from van Groenestijn and Verschuur (2009) for
                  primary estimation, can be processed together with
                  the demultiplexing problem to achieve a better
                  result compared to a separate treatment of these
                  problems.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255502},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs.pdf}
}


@CONFERENCE{lin2009DELPHIrwi,
  author       = {Tim T.Y. Lin and Felix J. Herrmann and Yogi A. Erlangga},
  title        = {Randomized wavefield inversion presented at the DELPHI meeting. The Hague.},
  booktitle    = {DELPHI},
  year         = {2009},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Delphi2009.pdf}
}


@CONFERENCE{saab2009SAMPTAnccs,
  author       = {Rayan Saab and Ozgur Yilmaz},
  title        = {A short note on non-convex compressed sensing},
  year         = {2009},
  booktitle    = {SAMPTA technical program},
  organization = {SAMPTA},
  abstract     = {In this note, we summarize the results we recently
                  proved in\cite{SY08} on the theoretical performance
                  guarantees of the decoders $¿î_p$.  These decoders
                  rely on $\ell^p$ minimization with $p {\i}n (0,1)$
                  to recover estimates of sparse and compressible
                  signals from incomplete and inaccurate
                  measurements. Our guarantees generalize the results
                  of \cite{CRT05} and \cite{Wojtaszczyk08} about
                  decoding by $\ell_p$ minimization with $p=1$, to the
                  setting where $p {\i}n (0,1)$ and are obtained under
                  weaker sufficient conditions. We also present novel
                  extensions of our results in \cite{SY08} that follow
                  from the recent work of DeVore et al. in
                  \cite{DPW08}. Finally, we show some insightful
                  numerical experiments displaying the trade-off in
                  the choice of $p {\i}n (0,1]$ depending on certain
                  properties of the input signal.},
  keywords     = {Presentation},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2009/saab09SAMPTAnccs/saab09SAMPTAnccs.pdf}
}


@CONFERENCE{shahidi2009SEGcmf,
  author       = {Reza Shahidi and Felix J. Herrmann},
  title        = {Curvelet-domain matched filtering with frequency-domain regularization},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3645-3650},
  organization = {SEG},
  abstract     = {In Herrmann et al. (2008), it is shown that
                  zero-order pseudodifferential operators, which model
                  the migration-demigration operator and the operator
                  mapping the predicted multiples to the true
                  multiples, can be represented by a diagonal
                  weighting in the curvelet domain. In that paper, a
                  smoothness constraint was introduced in the phase
                  space of the operator in order to regularize the
                  solution to make it unique.  In this paper, we use
                  recent results in Demanet and Ying (2008) on the
                  discrete symbol calculus to impose a further
                  smoothness constraint, this time in the frequency
                  domain. It is found that with this additional
                  constraint, faster convergence is realized. Results
                  on a synthetic pseudodifferential operator as well
                  as on an example of primary-multiple separation in
                  seismic data are included, comparing the model with
                  and without the new smoothness constraint, from
                  which it is found that results of improved quality
                  are also obtained.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255624},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/shahidi09SEGcmf/shahidi09segcmf.pdf}
}


@CONFERENCE{tang2009SEGhdb,
  author       = {Gang Tang and Reza Shahidi and Felix J. Herrmann and Jianwei Ma},
  title        = {Higher dimensional blue-noise sampling schemes for curvelet-based seismic data recovery},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {191-195},
  organization = {SEG},
  abstract     = {In combination with compressive sensing, a
                  successful reconstruction scheme called
                  Curvelet-based Recovery by Sparsity-promoting
                  Inversion (CRSI) has been developed, and has proven
                  to be useful for seismic data processing. One of the
                  most important issues for CRSI is the sampling
                  scheme, which can greatly affect the quality of
                  reconstruction.  Unlike usual regular undersampling,
                  stochastic sampling can convert aliases to
                  easy-to-eliminate noise. Some stochastic sampling
                  methods have been developed for CRSI, e.g. jittered
                  sampling, however most have only been applied to 1D
                  sampling along a line. Seismic datasets are usually
                  higher dimensional and very large, thus it is
                  desirable and often necessary to develop higher
                  dimensional sampling methods to deal with these
                  data. For dimensions higher than one, few results
                  have been reported, except uniform random sampling,
                  which does not perform well. In the present paper,
                  we explore 2D sampling methodologies for
                  curvelet-based reconstruction, possessing sampling
                  spectra with blue noise characteristics, such as
                  Poisson Disk sampling, Farthest Point Sampling, and
                  the 2D extension of jittered sampling. These
                  sampling methods are shown to lead to better
                  recovery and results are compared to the other more
                  traditional sampling protocols.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255230},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/tang09SEGhdb/tang09SEGhdb.pdf}
}


@CONFERENCE{vandenberg2009SLIMocf,
  author        = {Ewout {van den Berg} and Mark Schmidt and Michael P. Friedlander and K. Murphy},
  title         = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm},
  booktitle     = {SLIM},
  year          = {2009},
  volume        = {12},
  series        = {Twelfth International Conference on Artificial Intelligence and Statistics},
  optmonth      = {04/2009},
  abstract      = {An optimization algorithm for minimizing a smooth
                  function over a convex set is described. Each
                  iteration of the method computes a descent direction
                  by minimizing, over the original constraints, a
                  diagonal-plus low-rank quadratic approximation to
                  the function. The quadratic approximation is
                  constructed using a limited-memory quasi-Newton
                  update. The method is suitable for large-scale
                  problems where evaluation of the function is
                  substan- tially more expensive than projection onto
                  the constraint set. Numerical experiments on one-
                  norm regularized test problems indicate that the
                  proposed method is competitve with state- of-the-art
                  methods such as bound-constrained L-BFGS and
                  orthant-wise descent. We further show that the
                  method generalizes to a wide class of problems, and
                  substantially improves on state-of-the-art methods
                  for problems such as learning the structure of
                  Gaussian graphi- cal models (involving
                  positive-definite matrix constraints) and Markov
                  random fields (in- volving second-order cone
                  constraints).},
  date-added    = {2009-01-29 17:16:34 -0800},
  date-modified = {2009-01-29 17:16:34 -0800},
  keywords      = {SLIM},
  url           = {http://www.cs.ubc.ca/~mpf/papers/SchmidtBergFriedMurph09.pdf}
}


@CONFERENCE{yan2009SEGgpb,
  author       = {Jiupeng Yan and Felix J. Herrmann},
  title        = {Groundroll prediction by interferometry and separation by curvelet-domain matched filtering},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3297-3301},
  organization = {SEG},
  abstract     = {The removal of groundroll in land based seismic data
                  is a critical step for seismic imaging. In this
                  paper, we introduce a work flow to predict the
                  groundroll by interferometry and then separate the
                  groundroll in the curvelet domain. Thus workflow is
                  similar to the workflow of surface-related multiple
                  elimination (SRME). By exploiting the adaptability
                  and sparsity of curvelets, we are able to
                  significantly improve the separation of groundroll
                  in comparison to results yielded by frequency-domain
                  adaptive subtraction methods. We provide synthetic
                  data example to illustrate our claim.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255544},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb.pdf}
}


@CONFERENCE{yan2009SEGgpb2,
  author       = {Jiupeng Yan and Felix J. Herrmann},
  title        = {Groundroll prediction by interferometry and
                  separation by curvelet-domain filtering. Presented
                  at the 79th SEG Meeting, Houston},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  abstract     = {The removal of groundroll in land based seismic data
                  is a critical step for seismic imaging. In this
                  paper, we introduce a work flow to predict the
                  groundroll by interferometry and then separate the
                  groundroll in the curvelet domain. Thus workflow is
                  similar to the workflow of surface-related multiple
                  elimination (SRME). By exploiting the adaptability
                  and sparsity of curvelets, we are able to
                  significantly improve the separation of groundroll
                  in comparison to results yielded by frequency-domain
                  adaptive subtraction methods. We provide synthetic
                  data example to illustrate our claim.},
  keywords     = {Presentation},
  url          = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/yan09seggpi.pdf}
}



%-------------------------------------------------2008-------------------------------------------------

@CONFERENCE{vandenberg2008SINBADsat,
  author        = {Ewout {van den Berg}},
  title         = {Sparco: A testing framework for sparse reconstruction},
  booktitle     = {SINBAD 2008},
  year          = {2008},
  abstract      = {Sparco is a framework for testing and benchmarking
                  algorithms for sparse reconstruction. It includes a
                  large collection of sparse reconstruction problems
                  drawn from the imaging, compressed sensing, and
                  geophysics literature. Sparco is also a framework
                  for implementing new test problems and can be used
                  as a tool for reproducible research. We describe the
                  software environment, and demonstrate its usefulness
                  for testing and comparing solvers for sparse
                  reconstruction.},
  date-modified = {2008-08-22 12:54:25 -0700},
  keywords      = {SLIM, SINBAD, Presentation},
  url           = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf}
}


@CONFERENCE{erlangga2008SEGaim,
  author       = {Yogi A. Erlangga and Felix J. Herrmann},
  title        = {An iterative multilevel method for computing wavefields in frequency-domain seismic inversion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {1957-1960},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {We describe an iterative multilevel method for
                  solving linear systems representing forward modeling
                  and back propagation of wavefields in
                  frequency-domain seismic inversions. The workhorse
                  of the method is the so-called multilevel Krylov
                  method, applied to a multigrid-preconditioned linear
                  system, and is called multigrid-multilevel Krylov
                  (MKMG) method.  Numerical experiments are presented
                  for 2D Marmousi synthetic model for a range of
                  frequencies. The convergence of the method is fast,
                  and depends only mildly on frequency. The method can
                  be considered as the first viable alternative to LU
                  factorization, which is practically prohibitive for
                  3D seismic inversions.},
  keywords     = {Presentation,SLIM,SEG},
  optdoi       = {10.1190/1.3059279},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim_pres.pdf },
  url          = { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim.pdf }

}


@CONFERENCE{eso2008SEGira,
  author       = {R. A. Eso and S. Napier and Felix J. Herrmann and D. W. Oldenburg},
  title        = {Iterative reconstruction algorithm for non-linear operators},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {579-583},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {Iterative soft thresholding of a models wavelet
                  coefficients can be used to obtain models that are
                  sparse with respect to a known basis function. We
                  generate sparse models for non-linear forward
                  operators by applying the soft thresholding operator
                  to the model obtained through a Gauss-Newton
                  iteration and apply the technique in a synthetic
                  2.5D DC resistivity crosswell tomographic example.},
  keywords     = {SLIM, SEG},
  optdoi       = {10.1190/1.3063719},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/eso08SEGira/eso08SEGira.pdf }
}


@CONFERENCE{friedlander2008SINBADafl,
  author    = {Michael P. Friedlander},
  title     = {Algorithms for Large-Scale Sparse Reconstruction},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {Many signal processing applications seek to approximate
                  a signal as a linear combination of only a few
                  elementary atoms drawn from a large collection. This
                  is known as sparse reconstruction, and the theory of
                  compressed sensing allows us to pose it as a
                  structured convex optimization problem. I will
                  discuss the role of duality in revealing some
                  unexpected and useful properties of these problems,
                  and will show how they can lead to practical,
                  large-scale algorithms.  I will also describe some
                  applications of these algorithms.},
  keywords = {Presentation, SINBAD, SLIM},
  url      = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Michael_Alg.pdf}
}


@CONFERENCE{friedlander2008SIAMasa,
  author       = {Michael P. Friedlander},
  title        = {Active-set Approaches to Basis Pursuit Denoising},
  booktitle    = {SIAM Optimization},
  year         = {2008},
  optmonth     = {05/2008},
  organization = {SIAM Optimization},
  file         = {http//www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  keywords     = {Presentation, SLIM},
  url          = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}


@CONFERENCE{friedlander2008WCOMasm,
  author       = {Michael P. Friedlander and M. A. Saunders},
  title        = {Active-set methods for basis pursuit},
  booktitle    = {WCOM},
  organization = {West Coast Opitmization Meeting (WCOM)},
  year         = {2008},
  optmonth     = {07/2008},
  abstract     = {Many imaging and compressed sensing applications
                  seek sparse solutions to large under-determined
                  least-squares problems. The basis pursuit (BP)
                  approach minimizes the 1-norm of the solution, and
                  the BP denoising (BPDN) approach balances it against
                  the least-squares fit. The duals of these problems
                  are conventional linear and quadratic programs.  We
                  introduce a modified parameterization of the BPDN
                  problem and explore the effectiveness of active-set
                  methods for solving its dual.  Our basic algorithm
                  for the BP dual unifies several existing algorithms
                  and is applicable to large-scale examples.},
  file         = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  url          = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}


@CONFERENCE{hennenfent2008SINBADnii2,
  author    = {Gilles Hennenfent},
  title     = {New insights into one-norm solvers from the Pareto curve},
  booktitle = {SINBAD}, 
  year      = {2008},
  abstract  = {Several geophysical ill-posed inverse problems are
                  successfully solved by promoting sparsity using
                  one-norm regularization. The practicality of this
                  approach depends on the effectiveness of the
                  one-norm solver used and on its robustness under
                  limited number of iterations. We propose an approach
                  to understand the behavior and evaluate the
                  performance of one-norm solvers. The technique
                  consists of tracking on a graph the data misfit
                  versus the one norm of successive iterates. By
                  comparing the solution paths to the Pareto curve, we
                  are able to assess the performance of the solvers
                  and the quality of the solutions. Such an assessment
                  is particularly relevant given the renewed interest
                  in one-norm regularization.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_New.pdf}
}


@CONFERENCE{hennenfent2008SEGonri,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {One-norm regularized inversion: learning from the Pareto curve},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  organization = {SEG},
  abstract     = {Geophysical inverse problems typically involve a
                  trade off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain rela- tively
                  unexplored. First, we show how these curves provide
                  an objective criterion to gauge how robust one-norm
                  solvers are when they are limited by a maximum
                  number of matrix-vector products that they can
                  perform. Second, we use Pareto curves and their
                  properties to define and compute one-norm
                  compressibilities.  We argue this notion is key to
                  understand one-norm regularized inversion.  Third,
                  we illustrate the correlation between the one-norm
                  compressibility and the perfor- mance of Fourier and
                  curvelet reconstructions with sparsity promoting
                  inversion.},
  keywords     = {SEG},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/hennenfent08SEGonri/hennenfent08SEGonri.pdf}
}


@CONFERENCE{hennenfent2008SINBADsdw2,
  author    = {Gilles Hennenfent},
  title     = {Simply denoise: wavefield reconstruction via jittered undersampling},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {We present a new discrete undersampling scheme designed
                  to favor wavefield reconstruction by
                  sparsity-promoting inversion with transform elements
                  that are localized in the Fourier domain. Our work
                  is motivated by empirical observations in the
                  seismic community, corroborated by recent results
                  from compressive sampling, which indicate favorable
                  (wavefield) reconstructions from random as opposed
                  to regular undersampling.  As predicted by theory,
                  random undersampling renders coherent aliases into
                  harmless incoherent random noise, effectively
                  turning the interpolation problem into a much
                  simpler denoising problem. A practical requirement
                  of wavefield reconstruction with localized
                  sparsifying transforms is the control on the maximum
                  gap size. Unfortunately, random undersampling does
                  not provide such a control and the main purpose of
                  this paper is to introduce a sampling scheme, coined
                  jittered undersampling, that shares the benefits of
                  random sampling, while offering control on the
                  maximum gap size. Our contribution of jittered
                  sub-Nyquist sampling proofs to be key in the
                  formulation of a versatile wavefield
                  sparsity-promoting recovery scheme that follows the
                  principles of compressive sampling. After studying
                  the behavior of the jittered-undersampling scheme in
                  the Fourier domain, its performance is studied for
                  curvelet recovery by sparsity-promoting inversion
                  (CRSI). Our findings on synthetic and real seismic
                  data indicate an improvement of several decibels
                  over recovery from regularly-undersampled data for
                  the same amount of data collected.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_jit.pdf}
}


@CONFERENCE{herrmann2008SEGcdm,
  title        = {Curvelet-domain matched filtering},
  author       = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  optmonth     = {08/2008},
  year         = {2008},
  abstract     = {Matching seismic wavefields and images lies at the
                  heart of many pre-/post-processing steps part of
                  seismic imaging{\textendash}- whether one is
                  matching predicted wavefield components, such as
                  multiples, to the actual to-be-separated wavefield
                  components present in the data or whether one is
                  aiming to restore migration amplitudes by scaling,
                  using an image-to-remigrated-image matching
                  procedure to calculate the scaling coefficients. The
                  success of these wavefield matching procedures
                  depends on our ability to (i) control possible
                  overfitting, which may lead to accidental removal of
                  energy or to inaccurate image-amplitude corrections,
                  (ii) handle data or images with nonunique dips, and
                  (iii) apply subsequent wavefield separations or
                  migraton amplitude corrections stably. In this
                  paper, we show that the curvelet transform allows us
                  to address all these issues by imposing smoothness
                  in phase space, by using their capability to handle
                  conflicting dips, and by leveraging their ability to
                  represent seismic data and images sparsely. This
                  latter property renders curvelet-domain sparsity
                  promotion an effective prior.},
  keywords     = {SLIM,Presentation, SEG},
  number       = {TR-2008-6},
  organization = {SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm.pdf }
}


@CONFERENCE{herrmann2008SINBADacd2,
  author       = {Felix J. Herrmann},
  title        = {Adaptive curvelet-domain primary-multiple separation},
  booktitle    = {SINBAD},
  organization = {SINBAD},
  year         = {2008},
  note         = {SINBAD 2008},
  abstract     = {In many exploration areas, successful separation of
                  primaries and multiples greatly determines the
                  quality of seismic imaging. Despite major advances
                  made by Surface-Related Multiple Elimination (SRME),
                  amplitude errors in the predicted multiples remain a
                  problem. When these errors vary for each type of
                  multiple differently (as a function of offset, time
                  and dip), these amplitude errors pose a serious
                  challenge for conventional least-squares matching
                  and for the recently introduced separation by
                  curvelet-domain thresholding. We propose a
                  data-adaptive method that corrects amplitude errors,
                  which vary smoothly as a function of location, scale
                  (frequency band) and angle. In that case, the
                  amplitudes can be corrected by an element-wise
                  curvelet-domain scaling of the predicted
                  multiples. We show that this scaling leads to a
                  successful estimation of the primaries, despite
                  amplitude, sign, timing and phase errors in the
                  predicted multiples. Our results on synthetic and
                  real data show distinct improvements over
                  conventional least-squares matching, in terms of
                  better suppression of multiple energy and
                  high-frequency clutter and better recovery of the
                  estimated primaries.},
  keywords     = {Presentation, SINBAD, SLIM},
  url          = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ada.pdf}
}


@CONFERENCE{herrmann2008SEGcdm3,
  author       = {Felix J. Herrmann},
  title        = {Curvelet-domain matched filtering},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {3643-3649},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {Matching seismic wavefields lies at the heart of
                  seismic processing whether one is adaptively
                  subtracting multiples predictions or groundroll.  In
                  both cases, the predictions are matched to the
                  actual to-be-separated wavefield components in the
                  observed data. The success of these wavefield
                  matching procedures depends on our ability to (i)
                  control possible overfitting, which may lead to
                  accidental removal of primary energy, (ii) handle
                  data with nonunique dips, and (iii) apply wavefield
                  separation after matching stably. In this paper, we
                  show that the curvelet transform allows us to
                  address these issues by imposing smoothness in phase
                  space, by using their capability to handle
                  conflicting dips, and by leveraging their ability to
                  represent seismic data sparsely.},
  keywords     = {SEG, SLIM},
  optdoi       = {10.1190/1.3064089},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/Herrmann08SEGcdm3/Herrmann08SEGcdm3.pdf }
}


@CONFERENCE{herrmann2008IONcsa,
  author       = {Felix J. Herrmann},
  title        = {Compressive sampling: a new paradigm for seismic data acquistion and processing?},
  booktitle    = {ION},
  year         = {2008},
  abstract     = {Seismic data processing and imaging are firmly
                  rooted in the well-established paradigm of regular
                  Nyquist sampling. Faced with a typical uncooperative
                  environment, practitioners of seismic data
                  acquisition make all efforts to comply to this
                  theory by creating regularly-sampled seismic-data
                  volumes that are suitable for Fourier-based
                  processing flows. The current advent of new
                  alternative transform domains{\textendash}- such as
                  the sparsifying curvelet domain, where seismic data
                  is decomposed into localized, multiscale and
                  multidirectional plane waves{\textendash}- opens the
                  possibility to change this paradigm by no longer
                  combating sampling irregularity but by embracing
                  it. During this talk, we show that as long as
                  seismic data volumes permit a compressible
                  representation{\textendash}-i.e., data can be
                  represented as a superposition of relatively few
                  number of elementary waveforms{\textendash}- Nyquist
                  sampling is unnecessary pessimistic. So far, nothing
                  new, we all know from the work on Fourier- or other
                  transform-based seismic-data regularization
                  methodologies that wavefields can be recovered
                  accurately from sub-Nyquist samplings through some
                  sort of optimization procedure. What is new,
                  however, are recent insights from the field of
                  "compressive sampling", which dictate the conditions
                  that guarantee or, at least, in practice provide
                  conditions that favor sparsity-promoting recovery
                  from sub-Nyquist sampling. Random sub-sampling, or
                  to be more precise, jitter sub-sampling creates
                  favorable conditions for curvelet-based recovery. We
                  explain this phenomenon by arguing that this type of
                  sampling leads to noisy data, hence our slogan
                  "Simply denoise: wavefield reconstruction via
                  jittered undersampling", where we bank on separating
                  incoherent sub-sampling noise with curvelet-domain
                  sparsity promotion. During our presentation, we
                  introduce you to what curvelets are, why random
                  jitter sampling is important and why this opens a
                  pathway towards a new paradigm of curvelet-domain
                  seismic data processing. Our claims will be
                  supported by examples on synthetic and field
                  data. This is joint work with Gilles Hennenfent,
                  PhD. student at SLIM.},
  keywords     = {ION, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08ion_pres.pdf}
}


@CONFERENCE{herrmann2008SINBADfwr,
  author    = {Felix J. Herrmann},
  title     = {(De)-Focused wavefield reconstructions},
  booktitle = {SINBAD 2008},
  year      = {2008},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Dfo.pdf}
}


@CONFERENCE{herrmann2008SEGgbu,
  author       = {Felix J. Herrmann},
  title        = {Seismic noise: the good, the bad, \& the ugly},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  keywords     = {Presentation, SEG, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGgbu/herrmann08SEGgbu.pdf }
}


@CONFERENCE{herrmann2008SINBADpsm,
  author    = {Felix J. Herrmann},
  title     = {Phase-space matched filtering and migration preconditioning},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {During this talk, I will report on new phase-space
                  regularization functionals defined in terms of
                  splines. This spline representation reduces the
                  dimensionality of estimating our phase-space matched
                  filter. We will discuss how this filter can be used
                  in migration preconditioning. This is joint work
                  with Christiaan Stolk.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Pha.pdf}
}


@CONFERENCE{herrmann2008SINBADs2c,
  author    = {Felix J. Herrmann},
  title     = {SINBAD 2008 Consortium meeting},
  booktitle = {SINBAD 2008},
  year      = {2008},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ope.pdf}
}


@CONFERENCE{herrmann2008SIAMcsm,
  author    = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title     = {Compressive sampling meets seismic imaging},
  booktitle = {SIAM},
  year      = {2008},
  abstract  = {Compressive sensing has led to fundamental new insights
                  in the recovery of compressible signals from
                  sub-Nyquist samplings. It is shown how jittered
                  subsampling can be used to create favorable recovery
                  conditions.  Applications include mitigation of
                  incomplete acquisitions and wavefield
                  computations. While the former is a direct
                  adaptation of compressive sampling, the latter
                  application represents a new way of compressing
                  wavefield extrapolation operators. Operators are not
                  diagonalized but are compressively sampled reducing
                  the computational costs.},
  keywords  = {Presentation, SIAM, SLIM},
  url       = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08siam08.pdf}
}


@CONFERENCE{herrmann2008SINBADitc,
  author    = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin and Cody R. Brown},
  title     = {Introduction to compressive (wavefield) computation},
  booktitle = {SINBAD 2008},
  year      = {2008},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Int.pdf}
}


@CONFERENCE{herrmann2008SEGswi,
  author       = {Felix J. Herrmann and Deli Wang},
  title        = {Seismic wavefield inversion with curvelet-domain sparsity promotion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2497-2501},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {Inverting seismic wavefields lies at the heart of
                  seismic data processing and imaging{\textendash}-
                  whether one is applying
                  {\textquoteleft}{\textquoteleft}a poor
                  man{\textquoteright}s
                  inverse{\textquoteright}{\textquoteright} by
                  correlating wavefields during imaging or whether one
                  inverts wavefields as part of a focal transform
                  interferrometric deconvolution or as part of
                  computing the {\textquoteright}data
                  verse{\textquoteright}.  The success of these
                  wavefield inversions depends on the stability of the
                  inverse with respect to data imperfections such as
                  finite aperture, bandwidth limitation, and missing
                  data. In this paper, we show how curvelet domain
                  sparsity promotion can be used as a suitable prior
                  to invert seismic wavefields. Examples include,
                  seismic data regularization with the focused
                  curvelet-based recovery by sparsity-promoting
                  inversion (fCRSI), which involves the inversion of
                  the primary-wavefield operator, the prediction of
                  multiples by inverting the adjoint of the primary
                  operator, and finally the inversion of the data
                  itself {\textendash}- the so-called
                  {\textquoteright}data inverse{\textquoteright}.  In
                  all cases, curvelet-domain sparsity leads to a
                  stable inversion.},
  keywords     = {Presentation,SLIM},
  optdoi       = {10.1190/1.3063862},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi.pdf }
}


@CONFERENCE{johnson2008SINBADsdi,
  author    = {James Johnson and Gilles Hennenfent},
  title     = {Seismic Data Interpolation with Symmetry},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {Due to the physics of reciprocity seismic data sets are
                  symmetric in the source and receiver
                  coordinates. Often seismic data sets are incomplete
                  and the missing data must be
                  interpolated. Typically, missing traces do not occur
                  symmetrically. The purpose of this project is to
                  extend the current formulation for solving the
                  seismic interpolation problems in such a way that
                  they enforce reciprocity. The method decomposes the
                  seismic data volume into symmetric and antisymmetric
                  parts. This decomposition leads to an augmented
                  system of equations for the L1-solver that promotes
                  sparsity in the curvelet domain.  Interpolation is
                  carried out on the entire system during which the
                  asymmetric component of the volume is forced to
                  zero, while the symmetric part of the data volume is
                  matched to the measured data.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_James_Sei.pdf}
}


@CONFERENCE{kumar2008SINBADcd,
  author    = {Vishal Kumar},
  title     = {Curvelet Denoising},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {The separation of signal and noise is an important
                  issue in seismic data processing. By noise we refer
                  to the incoherent noise which is present in the
                  data. In our case, we showed curvelets concentrate
                  seismic signal energy in few significant
                  coefficients unlike noise energy that is spread all
                  over the coefficients. The sparsity of seismic data
                  in the curvelet domain makes curvelets an ideal
                  choice for separating the noise from the seismic
                  data. In our approach the denoising problem is
                  framed as curvelet-regularized inversion problem.
                  After initial processing, we applied the algorithm
                  to the poststack data and compared our results with
                  conventional wavelet denoising.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Den.pdf}
}


@CONFERENCE{kumar2008SINBADcrd,
  author       = {Vishal Kumar},
  title        = {Curvelet-Regularized Deconvolution},
  booktitle    = {SINBAD 2008},
  year         = {2008},
  abstract     = {The removal of source signature from seismic data is
                  an important step in seismic data processing. The
                  Curvelet transform provides sparse representations
                  for images that comprise smooth objects separated by
                  piece-wise smooth discontinuities (e.g. seismic
                  reflectivity).  In this approach the sparseness of
                  reflectivity in Curvelet domain is used as a prior
                  to stabilize the inversion process. Our
                  Curvelet-regularized deconvolution algorithm uses
                  recently developed SPGL1 solver which does adaptive
                  sampling of the trade-off curve. We applied the
                  algorithm on a synthetic example and compared our
                  results with that of Spiky deconvolution approach.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SINBAD/2008/kumar08SINBADcrd/kumar08SINBADcrd_pres.pdf },
  url          = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Dec.pdf}
}


@CONFERENCE{kumar2008CSEGcrs,
  author       = {Vishal Kumar and Felix J. Herrmann},
  title        = {Curvelet-regularized seismic deconvolution},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2008},
  optmonth     = {05/2008},
  organization = {CSEG},
  abstract     = {There is an inherent continuity along reflectors of
                  a seismic image.  We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.},
  keywords     = {Presentation,SLIM},
  presentation = {http://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs_pres.pdf},
  url          = {http://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs.pdf}
}


@CONFERENCE{kumar2008SEGdwc,
  author       = {Vishal Kumar and Felix J. Herrmann},
  title        = {Deconvolution with curvelet-domain sparsity},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {1996-2000},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {There is an inherent continuity along reflectors of
                  a seismic image.  We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.},
  keywords     = {SLIM,Presentation, SEG},
  optdoi       = {10.1190/1.3059287},
  presentation = { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc.pdf }
}


@CONFERENCE{lebed2008SINBADaoc,
  author    = {Evgeniy Lebed},
  title     = {Curvelet / Surfacelet comparison},
  booktitle = {SINBAD},
  year      = {2008},
  abstract  = {Curvelets and Surfacelets are two transforms that aim
                  to achieve a multiscale and a multidirectional
                  decomposition of arbitrary N-dimensional ($N>=2$)
                  signals. While both transforms are Fourier-based,
                  their construction is intrinsically different. In
                  this talk we will give and overview of the
                  construction of the two transforms, and explore
                  their properties such as frequency domain / spatial
                  domain coherence, sparsity, redundancy and
                  computational complexity.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_Curv.pdf}
}


@CONFERENCE{lebed2008SEGhggt,
  author       = {Evgeniy Lebed and Felix J. Herrmann},
  title        = {A hitchhiker{\textquoteright}s guide to the galaxy of transform-domain sparsification},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  organization = {SEG},
  abstract     = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last decade many
                  transforms such as wavelets, curvelets, contourlets,
                  surfacelets, shearlets, and many other types of
                  {\textquoteright}x-lets{\textquoteright} have been
                  developed to try to resolve this issue. In this
                  abstract we compare the properties of four of these
                  commonly used transforms, namely the shift-invariant
                  wavelets, complex wavelets, curvelets and
                  surfacelets.  We also briefly explore the
                  performance of these transforms for the problem of
                  recovering seismic wavefields from incomplete
                  measurements.},
  keywords     = {SEG},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lebed08SEGhggt/lebed08SEGhggt.pdf}
}


@CONFERENCE{lebed2008SINBADaoc1,
  author    = {Evgeniy Lebed},
  title     = {Applications of Curvelets/Surfacelets to seismic data processing},
  booktitle = {SINBAD},
  year      = {2008},
  abstract  = {In this talk we explore several applications of the
                  curvelet and surfacelet transforms to seismic data
                  processing. The first application is stable signal
                  recovery in the physical domain - seismic data
                  acquisition is often limited by physical and
                  economic constraints, and the goal is to interpolate
                  the data from a given subset of seismic traces.  The
                  second application is signal recovery in a transform
                  domain - we assume that our data comes in a form of
                  a random subset of temporal frequencies and the goal
                  is to recover the missing frequencies from this
                  data. Since seismic signals are generally not
                  bandwidth limited, this in fact becomes an
                  anti-aliasing problem. In both these problems the
                  recovery is resolved via a robust l_1 solver that
                  exploits the sparsity of the signals in
                  curvelet/surfacelet domains. In the last application
                  we explore the problem of primary-multiple
                  separation by simple thresholding.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_App.pdf}
}


@CONFERENCE{lin2008SINBADcwe,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Compressed wavefield extrapolation},
  booktitle    = {SINBAD},
  year         = {2008},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/sites/data/Papers/lin08cwe.pdf}
}


@CONFERENCE{lin2008SEGiso,
  author       = {Tim T.Y. Lin and Evgeniy Lebed and Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Interpolating solutions of the Helmholtz equation with compressed sensing},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2122-2126},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {We present an algorithm which allows us to model
                  wavefields with frequency-domain methods using a
                  much smaller number of frequencies than that
                  typically required by the classical sampling theory
                  in order to obtain an alias-free result. The
                  foundation of the algorithm is the recent results on
                  the compressed sensing, which state that data can be
                  successfully recovered from an incomplete
                  measurement if the data is sufficiently
                  sparse. Results from numerical experiment show that
                  only 30\% of the total frequency spectrum is need to
                  capture the full wavefield information when working
                  in the hard 2D synthetic Marmousi model.},
  keywords     = {Presentation,SLIM, SEG},
  optdoi       = {10.1190/1.3059307},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf } 
}


@CONFERENCE{maysami2008SEGlcf,
  author       = {Mohammad Maysami and Felix J. Herrmann},
  title        = {Lithological constraints from seismic waveforms: application to opal-A to opal-CT transition},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2011-2015},
  optmonth     = {November},
  organization = {SEG},
  abstract     = {In this paper, we present a new method for seismic
                  waveform characterization whose aim is threefold,
                  namely (i) extraction of detailed information on the
                  sharpness of transitions in the subsurface from
                  seismic waveforms, (ii) reflector modeling, based on
                  binary-mixture and percolation theory, and (iii)
                  establishment of well-seismic ties, through
                  parameterizations of our waveform and critical
                  reflector model. We test this methodology on the
                  opal-A (Amorphous) to opal-CT
                  (Cristobalite/Tridymite) transition imaged in a
                  migrated section of North Sea field data West of the
                  Shetlands.}, 
  keywords     = {SEG, SLIM},
  optdoi       = {10.1190/1.3059400},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/maysami08SEGlcf/maysami08SEGlcf.pdf }
}


@CONFERENCE{modzelewski2008SINBADdas,
  author    = {Henryk Modzelewski},
  title     = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {The SLIM group is actively developing software for
                  seismic imaging.  This talk will give a general
                  overview of the software development during SINBAD
                  project with focus on the final release in February
                  2008. The covered topics will include: 1) adopting
                  Python for object-oriented programming, 2) including
                  parallelism into the algorithms used in seismic
                  imaging/modeling, 3) in-house algorithms for seismic
                  imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session "SINBAD Software
                  releases".},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Henryk_Des.pdf}
}


@CONFERENCE{moghaddam2008SINBADrtm,
  author    = {Peyman P. Moghaddam},
  title     = {Reverse-time Migration Amplitude Recovery with Curvelets},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {We recover the amplitude of a seismic image by
                  approximating the normal (demigration-migration)
                  operator. In this approximation, we make use of the
                  property that curvelets remain invariant under the
                  action of the normal operator. We propose a seismic
                  amplitude recovery method that employs an eigenvalue
                  like decomposition for the normal operator using
                  curvelets as eigenvectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Peyman_Mig.pdf}
}


@CONFERENCE{moghaddam2008SEGcbm,
  author       = {Peyman P. Moghaddam and Cody R. Brown and Felix J. Herrmann},
  title        = {Curvelet-based migration preconditioning},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2211-2215},
  optmonth     = {November},
  organization = {SEG},
  abstract     = {In this paper, we introduce a preconditioner for
                  seismic imaging{\textendash}-i.e., the inversion of
                  the linearized Born scattering operator. This
                  preconditioner approximately corrects for the
                  {\textquoteleft}{\textquoteleft}square
                  root{\textquoteright}{\textquoteright} of the
                  normal{\textendash}-i.e., the demigration-migration
                  operator. This approach consists of three parts,
                  namely (i) a left preconditoner, defined by a
                  fractional time integration designed to make the
                  migration operator zero order, and two right
                  preconditioners that apply (ii) a scaling in the
                  physical domain accounting for a spherical
                  spreading, and (iii) a curvelet-domain scaling that
                  corrects for spatial and reflector-dip dependent
                  amplitude errors. We show that a combination of
                  these preconditioners lead to a significant
                  improvement of the convergence for iterative
                  least-squares solutions to the seismic imaging
                  problem based on reverse-time migration operators.},
  keywords     = {Presentation,SLIM,SEG},
  optdoi       = {10.1190/1.3059325},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm.pdf }
}


@CONFERENCE{ross2008SINBADsit,
  author    = {Sean Ross-Ross},
  title     = {Seismic inversion through operator overloading},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {Geophysical processing is dominated by many different
                  out of core memory software environments
                  (OOCE). Such environments include Madagascar and SU
                  and are designed to handle data that can not be
                  operated on in memory. Each base operation is
                  created as a main program that reads data from disk
                  and writes the result to disk. The main programs can
                  also be chained together on stdin/out pipes using a
                  shell only writing data to disk at the end. To be
                  efficient, the algorithm using an OOCE must chain
                  together the longest pipe to avoid disk I/O, as a
                  result it is very difficult to use iterative
                  techniques. The algorithms are written in shell
                  scripts can be difficult to read and understand.
                  SLIMpy is a software library that contains
                  definitions of coordinate free vectors and linear
                  operators. It allows the user to design and run
                  algorithms with any out of core package, in a Matlab
                  style interface while maintaining optimal efficiency
                  and speed. SLIMpy looks at each main program of each
                  OOCE as a Matrix vector operation or vector
                  reduction/transformation operation. It uses operator
                  overloading to generate an abstract syntax tree
                  (AST) which can be optimized in many ways before
                  executing its commands. The AST also provides a
                  pathway for embarrassingly parallel applications by
                  splitting the tree over different nodes and
                  processors. SLIMpy provides an interface to these
                  OOCE that allows for optimal construction of
                  commands and allows for iterative techniques. It
                  smoothes the transition from other languages such as
                  Matlab and allows the algorithm designer to write
                  readable and reusable code. SLIMpy also adds to OOCE
                  by allowing for easy parallelization.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Sean_Sli.pdf}
}


@CONFERENCE{saab2008ICASSPssa,
  author       = {Rayan Saab and  Rick Chartrand and Ozgur Yilmaz},
  title        = {Stable sparse approximations via nonconvex optimization},
  booktitle    = {ICASSP},
  year         = {2008},
  organization = {ICASSP},
  keywords     = {ICASSP}, 
  abstract     = {We present theoretical results pertaining to the
                  ability of lp minimization to recover sparse and
                  compressible signals from incomplete and noisy
                  measurements. In particular, we extend the results
                  of Cande`s, Romberg and Tao [1] to the p < 1
                  case. Our results indicate that depending on the
                  restricted isometry constants (see, e.g.,[2] and
                  [3]) and the noise level, lp minimization with
                  certain values of p < 1 provides better theoretical
                  guarantees in terms of stability and robustness than
                  l1 minimization does. This is especially true when
                  the restricted isometry constants are relatively
                  large.},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2008/saab08ICASSPssa/saab08ICASSPssa.pdf }
}


@CONFERENCE{saab2008SINBADcps,
  author        = {Rayan Saab},
  title         = {Curvelet-Based Primary-Multiple Separation from a Bayesian Perspective},
  booktitle     = {SINBAD 2008},
  year          = {2008},
  abstract      = {We present a novel primary-multiple separation
                  scheme which makes use of the sparsity of both
                  primaries and multiples in a transform domain, such
                  as the curvelet transform, to provide estimates of
                  each.  The proposed algorithm utilizes seismic data
                  as well as the output of a preliminary step that
                  provides (possibly) erroneous predictions of the
                  multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm.},
  date-modified = {2008-08-22 12:45:53 -0700},
  keywords      = {SLIM, SINBAD, Presentation},
  url           = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Rayan_Curv.pdf}
}


@CONFERENCE{vandenberg2008IAMesr,
  author        = {Ewout {van den Berg}},
  title         = {Exact sparse reconstruction and neighbourly polytopes},
  booktitle     = {IAM},
  year          = {2008},
  bdsk-url-1    = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf},
  date-added    = {2008-08-26 15:44:44 -0700},
  date-modified = {2008-08-26 15:45:58 -0700},
  keywords      = {SLIM, IAM, Presentation},
  presentation  = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf}
}


@CONFERENCE{wang2008SINBADrri,
  author    = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title     = {Recent results in curvelet-based primary-multiple separation},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {We present a nonlinear curvelet-based
                  sparsity-promoting formulation for the
                  primary-multiple separation problem. We show that
                  these coherent signal components can be separated
                  robustly by explicitly exploiting the locality of
                  curvelets in phase space (space-spatial frequency
                  plane) and their ability to compress data volumes
                  that contain wavefronts.  This work is an extension
                  of earlier results and the presented algorithms are
                  shown to be stable under noise and moderately
                  erroneous multiple predictions.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Deli_Rec.pdf}
}


@CONFERENCE{yan2008SINBADwru,
  author    = {Jiupeng Yan},
  title     = {Wavefield Reconstruction Using Simultaneous Denoising Interpolation vs. Denoising after Interpolation},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {This report represents and compares two methods of
                  wavefield reconstruction from noisy seismic data
                  with missing traces. The two methods are (i) First
                  interpolate incomplete noisy data to get complete
                  noisy data and then denoise, and (ii) Interpolate
                  and denoise the incomplete noisy data
                  simultaneously. A sample test of synthetic data will
                  be presented. The results of tests show that
                  denoising after interpolation is better than
                  simultaneous denoising and interpolation if the
                  parameter of the denoising problem is chosen
                  appropriately.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Jiapeng_Wav.pdf}
}


@CONFERENCE{yarham2008SINBADbss,
  author        = {Carson Yarham},
  title         = {Bayesian signal separation applied to ground-roll removal},
  booktitle     = {SINBAD 2008},
  year          = {2008},
  abstract      = {Accurate and adaptive noise removal is a critical
                  part in seismic processing. Recent developments in
                  signal separation methods have allowed a more
                  flexible and accurate framework in which to perform
                  ground roll and reflector separation. The use of a
                  new Bayesian separation scheme developed at the SLIM
                  group that contains control parameters to adjust for
                  the uniqueness of specific problems is used. The
                  sensitivity and variation of the control parameters
                  is examined and this method is applied to synthetic
                  and real data and the results are compared to
                  previous methods.},
  date-modified = {2008-08-22 12:42:58 -0700},
  keywords      = {Presentation,SLIM},
  url           = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Carson_Gro.pdf}
}


@CONFERENCE{yarham2008SEGbgr,
  author       = {Carson Yarham and Felix J. Herrmann},
  title        = {Bayesian ground-roll seperation by curvelet-domain sparsity promotion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2576-2580},
  optmonth     = {November},
  organization = {SEG},
  abstract     = {The removal of coherent noise generated by surface
                  waves in land based seismic is a prerequisite to
                  imaging the subsurface. These surface waves, termed
                  as ground roll, overlay important reflector
                  information in both the t-x and f-k
                  domains. Standard techniques of ground-roll removal
                  commonly alter reflector information. We propose the
                  use of the curvelet domain as a sparsifying
                  transform in which to preform signal-separation
                  techniques that preserves reflector information
                  while increasing ground-roll removal. We look at how
                  this method preforms on synthetic data for which we
                  can build quantitative results and a real field data
                  set.},
  keywords     = {Presentation,SLIM},
  optdoi       = {10.1190/1.3063878},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr_pres.pdf  },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr.pdf  }
}


@CONFERENCE{yilmaz2008SINBADsse,
  author    = {Ozgur Yilmaz},
  title     = {Stable sparse expansions via non-convex optimization},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {We present theoretical results pertaining to the
                  ability of p-(quasi)norm minimization to recover
                  sparse and compressible signals from incomplete and
                  noisy measurements. In particular, we extend the
                  results of Candes, Romberg and Tao for 1-norm to the
                  p<1 case. Our results indicate that depending on the
                  restricted isometry constants and the noise level,
                  p-norm minimization with certain values of p<1
                  provides better theoretical guarantees in terms of
                  stability and robustness compared to 1-norm
                  minimization. This is especially true when the
                  restricted isometry constants are relatively large,
                  or equivalently, when the data is significantly
                  undersampled.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ozgur_Sta.pdf}
}



%-------------------------------------------------2007-------------------------------------------------

@CONFERENCE{challa2007EAGEsrf,
  author       = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title        = {Signal reconstruction from incomplete and misplaced measurements},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Constrained by practical and economical
                  considerations, one often uses seismic data with
                  missing traces. The use of such data results in
                  image artifacts and poor spatial
                  resolution. Sometimes due to practical limitations,
                  measurements may be available on a perturbed grid,
                  instead of on the designated grid. Due to
                  algorithmic requirements, when such measurements are
                  viewed as those on the designated grid, the recovery
                  procedures may result in additional artifacts. This
                  paper interpolates incomplete data onto regular grid
                  via the Fourier domain, using a recently developed
                  greedy algorithm. The basic objective is to study
                  experimentally as to what could be the size of the
                  perturbation in measurement coordinates that allows
                  for the measurements on the perturbed grid to be
                  considered as on the designated grid for faithful
                  recovery. Our experimental work shows that for
                  compressible signals, a uniformly distributed
                  perturbation can be offset with slightly more number
                  of measurements.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/challa07EAGEsrf/challa07EAGEsrf.pdf}
}


@CONFERENCE{fomel2007ICASSPrepro,
  author       = {Sergey Fomel and Gilles Hennenfent},
  title        = {Reproducible computational experiments using scons},
  booktitle    = {ICASSP},
  organization = {ICASSP},
  year         = {2007},
  keywords     = {ICASSP},
  abstract     = {SCons (from Software Construction) is a well-known
                  open- source program designed primarily for building
                  software. In this paper, we describe our method of
                  extending SCons for managing data processing flows
                  and reproducible computational experiments. We
                  demonstrate our usage of SCons with a simple
                  example.},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2007/fomel07ICASSPrepro/fomel07ICASSPrepro.pdf  }
}


@CONFERENCE{hennenfent2007SINBADjdn,
  author    = {Gilles Hennenfent},
  title     = {Just denoise. Nonlinear recovery from randomly sampled data},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {In this talk, we turn the interpolation problem of
                  coarsely-sampled data into a denoising problem. From
                  this point of view, we illustrate the benefit of
                  random sampling at sub-Nyquist rate over regular
                  sampling at the same rate. We show that, using
                  nonlinear sparsity-promoting optimization, coarse
                  random sampling may actually lead to significantly
                  better wavefield reconstruction than equivalent
                  regularly sampled data.},
  keywords  = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{hennenfent2007EAGEcrw,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Curvelet reconstruction with sparsity-promoting inversion: successes and challenges},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {In this overview of the recent Curvelet
                  Reconstruction with Sparsity-promoting Inver- sion
                  (CRSI) method, we present our latest 2-D and 3-D
                  interpolation results on both synthetic and real
                  datasets. We compare these results to interpolated
                  data using other ex- isting methods. Finally, we
                  discuss the challenges related to sparsity-promoting
                  solvers for the large-scale problems the industry
                  faces.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw_pres.pdf }
}


@CONFERENCE{hennenfent2007EAGEisf,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Irregular sampling: from aliasing to noise},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Seismic data is often irregularly and/or sparsely
                  sampled along spatial coordinates. We show that
                  these acquisition geometries are not necessarily a
                  source of adversity in order to accurately
                  reconstruct adequately-sampled data. We use two
                  examples to illustrate that it may actually be
                  better than equivalent regularly subsampled
                  data. This comment was already made in earlier works
                  by other authors. We explain this behavior by two
                  key observations. Firstly, a noise-free
                  underdetermined problem can be seen as a noisy
                  well-determined problem. Secondly, regularly
                  subsampling creates strong coherent acquisition
                  noise (aliasing) difficult to remove unlike the
                  noise created by irregularly subsampling that is
                  typically weaker and Gaussian-like.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf_pres.pdf },
  url2         = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07eage_pres.pdf }
}


@CONFERENCE{hennenfent2007SINBADrii,
  author    = {Gilles Hennenfent and Felix J. Herrmann},
  title     = {Recent insights in $L_1$ solvers},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {During this talk, an overview is given on our work on
                  norm-one solvers as part of the DNOISE
                  project. Gilles will explain the ins and outs of our
                  iterative thresholding solver based on log cooling
                  while Felix will present the work of Michael
                  Friedlander "A Newton root-finding algorithms for
                  large-scale basis pursuit denoise". Both approaches
                  involve the solution of the basis pursuit problem
                  that seeks a minimum one-norm solution of an
                  underdetermined least-squares problem. Basis pursuit
                  denoise (BPDN) fits the least-squares problem only
                  approximately, and a single parameter determines a
                  curve that traces the trade-off between the
                  least-squares fit and the one-norm of the
                  solution. In the work of Friedlander, it is shown
                  show that the function that describes this curve is
                  convex and continuously differentiable over all
                  points of interest. They describe an efficient
                  procedure for evaluating this function and its
                  derivatives. As a result, they can compute arbitrary
                  points on this curve. Their method is suitable for
                  large-scale problems. Only matrix-vector operations
                  are required.  This is joint work with Ewout van der
                  Berg and Michael P. Friedlander},
  keywords  = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{hennenfent2007SEGrsn,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Random sampling: New insights into the reconstruction of coarsely sampled wavefields},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2575-2579},
  organization = {SEG},
  abstract     = {In this paper, we turn the interpolation problem of
                  coarsely-sampled data into a denoising problem. From
                  this point of view, we illustrate the benefit of
                  random sampling at sub-Nyquist rate over regular
                  sampling at the same rate. We show that, using
                  nonlinear sparsity-promoting optimization, coarse
                  random sampling may actually lead to significantly
                  better wavefield reconstruction than equivalent
                  regularly sampled data. {\copyright}2007 Society of
                  Exploration Geophysicists},
  optdoi       = {10.1190/1.2793002},
  keywords     = {Presentation, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn_pres.pdf }
}


@CONFERENCE{herrmann2007AMScsi,
  author       = {Felix J. Herrmann},
  title        = {Compressive seismic imaging},
  booktitle    = {AMS Von Neumann},
  year         = {2007},
  abstract     = {Seismic imaging involves the solution of an
                  inverse-scattering problem during which the energy
                  of (extremely) large data volumes is collapsed onto
                  the Earth's reflectors. We show how the ideas from
                  "compressive sampling" can alleviate this task by
                  exploiting the curvelet transform's "wavefront-set
                  detection" capability and "invariance" property
                  under wave propagation. First, a wavelet-vaguellete
                  technique is reviewed, where seismic amplitudes are
                  recovered from complete data by diagonalizing the
                  Gramm matrix of the linearized scattering
                  problem. Next, we show how the recovery of seismic
                  wavefields from incomplete data can be cast into a
                  compressive sampling problem, followed by a proposal
                  to compress wavefield extrapolation operators via
                  compressive sampling in the modal domain. During the
                  latter approach, we explicitly exploit the mutual
                  incoherence between the eigenfunctions of the
                  Helmholtz operator and the curvelet frame elements
                  that compress the extrapolated wavefield. This is
                  joint work with Gilles Hennenfent, Peyman Moghaddam,
                  Tim Lin, Chris Stolk and Deli Wang.},
  keywords     = {AMS Von Neumann, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/vonNeuman/2007/herrmann07AMScsi/herrmann07AMScsi_pres.pdf }
}


@CONFERENCE{herrmann2007PIMScsm,
  author       = {Felix J. Herrmann},
  title        = {Compressive sampling meets seismic imaging},
  booktitle    = {PIMS},
  year         = {2007},
  keywords     = {PIMS, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/PIMS/2007/herrmann07PIMScsm/herrmann07PIMScsm_pres.pdf  }
}


@CONFERENCE{herrmann2007SEGmpf,
  author       = {Felix J. Herrmann},
  title        = {Multiple prediction from incomplete data with the focused curvelet transform},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2505-2600},
  abstract     = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach. During the first step, the
                  recenly developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation.},
  keywords     = {SEG, Presentation, SLIM}, 
  optdoi       = {10.1190/1.2792987},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf_pres.pdf}
}


@CONFERENCE{herrmann2007SINBADcwe,
  author    = {Felix J. Herrmann},
  title     = {Compressed wavefield extrapolation},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {An explicit algorithm for the extrapolation of one-way
                  wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation.  Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright}, we are
                  able to formulate the (inverse) wavefield
                  extrapolation problem on small subsets of the data
                  volume, thereby reducing the size of the
                  operators. According to compressed sensing theory,
                  signals can successfully be recovered from an
                  incomplete set of measurements when the measurement
                  basis is incoherent with the representation in which
                  the wavefield is sparse. In this new approach, the
                  eigenfunctions of the Helmholtz operator are
                  recognized as a basis that is incoherent with
                  curvelets that are known to compress seismic
                  wavefields. By casting the wavefield extrapolation
                  problem in this framework, wavefields can
                  successfully be extrapolated in the modal domain via
                  a computationally cheaper operation. A proof of
                  principle for the
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. This is joint
                  work with Tim Lin.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/Publications/Public/Journals/CompressedExtrap.pdf}
}


@CONFERENCE{herrmann2007SINBADfrw,
  author       = {Felix J. Herrmann},
  title        = {Focused recovery with the curvelet transform},
  booktitle    = {SINBAD 2007},
  year         = {2007},
  abstract     = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach.  During the first step, the
                  recently developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation. This is joint work
                  with Deli Wang and Gilles Hennenfent.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/sites/data/Papers/Herrmann2007SINBADfoc.pdf }
}


@CONFERENCE{herrmann2007SLIMfsd,
  author    = {Felix J. Herrmann},
  title     = {From seismic data to the composition of rocks: an
                  interdisciplinary and multiscale approach to
                  exploration seismology},
  booktitle = {Berkhout{\textquoteright}s valedictory address: the
                  conceptual approach of understanding},
  year      = {2007},
  abstract  = {In this essay, a nonlinear and multidisciplinary
                  approach is presented that takes seismic data to the
                  composition of rocks. The presented work has deep
                  roots in the
                  {\textquoteleft}gedachtengoed{\textquoteright}
                  (philosophy) of Delphi spearheaded by Guus
                  Berkhout. Central themes are multiscale,
                  object-orientation and a multidisciplinary
                  approach.},
  keywords  = {SLIM},
  url       = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Misc/herrmann07SLIMfsd/herrmann07SLIMfsd.pdf }
}


@CONFERENCE{herrmann2007COIPpti,
  author       = {Felix J. Herrmann},
  title        = {Phase transitions in explorations seismology: statistical mechanics meets information theory},
  booktitle    = {COIP},
  year         = {2007},
  abstract     = {In this paper, two different applications of phase
                  transitions to exploration seismology will be
                  discussed. The first application concerns a phase
                  diagram ruling the recovery conditions for seismic
                  data volumes from incomplete and noisy data while
                  the second phase transition describes the behavior
                  of bi-compositional mixtures as a function of the
                  volume fraction. In both cases, the phase
                  transitions are the result of randomness in large
                  system of equations in combination with
                  nonlinearity. The seismic recovery problem from
                  incomplete data involves the inversion of a
                  rectangular matrix. Recent results from the field of
                  "compressive sensing" provide the conditions for a
                  successful recovery of functions that are sparse in
                  some basis (wavelet) or frame (curvelet)
                  representation, by means of a sparsity
                  ($\ell_1$-norm) promoting nonlinear program. The
                  conditions for a successful recovery depend on a
                  certain randomness of the matrix and on two
                  parameters that express the matrix{\textquoteright}
                  aspect ratio and the ratio of the number of nonzero
                  entries in the coefficient vector for the sparse
                  signal representation over the number of
                  measurements. It appears that the ensemble average
                  for the success rate for the recovery of the sparse
                  transformed data vector by a nonlinear sparsity
                  promoting program, can be described by a phase
                  transition, demarcating the regions for the two
                  ratios for which recovery of the sparse entries is
                  likely to be successful or likely to
                  fail. Consistent with other phase transition
                  phenomena, the larger the system the sharper the
                  transition. The randomness in this example is
                  related to the construction of the matrix, which for
                  the recovery of spike trains corresponds to the
                  randomly restricted Fourier matrix. It is shown,
                  that these ideas can be extended to the curvelet
                  recovery by sparsity-promoting inversion (CRSI)
                  . The second application of phase transitions in
                  exploration seismology concerns the upscaling
                  problem. To counter the intrinsic smoothing of
                  singularities by conventional equivalent medium
                  upscaling theory, a percolation-based nonlinear
                  switch model is proposed. In this model, the
                  transport properties of bi-compositional mixture
                  models for rocks undergo a sudden change in the
                  macroscopic transport properties as soon as the
                  volume fraction of the stronger material reaches a
                  critical point. At this critical point, the stronger
                  material forms a connected cluster, which leads to
                  the creation of a cusp-like singularity in the
                  elastic moduli, which in turn give rise to specular
                  reflections. In this model, the reflectivity is no
                  longer explicitly due to singularities in the rocks
                  composition.  Instead, singularities are created
                  whenever the volume fraction exceeds the critical
                  point. We will show that this concept can be used
                  for a singularity-preserved lithological upscaling.},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/COIP/2007/herrmann07COIPpti/herrmann07COIPpti_pres.pdf }
}


@CONFERENCE{herrmann2007EAGErdi,
  author       = {Felix J. Herrmann},
  title        = {Recent developments in curvelet-based seismic processing},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Combinations of parsimonious signal representations
                  with nonlinear sparsity promoting programs hold the
                  key to the next-generation of seismic data
                  processing algorithms ... Since they allow for a
                  formulation that is stable w.r.t. noise \&
                  incomplete data do not require prior information on
                  the velocity or locations and dips of the events},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi.pdf }
}


@CONFERENCE{herrmann2007SINBADrdi2,
  author       = {Felix J. Herrmann},
  title        = {Recent developments in primary-multiple separation},
  booktitle    = {SINBAD 2007},
  year         = {2007},
  abstract     = {In this talk, we present a novel primary-multiple
                  separation scheme which makes use of the sparsity of
                  both primaries and multiples in a transform domain,
                  such as the curvelet transform, to provide estimates
                  of each. The proposed algorithm utilizes seismic
                  data as well as the output of a preliminary step
                  that provides (possibly) erroneous predictions of
                  the multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm. Time permitted, we will also briefly
                  discuss a propasal for adaptive curvelet-domain
                  matched filtering. This is joint work with Deli
                  Wang, Rayan Saaba, {\o}zgur Yilmaz and Eric
                  Verschuur.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SINBAD/2007/herrmann07SINBADrdi2/herrmann07SINBADrdi2_pres.pdf }
}


@CONFERENCE{herrmann2007SINBADsia2,
  author    = {Felix J. Herrmann},
  title     = {Seismic image amplitude recovery},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {In this talk, we recover the amplitude of a seismic
                  image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave equation
                  on the SEG-AA salt model. This is joint work with
                  Peyman Moghaddam and Chris Stolk (University of
                  Twente)},
  keywords  = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{herrmann2007AIPsit,
  author       = {Felix J. Herrmann},
  title        = {Seismic inversion through operator overloading},
  booktitle    = {AIP},
  year         = {2007},
  abstract     = {Inverse problems in (exploration) seismology are
                  known for their large to very large scale. For
                  instance, certain sparsity-promoting inversion
                  techniques involve vectors that easily exceed 230
                  unknowns while seismic imaging involves the
                  construction and application of matrix-free
                  discretized operators where single matrix-vector
                  evaluations may require hours, days or even weeks on
                  large compute clusters. For these reasons, software
                  development in this field has remained the domain of
                  highly technical codes programmed in low-level
                  languages with little eye for easy development, code
                  reuse and integration with (nonlinear) programs that
                  solve inverse problems. Following ideas from the
                  Symes{\textquoteright} Rice Vector Library and
                  Bartlett{\textquoteright}s C++ object-oriented
                  interface, Thyra, and Reduction/Transformation
                  operators (both part of the Trilinos software
                  package), we developed a software-development
                  environment based on overloading. This environment
                  provides a pathway from in-core prototype
                  development to out-of-core and MPI
                  {\textquoteright}production{\textquoteright} code
                  with a high level of code reuse. This code reuse is
                  accomplished by integrating the out-of-core and MPI
                  functionality into the dynamic object-oriented
                  programming language Python. This integration is
                  implemented through operator overloading and allows
                  for the development of a coordinate-free solver
                  framework that (i) promotes code reuse; (ii)
                  analyses the statements in an abstract syntax tree
                  and (iii) generates executable statements. In the
                  current implementation, we developed an interface to
                  generate executable statements for the out-of-core
                  unix-pipe based (seismic) processing package
                  RSF-Madagascar (rsf.sf.net). The modular design
                  allows for interfaces to other seismic processing
                  packages and to in-core Python packages such as
                  numpy. So far, the implementation overloads linear
                  operators and elementwise reduction/transformation
                  operators. We are planning extensions towards
                  nonlinear operators and integration with existing
                  (parallel) solver frameworks such as Trilinos.},
  keywords     = {AIP, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/AIP/2007/herrmann07AIPsit/herrmann07AIPsit.pdf }
}


@CONFERENCE{herrmann2007CYBERsmc,
  author       = {Felix J. Herrmann},
  title        = {Seismology meets compressive sampling presented at the joint NSF-IPAM meeting. Los Angeles. October, 2007.},
  booktitle    = {Cyber},
  year         = {2007},
  abstract     = {Presented at Cyber-Enabled Discovery and Innovation:
                  Knowledge Extraction as a success story lecture. See
                  for more detail
                  https://www.ipam.ucla.edu/programs/cdi2007/},
  keywords     = {Cyber, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Cyber/2007/herrmann07CYBERsmc/herrmann07CYBERsmc.pdf }
}


@CONFERENCE{herrmann2007EAGEsrm,
  author       = {Felix J. Herrmann},
  title        = {Surface related multiple prediction from incomplete data},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmont      = {06/2007},
  organization = {EAGE},
  abstract     = {Incomplete data, unknown source-receiver signatures
                  and free-surface reflectivity represent challenges
                  for a successful prediction and subsequent removal
                  of multiples. In this paper, a new method will be
                  represented that tackles these challenges by
                  combining what we know about wavefield
                  (de-)focussing, by weighted
                  convolutions/correlations, and recently developed
                  curvelet-based recovery by sparsity-promoting
                  inversion (CRSI). With this combination, we are able
                  to leverage recent insights from wave physics to-
                  wards a nonlinear formulation for the
                  multiple-prediction problem that works for
                  incomplete data and without detailed knowledge on
                  the surface effects.},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm.pdf}
}


@CONFERENCE{herrmann2007AIPssd,
  author       = {Felix J. Herrmann},
  title        = {Stable seismic data recovery},
  booktitle    = {AIP},
  year         = {2007},
  abstract     = {In this talk, directional frames, known as
                  curvelets, are used to recover seismic data and
                  images from noisy and incomplete data. Sparsity and
                  invariance properties of curvelets are exploited to
                  formulate the recovery by a {\textquoteleft}1-norm
                  promoting program. It is shown that our data
                  recovery approach is closely linked to the recent
                  theory of
                  {\textquoteleft}{\textquoteleft}compressive
                  sensing{\textquoteright}{\textquoteright} and can be
                  seen as a first step towards a nonlinear sampling
                  theory for wavefields. The second problem that will
                  be discussed concerns the recovery of the amplitudes
                  of seismic images in clutter. There, the invariance
                  of curvelets is used to approximately invert the
                  Gramm operator of seismic imaging. In the
                  high-frequency limit, this Gramm matrix corresponds
                  to a pseudo-differential operator, which is near
                  diagonal in the curvelet domain.In this talk,
                  directional frames, known as curvelets, are used to
                  recover seismic data and images from noisy and
                  incomplete data. Sparsity and invariance properties
                  of curvelets are exploited to formulate the recovery
                  by a l1-norm promoting program. It is shown that our
                  data recovery approach is closely linked to the
                  recent theory of
                  {\textquoteleft}{\textquoteleft}compressive
                  sensing{\textquoteright}{\textquoteright} and can be
                  seen as a first step towards a nonlinear sampling
                  theory for wavefields. The second problem that will
                  be discussed concerns the recovery of the amplitudes
                  of seismic images in clutter. There, the invariance
                  of curvelets is used to approximately invert the
                  Gramm operator of seismic imaging.  In the
                  high-frequency limit, this Gramm matrix corresponds
                  to a pseudo-differential operator, which is near
                  diagonal in the curvelet domain.},
  keywords     = {AIP, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/AIP/2007/herrmann07AIPssd/herrmann07AIPssd.pdf }
}


@CONFERENCE{herrmann2007EAGEsia,
  author       = {Felix J. Herrmann and Gilles Hennenfent and Peyman P. Moghaddam},
  title        = {Seismic imaging and processing with curvelets},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {06/2007},
  organization = {EAGE},
  abstract     = {In this paper, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for three problems in
                  seismic processing and imaging namely, seismic data
                  regularization from data with large percentages of
                  traces missing; seismic amplitude recovery for
                  sub-salt images obtained by reverse-time migration
                  and primary-multiple separation, given an inaccurate
                  multiple prediction. We argue why these nonlinear
                  formulations are beneficial.},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia_pres.pdf },  
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia.pdf }
}


@CONFERENCE{herrmann2007EAGEjda,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Just diagonalize: a curvelet-based approach to seismic amplitude recovery},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {In his presentation we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  the recovery of seismic amplitudes. We show that the
                  curvelet's wavefront detection capability and
                  invariance under wave propagation lead to a
                  formulation of this recovery problem that is stable
                  under noise and missing data. {\copyright}2007
                  Society of Exploration Geophysicists},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSIM.pdf},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/herrmann07jda.pdf}
}


@CONFERENCE{herrmann2007SEGsdp,
  author       = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman P. Moghaddam},
  title        = {Seismic data processing with curvelets: a multiscale and nonlinear approach},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2220-2224},
  organization = {SEG},
  abstract     = {In this abstract, we present a nonlinear
                  curvelet-based sparsity-promoting formulation of a
                  seismic processing flow, consisting of the following
                  steps: seismic data regularization and the
                  restoration of migration amplitudes. We show that
                  the curvelet{\textquoteright}s wavefront detection
                  capability and invariance under the
                  migration-demigration operator lead to a formulation
                  that is stable under noise and missing
                  data. {\copyright}2007 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.2792927},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp.pdf }
}


@CONFERENCE{herrmann2007SEGsnt,
  author    = {Felix J. Herrmann and D. Wilkinson},
  title     = {Seismic noise: the good, the bad and the ugly},
  booktitle = {SEG Summer Research Workshop: Seismic Noise: Origins Preventions, Mitigation, Utilization},
  year      = {2007},
  note      = {Presented at SEG Summer Research Workshop: Seismic Noise: Origins, Prevention, Mitigation, Utilization},
  abstract  = {In this paper, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for three problems
                  related to seismic noise, namely the
                  {\textquoteright}good{\textquoteright},
                  corresponding to noise generated by random sampling;
                  the {\textquoteright}bad{\textquoteright},
                  corresponding to coherent noise for which
                  (inaccurate) predictions exist and the
                  {\textquoteright}ugly{\textquoteright} for which no
                  predictions exist.  We will show that the
                  compressive capabilities of curvelets on seismic
                  data and images can be used to tackle these three
                  categories of noise-related problems.},
  keywords  = {SLIM, SEG},
  url       = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsnt/herrmann07SEGsnt.pdf }
}


@CONFERENCE{lin2007SEGcwe1,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Compressed wavefield extrapolation with curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {1997-2001},
  organization = {SEG},
  abstract     = {An explicit algorithm for the extrapolation of
                  one-way wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation.  Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright}, we are
                  able to formulate the (inverse) wavefield
                  extrapolation problem on small subsets of the data
                  volume, thereby reducing the size of the
                  operators. According to compressed sensing theory,
                  signals can successfully be recovered from an
                  imcomplete set of measurements when the measurement
                  basis is incoherent with the representation in which
                  the wavefield is sparse. In this new approach, the
                  eigenfunctions of the Helmholtz operator are
                  recognized as a basis that is incoherent with
                  curvelets that are known to compress seismic
                  wavefields. By casting the wavefield extrapolation
                  problem in this framework, wavefields can
                  successfully be extrapolated in the modal domain via
                  a computationally cheaper operation. A proof of
                  principle for the
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. {\copyright}2007
                  Society of Exploration Geophysicists},
  optdoi       = {10.1190/1.2792882},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1.pdf }
}


@CONFERENCE{maysami2007EAGEsrc,
  author       = {Mohammad Maysami and Felix J. Herrmann},
  title        = {Seismic reflector characterization by a multiscale detection-estimation method},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Seismic transitions of the subsurface are typically
                  considered as zero-order singularities (step
                  functions). According to this model, the
                  conventional deconvolution problem aims at
                  recovering the seismic reflectivity as a sparse
                  spike train. However, recent multiscale analysis on
                  sedimentary records revealed the existence of
                  accumulations of varying order singularities in the
                  subsurface, which give rise to fractional-order
                  discontinuities. This observation not only calls for
                  a richer class of seismic reflection waveforms, but
                  it also requires a different methodology to detect
                  and characterize these reflection events. For
                  instance, the assumptions underlying conventional
                  deconvolution no longer hold. Because of the
                  bandwidth limitation of seismic data, multiscale
                  analysis methods based on the decay rate of wavelet
                  coefficients may yield ambiguous results. We avoid
                  this problem by formulating the estimation of the
                  singularity orders by a parametric nonlinear
                  inversion method.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc_pres.pdf }
}


@CONFERENCE{moghaddam2007CSEGmar,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title        = {Migration amplitude recovery using curvelets},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {In this paper, we recover the amplitude of a seismic
                  image by approximating the normal operator and
                  subsequently inverting it. Normal operator
                  (migration followed by modeling) is an example of
                  pseudo-differential.  curvelets are proven to be
                  invariant under the action of pseudo-differential
                  operators under certain conditions. Subsequently,
                  curvelets are forming as eigen-vectors for such an
                  operator. We propose a seismic amplitude recovery
                  method that employs an eigen-value decomposition for
                  normal operator using curvelets as eigen-vectors and
                  to be estimated eigenvalues.  A post-stack
                  reverse-time, wave-equation migration is used for
                  evaluation of the proposed method.},
  file         = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf:PDF},
  keywords     = {SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf}
}


@CONFERENCE{moghaddam2007SEGrsi,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title        = {Robust seismic-images amplitude recovery using curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2225-2229},
  organization = {SEG},
  abstract     = {In this paper, we recover the amplitude of a seismic
                  image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate non-linear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteleft}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave equation
                  on the SEG-AA salt model. {\copyright}2007 Society
                  of Exploration Geophysicists},
  optdoi       = {10.1190/1.2792928},
  keywords     = {Presentation, SLIM, SEG},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi_pres.pdf }
}


@CONFERENCE{moghaddam2007CSEGsac,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title        = {Sparsity and continuity enhancing seismic imaging},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {A non-linear singularity-preserving solution to the
                  least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes. Our method is based on the estimation of
                  the normal operator in the form of an
                  {\textquoteright}eigenvalue{\textquoteright}
                  decompsoition with curvelets as the
                  eigenvectors{\textquoteright}. Subsequently, we
                  propose an inversion method that derives from
                  estimation of the normal operator and is formulated
                  as a convex optimization problem.  Sparsity in the
                  curvelet domain as well as continuity along the
                  reflectors in the image domain are promoted as part
                  of this optimization. Our method is tested with a
                  reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  file         = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf:PDF},
  keywords     = {SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf}
}


@CONFERENCE{moghaddam2007EAGEsar,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title        = {Seismic amplitude recovery with curvelets},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {A non-linear singularity-preserving solution to the
                  least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes. Our method is based on the estimation of
                  the normal operator in the form of an
                  {\textquoteright}eigenvalue{\textquoteright}
                  decompsoition with curvelets as the
                  {\textquoteright}eigenvectors{\textquoteright}.
                  Subsequently, we propose an inversion method that
                  derives from estimation of the normal operator and
                  is formulated as a convex optimization
                  problem. Sparsity in the curvelet domain as well as
                  continuity along the reflectors in the image domain
                  are promoted as part of this optimization.  Our
                  method is tested with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/moghaddam07EAGEsar/moghaddam07EAGEsar.pdf }
}


@CONFERENCE{saab2007SEGcbp,
  author       = {Rayan Saab and Deli Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title        = {Curvelet-based primary-multiple separation from a {B}ayesian perspective},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2510-2514},
  organization = {SEG},
  abstract     = {In this abstract, we present a novel
                  primary-multiple separation scheme which makes use
                  of the sparsity of both primaries and multiples in a
                  transform domain, such as the curvelet transform, to
                  provide estimates of each. The proposed algorithm
                  utilizes seismic data as well as the output of a
                  preliminary step that provides (possibly) erroneous
                  predictions of the multiples. The algorithm
                  separates the signal components, i.e., the primaries
                  and multiples, by solving an optimization problem
                  that assumes noisy input data and can be derived
                  from a Bayesian perspective. More precisely, the
                  optimization problem can be arrived at via an
                  assumption of a weighted Laplacian distribution for
                  the primary and multiple coefficients in the
                  transform domain and of white Gaussian noise
                  contaminating both the seismic data and the
                  preliminary prediction of the multiples, which both
                  serve as input to the algorithm. {\copyright}2007
                  Society of Exploration Geophysicists},
  optdoi       = {10.1190/1.2792988},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp.pdf }
}


@CONFERENCE{sastry2007SINBADnor,
  author    = {Challa S. Sastry},
  title     = {Norm-one recovery from irregular sampled data},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {Seismic traces are sampled irregularly and
                  insufficiently due to practical and economical
                  limitations. The use of such data in seismic imaging
                  results in image artifacts and poor spatial
                  resolution. Therefore, before being used, the
                  measurements are to be interpolated onto a regular
                  grid. One of the methods achieving this objective is
                  based on the Fourier reconstruction, which deals
                  with the under-determined system of equations. The
                  recent pursuit techniques (namely, basis pursuit,
                  matching pursuit etc) admit certain promising
                  features such as faster and simpler implementation
                  even in large scale settings.  The presentation
                  discusses the application of the pursuit algorithms
                  to the Fourier-based interpolation problem for the
                  signals that have sparse Fourier spectra. In
                  particular, the objective of the presentation
                  includes: 1). studying the performance of the
                  algorithm if, and how far, the measurement
                  coordinates can be shifted from uniform distribution
                  on the continuous interval. 2). studying what could
                  be the allowable misplacement in the measurement
                  coordinates that does not alter the quality of the
                  reconstruction process},
  keywords  = {SLIM, SINBAD, Presentation}
}


@CONFERENCE{vandenberg2007SINBADipo1,
  author    = {Ewout {van den Berg} and Michael P. Friedlander},
  title     = {In Pursuit of a Root},
  booktitle = {2007 Von Neumann Symposium},
  year      = {2007},
  keywords  = {minimization, Presentation, SLIM},
  url       = {http://slim.eos.ubc.ca/Publications/Private/Presentations/sinbad/2007/friedlander07ipo.pdf}
}


@CONFERENCE{verschuur2007SEGmmp,
  author       = {D. J. Verschuur and Deli Wang and Felix J. Herrmann},
  title        = {Multiterm multiple prediction using separated
                  reflections and diffractions combined with
                  curvelet-based subtraction},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2535-2539},
  organization = {SEG},
  abstract     = {The surface-related multiple elimination (SRME)
                  method has proven to be successful on a large number
                  of data cases. Most of the applications are still
                  2D, as the full 3D implementation is still expensive
                  and under development. However, the earth is a 3D
                  medium, such that 3D effects are difficult to
                  avoid. Most of the 3D effects come from diffractive
                  structures, whereas the specular reflections
                  normally have less of a 3D behavior. By separating
                  the seismic data in a specular reflecting and a
                  diffractive part, multiple prediction can be carried
                  out with these different subsets of the input data,
                  resulting in several categories of predicted
                  multiples. Because each category of predicted
                  multiples can be subtracted from the input data with
                  different adaptation filters, a more flexible SRME
                  procedure is obtained.  Based on some initial
                  results from a Gulf of Mexico dataset, the potential
                  of this approach is investigated. {\copyright}2007
                  Society of Exploration Geophysicists},
  optdoi       = {10.1190/1.2792993},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp.pdf }
}


@CONFERENCE{wang2007SEGrri,
  author       = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title        = {Recent results in curvelet-based primary-multiple separation: application to real data},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2500-2504},
  organization = {SEG},
  abstract     = {In this abstract, we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  the primary-multiple separation problem. We show
                  that these coherent signal components can be
                  separated robustly by explicitly exploting the
                  locality of curvelets in phase space (space-spatial
                  frequency plane) and their ability to compress data
                  volumes that contain wavefronts. This work is an
                  extension of earlier results and the presented
                  algorithms are shown to be stable under noise and
                  moderately erroneous multiple
                  predictions. {\copyright}2007 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.2792986},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri.pdf }
}


@CONFERENCE{yarham2007SINBADnsw,
  author    = {Carson Yarham},
  title     = {Nonlinear surface wave prediction and separation},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {Removal of surface waves is an integral step in seismic
                  processing.  There are many standard techniques for
                  removal of this type of coherent noise, such as f-k
                  filtering, but these methods are not always
                  effective.  One of the common problems with removal
                  of surface waves is that they tend to be aliased in
                  the frequency domain. This can make removal
                  difficult and affect the frequency content of the
                  reflector signals, as this signals will not be
                  completely separated. As seen in (Hennenfent, G. and
                  F. Herrmann, 2006, Application of stable signal
                  recovery to seismic interpolation) interpolation can
                  be used effectively to resample the seismic record
                  thus dealiasing the surface waves. This separates
                  the signals in the frequency domain allowing for a
                  more precise and complete removal. The use of this
                  technique with curvelet based surface wave
                  predictions and an iterative L1 separation scheme
                  can be used to remove surface waves from shot
                  records more completely that with standard
                  techniques.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2007/yarham07eage.pdf}
}


@CONFERENCE{yarham2007EAGEcai,
  author       = {Carson Yarham and Gilles Hennenfent and Felix J. Herrmann},
  title        = {Curvelet applications in surface wave removal},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Ground roll removal of seismic signals can be a
                  challenging prospect.  Dealing with undersampleing
                  causing aliased waves amplitudes orders of magnitude
                  higher than reflector signals and low frequency loss
                  of information due to band ...},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/yarham07EAGEcai/yarham07EAGEcai.pdf}
}



%-------------------------------------------------2006-------------------------------------------------

@CONFERENCE{hennenfent2006SINBADapo,
  author       = {Gilles Hennenfent},
  title        = {A primer on stable signal recovery},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation an introduction will be
                  given on the method of stable recovery from noisy
                  and incomplete data. Strong recovery conditions that
                  guarantee the recovery for arbitrary acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/4-Felix3.pdf}
}


@CONFERENCE{hennenfent2006SINBADros,
  author       = {Gilles Hennenfent},
  title        = {Recovery of seismic data: practical considerations},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {We propose a method for seismic data interpolation
                  based on 1) the reformulation of the problem as a
                  stable signal recovery problem and 2) the fact that
                  seismic data is sparsely represented by curvelets.
                  This method does not require information on the
                  seismic velocities.  Most importantly, this
                  formulation potentially leads to an explicit
                  recovery condition. We also propose a large-scale
                  problem solver for the l1-regularization
                  minimization involved in the recovery and
                  successfully illustrate the performance of our
                  algorithm on 2D synthetic and real examples.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/5-Gilles2.pdf}
}


@CONFERENCE{hennenfent2006SINBADtnf,
  author       = {Gilles Hennenfent},
  title        = {The Nonuniform Fast Discrete Curvelet Transform (NFDCT)},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {The authors present an extension of the fast
                  discrete curvelet transform (FDCT) to nonuniformly
                  sampled data. This extension not only restores
                  curvelet compression rates for nonuniformly sampled
                  data but also removes noise and maps the data to a
                  regular grid.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/10-Gilles3.pdf}
}


@CONFERENCE{hennenfent2006SEGaos,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Application of stable signal recovery to seismic data interpolation},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2006},
  volume       = {25},
  pages        = {2797-2801},
  organization = {SEG},
  abstract     = {We propose a method for seismic data interpolation
                  based on 1) the reformulation of the problem as a
                  stable signal recovery problem and 2) the fact that
                  seismic data is sparsely represented by curvelets.
                  This method does not require information on the
                  seismic velocities.  Most importantly, this
                  formulation potentially leads to an explicit
                  recovery condition. We also propose a large-scale
                  problem solver for the 1-regularization minimization
                  involved in the recovery and successfully illustrate
                  the performance of our algorithm on 2D synthetic and
                  real examples. {\copyright}2006 Society of
                  Exploration Geophysicists},
  optdoi       = {10.1190/1.2370105},
  keywords     = {Presentation, SLIM, curvelets, interpolation,
                  seismic data, regularization minimization, iterative
                  thresholding, amplitude, SEG, continuity, fast
                  transform},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos.pdf },
  url2         = {http://dx.doi.org/10.1190/1.2370105 }
}


@CONFERENCE{herrmann2006SINBADapo1,
  author       = {Felix J. Herrmann},
  title        = {A primer on sparsity transforms: curvelets and wave atoms},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation an overview will be given
                  on the different sparsity transforms that are used
                  at SLIM. Emphasis will be on two directional and
                  multiscale wavelet transforms, namely the curvelet
                  and the recently introduced wave-atom
                  transforms. The main properties of these transforms
                  will be listed and their performance on seismic data
                  will be discussed.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/4-Felix3.pdf}
}


@CONFERENCE{herrmann2006SINBADapow,
  author       = {Felix J. Herrmann},
  title        = {A primer on weak conditions for stable recovery},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation an introduction will be
                  given on the method of stable recovery from noisy
                  and incomplete data. Weak recovery conditions that
                  guarantee the recovery for typical acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented.  The advantage of these
                  weak conditions is that they are less pessimistic
                  and {\textquoteleft}verifiable{\textquoteright} or
                  very large-scale acquisition geometries.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/6-Felix4.pdf}
}


@CONFERENCE{herrmann2006SINBADmpf,
  author       = {Felix J. Herrmann},
  title        = {Multiple prediction from incomplete data},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/16-Felix7.pdf}
}


@CONFERENCE{herrmann2006SINBADom,
  author       = {Felix J. Herrmann},
  title        = {Opening meeting},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/1-Felix1.pdf}
}


@CONFERENCE{herrmann2006SINBADpms,
  author       = {Felix J. Herrmann and Urs Boeniger and D. J.  Verschuur},
  title        = {Primary-multiple separation by curvelet frames},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, during which
                  multiples are predicted from seismic data, and a
                  primary-multiple separation step, during which the
                  predicted multiples are
                  {\textquoteright}matched{\textquoteright} with the
                  true multiples in the data and subsequently
                  removed. The last step is crucial in practice: an
                  incorrect separation will cause residual multiple
                  energy in the result or may lead to a distortion of
                  the primaries, or both. To reduce these adverse
                  effects, a new transformed-domain method is proposed
                  where primaries and multiples are separated rather
                  than matched. This separation is carried out on the
                  basis of differences in the multiscale and
                  multidirectional characteristics of these two signal
                  components. Our method uses the curvelet transform,
                  which maps multidimensional data volumes into almost
                  orthogonal localized multidimensional prototype
                  waveforms that vary in directional and
                  spatio-temporal content. Primaries-only and
                  multiples-only signal components are recovered from
                  the total data volume by a nonlinear optimization
                  scheme that is stable under noisy input data. During
                  the optimization, the two signal components are
                  separated by enhancing sparseness (through weighted
                  l1-norms) in the transformed domain subject to
                  fitting the observed data as the sum of the
                  separated components to within a user-defined
                  tolerance level. Whenever the prediction for the two
                  signal components in the transformed domain
                  correlate, the recovery is suppressed while for
                  regions where the correlation is small the method
                  seeks the sparsest set of coefficients that
                  represent each signal component. Our algorithm does
                  not seek a matched filter and as such it differs
                  fundamentally from traditional adaptive subtraction
                  methods. The method derives its stability from the
                  sparseness obtained by a non-parametric multiscale
                  and multidirectional overcomplete signal
                  representation. This sparsity serves as prior
                  information and allows for a Bayesian interpretation
                  of our method during which the log-likelihood
                  function is minimized while the two signal
                  components are assumed to be given by a
                  superposition of prototype waveforms, drawn
                  independently from a probability function that is
                  weighted by the predicted primaries and
                  multiples. In this paper, the predictions are based
                  on the data-driven surface-related multiple
                  elimination (SRME) method. Synthetic and field data
                  examples show a clean separation leading to a
                  considerable improvement in multiple suppression
                  compared to the conventional method of adaptive
                  matched filtering. This improved separation
                  translates into an improved stack.},
  keywords     = {Presentation, SINBAD, SLIM},
  optdoi       = {10.1111/j.1365-246X.2007.03360.x},
  Volume       = {170},
  pages        = {781-799},
  organization = {Geophysical Journal International},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann07npm.pdf}
}


@CONFERENCE{herrmann2006SINBADsac,
  author       = {Felix J. Herrmann},
  title        = {Sparsity- and continuity-promoting seismic image recovery with curvelets},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {A nonlinear singularity-preserving solution to
                  seismic image recovery with sparseness and
                  continuity constraints is proposed. The method
                  explicitly explores the curvelet transform as a
                  directional frame expansion that, by virtue of its
                  sparsity on seismic images and its invariance under
                  the Hessian of the linearized imaging problem,
                  allows for a stable recovery of the migration
                  amplitudes from noisy data.  The method corresponds
                  to a preconditioning that corrects the amplitudes
                  during a post-processing step. The solution is
                  formulated as a nonlinear optimization problem where
                  sparsity in the curvelet domain as well as
                  continuity along the imaged reflectors are jointly
                  promoted. To enhance sparsity, the l1-norm on the
                  curvelet coefficients is minimized while continuity
                  is promoted by minimizing an anisotropic diffusion
                  norm on the image. The performance of the recovery
                  scheme is evaluated with
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code on a synthetic dataset. This is joint
                  work with Peyman Moghaddam.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/14-Felix6.pdf}
}


@CONFERENCE{herrmann2006SINBADsra,
  author       = {Felix J. Herrmann},
  title        = {Stable recovery and separation of seismic data},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation an overview will be given
                  on how seismic data regularization and separation
                  problems can be cast into the framework of stable
                  signal recovery. It is shown that the successful
                  solution of these two problems depends on the
                  existence of signal expansions that are
                  compressible. Preliminary examples will be shown.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/3-Felix2.pdf}
}


@CONFERENCE{lin2006SINBADci,
  author       = {Tim T.Y. Lin},
  title        = {Compressed imaging},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {In 1998 Grimbergen et. al. introduced a new method
                  for computing wavefield propagation which improved
                  on the previously employed local explicit operator
                  method in that it exhibited no dip limitation,
                  accurately handled laterally varying background
                  ground velocity models, and is unconditionally
                  stable. These desirable properties are mainly
                  attributed to bringing the propagation problem into
                  an eigenvector basis that diagonalizes the
                  propagation operators. This modal-transform method,
                  however, requires at each depth-level the solution
                  of a large-scale sparse eigenvalue problem to
                  compute the square-root of the Helmholtz
                  operator. By using recent results from compressed
                  sensing, we hope to reduce these computational costs
                  that typically involve the synthesizes of the
                  imaging operators and the cost of matrix-vector
                  products.  To reduce these costs, we compress the
                  extrapolation operators by using only a fraction of
                  the positive eigenvalues and temporal frequencies.
                  This reduction not only leads to smaller matrices
                  but also to reduced synthesis costs. These
                  reductions go at the expense of solving a recovery
                  problem from incomplete data. During the
                  presentation, we show that wavefields can accurately
                  be extrapolated with a compressed operators and
                  competitive costs.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/17-Tim1.pdf}
}


@CONFERENCE{maysami2006SINBADrro,
  author    = {Mohammad Maysami},
  title     = {Recent results on seismic deconvolution},
  booktitle = {SINBAD 2006},
  year      = {2006},
  abstract  = {One of the important steps in seismic imaging is to
                  provide suitable information about boundaries. Sharp
                  variation of physical properties at a layer boundary
                  cause reflection the wavefield. In previous work
                  done by C. M. Dupuis, seismic signal
                  characterization is divided into two steps:
                  detection and estimation. In the detection phase,
                  the goal is to find all singularities in a seismic
                  section regardless of their order and then to
                  categorize the data to different events by windowing
                  each singularity. In the estimation step, we
                  determine the order of singularity more precisely by
                  using a rough estimate based on the detection
                  phase. Traditionally, a redundant dictionary method
                  is employed for the detection part. However, we
                  attempt to instead use a new L1-solver developed by
                  D.L. Donoho: the Stagewise Orthogonal Matching
                  Pursuit (StOMP). It approximates the solution to
                  inverse problems while promoting the sparsity in the
                  solution vector. This algorithm will allow us to
                  experimentally confirm the recent analysis by
                  S. Mallat on spiky deconvolution limits, which
                  imposes a required minimum distance between
                  spikes. This required minimum distance between
                  different spikes is dependent on the number of
                  spikes as well as the width of the chosen source
                  wavelet used in convolution with the train. These
                  results allow for the design of more robust and
                  accurate detection schemes for seismic signal
                  characterization.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SINBAD/2006/maysami06SINBADrro/maysami06SINBADrro_pres.pdf }
}


@CONFERENCE{modzelewski2006SINBADdas,
  author       = {Henryk Modzelewski},
  title        = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/19-Henryk1.pdf}
}


@CONFERENCE{moghaddam2006SINBADioa,
  author       = {Peyman P. Moghaddam},
  title        = {Imaging operator approximation using Curvelets},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {In this presentation, the normal
                  (demigation-migration) operator is studied in terms
                  of a pseudo-differential operator. The invariance of
                  curvelets under this operator and their sparsity on
                  the seismic images is used to precondition the
                  migration operator. A brief overview will be given
                  on some of the theory from micro-local analysis
                  which proofs that curvelets remain approximately
                  invariant under the operator.  The proper setting
                  for which a diagonal approximation in the curvelet
                  domain is accurate is discussed together with
                  different methods that estimate this diagonal from
                  of-the-shelf migration operators. This is joint work
                  with Chris Stolk.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/15-Peyman2.pdf}
}


@CONFERENCE{moghaddam2006SINBADsac,
  author       = {Peyman P. Moghaddam},
  title        = {Sparsity- and continuity-promoting norms for seismic images},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation, the importance of sparsity
                  and continuity enhancing energy norms is emphasized
                  for seismic imaging and inversion.  The continuity
                  promoting energy norm is justified by the apparent
                  smoothness of reflectors in the direction along and
                  the oscillatory behavior across the interfaces. This
                  energy norm is called anisotropic diffusion and will
                  be defined mathematically. Denoising examples will
                  be given during which seismic images are recovered
                  from the noise by a joint norm-one and continuity
                  promoting minimization.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/13-Peyman1.pdf}
}


@CONFERENCE{sastry2007SINBADrfu,
  author       = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title        = {Recovery from unstructured data},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  keywords     = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/11-Sastry1.pdf}
}


@CONFERENCE{thomson2006SINBADlss,
  author       = {Darren Thomson},
  title        = {Large-scale seismic data recovery by the Parallel Windowed Curvelet Transform},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {We propose using overlapping, tapered windows to
                  process seismic data in parallel. This method
                  consists of numerically tight linear operators and
                  adjoints that are suitable for use in iterative
                  algorithms. This method is also highly scalable and
                  makes parallelprocessing of large seismic data sets
                  feasible. We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we have applied to a seismic data
                  interpolation algorithm. Some preliminary results
                  will be shown. Henryk Modzeleweski: Design and
                  specifications for SLIMPy's software framework The
                  SLIM group is actively developing software for
                  seismic imaging. This talk will give a general
                  overview of the software development philosophy
                  adopted by SLIM. The covered topics will include: 1)
                  adopting Python for object-oriented programming, 2)
                  including parallelism into the algorithms used in
                  seismic imaging/modeling, 3) in-house algorithms for
                  seismic imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session ``SINBAD Software
                  releases".},
  keywords     = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/18-Darren1.pdf}
}


@CONFERENCE{thomson2006SEGpwfd,
  author       = {Darren Thomson and Gilles Hennenfent and Henryk Modzelewski and Felix J. Herrmann},
  title        = {A parallel windowed fast discrete curvelet transform applied to seismic processing},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2006},
  volume       = {25},
  pages        = {2767-2771},
  organization = {SEG},
  abstract     = {We propose using overlapping, tapered windows to
                  process seismic data in parallel. This method
                  consists of numerically tight linear oper ators and
                  adjoints that are suitable for use in iterative
                  algorithms.  This method is also highly scalable and
                  makes parallel processing of large seismic data sets
                  feasible. We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we apply to a seismic data interpolation
                  algorithm. The successful performance of our
                  parallel processing scheme and algorithm on a
                  two-dimensional synthetic data is shown.},
  keywords     = {SEG},
  optdoi       = {10.1190/1.2370099},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/thomson06SEGpwfd/thomson06SEGpwfd.pdf }
}


@CONFERENCE{thomson2006SINBADppe,
  author       = {Darren Thomson},
  title        = {(P)SLIMPy: parallel extension},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {The parallel extensions to the SLIMpy environment
                  enable pipe-based processing of large data sets in
                  an MPI-based parallel environment.  Parallel
                  processing can be done by straightforward slicing of
                  data, or by using an overlapping domain
                  decomposition that requires communication between
                  different processors. The principal aim of the
                  parallel extensions is to leave abstract numerical
                  algorithms (ANA's) and applications programmed for
                  use in SLIMpy untouched when moving to parallel
                  processing.  The object-oriented functionality of
                  Python makes this possible.},
  keywords     = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/21-Darren2.pdf}
}


@CONFERENCE{yarham2006SEGcgrr,
  author       = {Carson Yarham and Urs Boeniger and Felix J. Herrmann},
  title        = {Curvelet-based ground roll removal},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2006},
  volume       = {25},
  pages        = {2777-2782},
  organization = {SEG},
  abstract     = {We have effectively identified and removed ground
                  roll through a two- step process. The first step is
                  to identify the major components of the ground roll
                  through various methods including multiscale
                  separation, directional or frequency filtering or by
                  any other method that identifies the ground
                  roll. Given this estimate for ground roll, the
                  recorded signal is separated during the second step
                  through a block-coordinate relaxation method that
                  seeks the sparsest set for weighted curvelet
                  coefficients of the ground roll and the sought-after
                  reflectivity.  The combination of these two methods
                  allows us to separate out the ground roll signal
                  while preserving the reflector information. Since
                  our method is iterative, we have control of the
                  separation process.  We successfully tested our
                  algorithm on a real data set with a complex ground
                  roll and reflector structure.},
  keywords     = {SEG},
  optdoi       = {10.1190/1.2370101},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/yarham06SEGcgrr/yarham06SEGcgrr.pdf } 
}



%-------------------------------------------------2005-------------------------------------------------

@CONFERENCE{beyreuther2005SEGcot,
  author       = {Moritz Beyreuther and Jamin Cristall and Felix J. Herrmann},
  title        = {Computation of time-lapse differences with {3-D} directional frames},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2005},
  volume       = {24},
  pages        = {2488-2491},
  organization = {SEG},
  abstract     = {We present an alternative method of extracting
                  production related differences from time-lapse
                  seismic data sets. Our method is not based on the
                  actual subtraction of the two data sets, risking the
                  enhancement of noise and introduction of artifacts
                  due to local phase rotation and slightly misaligned
                  events. Rather, it mutes events of the monitor
                  survey with respect to the baseline survey based on
                  the magnitudes of coefficients in a sparse and local
                  atomic decomposition.  Our technique is demonstrated
                  to be an effective tool for enhancing the time-lapse
                  signal from surveys which have been cross-equalized.
                  {\copyright}2005 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.2148227},
  keywords     = {SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Beyreuther05SEGcot/Beyreuther05SEGcot.pdf },   
  url2         = {http://dx.doi.org/10.1190/1.2148227}
}


@CONFERENCE{hennenfent2005SEGscd,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Sparseness-constrained data continuation with
                  frames: applications to missing traces and aliased
                  signals in {2/3-D}},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2005},
  volume       = {24},
  pages        = {2162-2165},
  organization = {SEG},
  abstract     = {We present a robust iterative sparseness-constrained
                  interpolation algorithm using 2-/3-D curvelet frames
                  and Fourier-like transforms that exploits continuity
                  along reflectors in seismic data. By choosing
                  generic transforms, we circumvent the necessity to
                  make parametric assumptions (e.g. through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. Simulation and real
                  data examples for data with moderately sized gaps
                  demonstrate that our algorithm provides interpolated
                  traces that accurately reproduce the wavelet shape
                  as well as the AVO behavior. Our method also shows
                  good results for de-aliasing judged by the behavior
                  of the ($f-k$)-spectrum before and after
                  regularization. {\copyright}2005 Society of
                  Exploration Geophysicists},
  optdoi       = {10.1190/1.2148142},
  keywords     = {Presentation, SEG, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd_pres.pdf  }
}


@CONFERENCE{hennenfent2005CSEGscs,
  author       = {Gilles Hennenfent and Felix J. Herrmann and R. Neelamani},
  title        = {Sparseness-constrained seismic deconvolution with curvelets},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {Continuity along reflectors in seismic images is
                  used via Curvelet representation to stabilize the
                  convolution operator inversion. The Curvelet
                  transform is a new multiscale transform that
                  provides sparse representations for images that
                  comprise smooth objects separated by piece-wise
                  smooth discontinuities (e.g. seismic images). Our
                  iterative Curvelet-regularized deconvolution
                  algorithm combines conjugate gradient-based
                  inversion with noise regularization performed using
                  non-linear Curvelet coefficient thresholding. The
                  thresholding operation enhances the sparsity of
                  Curvelet representations. We show on a synthetic
                  example that our algorithm provides improved
                  resolution and continuity along reflectors as well
                  as reduced ringing effect compared to the iterative
                  Wiener-based deconvolution approach.},
  keywords     = {Presentation, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs_pres.pdf }
}


@CONFERENCE{hennenfent2005EAGEsdr,
  author       = {Gilles Hennenfent and R. Neelamani and Felix J. Herrmann},
  title        = {Seismic deconvolution revisited with curvelet frames},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {We propose an efficient iterative
                  curvelet-regularized deconvolution algorithm that
                  exploits continuity along reflectors in seismic
                  images.  Curvelets are a new multiscale transform
                  that provides sparse representations for images
                  (such as seismic images) that comprise smooth
                  objects separated by piece-wise smooth
                  discontinuities. Our technique combines conjugate
                  gradient-based convolution operator inversion with
                  noise regularization that is performed using
                  non-linear curvelet coefficient shrinkage
                  (thresholding). The shrinkage operation leverages
                  the sparsity of curvelets
                  representations. Simulations demonstrate that our
                  algorithm provides improved resolution compared to
                  the traditional Wiener-based deconvolution
                  approach.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Hennenfent05EAGEsdr/Hennenfent05EAGEsdr.pdf },
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/hennenfent05eage_poster.pdf}
}




@CONFERENCE{herrmann2005CSEGnld,
  author       = {Felix J. Herrmann and Gilles Hennenfent},
  title        = {Non-linear data continuation with redundant frames},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {05/2005},
  organization = {CSEG},
  abstract     = {We propose an efficient iterative data interpolation
                  method using continuity along reflectors in seismic
                  images via curvelet and discrete cosine
                  transforms. The curvelet transform is a new
                  multiscale transform that provides sparse
                  representations for images that comprise smooth
                  objects separated by piece-wise smooth
                  discontinuities (e.g. seismic images). The advantage
                  of using curvelets is that these frames are sparse
                  for high-frequency caustic-free solutions of the
                  wave-equation. Since we are dealing with less than
                  ideal data (e.g. bandwidth-limited), we compliment
                  the curvelet frames with the discrete cosine
                  transform. The latter is motivated by the successful
                  data continuation with the discrete Fourier
                  transform. By choosing generic basis functions we
                  circumvent the necessity to make parametric
                  assumptions (e.g. through linear/parabolic Radon or
                  demigration) regarding the shape of events in
                  seismic data. Synthetic and real data examples
                  demonstrate that our algorithm provides interpolated
                  traces that accurately reproduce the wavelet shape
                  as well as the AVO behavior along events in shot
                  gathers.},
  keywords     = {Presentation, SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2005/2005abstracts/101S0201-Herrmann_F_Non_Linear_Data_Continuation.pdf},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/herrmann05cseg_pres.pdf}
}


@CONFERENCE{herrmann2005CSEGnlr,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Non-linear regularization in seismic imaging},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {05/2005},
  organization = {CSEG},
  abstract     = {Two complementary solution strategies to the
                  least-squares imaging problem with sparseness \&
                  continuity continuity constraints are proposed. The
                  applied formalism explores the sparseness of
                  curvelets coefficients of the reflectivity and their
                  invariance under the demigration-migration
                  operator. We achieve the solution by jointly
                  minimizing a weighted l1-norm on the curvelet
                  coefficients and an anisotropic difussion or total
                  variation norm on the imaged reflectivity model. The
                  l1-norm exploits the sparsenss of the reflectivity
                  in the curvelet domain whereas the anisotropic norm
                  enhances the continuity along the reflections while
                  removing artifacts residing in between
                  reflectors. While the two optimization methods
                  (convex versus non-convex) share the same type of
                  regularization, they differ in flexibility how to
                  handle additional constraints on the coefficients of
                  the imaged reflectivity and in computational
                  expense. A brief sketch of the theory is provided
                  along with a number of synthetic examples.},
  keywords     = {Presentation, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr_pres.pdf }
}


@CONFERENCE{herrmann2005EAGEosf,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam and R. Kirlin},
  title        = {Optimization strategies for sparseness- and continuity-enhanced imaging: theory},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Two complementary solution strategies to the
                  least-squares migration problem with sparseness- and
                  continuity constraints are proposed.  The applied
                  formalism explores the sparseness of curvelets on
                  the reflectivity and their invariance under the
                  demigration-migration operator. Sparseness is
                  enhanced by (approximately) minimizing a (weighted)
                  l1-norm on the curvelet coefficients. Continuity
                  along imaged reflectors is brought out by minimizing
                  the anisotropic diffusion or total variation norm
                  which penalizes variations along and in between
                  reflectors. A brief sketch of the theory is provided
                  as well as a number of synthetic examples. Technical
                  details on the implementation of the optimization
                  strategies are deferred to an accompanying paper:
                  implementation.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGEosf/Herrmann05EAGEosf.pdf}
}


@CONFERENCE{herrmann2005EAGErcd,
  author       = {Felix J. Herrmann and Gilles Hennenfent},
  title        = {Robust curvelet-domain data continuation with sparseness constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {06/2005},
  organization = {EAGE},
  abstract     = {A robust data interpolation method using curvelets
                  frames is presented.  The advantage of this method
                  is that curvelets arguably provide an optimal sparse
                  representation for solutions of wave equations with
                  smooth coefficients. As such curvelets frames
                  circum- vent {\textendash} besides the assumption of
                  caustic-free data {\textendash} the necessity to
                  make parametric assumptions (e.g. through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. A brief sketch of
                  the theory is provided as well as a number of
                  examples on synthetic and real data.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf}
}


@CONFERENCE{herrmann2005EAGErcd1,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Robust curvelet-domain primary-multiple separation with sparseness constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {A non-linear primary-multiple separation method
                  using curvelets frames is presented. The advantage
                  of this method is that curvelets arguably provide an
                  optimal sparse representation for both primaries and
                  multiples.  As such curvelets frames are ideal
                  candidates to separate primaries from multiples
                  given inaccurate predictions for these two data
                  components.  The method derives its robustness
                  regarding the presence of noise; errors in the
                  prediction and missing data from the curvelet
                  frame{\textquoteright}s ability (i) to represent
                  both signal components with a limited number of
                  multi-scale and directional basis functions; (ii) to
                  separate the components on the basis of differences
                  in location, orientation and scales and (iii) to
                  minimize correlations between the coefficients of
                  the two components. A brief sketch of the theory is
                  provided as well as a number of examples on
                  synthetic and real data.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd1/Herrmann05EAGErcd1.pdf}
}



%-------------------------------------------------2004-------------------------------------------------

@CONFERENCE{beyreuther2004EAGEcdo,
  author       = {Moritz Beyreuther and Felix J. Herrmann and Jamin Cristall},
  title        = {Curvelet denoising of {4-D} seismic},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {06/2004},
  organization = {EAGE},
  abstract     = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging is an emerging technology that holds great
                  promise to better monitor and optimise reservoir
                  production. The basic idea behind 4D seismic is that
                  when multiple 3D surveys are acquired at separate
                  calendar times over a producing field, the reservoir
                  geology will not change from survey to survey but
                  the state of the reservoir fluids will change. Thus,
                  taking the difference between two 3D surveys should
                  remove the static geologic contribution to the data
                  and isolate the time- varying fluid flow
                  component. However, a major challenge in 4D seismic
                  is that acquisition and processing differences
                  between 3D surveys often overshadow the changes
                  caused by fluid flow. This problem is compounded
                  when 4D effects are sought to be derived from
                  vintage 3D data sets that were not originally
                  acquired with 4D in mind. The goal of this study is
                  to remove the acquisition and imaging artefacts from
                  a 4D seismic difference cube using Curveket
                  processing techniques.},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Beyreuther04EAGEcdo/Beyreuther04EAGEcdo_pres.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/EAGE4D2004.pdf},
  url2         = {https://circle.ubc.ca/bitstream/handle/2429/453/EAGE4D2004.pdf?sequence=1}
}


@CONFERENCE{cristall2004CSEGcpa,
  author       = {Jamin Cristall and Moritz Beyreuther and Felix J. Herrmann},
  title        = {Curvelet processing and imaging: {4-D} adaptive subtraction},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging holds great promise to better monitor and
                  optimise reservoir production. The basic idea behind
                  4D seismic is that when multiple 3D surveys are
                  acquired at separate calendar times over a producing
                  field, the reservoir geology will not change from
                  survey to survey but the state of the reservoir
                  fluids will change. Thus, taking the difference
                  between two 3D surveys should remove the static
                  geologic contribution to the data and isolate the
                  time-varying fluid flow component. However, a major
                  challenge in 4D seismic is that acquisition and
                  processing differences between 3D surveys often
                  overshadow the changes caused by fluid flow. This
                  problem is compounded when 4D effects are sought to
                  be derived from legacy 3D data sets that were not
                  originally acquired with 4D in mind. The goal of
                  this study is to remove the acquisition and imaging
                  artefacts from a 4D seismic difference cube using
                  Curvelet processing techniques.},
  keywords     = {SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/059S0201-Cristall_J_Curvelet_4D.pdf}
}


@CONFERENCE{hennenfent2004SEGtta,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by curvelet and wavelet transforms},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  pages        = {211-214},
  organization = {SEG},
  abstract     = {We present a new method to stabilize the three-term
                  AVO inversion using Curvelet and Wavelet
                  transforms. Curvelets are basis functions that
                  effectively represent otherwise smooth objects
                  having discontinuities along smooth curves. The
                  applied formalism explores them to make the most of
                  the continuity along reflectors in seismic
                  images. Combined with Wavelets, Curvelets are used
                  to denoise the data by penalizing high frequencies
                  and small contributions in the AVO-cube. This
                  approach is based on the idea that rapid amplitude
                  changes along the ray-parameter axis are most likely
                  due to noise. The AVO-inverse problem is linearized,
                  formulated and solved for all (x, z) at once. Using
                  densities and velocities of the Marmousi model to
                  define the fluctuations in the elastic properties,
                  the performance of the proposed method is studied
                  and compared with the smoothing along the
                  ray-parameter direction only. We show that our
                  method better approximates the true data after the
                  denoising step, especially when noise level
                  increases. {\copyright}2004 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.1851201},
  keywords     = {Presentation, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta.pdf }, 
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta_pres.pdf }
}


@CONFERENCE{herrmann2004SEGcbn,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-based non-linear adaptive subtraction with sparseness constraints},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  pages        = {1977-1980},
  organization = {SEG},
  abstract     = {In this paper an overview is given on the
                  application of directional basis functions, known
                  under the name Curvelets/Contourlets, to various
                  aspects of seismic processing and imaging, which
                  involve adaptive subtraction. Key concepts in the
                  approach are the use of directional basis functions
                  that localize in both domains (e.g. space and
                  angle); non-linear estimation, which corresponds to
                  localized muting on the coefficients, possibly
                  supplemented by constrained optimization.  We will
                  discuss applications that include multiple,
                  ground-roll removal and migration
                  denoising. {\copyright}2004 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.1851181},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn_pres.pdf  },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn.pdf },
  url2         = {http://slim.eos.ubc.ca/~felix/public/SEGAD2004.pdf}
}


@CONFERENCE{herrmann2004EAGEcdp,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-domain preconditioned 'wave-equation' depth-migration with sparseness and illumination constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal- to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann04mpw.pdf}
}


@CONFERENCE{herrmann2004EAGEcdl,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-domain least-squares migration with sparseness constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {May},
  organization = {EAGE},
  abstract     = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl_pres.pdf }
}


@CONFERENCE{herrmann2004SEGcdm,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Curvelet-domain multiple elimination with sparseness constraints},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  pages        = {1333-1336},
  organization = {SEG},
  abstract     = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, in which
                  multiples are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.
                  {\copyright}2004 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.1851110},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm.pdf  },
  url2         = {http://slim.eos.ubc.ca/~felix/public/SEGM2004.pdf}
}


@CONFERENCE{herrmann2004CSEGcia,
  author       = {Felix J. Herrmann},
  title        = {Curvelet imaging and processing: an overview},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {05/2004},
  organization = {CSEG},
  abstract     = {In this paper an overview is given on the
                  application of directional basis functions, known
                  under the name Curvelets/Contourlets, to various
                  aspects of seismic processing and imaging. Key
                  conceps in the approach are the use of (i)
                  directional basis functions that localize in both
                  domains (e.g. space and angle); (ii) non-linear
                  estimation, which corresponds to localized muting on
                  the coefficients, possibly supplemented by
                  constrained optimization (iii) invariance of the
                  basis functions under the imaging operators. We will
                  discuss applications that include multiple and
                  ground roll removal; sparseness-constrained
                  least-squares migration and the computation of 4-D
                  difference cubes.},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia/Herrmann04CSEGcia.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_overview.doc}
}


@CONFERENCE{herrmann2004CSEGcia1,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Curvelet imaging and processing: adaptive multiple elimination},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {05/2004},
  organization = {CSEG},
  abstract     = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, in which
                  multiples are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia1/Herrmann04CSEGcia1.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_multiple.doc}
}


@CONFERENCE{herrmann2004CSEGcia2,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet imaging and processing: sparseness-constrained least-squares migration},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is, like-wise to the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia2/Herrmann04CSEGcia2.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_imaging.doc}
}


@CONFERENCE{herrmann2004EAGEsop,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Separation of primaries and multiples by non-linear estimation in the curvelet domain},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, in which
                  multiples are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/EAGEM2004.pdf}
}


@CONFERENCE{moghaddam2004SEGmpw,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann},
  title        = {Migration preconditioning with curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  pages        = {2204-2207},
  organization = {SEG},
  abstract     = {In this paper, the property of Curvelet transforms
                  for preconditioning the migration and normal
                  operators is investigated. These operators belong to
                  the class of Fourier integral operators and
                  pseudo-differential operator, respectively. The
                  effect of this pre-conditioner is shown in term of
                  improvement of sparsity, convergence rate, number of
                  iteration for the Krylov-subspace solver and
                  clustering of singular(eigen) values. The migration
                  operator, which we employed in this work is the
                  common-offset Kirchoff-Born
                  migration. {\copyright}2004 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.1845213},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Moghaddam04SEGmpw/Moghaddam04SEGmpw_pres.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/SEGLAN2004.pdf}
}


@CONFERENCE{yarham2004CSEGcpa,
  author       = {Carson Yarham and Daniel Trad and Felix J. Herrmann},
  title        = {Curvelet processing and imaging: adaptive ground roll removal},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {05/2005},
  organization = {CSEG},
  abstract     = {In this paper we present examples of ground roll
                  attenuation for synthetic and real data gathers by
                  using Contourlet and Curvelet transforms.  These
                  non-separable wavelet transforms are locoalized both
                  (x,t)- and (k,f)-domains and allow for adaptive
                  seperation of signal and ground roll. Both linear
                  and non-linear filtering are discussed using the
                  unique properties of these basis that allow for
                  simultaneous localization in the both
                  domains. Eventhough, the linear filtering techniques
                  are encouraging the true added value of these
                  basis-function techniques becomes apparent when we
                  use these decompositions to adaptively substract
                  modeled ground roll from data using a non-linear
                  thesholding procedure. We show real and synthetic
                  examples and the results suggest that these
                  directional-selective basis functions provide a
                  usefull tool for the removal of coherent noise such
                  as ground roll.},
  keywords     = {Presentation, SLIM, CSEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/Yarham04CSEGcpa_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/yarham04csegcpa.pdf}
}


@CONFERENCE{yarham2004CSEGgrr,
  author       = {Carson Yarham and Felix J. Herrmann and Daniel Trad},
  title        = {Ground Roll Removal Using Non-Separable Wavelet Transforms },
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  keywords     = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGgrr/Yarham04CSEGgrr.pdf }
}



%-------------------------------------------------2003-------------------------------------------------

@CONFERENCE{herrmann2003SEGoiw,
  author       = {Felix J. Herrmann},
  title        = {"Optimal" imaging with curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2003},
  volume       = {22},
  pages        = {997-1000},
  abstract     = {In this paper we present a non-linear
                  edge-preserving solution to linear inverse
                  scattering problems based on optimal basis- function
                  decompositions. Optimality of the basis functions
                  allow us to (i) reduce the dimensionality of the
                  inverse problem; (ii) devise non-linear thresholding
                  operators that approximate minimax (minimize the
                  maximal mean square error given the worst possible
                  prior) and that significantly improve the
                  signal-to-noise ratio on the image. We present a
                  reformulation of the standard generalized
                  least-squares formulation of the seismic inversion
                  problem into a formulation based on thresh- olding,
                  where the singular values, vectors and linear
                  estimators are replaced by quasi-singular values,
                  basis-functions and thresholding. To limit the
                  computational burden we use a Monte-Carlo sampling
                  method to compute the quasi-singular values. With
                  the proposed method, we aim to significantly improve
                  the signal-to-noise ratio (SNR) on the model space
                  and hence the resolution of the seismic image. While
                  classical Tikhonov-regularized methods only gain the
                  square-root of the SNR on the data for the SNR on
                  the model our method scales almost linearly. This
                  significant improvement of the SNR allows us to
                  discern events at high frequencies which would
                  normally be in the noise.},
  keywords     = {Presentation, SEG, SLIM},
  optdoi       = {10.1190/1.1818117},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2003/Herrmann03SEGoiw/Herrmann03SEGoiw_pres.pdf}

}


@CONFERENCE{herrmann2003SPIEmsa,
  author       = {Felix J. Herrmann},
  title        = {Multifractional splines: application to seismic imaging},
  booktitle    = {Proceedings of SPIE Technical Conference on Wavelets: Applications in Signal and Image Processing X},
  year         = {2003},
  editor       = {Michael A. Unser and Akram Aldroubi and Andrew F. Laine},
  volume       = {5207},
  pages        = {240-258},
  organization = {SPIE},
  abstract     = {Seismic imaging commits itself to locating
                  singularities in the elastic properties of the
                  Earth{\textquoteright}s subsurface. Using the
                  high-frequency ray-Born approximation for scattering
                  from non-intersecting smooth interfaces, seismic
                  data can be represented by a generalized Radon
                  transform mapping the singularities in the medium to
                  seismic data.  Even though seismic data are
                  bandwidth limited, signatures of the singularities
                  in the medium carry through this transform and its
                  inverse and this mapping property presents us with
                  the possibility to develop new imaging techniques
                  that preserve and characterize the singularities
                  from incomplete, bandwidth-limited and noisy data.
                  In this paper we propose a non-adaptive
                  Curvelet/Contourlet technique to image and preserve
                  the singularities and a data-adaptive Matching
                  Pursuit method to characterize these imaged
                  singularities by Multi-fractional Splines. This
                  first technique borrows from the ideas within the
                  Wavelet-Vaguelette/Quasi-SVD approach. We use the
                  almost diagonalization of the scattering operator to
                  approximately compensate for (i) the coloring of the
                  noise and hence facilitate estimation; (ii) the
                  normal operator itself. Results of applying these
                  techniques to seismic imaging are encouraging
                  although many open fundamental questions remain.},
  keywords     = {Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/herrmann03msa.pdf},
  url          = {http://www.eos.ubc.ca/~felix/Preprint/SPIE03DEF.pdf}

}



%-------------------------------------------------2001-------------------------------------------------

@CONFERENCE{herrmann2001EAGEsas,
  author       = {Felix J. Herrmann},
  title        = {Scaling and seismic reflectivity: implications of scaling on {AVO}},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2001},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {AVO analysis of seismic data is based on the
                  assumption that transitions in the earth consist of
                  jump discontinuities only. Generalization of these
                  transitions to more realistic transitions shows a
                  drastic change in observed AVO behavior, especially
                  for the large angles currently attained by
                  increasing cable lengths. We propose a simple
                  ities. After renormalization, the inverted
                  fluctuations regain their relative magnitudes which,
                  due to the scaling, may have been significantly
                  distorted.},
  keywords     = {SLIM},
}

@MASTERSTHESIS{lin08THccl,
  author = {Tim T.Y. Lin},
  title = {Compressed computation of large-scale wavefield extrapolation in
	inhomogeneous medium},
  year = {2008},
  type = {masters},
  school = {University of British Columbia},
  optmonth = {04/2008},
  abstract = {In this work an explicit algorithm for the extrapolation of one-way
	wavefields is proposed which combines recent developments in information
	theory and theoretical signal processing with the physics of wave
	propagation. Because of excessive memory requirements, explicit formulations
	for wave propagation have proven to be a challenge in 3-D. By using
	ideas from {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright},
	we are able to formulate the (inverse) wavefield extrapolation problem
	on small subsets of the data volume, thereby reducing the size of
	the operators. Compressed sensing entails a new paradigm for signal
	recovery that provides conditions under which signals can be recovered
	from incomplete samplings by \emph{nonlinear} recovery methods that
	promote sparsity of the to-be-recovered signal. According to this
	theory, signals can successfully be recovered when the measurement
	basis is \emph{incoherent} with the representation in which the wavefield
	is sparse. In this new approach, the eigenfunctions of the Helmholtz
	operator are recognized as a basis that is incoherent with sparsity
	transforms that are known to compress seismic wavefields. By casting
	the wavefield extrapolation problem in this framework, wavefields
	can successfully be extrapolated in the modal domain, despite evanescent
	wave modes. The degree to which the wavefield can be recovered depends
	on the number of missing (evanescent) wave modes and on the complexity
	of the wavefield. A proof of principle for the {\textquoteleft}{\textquoteleft}compressed
	sensing{\textquoteright}{\textquoteright} method is given for inverse
	wavefield extrapolation in 2-D. The results show that our method
	is stable, has reduced dip limitations and handles evanescent waves
	in inverse extrapolation.},
  keywords = {BSc, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/timlin08bsch.pdf}
}
% This file was created with JabRef 2.6.
% Encoding: MacRoman
@MASTERSTHESIS{almatar10THesd,
  author = {Mufeed H. AlMatar},
  title = {Estimation of Surface-free Data by Curvelet-domain Matched Filtering
	and Sparse Inversion},
  school = {University of British Columbia},
  year = {2010},
  type = {masters},
  abstract = {A recent robust multiple-elimination technique, based on the underlying
	principle that relates primary impulse response to total upgoing
	wavefield, tries to change the paradigm that sees surface-related
	multiples as noise that needs to be removed from the data prior to
	imaging. This technique, estimation of primaries by sparse inversion
	(EPSI), (van Groenestijn and Verschuur, 2009; Lin and Herrmann, 2009),
	proposes an inversion procedure during which the source function
	and surface- free impulse response are directly calculated from the
	upgoing wavefield using an alternating optimization procedure. EPSI
	hinges on a delicate interplay between surface-related multiples
	and pri- maries. Finite aperture and other imperfections may violate
	this relationship. In this thesis, we investigate how to make EPSI
	more robust by incorporating curvelet- domain matching in its formulation.
	Compared to surface-related multiple removal (SRME), where curvelet-domain
	matching was used successfully, incorporating this step has the additional
	advantage that matches multiples to multiples rather than predicated
	multiples to total data as in SRME.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/public/theses/2010/AlMatarThesis.pdf}
}

@MASTERSTHESIS{alhashim09THsdp,
  author = {Fadhel Abbas Alhashim},
  title = {Seismic Data Processing with the Parallel Windowed Curvelet Transform},
  school = {University of British Columbia},
  year = {2009},
  type = {masters},
  abstract = {The process of obtaining high quality seismic images is very challenging
	when exploring new areas that have high complexities. The to be processed
	seismic data comes from the field noisy and commonly incomplete.
	Recently, major advances were accomplished in the area of coherent
	noise removal, for example, Surface Related Multiple Elimination
	(SRME). Predictive multiple elimination methods, such as SRME, consist
	of two steps: The first step is the prediction step, in this step
	multiples are predicted from the seismic data. The second step is
	the separation step in which primary reflection and surface related
	multiples are separated, this involves predicted multiples from the
	first step to be matched
	with the true multiples in the data and eventually removed . A recent
	robust Bayesian wavefield separation method have been recently introduced
	to improve on the separation by matching methods. This method utilizes
	the effectiveness of using the multi scale and multi angular curvelet
	transform in processing seismic images. The method produced excellent
	results and improved multiple removal. A considerable problem in
	the seismic processing field is the fact that seismic data are large
	and require a correspondingly large memory size and processing time.
	The fact that curvelets are redundant also increases the need for
	large memory to process seismic data. In this thesis we propose a
	parallel approach based windowing operator that divides large seismic
	data into smaller more managable datasets that can fit in memory
	so that it is possible to apply the Bayesian separation pro- cess
	in parallel with minimal harm to the image quality and data integrity.
	However, by dividing the data, we introduce discontinuities. We take
	these discontinuities into account and compare two ways that different
	windows may communicate. The first method is to communicate edge
	information at only two steps, namely, data scattering and gathering
	processes while applying the multiple separation on each window separately.
	The second method is to define our windowing operator as a global
	operator, which exchanges window edge information at each forward
	and inverse curvelet transform. We discuss the trade off between
	the two methods trying to minimize complexity and I/O time spent
	in the process. We test our windowing operator on a seismic denoising
	problem and then apply the windowing operator on our sparse-domain
	Bayesian primary- multiple separation.},
  keywords = {MSc},
  URL = {http://slim.eos.ubc.ca/Publications/public/theses/2009/alhashim09sdp.pdf},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/2009/alhashim09sdp1.pdf}
}

@MASTERSTHESIS{kumar09THins,
  author = {Vishal Kumar},
  title = {Incoherent noise suppression and deconvolution using curvelet-domain
	sparsity},
  school = {University of British Columbia},
  year = {2009},
  type = {masters},
  abstract = {Curvelets are a recently introduced transform domain that belongs
	to a family of multiscale and also multidirectional data expansions.
	As such, curvelets can be applied to resolution of the issues of
	complicated seismic wavefronts. We make use of this multiscale, multidirectional
	and hence sparsifying ability of the curvelet transform to suppress
	incoherent noise from crustal data where the signal-to-noise ratio
	is low and to develop an improved deconvolution procedure. Incoherent
	noise present in seismic reflection data corrupts the quality of
	the signal and can often lead to misinterpretation. The curvelet
	domain lends itself particularly well for denoising because coherent
	seismic energy maps to a relatively small number of significant curvelet
	coefficents.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2009/kumar09ins.pdf}
}

@MASTERSTHESIS{lebed08THssr,
  author = {Evgeniy Lebed},
  title = {Sparse Signal Recovery in a Transform Domain},
  school = {The University of British Columbia},
  year = {2008},
  type = {masters},
  optmonth = {08/2008},
  abstract = {The ability to efficiently and sparsely represent seismic data is
	becoming an increasingly important problem in geophysics. Over the
	last thirty years many transforms such as wavelets, curvelets, contourlets,
	surfacelets, shear- lets, and many other types of x-lets
	have been developed. Such transform were leveraged to resolve this
	issue of sparse representations. In this work we compare the properties
	of four of these commonly used transforms, namely the shift-invariant
	wavelets, complex wavelets, curvelets and surfacelets. We also explore
	the performance of these transforms for the problem of recov- ering
	seismic wavefields from incomplete measurements.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/lebed08msc.pdf}
}

@MASTERSTHESIS{maysami08THlcs,
  author = {Mohammad Maysami},
  title = {Lithology constraints from seismic waveforms: application to opal-A
	to opal-CT transition},
  school = {The University of British Columbia},
  year = {2008},
  type = {masters},
  address = {Vancouver, BC Canada},
  abstract = {In this work, we present a new method for seismic waveform characterization,
	which is aimed at extracting detailed litho-stratigraphical information
	from seismic data. We attempt to estimate the lithological attributes
	from seismic data according to our parametric representation of stratigraphical
	horizons, where the parameter values provide us with a direct link
	to nature of lithological transitions. We test our method on a seismic
	dataset with a strong diagenetic transition (opal-A to opal-CT transition).
	Given some information from cutting samples of well, we use a percolation-based
	model to construct the elastic profile of lithological transitions.
	Our goal is to match parametric representation for the diagenetic
	transition in both real data and synthetic data given by these elastic
	profiles. This match may be interpreted as a well-seismic tie, which
	reveals lithological information about stratigraphical horizons.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/maysami08msc.pdf}
}

@MASTERSTHESIS{yarham08THsgs,
  author = {Carson Yarham},
  title = {Seismic ground-roll separation using sparsity promoting l1 minimization},
  school = {The University of British Columbia},
  year = {2008},
  address = {Vancouver, BC Canada},
  optmonth = {05/2008},
  abstract = {The removal of coherent noise generated by surface waves in land based
	seismic is a prerequisite to imaging the subsurface. These surface
	waves, termed as ground roll, overlay important reflector information
	in both the t-x and f-k domains. Standard techniques of ground roll
	removal commonly alter reflector information as a consequence of
	the ground roll removal. We propose the combined use of the curvelet
	domain as a sparsifying basis in which to perform signal separation
	techniques that can preserve reflector informa- tion while increasing
	ground roll removal. We examine two signal separation techniques,
	a block-coordinate relaxation method and a Bayesian separation method.
	The derivations and background for both methods are presented and
	the parameter sensitivity is examined. Both methods are shown to
	be effective in certain situations regarding synthetic data and erroneous
	surface wave predictions. The block-coordinate relaxation method
	is shown to have ma jor weaknesses when dealing with seismic signal
	separation in the pres- ence of noise and with the production of
	artifacts and reflector degradation. The Bayesian separation method
	is shown to improve overall separation for both seismic and real
	data. The Bayesian separation scheme is used on a real data set with
	a surface wave prediction containing reflector information. It is
	shown to improve the signal separation by recovering reflector information
	while improving the surface wave removal. The abstract contains a
	separate real data example where both the block-coordinate relaxation
	method and the Bayesian separation method are compared. },
  url= {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/cyarham08msc.pdf},
  keywords = {MSc}
}

@MASTERSTHESIS{dupuis05THssc,
  author = {Catherine Dupuis},
  title = {Seismic singularity characterization with redundant dictionaries},
  school = {The University of British Columbia},
  year = {2005},
  type = {masters},
  address = {Vancouver, BC Canada},
  abstract = {We consider seismic signals as a superposition of waveforms parameterized
	by their fractional- orders. Each waveform models the reflection
	of a seismic wave at a particular transition between two lithological
	layers in the subsurface. The location of the waveforms in the seismic
	signal corresponds to the depth of the transitions in the subsurface,
	whereas their fractional-order constitutes a measure of the sharpness
	of the transitions. By considering fractional-order tran- sitions,
	we generalize the zero-order transition model of the conventional
	deconvolution problem, and aim at capturing the different types of
	transitions. The goal is to delineate and characterize transitions
	from seismic signals by recovering the locations and fractional-orders
	of its corre- sponding waveforms. This problem has received increasing
	interest, and several methods have been proposed, including multi-
	and monoscale analysis based on Mallat{\textquoteright}s wavelet
	transform modulus maxima, and seismic atomic decomposition. We propose
	a new method based on a two-step approach, which divides the initial
	problem of delineating and characterizing transitions over the whole
	seismic signal, into two easier sub- problems. The algorithm first
	partitions the seismic signal into its ma jor components, and then
	estimates the fractional-orders and locations of each component.
	Both steps are based on the sparse decomposition of seismic signals
	in overcomplete dictionaries of waveforms parameter- ized by their
	fractional-orders, and involve  1 minimizations solved by an iterative
	thresholding algorithm. We present the method and show numerical
	results on both synthetic and real data.},
  keywords = {MSc},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2005/dupuis05msc.pdf}
}
% This file was created with JabRef 2.6.
% Encoding: MacRoman

@techreport{vanleeuwen2012SEGparallel,
author = {Tristan {van Leeuwen} and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
number = {TR-2012-03},
institution = {Department of Earth and Ocean Sciences},
address = {University of British Columbia, Vancouver},
title = {A parallel, object-oriented framework for frequency-domain wavefield 
imaging and inversion.},
abstract = {We present a parallel object-oriented matrix-free framework for frequency-domain 
seismic modeling, imaging and inversion. 
The key aspects of the framework are its modularity and level of abstraction, 
which allows us to write code that reflects the underlying mathematical 
structure and develop unit-tests that guarantee the fidelity of the code. 
By overloading standard linear-algebra operations, such as matrix-vector 
multiplications, we can use standard optimization packages to work with our code 
without any modification. This leads to a scalable testbed on 
which new methods can be rapidly prototyped and tested on medium-sized 2D problems. 
Although our current implementation uses (parallel) Matlab, all of 
these design principles can also be met by using lower-level languages 
which is important when we want to scale to realistic 3D problems. 
We present some numerical examples on synthetic data.},
keywords = {modeling, imaging, inversion, SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2012/vanleeuwen2012SEGparallel/vanleeuwen2012SEGparallel.pdf}
}

@techreport{rajiv2012SEGFRM,
author = {Rajiv Kumar and Aleksandr Y. Aravkin and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
number = {TR-2012-04},
institution = {Department of Earth and Ocean Sciences},
address = {University of British Columbia, Vancouver},
title = {Fast Methods for Rank Minimization with Applications to Seismic-Data Interpolation},
abstract = {Rank penalizing techniques are an important direction 
in seismic inverse problems, since they allow improved recovery by 
exploiting low-rank structure. A major downside of current state of 
the art techniques is their reliance on the SVD of seismic data 
structures, which can be prohibitively expensive. Fortunately, recent 
work allows us to circumvent this problem by working with matrix 
factorizations. We review a novel approach to rank penalization, and 
successfully apply it to the seismic interpolation problem by exploiting 
the low-rank structure of seismic data in the midpoint-offset domain. 
Experiments for the recovery of 2D monochromatic data matrices and 
seismic lines represented as 3D volumes support the feasibility and 
potential of the new approach.},
keywords = {Rank, optimization, Seismic data Interpolation},
url = {https://www.slim.eos.ubc.ca/Publications//Public/TechReport/2012/rajiv2012SEGFRM/rajiv2012SEGFRM.pdf}
}


@techreport{herrmann10SEGerc,
  author = {Felix J. Herrmann},
  title = {Empirical recovery conditions for seismic sampling},
  year = {2010},
  organization = {SEG},
  institution = {Department of Earth and Ocean Sciences},
  address = {University of British Columbia, Vancouver},
  abstract = {In this paper, we offer an alternative sampling method leveraging
        recent insights from compressive sensing towards seismic acquisition
        and processing for data that are traditionally considered to be undersampled.
        The main outcome of this approach is a new technology where acquisition
        and processing related costs are no longer determined by overly stringent
        sampling criteria, such as Nyquist. At the heart of our approach
        lies randomized incoherent sampling that breaks subsampling related
        interferences by turning them into harmless noise, which we subsequently
        remove by promoting transform-domain sparsity. Now, costs no longer
        grow with resolution and dimensionality of the survey area, but instead
        depend on transform-domain sparsity only. Our contribution is twofold.
        First, we demon- strate by means
        of carefully designed numerical experiments that compressive sensing
        can successfully be adapted to seismic acquisition. Second, we show
        that accurate recovery can be accomplished for compressively sampled
        data volumes sizes that exceed the size of conventional transform-domain
        data volumes by only a small factor. Because compressive sens- ing
        combines transformation and encoding by a single linear encoding
        step, this technology is directly applicable to acqui- sition and
        to dimensionality reduction during processing. In either case, sampling,
        storage, and processing costs scale with transform-domain sparsity.},
  keywords = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Publications/Public/Conferences/SEG/2010/herrmann10SEGerc/herrmann10SEGerc.pdf }

}


@techreport{almatar10SEGesfd,
  author = {Mufeed H. AlMatar and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of surface-free data by curvelet-domain matched filtering and sparse inversion},
  year = {2010},
  organization = {SEG},
  institution = {Department of Earth and Ocean Sciences},
  address = {University of British Columbia, Vancouver},
  abstract = {Matching seismic wavefields and images lies at the heart of many 
        pre-post-processing steps part of seismic imaging- whether one is matching predicted
        wavefield components, such as multiples, to the actual to-be-separated
        wavefield compo- nents present in the data or whether one is aiming
        to restore migration amplitudes by scaling, using an image-to-remigrated-
        image matching procedure to calculate the scaling coefficients. The
        success of these wavefield matching procedures depends on our ability
        to (i) control possible overfitting, which may lead to accidental
        removal of energy or to inaccurate image-amplitude corrections, (ii)
        handle data or images with nonunique dips, and (iii) apply subsequent
        wavefield separations or migraton amplitude corrections stably. In
        this paper, we show that the curvelet transform allows us to address
        all these issues by im- posing smoothness in phase space, by using
        their capability to handle conflicting dips, and by leveraging their
        ability to represent seismic data and images sparsely. This latter
        property renders curvelet-domain sparsity promotion an effective
        prior.},
 keywords = {SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/almatar10SEGesfd/almatar10SEGesfd.pdf }
}


@TECHREPORT{vandenberg10TRsol,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Sparse optimization with least-squares constraints},
  institution = {Department of Computer Science},
  year = {2010},
  type = {Tech. Rep.},
  number = {TR-2010-02},
  address = {University of British Columbia, Vancouver},
  optmonth = {01/2010},
  abstract = {The use of convex optimization for the recovery of sparse signals
	from incomplete or compressed data is now common practice. Motivated
	by the success of basis pursuit in recovering sparse vectors, new
	formulations have been proposed that take advantage of different
	types of sparsity. In this paper we propose an efficient algorithm
	for solving a general class of sparsifying formulations. For several
	common types of sparsity we provide applications, along with details
	on how to apply the algorithm, and experimental results.},
  url = {http://www.cs.ubc.ca/~mpf/papers/vdBergFriedlander11.pdf}
}

@TECHREPORT{vandenberg09TRter,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Theoretical and empirical results for recovery from multiple measurements},
  institution = {Department of Computer Science},
  year = {2009},
  type = {Tech. Rep.},
  number = {TR-2009-7},
  address = {University of British Columbia, Vancouver},
  optmonth = {09/2009},
  abstract = {The joint-sparse recovery problem aims to recover, from sets of compressed
	measurements, unknown sparse matrices with nonzero entries restricted
	to a subset of rows. This is an extension of the single-measurement-vector
	(SMV) problem widely studied in compressed sensing. We analyze the
	recovery properties for two types of recovery algorithms. First,
	we show that recovery using sum-of-norm minimization cannot exceed
	the uniform recovery rate of sequential SMV using L1 minimization,
	and that there are problems that can be solved with one approach
	but not with the other. Second, we analyze the performance of the
	ReMBo algorithm [M. Mishali and Y. Eldar, IEEE Trans. Sig. Proc.,
	56 (2008)] in combination with L1 minimization, and show how recovery
	improves as more measurements are taken. From this analysis it follows
	that having more measurements than number of nonzero rows does not
	improve the potential theoretical recovery rate.},
  url = {http://www.cs.ubc.ca/~mpf/papers/vdBergFriedlander09.pdf}
}

@TECHREPORT{vandenberg07TRipr,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {In pursuit of a root},
  institution = {Department of Computer Science},
  year = {2007},
  type = {Tech. Rep.},
  number = {TR-2007-19},
  address = {University of British Columbia, Vancouver},
  month = {06/2007},
  abstract = {The basis pursuit technique is used to find a minimum one-norm solution
	of an un- derdetermined least-squares problem. Basis pursuit denoise
	fits the least-squares problem only approximately, and a single parameter
	determines a curve that traces the trade-off between the least-squares
	fit and the one-norm of the solution. We show that the function that
	describes this curve is convex and continuously differ- entiable
	over all points of interest. The dual solution of a least-squares
	problem with an explicit one-norm constraint gives function and derivative
	information needed for a root-finding method. As a result, we can
	compute arbitrary points on this curve. Numerical experiments demonstrate
	that our method, which relies on only matrix-vector operations, scales
	well to large problems.},
  url = {http://www.optimization-online.org/DB_HTML/2007/06/1708.html}
}

@TECHREPORT{vandenberg07TRsat,
  author = {Ewout {van den Berg} and Michael P. Friedlander and Gilles Hennenfent
	and Felix J. Herrmann and Rayan Saab and Ozgur Yilmaz},
  title = {Sparco: a testing framework for sparse reconstruction},
  institution = {UBC Computer Science Department},
  year = {2007},
  number = {TR-2007-20},
  abstract = {Sparco is a framework for testing and benchmarking algorithms for
	sparse reconstruction. It includes a large collection of sparse reconstruction
	problems drawn from the imaging, compressed sensing, and geophysics
	literature. Sparco is also a framework for implementing new test
	problems and can be used as a tool for reproducible research. Sparco
	is implemented entirely in Matlab, and is released as open-source
	software under the GNU Public License.},
  keywords = {SLIM, Sparco},
  presentation = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf},
  url = {http://www.cs.ubc.ca/labs/scl/sparco/downloads.php?filename=sparco.pdf}
}


@TECHREPORT{hennenfent10TRnct,
  author = {Gilles Hennenfent and Lloyd Fenelon and Felix J. Herrmann},
  title = {Nonequispaced curvelet transform for seismic data reconstruction:
	a sparsity-promoting approach},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2010},
  number = {TR-2010-2},
  abstract = {Seismic data are typically irregularly sampled along spatial axes.
	This irregular sampling may adversely affect some key steps, e.g.,
	multiple prediction/attenuation or imaging, in the processing workflow.
	To overcome this problem almost every large dataset is regularized
	and/or interpolated. Our contribution is twofold. Firstly, we extend
	our earlier work on the nonequispaced fast discrete curvelet transform
	(NFDCT) and introduce a second generation of the transform. This
	new generation differs from the previous one by the approach taken
	to compute accurate curvelet coefficients from irregularly sampled
	data. The first generation relies on accurate Fourier coefficients
	obtained by an $\ell_2$-regularized inversion of the nonequispaced
	fast Fourier transform, while the second is based on a direct, $\ell_1$
	-regularized inversion of the operator that links curvelet coefficients
	to irregular data. Also, by construction, the NFDCT second generation
	is lossless, unlike the NFDCT first generation. This property is
	particularly attractive for processing irregularly sampled seismic
	data in the curvelet domain and bringing them back to their irregular
	recording locations with high fidelity. Secondly, we combine the
	NFDCT second generation with the standard fast discrete curvelet
	transform (FDCT) to form a new curvelet-based method, coined nonequispaced
	curvelet reconstruction with sparsity-promoting inversion (NCRSI),
	for the regularization and interpolation of irregularly sampled data.
	We demonstrate that, for a pure regularization problem, the reconstruction
	is very accurate. The signal-to-reconstruction error ratio is, in
	our example, above 40 dB. We also conduct combined interpolation
	and regularization experiments. The reconstructions for synthetic
	data are accurate, particularly when the recording locations are
	optimally jittered. The reconstruction in our real data example shows
	amplitudes along the main wavefronts smoothly varying with no obvious
	acquisition imprint; a result very competitive with results from
	other reconstruction methods overall.},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/hennenfent2010nct.pdf}
}

@TECHREPORT{hennenfent08TRori,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the Pareto curve},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-5},
  abstract = {Geophysical inverse problems typically involve a trade off between
	data misfit and some prior. Pareto curves trace the optimal trade
	off between these two competing aims. These curves are commonly used
	in problems with two-norm priors where they are plotted on a log-log
	scale and are known as L-curves. For other priors, such as the sparsity-promoting
	one norm, Pareto curves remain relatively unexplored. First, we show
	how these curves provide an objective criterion to gauge how robust
	one-norm solvers are when they are limited by a maximum number of
	matrix-vector products that they can perform. Second, we use Pareto
	curves and their properties to define and compute one-norm compressibilities.
	We argue this notion is key to understand one-norm regularized inversion.
	Third, we illustrate the correlation between the one-norm compressibility
	and the performance of Fourier and curvelet reconstructions with
	sparsity promoting inversion.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/hennenfent08seg.pdf}
}

@TECHREPORT{herrmann10TRrsa,
  author = {Felix J. Herrmann},
  title = {Randomized sampling and sparsity: getting more information from fewer
	samples},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2010},
  number = {TR-2010-1},
  abstract = {Many seismic exploration techniques rely on the collection of massive
	data volumes that are subsequently mined for information during processing.
	While this approach has been extremely successful in the past, current
	efforts toward higher- resolution images in increasingly complicated
	regions of the Earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly amongst these is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which disproportionately
	strains current acquisition and processing systems as the size and
	desired resolution of our survey areas continues to increase. In
	this paper, we offer an alternative sampling method leveraging recent
	insights from compressive sensing towards seismic acquisition and
	processing for data that are traditionally considered to be undersampled.
	The main outcome of this approach is a new technology where acquisition
	and processing related costs are no longer determined by overly stringent
	sampling criteria, such as Nyquist. At the heart of our approach
	lies randomized incoherent sampling that breaks subsampling related
	interferences by turning them into harmless noise, which we subsequently
	remove by promoting transform-domain sparsity. Now, costs no longer
	grow significantly with resolution and dimensionality of the survey
	area, but instead depend on transform-domain sparsity only. Our contribution
	is twofold. First, we demonstrate by means of carefully designed
	numerical experiments that compressive sensing can successfully be
	adapted to seismic exploration. Second, we show that accurate recovery
	can be accomplished for compressively sampled data volumes sizes
	that exceed the size of conventional transform-domain data volumes
	by only a small factor. Because compressive sensing combines transformation
	and encoding by a single linear encoding step, this technology is
	directly applicable to acquisition and to dimensionality reduction
	during processing. In either case, sampling, storage, and processing
	costs scale with transform-domain sparsity. We illustrate this principle
	by means of number of case studies.},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann2010rsa.pdf}
}


@TECHREPORT{lebed08TRhgg,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker's guide to the galaxy of transform-domain sparsification},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-4},
  abstract = {The ability to efficiently and sparsely represent seismic data is
	becoming an increasingly important problem in geophysics. Over the
	last decade many transforms such as wavelets, curervelets, contourlets,
	surfacelets, shearlets, and many other types of 'x-lets' have been
	developed to try to resolve this issue. In this abstract we compare
	the properties of four of these commonly used transforms, namely
	the shift-invariant wavelets, complex wavelets, curvelets and surfacelets.
	We also briefly explore the performance of these transforms for the
	problem of recovering seismic wavefields from incomplete measurements.},
  keywords = {SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/lebed08seg.pdf}
}

@TECHREPORT{tang09TRdtr,
  author = {Gang Tang and Reza Shahidi and Jianwei Ma},
  title = {Design of two-dimensional randomized sampling schemes for curvelet-based
	sparsity-promoting seismic data recovery},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2009},
  number = {TR-2009-03},
  abstract = {The tasks of sampling, compression and reconstruction are very common
	and often necessary in seismic data processing due to the large size
	of seismic data. Curvelet-based Recovery by Sparsity-promoting Inversion,
	motivated by the newly developed theory of compressive sensing, is
	among the best recovery strategies for seismic data. The incomplete
	data input to this curvelet-based recovery is determined by randomized
	sampling of the original complete data. Unlike usual regular undersampling,
	randomized sampling can convert aliases to easy-to-eliminate noise,
	thus facilitating the process of reconstruction of the complete data
	from the incomplete data. Randomized sampling methods such as jittered
	sampling have been developed in the past that are suitable for curvelet-based
	recovery, however most have only been applied to sampling in one
	dimension. Considering that seismic datasets are usually higher dimensional
	and extremely large, in the present paper, we extend the 1D version
	of jittered sampling to two dimensions, both with underlying Cartesian
	and hexagonal grids. We also study separable and non-separable two
	dimensional jittered sampling, the former referring to the Kronecker
	product of two one-dimensional jittered samplings. These different
	categories of jittered sampling are compared against one another
	in terms of signal-to-noise ratio and visual quality, from which
	we find that jittered hexagonal sampling is better than jittered
	Cartesian sampling, while fully non-separable jittered sampling is
	better than separable sampling. Because in the image processing and
	computer graphics literature, sampling patterns with blue-noise spectra
	are found to be ideal to avoid aliasing, we also introduce two other
	randomized sampling methods, possessing sampling spectra with beneficial
	blue noise characteristics, Poisson Disk sampling and Farthest Point
	sampling. We compare these methods, and apply the introduced sampling
	methodologies to higher dimensional curvelet-based reconstruction.
	These sampling schemes are shown to lead to better results from CRSI
	compared to the other more traditional sampling protocols, e.g. regular
	subsampling.},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/tang09dtr.pdf}
}

@TECHREPORT{wang08TRbss,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Bayesian-signal separation by sparsity promotion: application to
	primary-multiple separation},
  institution = {UBC Earth and Ocean Sciences Department},
  year = {2008},
  number = {TR-2008-1},
  optmonth = {01/2008},
  abstract = {Successful removal of coherent noise sources greatly determines the
	quality of seismic imaging. Major advances were made in this direction,
	e.g., Surface-Related Multiple Elimination (SRME) and interferometric
	ground-roll removal. Still, moderate phase, timing, amplitude errors
	and clutter in the predicted signal components can be detrimental.
	Adopting a Bayesian approach along with the assumption of approximate
	curvelet-domain independence of the to-be-separated signal components,
	we construct an iterative algorithm that takes the predictions produced
	by for example SRME as input and separates these components in a
	robust fashion. In addition, the proposed algorithm controls the
	energy mismatch between the separated and predicted components. Such
	a control, which was lacking in earlier curvelet-domain formulations,
	produces improved results for primary-multiple separation on both
	synthetic and real data.},
  keywords = {signal separation, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Private/Journals/wang08bayes/paper_html/paper.html}
}

% This file was created with JabRef 2.6.
% Encoding: MacRoman

@unpublished{vanderneut12irs,
author = {Joost {van der Neut} and Felix J. Herrmann},
year = {2012},
optmonth = {10/2012},
title = {Interferometric redatuming by sparse inversion},
abstract = {Assuming that exact transmission responses are known
                  between the surface and a particular depth level in
                  the subsurface, seismic sources can be eﬀectively
                  mapped to that level by a process called
                  interferometric redatuming. After redatuming, the
                  obtained waveﬁelds can be used for imaging below
                  this particular depth level. Interferometric
                  redatuming consists of two steps, namely (i) the
                  decomposition of the observed waveﬁelds into up- and
                  down-going constituents and (ii) a multidimensional
                  deconvolution of the up- and downgoing
                  waveﬁelds. While this method works in theory,
                  sensitivity to noise and artifacts due to incomplete
                  acquisition call for a diﬀerent formulation. In this
                  letter, we demonstrate the beneﬁts of formulating
                  the two steps that undergird interferometric
                  redatuming in terms of a transform-domain
                  sparsity-promoting program. By exploiting
                  compressibility of seismic waveﬁelds in the curvelet
                  domain, we not only become robust with respect to
                  noise but we are also able to remove certain
                  artifacts while preserving the frequency
                  content. These improvements lead to a better image
                  of the target from the redatumed data.},
keywords = {Controlled source seismology, Interferometry, Inverse theory},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/vanderneut12irs/vanderneut12irs.pdf}
}

@unpublished{mansour12iwr,
author = {Hassan Mansour and Felix J. Herrmann and Ozgur Yilmaz},
year = {2012},
optmonth = {10/2012},
title = {Improved wavefield reconstruction from randomized sampling via weighted one-norm minimization},
abstract = {Missing-trace interpolation aims to reconstruct regularly
                  sampled wavefields from periodically sampled data
                  with gaps caused by physical constraints. While
                  transform-domain sparsity promotion has proven to be
                  an effective tool to solve this recovery problem,
                  recent developments in randomized acquisition in
                  marine, via randomized coil sampling or randomized
                  streamers with deliberate feathering, and on land,
                  via randomization of the source and/or receiver
                  positions, expose vulnerabilities in the current
                  recovery techniques that aside from the selection of
                  the proper transform domain make no use of a priori
                  information on the transform-domain coefficients. To
                  overcome these vulnerabilities in solving the
                  recovery problem for large-scale problems, we
                  propose recovery by weighted one-norm minimization,
                  which exploits correlations between locations of
                  significant coefficients of different partitions,
                  e.g., shot records, common-offset gathers, or
                  frequency slices, of the acquired data. We use these
                  correlations to define a sequence of 2D
                  curvelet-based recovery problems that exploit 3D
                  continuity exhibited by seismic wavefields without
                  relying on the highly redundant 3D curvelet
                  transform. To illustrate the performance of our
                  weighted algorithm, we compare recoveries from
                  different scenarios of partitioning for a seismic
                  line from the Gulf of Suez. These examples
                  demonstrate that our method is superior to standard
                  ℓ1 minimization in terms of reconstruction quality
                  and computational memory requirements.},
keywords = {Trace interpolation, weighted one-norm minimization, compressed sensing, randomized sampling},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/mansour12iwr/mansour12iwr.pdf}
}

@unpublished{vanleeuwen2012smii,
author = {Tristan {van Leeuwen}},
year = {2012},
optmonth = {07/2012},
title = {A parallel matrix-free framework for frequency-domain seismic modelling, imaging and inversion in Matlab},
abstract = {I present a parallel matrix-free framework for
                  frequency-domain seismic modeling, imaging and
                  inversion. The framework provides basic building
                  blocks for designing and testing optimization-based
                  formulations of both linear and non-linear seismic
                  in- verse problems. By overloading standard
                  linear-algebra operations, such as matrix- vector
                  multiplications, standard optimization packages can
                  be used to work with the code without any
                  modification. This leads to a scalable testbed on
                  which new methods can be rapidly prototyped and
                  tested on medium-sized 2D problems. I present some
                  numerical examples on both linear and non-linear
                  seismic inverse problems.},
keywords = {Seismic imaging,optimization,Matlab,object-oriented programming},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/vanleeuwen2012smii/vanleeuwen2012smii.pdf}
}


@unpublished{lin2012robustepsi,
author = {Tim T.Y. Lin and Felix J. Herrmann},
year = {2012},
optmonth = {03/2012},
title = {Robust estimation of primaries by sparse inversion via one-norm minimization},
abstract = {Even though contemporary methods for the removal of multiple 
events in seismic data due to a free-surface are built upon reciprocity 
relationships between wavefields, they are often still implemented as 
prediction-subtraction processes. The subtraction process does not always 
compensate for imperfect prediction of multiple events, and itself often 
leads to distortion of primary events. A recently proposed method called 
Estimation of Primaries by Sparse Inversion avoids the subtraction process 
altogether by directly prediction the primary impulse response as a collection 
of band-limited spikes under sparsity-regulated wavefield inversion approach. 
Although it can be shown that the correct primary impulse response is 
obtained through the sparsest possible solution, the Estimation of Primaries 
by Sparse Inversion algorithm was not designed to seek such a solution, instead 
depending on a predetermined degree of sparsity as an inversion parameter. 
This leads to imperfect multiple rejection when the sparsity is overestimated, 
and problems with recovering late primary events when it is underestimated. 
In this paper, we propose a new algorithm where we make obtaining the 
sparsest solution our explicit goal. Our approach remains a gradient-based 
approach like the original algorithm, but is in turn derived from a new 
optimization framework based on an extended basis pursuit denoising formulation. 
We show that the sparsity-minimizing objective of our formulation enables it 
to operate successfully on a wide variety of synthetic and field marine dataset 
without excessive tweaking of inversion parameters. We also demonstrate that 
Robust EPSI produces a more artifact-free impulse response compared to the 
original algorithm, which has interesting implications for broadband seismic 
applications. Finally we demonstrate through field data that recovering the 
primary impulse response under transform domains can significantly improve 
the recovery of weak primary late arrivals, without appreciable change to the 
underlying algorithm.},
keywords = {multiples, optimization, sparsity, waveform inversion, pareto, biconvex, algorithm, EPSI},
notes = {submitted to Geophysics, March 12, 2012},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/lin2012robustepsi/lin2012robustepsi.pdf}
}

@unpublished{bander2012dre,
author = {Bander Jumah and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
title = {Dimensionality-reduced estimation of primaries by sparse inversion},
abstract = {Data-driven methods---such as the estimation of primaries by sparse 
inversion---suffer from the 'curse of dimensionality' that leads to 
disproportional growth in computational and storage demands when 
moving to realistic 3D field data. To remove this fundamental 
impediment, we propose a dimensionality-reduction technique where 
the 'data matrix' is approximated adaptively by a randomized 
low-rank factorization. Compared to conventional methods, which need 
for each iteration passage through all data possibly requiring 
on-the-fly interpolation, our randomized approach has the advantage 
that the total number of passes is reduced to only one to three. In 
addition, the low-rank matrix factorization leads to considerable 
reductions in storage and computational costs of the matrix 
multiplies required by the sparse inversion. Application of the 
proposed method to synthetic and real data shows that significant 
performance improvements in speed and memory use are achievable at a 
low computational up-front cost required by the low-rank 
factorization.},
keywords = {multiples,Processing,Optimization},
url = {https://www.slim.eos.ubc.ca/Publications/Private/Submitted/Journal/bander2012dre/bander2012dre.pdf}
}

@unpublished{Friedlander11TRhdm,
  author = {Michael P. Friedlander and Mark Schmidt},
  title = {Hybrid deterministic-stochastic methods for data fitting},
  institution = {Department of Computer Science},
  year = {2011},
  address = {University of British Columbia, Vancouver},
  optmonth = {04/2011},
  abstract = {Many structured data-fitting applications require the
                  solution of an optimization problem involving a sum
                  over a potentially large number of
                  measurements. Incremental gradient algorithms (both
                  deterministic and randomized) offer inexpensive
                  iterations by sampling only subsets of the terms in
                  the sum. These methods can make great progress
                  initially, but often slow as they approach a
                  solution. In contrast, full gradient methods achieve
                  steady convergence at the expense of evaluating the
                  full objective and gradient on each iteration. We
                  explore hybrid methods that exhibit the benefits of
                  both approaches. Rate of convergence analysis and
                  numerical experiments illustrate the potential for
                  the approach.},
  publisher = {Department of Computer Science},
  keywords ={Optimization},
  url = {http://www.cs.ubc.ca/~mpf/papers/FriedlanderSchmidt2011.pdf}
}

@unpublished{Mansour11TRwmmw,
  author = {Hassan Mansour and Ozgur Yilmaz.},
  title = {Weighted -$\ell_1$ minimization with multiple weighting sets},
  year = {2011},
  type = {Tech. Rep.},
  address = {University of British Columbia, Vancouver},
  optmonth = {09/2011},
notes= {TR-2011-07 },
  abstract = {In this paper, we study the support In this paper, we
                  study the support recovery conditions of weighted
                  -$\ell_1$ minimization for signal reconstruction
                  from compressed sensing measurements when multiple
                  support estimate sets with different accuracy are
                  available. We identify a class of signals for which
                  the recovered vector from -$\ell_1$ minimization
                  provides an accurate support estimate. We then
                  derive stability and robustness guarantees for the
                  weighted -$\ell_1$ minimization problem with more
                  than one support estimate. We show that applying a
                  smaller weight to support estimate that enjoy higher
                  accuracy improves the recovery conditions compared
                  with the case of a single support estimate and the
                  case with standard, i.e., non-weighted,-$\ell_1$
                  minimization. Our theoretical results are supported
                  by numerical simulations on synthetic signals and
                  real audio signals.},
keywords={Compressive Sensing,Optimization},
  url = {http://slim.eos.ubc.ca/Publications/private/Journals/MansourYilmaz2011.pdf }
}

% Check with Michael whether this will be publsihed or stays as an technical report
@unpublished{vandenberg08gsv,
  author = {Ewout {van den Berg} and Mark Schmidt and Michael P. Friedlander and K.
	Murphy},
  title = {Group sparsity via linear-time projection},
institution = {UBC-Computer Science Department},
  year = {2008},
    number = {TR-2008-1},
  abstract = {We present an efficient spectral projected-gradient algorithm for
	optimization subject to a group one-norm constraint. Our approach
	is based on a novel linear-time algorithm for Euclidean projection
	onto the one- and group one-norm constraints. Numerical experiments
	on large data sets suggest that the proposed method is substantially
	more efficient and scalable than existing methods.},
  keywords = {SLIM,Optimization},
  url = {http://www.optimization-online.org/DB_FILE/2008/07/2056.pdf}
}

% This file was created with JabRef 2.6.
% Encoding: MacRoman

% This file was created with JabRef 2.6.
% Encoding: MacRoman
@article{aravkin2012IPNuisance,
author = {Aleksandr Y. Aravkin and Tristan {van Leeuwen}},
year = {2012},
optmonth = {10/2012},
title = {Estimating Nuisance Parameters in Inverse Problems},
journal = {Inverse Problems},
volume = {28},
number = {11},
abstract = {Many inverse problems include nuisance parameters which,
                  while not of direct interest, are required to
                  recover primary parameters. Structure present in
                  these problems allows efficient optimization
                  strategies - a well known example is variable
                  projection, where nonlinear least squares problems
                  which are linear in some parameters can be very
                  efficiently optimized. In this paper, we extend the
                  idea of projecting out a subset over the variables
                  to a broad class of maximum likelihood (ML) and
                  maximum a posteriori likelihood (MAP) problems with
                  nuisance parameters, such as variance or degrees of
                  freedom. As a result, we are able to incorporate
                  nuisance parameter estimation into large-scale
                  constrained and unconstrained inverse problem
                  formulations. We apply the approach to a variety of
                  problems, including estimation of unknown variance
                  parameters in the Gaussian model, degree of freedom
                  (d.o.f.) parameter estimation in the context of
                  robust inverse problems, automatic calibration, and
                  optimal experimental design. Using numerical
                  examples, we demonstrate improvement in recovery of
                  primary parameters for several large- scale inverse
                  problems. The proposed approach is compatible with a
                  wide variety of algorithms and formulations, and its
                  implementation requires only minor modifications to
                  existing algorithms.},
keywords = {full waveform inversion, students t, variance},
optdoi = {10.1088/0266-5611/28/11/115016},
url1 = {http://arxiv.org/abs/1206.6532},
url2 = {https://www.slim.eos.ubc.ca/Publications/Public/Journals/InverseProblems/2012/aravkin2012IPNuisance/aravkin2012IPNuisance.pdf},
url3 = {http://iopscience.iop.org/0266-5611/28/11/115016/}
}


@article{VanLeeuwen11TRfwiwse,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast waveform inversion without source encoding},
  journal = {Geophysical Prospecting},
  optvolume = {Online publication},
  year = {2012},
  optmonth = {07/2012},
  abstract = {Randomized source encoding has recently been proposed as
                  a way to dramatically reduce the costs of full
                  waveform inversion. The main idea is to replace all
                  sequential sources by a small number of simultaneous
                  sources. This introduces random crosstalk in the
                  model updates and special stochastic optimization
                  strategies are required to deal with this. Two
                  problems arise with this approach: i) source
                  encoding can only be applied to fixed-spread
                  acquisition setups, and ii) stochastic optimization
                  methods tend to converge very slowly, relying on
                  averaging to get rid of the cross-talk. Although the
                  slow convergence is partly offset by the low
                  iteration cost, we show that conventional
                  optimization strategies are bound to outperform
                  stochastic methods in the long run. In this paper we
                  argue that we don¬øt need randomized source encoding
                  to reap the benefits of stochastic optimization and
                  we review an optimization strategy that combines the
                  benefits of both conventional and stochastic
                  optimization. The method uses a gradually increasing
                  batch of sources. Thus, iterations are very cheap
                  initially and this allows the method to make fast
                  progress in the beginning. As the batch size grows,
                  the method behaves like conventional optimization,
                  allowing for fast convergence. Numerical examples
                  suggest that the stochastic and hybrid method
                  perform equally well with and without source
                  encoding and that the hybrid method outperforms both
                  conventional and stochastic optimization. The method
                  does not rely on source encoding techniques and can
                  thus be applied to non fixed-spread data.},
keywords = {SLIM,Full-waveform inversion,Optimization},
optdoi = {10.1111/j.1365-2478.2012.01096.x},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2478.2012.01096.x/abstract}
}

@article{haber10TRemp,
author = {Eldad Haber and Matthias Chung and Felix J. Herrmann},
title = {An effective method for parameter estimation with PDE constraints
        with multiple right hand sides},
journal = {SIAM Journal on Optimization},
volume = {22},
number = {3},
year = {2012},
optmonth = {07/2012},
abstract = {Often, parameter estimation problems of
                  parameter-dependent PDEs involve multiple right-hand
                  sides. The computational cost and memory
                  requirements of such problems increase linearly with
                  the number of right-hand sides. For many
                  applications this is the main bottleneck of the
                  computation. In this paper we show that problems
                  with multiple right-hand sides can be reformulated
                  as stochastic programming problems by combining the
                  right-hand sides into a few „simultaneous”
                  sources. This effectively reduces the cost of the
                  forward problem and results in problems that are
                  much cheaper to solve. We discuss two solution
                  methodologies: namely sample average approximation
                  and stochastic approximation. To illustrate the
                  effectiveness of our approach we present two model
                  problems, direct current resistivity and seismic
                  tomography.},
  keywords = {SLIM,Full-waveform inversion,Optimization},
  url = {http://dx.doi.org/10.1137/11081126X}
}

@article{Li11TRfrfwi,
author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
title = {Fast randomized full-waveform inversion with compressive sensing},
year = {2012},
journal = {Geophysics},
volume = {77},
number = {3},
pages = {A13-A17},
optmonth = {05/2012},
abstract = { Wave-equation based seismic inversion can be formulated
                  as a nonlinear inverse problem where the medium
                  properties are obtained via minimization of a least-
                  squares misfit functional. The demand for higher
                  resolution models in more geologically complex areas
                  drives the need to develop techniques that explore
                  the special structure of full-waveform inversion to
                  reduce the computational burden and to regularize
                  the inverse problem. We meet these goals by using
                  ideas from compressive sensing and stochastic
                  optimization to design a novel Gauss-Newton method,
                  where the updates are computed from random subsets
                  of the data via curvelet-domain sparsity
                  promotion. Application of this idea to a realistic
                  synthetic shows improved results compared to
                  quasi-Newton methods, which require passes through
                  all data. Two different subset sampling strategies
                  are considered: randomized source encoding, and
                  drawing sequential shots firing at random source
                  locations from marine data with missing near and far
                  offsets. In both cases, we obtain excellent
                  inversion results compared to conventional methods
                  at reduced computational costs. },
keywords ={SLIM,Full-waveform inversion,Compressive Sensing,Optimization},
optdoi = {10.1190/geo2011-0410.1},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2012/Li11TRfrfwi/Li11TRfrfwi.pdf}
}

@article{Mansour11TRssma,
author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
title = {Randomized marine acquisition with compressive sampling matrices},
journal = {Geophysical Prospecting},
volume = {60},
number = {4},
pages = {648-662},
year = {2012},
optmonth = {07/2012},
abstract = {Seismic data acquisition in marine environments is a
                  costly process that calls for the adoption of
                  simultaneous-source or randomized acquisition - an
                  emerging technology that is stimulating both
                  geophysical research and commercial
                  efforts. Simultaneous marine acquisition calls for
                  the development of a new set of design principles
                  and post-processing tools. In this paper, we discuss
                  the properties of a specific class of randomized
                  simultaneous acquisition matrices and demonstrate
                  that sparsity-promoting recovery improves the
                  quality of reconstructed seismic data volumes. We
                  propose a practical randomized marine acquisition
                  scheme where the sequential sources fire airguns at
                  only randomly time-dithered instances. We
                  demonstrate that the recovery using sparse
                  approximation from random time-dithering with a
                  single source approaches the recovery from
                  simultaneous-source acquisition with multiple
                  sources. Established findings from the field of
                  compressive sensing indicate that the choice of the
                  sparsifying transform that is incoherent with the
                  compressive sampling matrix can significantly impact
                  the reconstruction quality. Leveraging these
                  findings, we then demonstrate that the compressive
                  sampling matrix resulting from our proposed sampling
                  scheme is incoherent with the curvelet
                  transform. The combined measurement matrix exhibits
                  better isometry properties than other transform
                  bases such as a non-localized multidimensional
                  Fourier transform. We illustrate our results with
                  simulations of ‘ideal’ simultaneous-source marine
                  acquisition, which dithers both in time and space,
                  compared with periodic and randomized time-dithering.},
keywords ={Curvelet transform,Fourier,Marine acquisition},
optdoi = {10.1111/j.1365-2478.2012.01075.x},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2478.2012.01075.x/abstract}
}

@article{Aravkin11TRridr,
author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Felix J. Herrmann and Tristan {van Leeuwen}},
title = {Robust inversion, dimensionality reduction, and randomized sampling},
journal = {Mathematical Programming},
volume = {134},
number = {1},
pages = {101-125},
year = {2012},
optmonth = {07/2012},
abstract = {We consider a class of inverse problems in which the
                  forward model is the solution operator to linear
                  ODEs or PDEs. This class admits several
                  dimensionality-reduction techniques based on data
                  averaging or sampling, which are especially useful
                  for large-scale problems. We survey these approaches
                  and their connection to stochastic optimization. The
                  data-averaging approach is only viable, however, for
                  a least-squares misfit, which is sensitive to
                  outliers in the data and artifacts unexplained by
                  the forward model. This motivates us to propose a
                  robust formulation based on the Student’s
                  t-distribution of the error. We demonstrate how the
                  corresponding penalty function, together with the
                  sampling approach, can obtain good results for a
                  large-scale seismic inverse problem with 50 %
                  corrupted data.},
keywords = {Inverse problems, Seismic inversion, Stochastic optimization, Robust estimation, Optimization, FWI},
optdoi = {10.1007/s10107-012-0571-6},
url = {http://www.springerlink.com/content/35rwr101h5736340/}
}

@article{Herrmann11TRfcd, 
author={Felix J. Herrmann and Michael P. Friedlander and Ozgur Yilmaz}, 
journal={Signal Processing Magazine, IEEE}, 
title={Fighting the Curse of Dimensionality: Compressive Sensing in Exploration Seismology}, 
year={2012}, 
optmonth={05/12}, 
volume={29}, 
number={3}, 
pages={88-100}, 
abstract={Many seismic exploration techniques rely on the collection 
of massive data volumes that are mined for information during processing. 
This approach has been extremely successful, but current efforts 
toward higher resolution images in increasingly complicated regions 
of Earth continue to reveal fundamental shortcomings in our typical 
workflows. The "curse" of dimensionality is the main 
roadblock and is exemplified by Nyquist's sampling criterion, which 
disproportionately strains current acquisition and processing systems 
as the size and desired resolution of our survey areas continues to increase.}, 
keywords={Earth, Nyquist sampling criterion;dimensionality curse,
higher-resolution images, massive data volumes,seismic exploration techniques,
strains current acquisition system,strains current processing system,
geographic information systems,seismology}, 
optdoi={10.1109/MSP.2012.2185859}, 
url = {https://www.slim.eos.ubc.ca/Publications/Public/Journals/IEEESignalProcessingMagazine/2012/Herrmann11TRfcd/Herrmann11TRfcd.pdf},
ISSN={1053-5888}}

@article{herrmann2011RECORDERcsse1,
author = {Felix J. Herrmann and Haneet Wason and Tim T.Y. Lin},
title = {Compressive sensing in seismic exploration: an outlook on a new paradigm},
year = {2011},
journal = {CSEG Recorder},
optmonth = {04/2011},
abstract = {Many seismic exploration techniques rely on the collection of 
massive data volumes that are subsequently mined for information during 
processing. While this approach has been extremely successful in the past, 
current efforts toward higher resolution images in increasingly complicated 
regions of the Earth continue to reveal fundamental shortcomings in our 
workflows. Chiefly amongst these is the so-called “curse of dimensionality” 
exemplified by Nyquist's sampling criterion, which disproportionately 
strains current acquisition and processing systems as the size and desired 
resolution of our survey areas continues to increase. We offer an alternative 
sampling method leveraging recent insights from compressive sensing towards 
seismic acquisition and processing for data that, from a traditional point 
of view, are considered to be undersampled. The main outcome of this approach 
is a new technology where acquisition and processing related costs are 
decoupled the stringent Nyquist sampling criterion. At the heart of our 
approach lies randomized incoherent sampling that breaks subsampling-related 
interferences by turning them into harmless noise, which we subsequently 
remove by promoting sparsity in a transform-domain. Acquisition schemes 
designed to fit into this regime no longer grow significantly in cost with 
increasing resolution and dimensionality of the survey area, but instead 
its cost ideally only depends on transform-domain sparsity of the expected 
data. Our contribution is split into two part.},
url = {http://www.cseg.ca/publications/recorder/2011/04apr/Apr2011-Compressive-sensing-in-seismic-expl.pdf},
volume = {36},
number = {4},
pages = {19-33}
}

@article{herrmann11GPelsqIm,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares imaging with sparsity promotion and compressive
        sensing},
  year = {2012},
  journal = {Geophysical Prospecting},
  volume = {60},
  number = {4},
  pages = {696-712},
  address = {University of British Columbia, Vancouver},
  optmonth = {07/2012},
  abstract = {Seismic imaging is a linearized inversion problem relying on the 
  minimization of a least-squares misfit functional as a function of the medium 
  perturbation. The success of this procedure hinges on our ability to handle 
  large systems of equations---whose size grows exponentially with the demand 
  for higher resolution images in more and more complicated areas---and our 
  ability to invert these systems given a limited amount of computational 
  resources. To overcome this ``curse of dimensionality'' in problem size and 
  computational complexity, we propose a combination of randomized 
  dimensionality-reduction and divide-and-conquer techniques. This approach 
  allows us to take advantage of sophisticated sparsity-promoting solvers that 
  work on a series of smaller subproblems each involving a small randomized 
  subset of data. These subsets correspond to artificial simultaneous-source 
  experiments made of random superpositions of sequential-source experiments. 
  By changing these subsets after each subproblem is solved, we are able to 
  attain an inversion quality that is competitive while requiring fewer 
  computational, and possibly, fewer acquisition resources. Application of this 
  concept to a controlled series of experiments showed the validity of our 
  approach and the relationship between its efficiency---by reducing the number 
  of sources and hence the number of wave-equation solves---and the image 
  quality. Application of our dimensionality-reduction methodology with sparsity 
  promotion to a complicated synthetic with well-log constrained structure also 
  yields excellent results underlining the importance of sparsity promotion.},
  keywords ={SLIM,Imaging,Optimization,Compressive Sensing},
  optdoi = {10.1111/j.1365-2478.2011.01041.x},
  url1 = {https://www.slim.eos.ubc.ca/Publications/Public/Journals/GeophysicalProspecting/2012/herrmann11GPelsqIm/herrmann11GPelsqIm.pdf},
  url2 = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2478.2011.01041.x/full}
}

@article{mansour2012IEEETITrcs,
  author = {Michael P. Friedlander and Hassan Mansour and Rayan Saab and Ozgur Yilmaz},
  title = {Recovering compressively sampled signals using partial support information},
  institution = {Department of Computer Science},
  year = 2012,
  volume = {58},
  number = {2},
  pages = {1122-1134},
  address = {University of British Columbia, Vancouver},
  optmonth = {02/2012},
  abstract = {We study recovery conditions of weighted $\ell_1$ minimization for
        signal reconstruction from compressed sensing measurements when partial
        support information is available. We show that if at least 50\% of
        the (partial) support information is accurate, then weighted $\ell_1$
        minimization is stable and robust under weaker sufficient conditions
        than the analogous conditions for standard $\ell_1$ minimization.
        Moreover, weighted $\ell_1$ minimization provides better upper bounds
        on the reconstruction error in terms of the measurement noise and
        the compressibility of the signal to be recovered. We illustrate
        our results with extensive numerical experiments on synthetic data
        and real audio and video signals.},
  journal = {IEEE Trans. on Information Theory},
  keywords = {Compressive Sensing},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/mansour2011.pdf},
  optdoi={10.1109/TIT.2011.2167214}
}

@ARTICLE{friedlander2011CoRRhybrid,
  author = {Michael P. Friedlander and Mark Schmidt},
  title = {Hybrid Deterministic-Stochastic Methods for Data Fitting},
  year = {2011},
  optmonth = {04/2011},
  abstract = {Many structured data-fitting applications require the solution of
	an optimization problem involving a sum over a potentially large
	number of measurements. Incremental gradient algorithms (both deterministic
	and randomized) offer inexpensive iterations by sampling only subsets
	of the terms in the sum. These methods can make great progress initially,
	but often slow as they approach a solution. In contrast, full gradient
	methods achieve steady convergence at the expense of evaluating the
	full objective and gradient on each iteration. We explore hybrid
	methods that exhibit the benefits of both approaches. Rate of convergence
	analysis and numerical experiments illustrate the potential for the
	approach.},
journal= {CoRR },
keywords={Optimization},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/CoRR/2011/friedlander11hybrid.pdf}
}

@ARTICLE{hennenfent2010GEOPnct,
  author = {Gilles Hennenfent and Lloyd Fenelon and Felix J. Herrmann},
  title = {Nonequispaced curvelet transform for seismic data reconstruction:
	A sparsity-promoting approach},
  journal = {Geophysics},
  year = {2010},
  volume = {75},
  optmonth = {12/2010},
  pages = {WB203-WB210},
  number = {6},
  abstract = {We extend our earlier work on the nonequispaced fast discrete curvelet
	transform (NFDCT) and introduce a second generation of the transform.
	This new generation differs from the previous one by the approach
	taken to compute accurate curvelet coefficients from irregularly
	sampled data. The first generation relies on accurate Fourier coefficients
	obtained by an l2-regularized inversion of the nonequispaced fast
	Fourier transform (FFT) whereas the second is based on a direct l1-regularized
	inversion of the operator that links curvelet coefficients to irregular
	data. Also, by construction the second generation NFDCT is lossless
	unlike the first generation NFDCT. This property is particularly
	attractive for processing irregularly sampled seismic data in the
	curvelet domain and bringing them back to their irregular record-ing
	locations with high fidelity. Secondly, we combine the second generation
	NFDCT with the standard fast discrete curvelet transform (FDCT) to
	form a new curvelet-based method, coined nonequispaced curvelet reconstruction
	with sparsity-promoting inversion (NCRSI) for the regularization
	and interpolation of irregularly sampled data. We demonstrate that
	for a pure regularization problem the reconstruction is very accurate.
	The signal-to-reconstruction error ratio in our example is above
	40 dB. We also conduct combined interpolation and regularization
	experiments. The reconstructions for synthetic data are accurate,
	particularly when the recording locations are optimally jittered.
	The reconstruction in our real data example shows amplitudes along
	the main wavefronts smoothly varying with limited acquisition imprint.},
  optdoi = {10.1190/1.3494032},
  keywords = {curvelet transforms, data acquisition, geophysical techniques, seismology,SLIM,Processing},
  publisher = {SEG},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent2010nct.pdf}
}

@ARTICLE{herrmann2010GEOPrsg,
  author = {Felix J. Herrmann},
  title = {Randomized sampling and sparsity: Getting more information from fewer
	samples},
  journal = {Geophysics},
  year = {2010},
  volume = {75},
  optmonth = {12/2010},
  pages = {WB173-WB187},
  number = {6},
  abstract = {Many seismic exploration techniques rely on the collection of massive
	data volumes that are subsequently mined for information during processing.
	Although this approach has been extremely successful in the past,
	current efforts toward higher-resolution images in increasingly complicated
	regions of the earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly among these is the so-called {\textquotedblleft}curse
	of dimensionality{\textquotedblright} exemplified by Nyquist{\textquoteright}s
	sampling criterion, which disproportionately strains current acquisition
	and processing systems as the size and desired resolution of our
	survey areas continue to increase. We offer an alternative sampling
	method leveraging recent insights from compressive sensing toward
	seismic acquisition and processing for data that are traditionally
	considered to be undersampled. The main outcome of this approach
	is a new technology where acquisition and processing related costs
	are no longer determined by overly stringent sampling criteria, such
	as Nyquist. At the heart of our approach lies randomized incoherent
	sampling that breaks subsampling related interferences by turning
	them into harmless noise, which we subsequently remove by promoting
	transform-domain sparsity. Now, costs no longer grow significantly
	with resolution and dimensionality of the survey area, but instead
	depend only on transform-domain sparsity. Our contribution is twofold.
	First, we demonstrate by means of carefully designed numerical experiments
	that compressive sensing can successfully be adapted to seismic exploration.
	Second, we show that accurate recovery can be accomplished for compressively
	sampled data volumes sizes that exceed the size of conventional transform-domain
	data volumes by only a small factor. Because compressive sensing
	combines transformation and encoding by a single linear encoding
	step, this technology is directly applicable to acquisition and to
	dimensionality reduction during processing. In either case, sampling,
	storage, and processing costs scale with transform-domain sparsity.
	We illustrate this principle by means of number of case studies.},
  optdoi = {10.1190/1.3506147},
  keywords = {data acquisition, geophysical techniques, Nyquist criterion, sampling
	methods, seismology,SLIM,Acquisition,Compressive Sensing,Optimization},
  publisher = {SEG},
  url = {http://link.aip.org/link/?GPY/75/WB173/1}
}

@article{kumar2010TNPecr,
  author = {Vishal Kumar and Jounada Oueity and Ron Clowes and Felix J. Herrmann},
  title = {Enhancing Crustal Reflection Data through Curvelet Denoising},
  year = {2011},
  optmonth = {07/2011},
  journal = {Technophysics},
  volume = {508},
  number = {1-4},
  pages = {106-116},
  abstract = {Suppression of incoherent noise, which is present in the
                  seismic signal and may often lead to ambiguous
                  interpretation, is a key step in processing
                  associated with crustal reflection data. In this
                  paper, we make use of the parsimonious
                  representation of seismic data in the curvelet
                  domain to perform the noise attenuation while
                  preserving the coherent energy and its amplitude
                  information. Curvelets are a recently developed
                  mathematical transform that has as one of its
                  properties minimal overlap between seismic signal
                  and noise in the transform domain, thereby
                  facilitating signal-noise separation. The problem is
                  cast as an inverse problem and the results are
                  obtained by updating the solution at each
                  iteration. We demonstrate the effectiveness of this
                  procedure at removing noise on both synthetic shot
                  gathers and a synthetic stacked seismic section. We
                  then apply curvelet denoising to deep crustal
                  seismic reflection data where the signal-to-noise
                  ratio is low. The reflection data were recorded
                  along Lithoprobe's SNORCLE Line 1 across
                  Paleoproterozoic-Archean domains in Canada's
                  Northwest Territories. After initial processing, we
                  apply the iterative curvelet denoising to both
                  pre-stack shot gathers and post-stack data. Ground
                  roll, random noise and much of the anomalous
                  vertical energy is removed from the pre-stack shot
                  gathers, to the extent that crustal reflections,
                  including those from the Moho, are clearly seen on
                  individual gathers. Denoised stacked data show a
                  series of dipping reflections in the lower crust
                  that extend into the Moho. The Moho itself is
                  relatively flat and characterized by a sharp, narrow
                  band of reflections. Comparing the results for the
                  stacked data with those from F-X deconvolution,
                  curvelet denoising outperforms the latter by
                  attenuating incoherent noise with minimal harm to
                  the signal. Because curvelet denoising retains
                  amplitude information, it provides opportunities for
                  further studies of seismic sections through
                  attribute analyses. Curvelet denoising provides an
                  important new tool in the processing toolbox for
                  crustal seismic reflection data.},
  optdoi = {10.1016/j.tecto.2010.07.01},
  keywords = {SLIM,Processing},
  url = {http://www.sciencedirect.com/science/article/pii/S0040195110003227}
}


@ARTICLE{vanLeeuwen2010IJGswi,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and  Felix J. Herrmann},
  title = {Seismic waveform inversion by stochastic optimization},
  journal = {International Journal of Geophysics},
  volume = {2011},
  notes = {Article ID: 689041, 18pages},
  year = {2011},
  optmonth = {12/2011},
  optdoi = {10.1155/2011/689041},
  abstract = {We explore the use of stochastic optimization methods
                  for seismic waveform inversion. The basic principle
                  of such methods is to randomly draw a batch of
                  realizations of a given misfit function and goes
                  back to the 1950s. The ultimate goal of such an
                  approach is to dramatically reduce the computational
                  cost involved in evaluating the misfit. Following
                  earlier work, we introduce the stochasticity in
                  waveform inversion problem in a rigorous way via a
                  technique called randomized trace estimation. We
                  then review theoretical results that underlie recent
                  developments in the use of stochastic methods for
                  waveform inversion. We present numerical experiments
                  to illustrate the behavior of different types of
                  stochastic optimization methods and investigate the
                  sensitivity to the batch size and the noise level in
                  the data. We find that it is possible to reproduce
                  results that are qualitatively similar to the
                  solution of the full problem with modest batch
                  sizes, even on noisy data. Each iteration of the
                  corresponding stochastic methods requires an order
                  of magnitude fewer PDE solves than a comparable
                  deterministic method applied to the full problem,
                  which may lead to an order of magnitude speedup for
                  waveform inversion in practice.},
  keywords = {SLIM,Full-waveform inversion,Optimization},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Journals/InternationJournalOfGeophysics/2011/vanLeeuwen10IJGswi/vanLeeuwen10IJGswi.pdf}
}

@ARTICLE{herrmann2009GEOPcbm,
  author = {Felix J. Herrmann and Cody R. Brown and Yogi A. Erlangga and Peyman
	P. Moghaddam},
  title = {Curvelet-based migration preconditioning and scaling},
  journal = {Geophysics},
  year = {2009},
  optmonth = {09/2009},
  volume = {74},
  pages = {A41},
  abstract = {The extremely large size of typical seismic imaging problems has been
	one of the major stumbling blocks for iterative techniques to attain
	accurate migration amplitudes. These iterative methods are important
	because they complement theoretical approaches that are hampered
	by difficulties to control problems such as finite-acquisition aperture,
	source-receiver frequency response, and directivity. To solve these
	problems, we apply preconditioning, which significantly improves
	convergence of least-squares migration. We discuss different levels
	of preconditioning that range from corrections for the order of the
	migration operator to corrections for spherical spreading, and position
	and reflector-dip dependent amplitude errors. While the first two
	corrections correspond to simple scalings in the Fourier and physical
	domain, the third correction requires phase-space (space spanned
	by location and dip) scaling, which we carry out with curvelets.
	We show that our combined preconditioner leads to a significant improvement
	of the convergence of least-squares {\textquoteleft}wave-equation{\textquoteright}
	migration on a line from the SEG AA{\textquoteright} salt model.},
  keywords = {migration,SLIM,Imaging},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08cmp-r.pdf}
}


@ARTICLE{herrmann2009GEOPcsf,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive simultaneous full-waveform simulation},
  journal = {Geophysics},
  year = {2009},
  volume = {74},
  optmonth = {08/2008},
  pages = {A35},
  abstract = {The fact that computational complexity of wavefield simulation is
	proportional to the size of the discretized model and acquisition
	geometry, and not to the complexity of the simulated wavefield, is
	a major impediment within seismic imaging. By turning simulation
	into a compressive sensing problem{\textendash}-where simulated data
	is recovered from a relatively small number of independent simultaneous
	sources{\textendash}-we remove this impediment by showing that compressively
	sampling a simulation is equivalent to compressively sampling the
	sources, followed by solving a reduced system. As in compressive
	sensing, this allows for a reduction in sampling rate and hence in
	simulation costs. We demonstrate this principle for the time-harmonic
	Helmholtz solver. The solution is computed by inverting the reduced
	system, followed by a recovery of the full wavefield with a sparsity
	promoting program. Depending on the wavefield{\textquoteright}s sparsity,
	this approach can lead to significant cost reductions, in particular
	when combined with the implicit preconditioned Helmholtz solver,
	which is known to converge even for decreasing mesh sizes and increasing
	angular frequencies. These properties make our scheme a viable alternative
	to explicit time-domain finite-differences.},
  keywords = {full-waveform,SLIM,Modelling,Compressive Sensing},
  url = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08csf-r.pdf}
}

@ARTICLE{berg2008SJSCpareto,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Probing the Pareto frontier for basis pursuit solutions},
  journal = {SIAM Journal on Scientific Computing},
  year = {2008},
  volume = {31},
  pages = {890-912},
  optmonth = {01/2008},
  number = {2},
  abstract = {The basis pursuit problem seeks a minimum one-norm solution of an
	underdetermined least-squares problem. Basis pursuit denoise (BPDN)
	fits the least-squares problem only approximately, and a single parameter
	determines a curve that traces the optimal trade-off between the
	least-squares fit and the one-norm of the solution. We prove that
	this curve is convex and continuously differentiable over all points
	of interest, and show that it gives an explicit relationship to two
	other optimization problems closely related to BPDN. We describe
	a root-finding algorithm for finding arbitrary points on this curve;
	the algorithm is suitable for problems that are large scale and for
	those that are in the complex domain. At each iteration, a spectral
	gradient-projection method approximately minimizes a least-squares
	problem with an explicit one-norm constraint. Only matrix-vector
	operations are required. The primal-dual solution of this problem
	gives function and derivative information needed for the root-finding
	method. Numerical experiments on a comprehensive set of test problems
	demonstrate that the method scales well to large problems.},
  optdoi = {10.1137/080714488},
  keywords = {basis pursuit, convex program, duality, Newton{\textquoteright}s method,
	one-norm regularization, projected gradient, root-finding, sparse
	solutions,Optimization},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/vanderberg08ptp.pdf},
  publisher = {SIAM}
}


@ARTICLE{hennenfent2008GEOPnii,
  author = {Gilles Hennenfent and Ewout {van den Berg} and Michael P. Friedlander
	and Felix J. Herrmann},
  title = {New insights into one-norm solvers from the {P}areto curve},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  number = {4},
  optmonth = {07/2008},
  abstract = {Geophysical inverse problems typically involve a trade off between
	data misfit and some prior. Pareto curves trace the optimal trade
	off between these two competing aims. These curves are commonly used
	in problems with two-norm priors where they are plotted on a log-log
	scale and are known as L-curves. For other priors, such as the sparsity-promoting
	one norm, Pareto curves remain relatively unexplored. We show how
	these curves lead to new insights in one-norm regularization. First,
	we confirm the theoretical properties of smoothness and convexity
	of these curves from a stylized and a geophysical example. Second,
	we exploit these crucial properties to approximate the Pareto curve
	for a large-scale problem. Third, we show how Pareto curves provide
	an objective criterion to gauge how different one-norm solvers advance
	towards the solution.},
  keywords = {Pareto, SLIM, Geophysics,Optimization,Acquisition,Processing},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOnii/hennenfent08GEOnii.pdf }
}

@ARTICLE{hennenfent2008GEOPsdw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Simply denoise: wavefield reconstruction via jittered undersampling},
  journal = {Geophysics},
  year = {2008},
  pages = {V19-V28},
  volume = {73},
  number = {3},
  optmonth = {05/2008},
  abstract = {In this paper, we present a new discrete undersampling scheme designed
	to favor wavefield reconstruction by sparsity-promoting inversion
	with transform elements that are localized in the Fourier domain.
	Our work is motivated by empirical observations in the seismic community,
	corroborated by recent results from compressive sampling, which indicate
	favorable (wavefield) reconstructions from random as opposed to regular
	undersampling. As predicted by theory, random undersampling renders
	coherent aliases into harmless incoherent random noise, effectively
	turning the interpolation problem into a much simpler denoising problem.
	A practical requirement of wavefield reconstruction with localized
	sparsifying transforms is the control on the maximum gap size. Unfortunately,
	random undersampling does not provide such a control and the main
	purpose of this paper is to introduce a sampling scheme, coined jittered
	undersampling, that shares the benefits of random sampling, while
	offering control on the maximum gap size. Our contribution of jittered
	sub-Nyquist sampling proves to be key in the formulation of a versatile
	wavefield sparsity-promoting recovery scheme that follows the principles
	of compressive sampling. After studying the behavior of the jittered
	undersampling scheme in the Fourier domain, its performance is studied
	for curvelet recovery by sparsity-promoting inversion (CRSI). Our
	findings on synthetic and real seismic data indicate an improvement
	of several decibels over recovery from regularly-undersampled data
	for the same amount of data collected.},
  optdoi = {10.1190/1.2841038},
  keywords = {sampling, Geophysics,SLIM,Acquisition,Processing,Optimization,Compressive Sensing},
  publisher = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/hennenfent08GEOsdw/hennenfent08GEOsdw.pdf}
}


@ARTICLE{herrmann2008GJInps,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Non-parametric seismic data recovery with curvelet frames},
  journal = {Geophysical Journal International},
  year = {2008},
  volume = {173},
  pages = {233-248},
  optmonth = {04/2011},
  abstract = {Seismic data recovery from data with missing traces on otherwise regular
	acquisition grids forms a crucial step in the seismic processing
	flow. For instance, unsuccessful recovery leads to imaging artifacts
	and to erroneous predictions for the multiples, adversely affecting
	the performance of multiple elimination. A non-parametric transform-based
	recovery method is presented that exploits the compression of seismic
	data volumes by recently developed curvelet frames. The elements
	of this transform are multidimensional and directional and locally
	resem- ble wavefronts present in the data, which leads to a compressible
	representation for seismic data. This compression enables us to formulate
	a new curvelet-based seismic data recovery algorithm through sparsity-promoting
	inversion. The concept of sparsity-promoting inversion is in itself
	not new to geophysics. However, the recent insights from the field
	of {\textquoteleft}compressed sensing{\textquoteright} are new since
	they clearly identify the three main ingredients that go into a successful
	formulation of a re- covery problem, namely a sparsifying transform,
	a sampling strategy that subdues coherent aliases and a sparsity-promoting
	program that recovers the largest entries of the curvelet-domain
	vector while explaining the measurements. These concepts are illustrated
	with a stylized experiment that stresses the importance of the degree
	of compression by the sparsifying transform. With these findings,
	a curvelet-based recovery algorithms is developed, which recovers
	seismic wavefields from seismic data volumes with large percentages
	of traces missing. During this construction, we benefit from the
	main three ingredients of compressive sampling, namely the curvelet
	compression of seismic data, the existence of a favorable sam- pling
	scheme and the formulation of a large-scale sparsity-promoting solver
	based on a cooling method. The recovery performs well on synthetic
	as well as real data and performs better by virtue of the sparsifying
	property of curvelets. Our results are applicable to other areas
	such as global seismology.},
  optdoi = {10.1111/j.1365-246X.2007.03698.x},
  keywords = {curvelet transform, reconstruction, SLIM,Acquisition},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysical Journal International/2008/herrmann08nps/herrmann08nps.pdf }
}

@ARTICLE{herrmann2008ACHAsac,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and Chris Stolk},
  title = {Sparsity- and continuity-promoting seismic image recovery with curvelet
	frames},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2008},
  volume = {24},
  pages = {150-173},
  number = {2},
  optmonth = {03/2011},
  abstract = {A nonlinear singularity-preserving solution to seismic image recovery
	with sparseness and continuity constraints is proposed. We observe
	that curvelets, as a directional frame expan- sion, lead to sparsity
	of seismic images and exhibit invariance under the normal operator
	of the linearized imaging problem. Based on this observation we derive
	a method for stable recovery of the migration amplitudes from noisy
	data. The method corrects the amplitudes during a post-processing
	step after migration, such that the main additional cost is one ap-
	plication of the normal operator, i.e. a modeling followed by a migration.
	Asymptotically this normal operator corresponds to a pseudodifferential
	operator, for which a convenient diagonal approximation in the curvelet
	domain is derived, including a bound for its error and a method for
	the estimation of the diagonal from a compound operator consisting
	of discrete implementations for the scattering operator and its adjoint
	the migration opera- tor. The solution is formulated as a nonlinear
	optimization problem where sparsity in the curvelet domain as well
	as continuity along the imaged reflectors are jointly promoted. To
	enhance sparsity, the l1 -norm on the curvelet coefficients is minimized,
	while continuity is promoted by minimizing an anisotropic diffusion
	norm on the image. The performance of the recovery scheme is evaluated
	with a time-reversed {\textquoteright}wave-equation{\textquoteright}
	migration code on synthetic datasets, including the complex SEG/EAGE
	AA salt model.},
  optdoi = {10.1016/j.acha.2007.06.007},
  keywords = {curvelet transform, imaging, SLIM,Imaging,Processing},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/Herrmann08sac.pdf}
}

@ARTICLE{herrmann2008GEOPcbs,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman
	P. Moghaddam},
  title = {Curvelet-based seismic data processing: a multiscale and nonlinear
	approach},
  journal = {Geophysics},
  year = {2008},
  optmonth = {03/2008},
  volume = {73},
  pages = {A1-A5},
  number = {1},
  abstract = {Mitigating missing data, multiples, and erroneous migration amplitudes
	are key factors that determine image quality. Curvelets, little {\textquoteleft}{\textquoteleft}plane
	waves,{\textquoteright}{\textquoteright} complete with oscillations
	in one direction and smoothness in the other directions, sparsify
	a property we leverage explicitly with sparsity promotion. With this
	principle, we recover seismic data with high fidelity from a small
	subset (20\%) of randomly selected traces. Similarly, sparsity leads
	to a natural decorrelation and hence to a robust curvelet-domain
	primary-multiple separation for North Sea data. Finally, sparsity
	helps to recover migration amplitudes from noisy data. With these
	examples, we show that exploiting the curvelet{\textquoteright}s
	ability to sparsify wavefrontlike features is powerful, and our results
	are a clear indication of the broad applicability of this transform
	to exploration seismology. {\copyright}2008 Society of Exploration
	Geophysicists},
  optdoi = {10.1190/1.2799517},
  keywords = {curvelet transform, SLIM,Acquisition,Processing},
  publisher = {SEG},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/herrmann08GEOcbs/herrmann08GEOcbs.pdf } 
}


@ARTICLE{herrmann2008GEOPacd,
  author = {Felix J. Herrmann and Deli Wang and D. J. Verschuur},
  title = {Adaptive curvelet-domain primary-multiple separation},
  journal = {Geophysics},
  year = {2008},
  optmonth = {08/2008},
  volume = {73},
  pages = {A17-A21},
  number = {3},
  abstract = {In many exploration areas, successful separation of primaries and
	multiples greatly determines the quality of seismic imaging. Despite
	major advances made by surface-related multiple elimination (SRME),
	amplitude errors in the predicted multiples remain a problem. When
	these errors vary for each type of multiple in different ways (as
	a function of offset, time, and dip), they pose a serious challenge
	for conventional least-squares matching and for the recently introduced
	separation by curvelet-domain thresholding. We propose a data-adaptive
	method that corrects amplitude errors, which vary smoothly as a function
	of location, scale (frequency band), and angle. With this method,
	the amplitudes can be corrected by an elementwise curvelet-domain
	scaling of the predicted multiples. We show that this scaling leads
	to successful estimation of primaries, despite amplitude, sign, timing,
	and phase errors in the predicted multiples. Our results on synthetic
	and real data show distinct improvements over conventional least-squares
	matching in terms of better suppression of multiple energy and high-frequency
	clutter and better recovery of estimated primaries. {\copyright}2008
	Society of Exploration Geophysicists},
  optdoi = {10.1190/1.2904986},
  keywords = {Geophysics, SLIM,Processing},
  publisher = {SEG},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/herrmann08GEOacd/herrmann08GEOacd.pdf }
}


@ARTICLE{saab2008ACHAsrb,
  author = {Rayan Saab and Ozgur Yilmaz},
  title = {Sparse Recovery by Non-Convex Optimization - Instance Optimality},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2010},
  optmonth = {07/2010},
  volume = {29},
  number = {1},
  pages = {30-48},
  abstract = {In this note, we address the theoretical properties of $Œî_p$, a class
	of compressed sensing decoders that rely on $l^p$ minimization with
	$p {\i}n (0, 1)$ to recover estimates of sparse and compressible
	signals from incomplete and inaccurate measurements. In particular,
	we extend the results of Cand{\textquoteleft}es, Romberg and Tao
	[3] and Wojtaszczyk [30] regarding the decoder $Œî_1$, based on $\ell^1$
	minimization, to $Œî p$ with $p {\i}n (0, 1)$. Our results are two-fold.
	First, we show that under certain sufficient conditions that are
	weaker than the analogous sufficient conditions for $Œî_1$ the decoders
	$Œî_p$ are robust to noise and stable in the sense that they are
	$(2, p)$ instance optimal. Second, we extend the results of Wojtaszczyk
	to show that, like $Œî_1$, the decoders $Œî_p$ are (2, 2) instance
	optimal in probability provided the measurement matrix is drawn from
	an appropriate distribution. While the extension of the results of
	[3] to the setting where $p {\i}n (0, 1)$ is straightforward, the
	extension of the instance optimality in probability result of [30]
	is non-trivial. In particular, we need to prove that the $LQ_1$ property,
	introduced in [30], and shown to hold for Gaussian matrices and matrices
	whose columns are drawn uniformly from the sphere, generalizes to
	an $LQ_p$ property for the same classes of matrices. Our proof is
	based on a result by Gordon and Kalton [18] about the Banach-Mazur
	distances of p-convex bodies to their convex hulls.},
  keywords = {non-convex,Compressive Sensing},
  url = {http://dx.doi.org/10.1016/j.acha.2009.08.002}
}

@ARTICLE{wang2008GEOPbws,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Bayesian wavefield separation by transform-domain sparsity promotion},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  number = {5},
  pages = {1-6},
  optmonth = {07/2008},
  abstract = {Successful removal of coherent noise sources greatly determines the
	quality of seismic imag- ing. Ma jor advances were made in this direction,
	e.g., Surface-Related Multiple Elimination (SRME) and interferometric
	ground-roll removal. Still, moderate phase, timing, amplitude errors
	and clutter in the predicted signal components can be detrimental.
	Adopting a Bayesian approach along with the assumption of approximate
	curvelet-domain independence of the to-be-separated signal components,
	we construct an iterative algorithm that takes the predictions produced
	by for example SRME as input and separates these components in a
	robust fashion. In addition, the proposed algorithm controls the
	energy mismatch between the separated and predicted components. Such
	a control, which was lacking in earlier curvelet-domain formulations,
	produces improved results for primary-multiple separation on both
	synthetic and real data.},
  optdoi = {10.1190/1.2952571},
  keywords = {curvelet transform, SLIM, Geophysics,Processing,Optimization},
url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2008/wang08GEObws/wang08GEObws.pdf }
}

@ARTICLE{friedlander2007TASdtd,
  author = {Michael P. Friedlander and M. A. Saunders},
  title = {Discussion: The Dantzig Selector: Statistical estimation when p is
	much larger then n},
  journal = {The Annals of Statistics},
  year = {2007},
  volume = {35},
  optmonth = {03/2007},
  pages = {2385-2391},
  number = {6},
  optdoi = {10.1214/009053607000000479},
  keywords = {dantzig, SLIM, statistics},
  url = {http://www.cs.ubc.ca/~mpf/downloads/FriedlanderSaunders08.pdf}
}

@ARTICLE{friedlander2007SJOero,
  author = {Michael P. Friedlander and P. Tseng},
  title = {Exact Regularization of Convex Programs},
  journal = {SIAM J. Optim},
  year = {2007},
  optmonth = {05/2007},
  volume = {18},
  pages = {1326-1350},
  number = {4},
  abstract = {The regularization of a convex program is exact if all solutions of
	the regularized problem are also solutions of the original problem
	for all values of the regularization parameter below some positive
	threshold. For a general convex program, we show that the regularization
	is exact if and only if a certain selection problem has a Lagrange
	multiplier. Moreover, the regularization parameter threshold is inversely
	related to the Lagrange multiplier. We use this result to generalize
	an exact regularization result of Ferris and Mangasarian [Appl. Math.
	Optim., 23(1991), pp. 266{\textendash}273] involving a linearized
	selection problem. We also use it to derive necessary and sufficient
	conditions for exact penalization, similar to those obtained by Bertsekas
	[Math. Programming, 9(1975), pp. 87{\textendash}99] and by Bertsekas,
	Nedi , Ozdaglar [Convex Analysis and Optimization, Athena Scientific,
	Belmont, MA, 2003]. When the regularization is not exact, we derive
	error bounds on the distance from the regularized solution to the
	original solution set. We also show that existence of a {\textquoteleft}{\textquoteleft}weak
	sharp minimum{\textquoteright}{\textquoteright} is in some sense
	close to being necessary for exact regularization. We illustrate
	the main result with numerical experiments on the l1 regularization
	of benchmark (degenerate) linear programs and semidefinite/second-order
	cone programs. The experiments demonstrate the usefulness of l1 regularization
	in finding sparse solutions.},
  optdoi = {10.1137/060675320},
  keywords = {SLIM,Optimization},
  url = {http://www.cs.ubc.ca/~mpf/index.php?q=cpreg.pdf}
}

@ARTICLE{herrmann2007GJInlp,
  author = {Felix J. Herrmann and U. Boeniger and D. J. Verschuur},
  title = {Non-linear primary-multiple separation with directional curvelet
	frames},
  journal = {Geophysical Journal International},
  year = {2007},
  optmonth = {08/2007},
  volume = {170},
  pages = {781-799},
  number = {2},
  abstract = {Predictive multiple suppression methods consist of two main steps:
	a prediction step, during which multiples are predicted from seismic
	data, and a primary-multiple separation step, during which the predicted
	multiples are {\textquoteright}matched{\textquoteright} with the
	true multiples in the data and subsequently removed. This second
	separation step, which we will call the estimation step, is crucial
	in practice: an incorrect separation will cause residual multiple
	energy in the result or may lead to a distortion of the primaries,
	or both. To reduce these adverse effects, a new transformed-domain
	method is proposed where primaries and multiples are separated rather
	than matched. This separation is carried out on the basis of differences
	in the multiscale and multidirectional characteristics of these two
	signal components. Our method uses the curvelet transform, which
	maps multidimensional data volumes into almost orthogonal localized
	multidimensional prototype waveforms that vary in directional and
	spatio-temporal content. Primaries-only and multiples-only signal
	components are recovered from the total data volume by a non-linear
	optimization scheme that is stable under noisy input data. During
	the optimization, the two signal components are separated by enhancing
	sparseness (through weighted l1-norms) in the transformed domain
	subject to fitting the observed data as the sum of the separated
	components to within a user-defined tolerance level. Whenever, during
	the optimization, the estimates for the primaries in the transformed
	domain correlate with the predictions for the multiples, the recovery
	of the coefficients for the estimated primaries will be suppressed
	while for regions where the correlation is small the method seeks
	the sparsest set of coefficients that represent the estimation for
	the primaries. Our algorithm does not seek a matched filter and as
	such it differs fundamentally from traditional adaptive subtraction
	methods. The method derives its stability from the sparseness obtained
	by a non-parametric (i.e. not depending on a parametrized physical
	model) multiscale and multidirectional overcomplete signal representation.
	This sparsity serves as prior information and allows for a Bayesian
	interpretation of our method during which the log-likelihood function
	is minimized while the two signal components are assumed to be given
	by a superposition of prototype waveforms, drawn independently from
	a probability function that is weighted by the predicted primaries
	and multiples. In this paper, the predictions are based on the data-driven
	surface-related multiple elimination method. Synthetic and field
	data examples show a clean separation leading to a considerable improvement
	in multiple suppression compared to the conventional method of adaptive
	matched filtering. This improved separation translates into an improved
	stack.},
  optdoi = {10.1111/j.1365-246X.2007.03360.x},
  keywords = {signal separation, SLIM,Processing},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysical Journal International/2007/herrmann07nlp/herrmann07nlp.pdf  }
}


@ARTICLE{lin2007GEOPcwe,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  optmonth = {08/2007},
  pages = {SM77-SM93},
  number = {5},
  abstract = {An explicit algorithm for the extrapolation of one-way wavefields
	is proposed that combines recent developments in information theory
	and theoretical signal processing with the physics of wave propagation.
	Because of excessive memory requirements, explicit formulations for
	wave propagation have proven to be a challenge in 3D. By using ideas
	from compressed sensing, we are able to formulate the (inverse) wavefield
	extrapolation problem on small subsets of the data volume, thereby
	reducing the size of the operators. Compressed sensing entails a
	new paradigm for signal recovery that provides conditions under which
	signals can be recovered from incomplete samplings by nonlinear recovery
	methods that promote sparsity of the to-be-recovered signal. According
	to this theory, signals can be successfully recovered when the measurement
	basis is incoherent with the representa-tion in which the wavefield
	is sparse. In this new approach, the eigenfunctions of the Helmholtz
	operator are recognized as a basis that is incoherent with curvelets
	that are known to compress seismic wavefields. By casting the wavefield
	extrapolation problem in this framework, wavefields can be successfully
	extrapolated in the modal domain, despite evanescent wave modes.
	The degree to which the wavefield can be recovered depends on the
	number of missing (evanescent) wavemodes and on the complexity of
	the wavefield. A proof of principle for the compressed sensing method
	is given for inverse wavefield extrapolation in 2D, together with
	a pathway to 3D during which the multiscale and multiangular properties
	of curvelets, in relation to the Helmholz operator, are exploited.
	The results show that our method is stable, has reduced dip limitations,
	and handles evanescent waves in inverse extrapolation. {\copyright}2007
	Society of Exploration Geophysicists},
  optdoi = {10.1190/1.2750716},
  keywords = {SLIM, wave propagation,Modelling},
  publisher = {SEG},
url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Geophysics/2007/lin07cwe/lin07cwe.pdf }
}

@ARTICLE{hennenfent2006CiSEsdn,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Seismic Denoising with Nonuniformly Sampled Curvelets},
  journal = {Computing in Science \& Engineering},
  year = {2006},
  volume = {8 Issue:3},
  optmonth = {05/2006},
  pages = {16 - 25},
  month = {May-June 2006},
  abstract = {The authors present an extension of the fast discrete curvelet transform
	(FDCT) to nonuniformly sampled data. This extension not only restores
	curvelet compression rates for nonuniformly sampled data but also
	removes noise and maps the data to a regular grid.},
  optdoi = {10.1109/MCSE.2006.49},
  keywords = {CiSE,Processing},
  url= {https://www.slim.eos.ubc.ca/Publications/Public/Journals/CiSE/2006/hennenfent06CiSEsdn/hennenfent06CiSEsdn.pdf } 
}

@ARTICLE{herrmann2005ICAEsdb,
  author = {Felix J. Herrmann},
  title = {Seismic deconvolution by atomic decomposition: a parametric approach
	with sparseness constraints},
  journal = {Integrated Computer-Aided Engineering},
  year = {2005},
  volume = {12},
  pages = {69-90},
  number = {1},
  optmonth = {01/2005},
  abstract = {In this paper an alternative approach to the blind seismic deconvolution
	problem is presented that aims for two goals namely recovering the
	location and relative strength of seismic reflectors, possibly with
	super-localization, as well as obtaining detailed parametric characterizations
	for the reflectors. We hope to accomplish these goals by decomposing
	seismic data into a redundant dictionary of parameterized waveforms
	designed to closely match the properties of reflection events associated
	with sedimentary records. In particular, our method allows for highly
	intermittent non-Gaussian records yielding a reflectivity that can
	no longer be described by a stationary random process or by a spike
	train. Instead, we propose a reflector parameterization that not
	only recovers the reflector{\textquoteright}s location and relative
	strength but which also captures reflector attributes such as its
	local scaling, sharpness and instantaneous phase-delay. The first
	set of parameters delineates the stratigraphy whereas the second
	provides information on the lithology. As a consequence of the redundant
	parameterization, finding the matching waveforms from the dictionary
	involves the solution of an ill-posed problem. Two complementary
	sparseness-imposing methods Matching and Basis Pursuit are compared
	for our dictionary and applied to seismic data.},
  address = {Amsterdam, The Netherlands, The Netherlands},
  issn = {1069-2509},
  keywords = {deconvolution, SLIM,Processing,Modelling},
  publisher = {IOS Press},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann05sdb.pdf}
}

@ARTICLE{bernabe2004JGRpas,
  author = {Y. Bernab{\'e} and U. Mok and B. Evans and Felix J. Herrmann},
  title = {Permeability and storativity of binary mixtures of high-and low-porosity
	materials},
  journal = {Journal of Geophysical Research},
  year = {2004},
  optmonth = {10/2004},
  volume = {109},
  pages = {B12207},
  abstract = {As a first step toward determining the mixing laws for the transport
	properties of rocks, we prepared binary mixtures of high- and low-permeability
	materials by isostatically hot-pressing mixtures of fine powders
	of calcite and quartz. The resulting rocks were marbles containing
	varying concentrations of dispersed quartz grains. Pores were present
	throughout the rock, but the largest ones were preferentially associated
	with the quartz particles, leading us to characterize the material
	as being composed of two phases, one with high permeability and the
	second with low permeability. We measured the permeability and storativity
	of these materials using the oscillating flow technique, while systematically
	varying the effective pressure and the period and amplitude of the
	input fluid oscillation. Control measurements performed using the
	steady state flow and pulse decay techniques agreed well with the
	oscillating flow tests. The hydraulic properties of the marbles were
	highly sensitive to the volume fraction of the high-permeability
	phase (directly related to the quartz content). Below a critical
	quartz content, slightly less than 20 wt \%, the high-permeability
	volume elements were disconnected, and the overall permeability was
	low. Above the critical quartz content the high-permeability volume
	elements formed throughgoing paths, and permeability increased sharply.
	We numerically simulated fluid flow through binary materials and
	found that permeability approximately obeys a percolation-based mixing
	law, consistent with the measured permeability of the calcite-quartz
	aggregates.},
  optdoi = {10.1029/2004JB00311},
  keywords = {permeability, porosity, SLIM, Modeling},
  url= { https://www.slim.eos.ubc.ca/Publications/Public/Journals/Journal of Geophysical Research/bernabe04JGRpas/bernabe04JGRpas.pdf}
}

@ARTICLE{herrmann2004GJIssa,
  author = {Felix J. Herrmann and Y. Bernab\'e },
  title = {Seismic singularities at upper-mantle phase transitions: a site percolation
	model},
  journal = {Geophysical Journal International},
  year = {2004},
  optmonth = {12/2004},
  volume = {159},
  pages = {949-960},
  number = {3},
  abstract = {Mineralogical phase transitions are usually invoked to account for
	the sharpness of globally observed upper-mantle seismic discontinuities.
	We propose a percolation-based model for the elastic properties of
	the phase mixture in the coexistence regions associated with these
	transitions. The major consequence of the model is that the elastic
	moduli (but not the density) display a singularity at the percolation
	threshold of the high-pressure phase. This model not only explains
	the sharp but continuous change in seismic velocities across the
	phase transition, but also predicts its abruptness and scale invariance,
	which are characterized by a non-integral scale exponent. Using the
	receiver-function approach and new, powerful signal-processing techniques,
	we quantitatively determine the singularity exponent from recordings
	of converted seismic waves at two Australian stations (CAN and WRAB).
	Using the estimated values, we construct velocity{\textendash}depth
	profiles across the singularities and verify that the calculated
	converted waveforms match the observations under CAN. Finally, we
	point out a series of additional predictions that may provide new
	insights into the physics and fine structure of the upper-mantle
	transition zone.},
  optdoi = {10.1111/j.1365-246X.2004.02464.x},
  keywords = {percolation, SLIM,Modelling},
  url = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann04ssa.pdf}
}
% This file was created with JabRef 2.6.
% Encoding: MacRoman


@MANUAL{hennenfent08MNrap,
  title = {Repro: a Python package for automating reproducible research
	in scientific computing},
  author = {Gilles Hennenfent and Sean Ross-Ross},
  optmonth = {08/2008},
  year = {2008},
  abstract = {repro is a Python package for automating reproducible research in
	scientific computing. Repro works in combination with SCons, a next-generation
	build tool. The package is freely available over the Internet. Downloading
	and installation instructions are provided in this gui de. The repro
	package is documented in various ways (many comments in source code,
	this guide{\textendash}-written using repro itself!{\textendash}-and
	a reference guide ). In this user{\textquoteright}s guide, we present
	a few pedagogical examples that uses Matlab, Python, Seismic Unix
	(SU), and Madagascar. We also include demo pa pers. These papers
	are written in LaTeX and compiled using repro. The figures they
	contain are automatically generated from the source codes prov ided.
	In that sense, the demo papers are a model of self-contained documents
	that are fully reproducible. The repro package is largely inspired
	by some parts of Madagascar, a geophysical software package for reproducible
	research. However, the repro package is intended for a broad audience
	co ming from a wide spectrum of interest areas.},
  keywords = {SLIM},
  url = {http://repro.sourceforge.net/Site/Home.html}
}

@MANUAL{rossross08MNsai,
  title = {{SLIMPy: a python interface for unix-pipe based coordinate-free
	scientific computing}},
  author = {Sean Ross-Ross and Henryk Modzelewski and Felix J. Herrmann},
  optmonth = {07/2008},
  year = {2008},
  abstract = {SLIMpy is a Python interface that exposes the functionality of seismic
	data processing packages, such as MADAGASCAR, through oper ator overloading.
	SLIMpy provides a concrete coordinate-free implementation of classes
	for out-of-core linear (implicit matrix-vector), and element -wise
	operations, including calculation of norms and other basic vector
	operations. The library is intended to provide the user with an abstract
	sc ripting language to program iterative algorithms from numerical
	linear algebra. These algorithms require repeated evaluation of operators
	that were initially designed to be run as part of batch-oriented
	processing flows. The current implementation supports a plugin for
	Madagascar{\textquoteright}s out-of-core UNIX pipe-based applications
	and is extenable to pipe-based collections of programs such as Seismic
	Un*x, SEPLib, and FreeUSP. To optimize perform ance, SLIMpy uses
	an Abstract Syntax Tree that parses the algorithm and optimizes the
	pipes.},
  url = {http://slim.eos.ubc.ca/SLIMpy/}
}

@MANUAL{rossross07MNsda,
  title = {{SLIMpy} development and programming interface for seismic processing},
  author = {Sean Ross-Ross and Henryk Modzelewski and Cody R. Brown and Felix
	J. Herrmann},
  year = {2007},
  abstract = {Inverse problems in (exploration) seismology are known for their large
	to very large scale. For instance, certain sparsity-promoting inversion
	techniques involve vectors that easily exceed unknowns while seismic
	imaging involves the construction and application of matrix-free
	discretized operators where single matrix-vector evaluations may
	require hours, days or even weeks on large compute clusters. For
	these reasons, software development in this field has remained the
	domain of highly technical codes programmed in low-level languages
	with little eye for easy development, code reuse and integration
	with (nonlinear) programs that solve inverse problems.Following ideas
	from the Symes{\textquoteright} Rice Vector Library and Bartlett{\textquoteright}s
	C++ object-oriented interface, Thyra, and Reduction/Transformation
	operators (both part of the Trilinos software package), we developed
	a software-development environment based on overloading. This environment
	provides a pathway from in-core prototype development to out-of-core
	and MPI {\textquoteright}production{\textquoteright} code with a
	high level of code reuse. This code reuse is accomplished by integrating
	the out-of-core and MPI functionality into the dynamic object-oriented
	programming language Python. This integration is implemented through
	operator overloading and allows for the development of a coordinate-free
	solver framework that (i) promotes code reuse; (ii) analyses the
	statements in an abstract syntax tree and (iii) generates executable
	statements. In the current implementation, we developed an interface
	to generate executable statements for the out-of-core unix-pipe based
	(seismic) processing package RSF-Madagascar (rsf.sf.net). The modular
	design allows for interfaces to other seismic processing packages
	and to in-core Python packages such as numpy. So far, the implementation
	overloads linear operators and element-wise reduction/transformation
	operators. We are planning extensions towards nonlinear operators
	and integration with existing (parallel) solver frameworks such as
	Trilinos.},
  keywords = {SLIM, software},
  url = {http://slim.eos.ubc.ca/SLIMpy}
}

@MANUAL{vandenberg07MNsat,
  title = {SPARCO: A toolbox for testing sparse reconstruction algorithms},
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  optmonth = {10/2007},
  year = {2007},
  abstract = {Sparco is a suite of problems for testing and benchmarking algorithms
	for sparse signal reconstruction. It is also an environment for creating
	new test problems, and a suite of standard linear operators is provided
	from which new problems can be assembled. Sparco is implement ed
	entirely in Matlab and is self contained. (A few optional test problems
	are based on the CurveLab toolbox, which can be installed separately.)
	At the core of the sparse recovery problem is the linear system $Ax+r=b$,
	where $A$ is an $m$-by-$n$ linear operator and the $m$-vector $b$
	is the observed signal. The goal is to find a sparse $n$-vector $x$
	such that $r$ is small in norm.},
  keywords = {SLIM},
  url = {http://www.cs.ubc.ca/labs/scl/sparco/}
}



@PHDTHESIS{moghaddam10phd,
  author = {Peyman P. Moghaddam},
  title = {Curvelet-Based Migration Amplitude Recovery},
  school = {The University of British Columbia},
  year = {2010},
  type = {phd},
  address = {Vancouver, BC Canada},
  optmonth = {05/2010},
  abstract = {Migration can accurately locate reflectors in the earth but in most
	cases fails to correctly resolve their amplitude. This might lead
	to mis-interpretation of the nature of reflector. In this thesis,
	I introduced a method to accurately recover the amplitude of the
	seismic reflector. This method relies on a new transform-based recovery
	that exploits the expression of seismic images by the recently developed
	curvelet transform. The elements of this transform, called curvelets,
	are multi-dimensional, multi-scale, and multi-directional. They also
	remain approximately invariant under the imaging operator. I exploit
	these properties of the curvelets to introduce a method called Curvelet
	Match Filtering (CMF) for recovering the seismic amplitude in presence
	of noise in both migrated image and data. I detail the method and
	illustrate its performance on synthetic dataset. I also extend CMF
	formulation to other geophysical applications and present results
	on multiple removal. In addition of that, I investigate preconditioning
	of the migration which results to rapid convergence rate of the iterative
	method using migration.},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2010/moghaddam10phd.pdf}
}


@PHDTHESIS{hennenfent08phd,
  author = {Gilles Hennenfent},
  title = {Sampling and reconstruction of seismic wavefields in the curvelet
	domain},
  school = {The University of British Columbia},
  year = {2008},
  type = {phd},
  address = {Vancouver, BC Canada},
  optmonth = {05/2008},
  abstract = {Wavefield reconstruction is a crucial step in the seismic processing
	flow. For instance, unsuccessful interpolation leads to erroneous
	multiple predictions that adversely affect the performance of multiple
	elimination, and to imaging artifacts. We present a new non-parametric
	transform-based reconstruction method that exploits the compression
	of seismic data by the recently developed curvelet transform. The
	elements of this transform, called curvelets, are multi-dimensional,
	multi-scale, and multi-directional. They locally resemble wavefronts
	present in the data, which leads to a compressible representation
	for seismic data. This compression enables us to formulate a new
	curvelet-based seismic data recovery algorithm through sparsity-promoting
	inversion (CRSI). The concept of sparsity-promoting inversion is
	in itself not new to geophysics. However, the recent insights from
	the field of {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright}
	are new since they clearly identify the three main ingredients that
	go into a successful formulation of a reconstruction problem, namely
	a sparsifying transform, a sub-Nyquist sampling strategy that subdues
	coherent aliases in the sparsifying domain, and a data-consistent
	sparsity-promoting program. After a brief overview of the curvelet
	transform and our seismic-oriented extension to the fast discrete
	curvelet transform, we detail the CRSI formulation and illustrate
	its performance on synthetic and real datasets. Then, we introduce
	a sub-Nyquist sampling scheme, termed jittered undersampling, and
	show that, for the same amount of data acquired, jittered data are
	best interpolated using CRSI compared to regular or random undersampled
	data. We also discuss the large-scale one-norm solver involved in
	CRSI. Finally, we extend CRSI formulation to other geophysical applications
	and present results on multiple removal and migration-amplitude recovery.},
  keywords = {curvelet transform, reconstruction, SLIM},
  url = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/hennenfent08phd.pdf}
}

% This file was created with JabRef 2.6.
% Encoding: MacRoman

@CONFERENCE{erlangga08SINBADimf,
  author = {Yogi A. Erlangga and K. Vuik and K. Oosterlee and D. Riyanti and
        R. Nabben},
  title = {Iterative methods for 2D/3D Helmholtz operator},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present an iterative method for solving the 2D/3D Helmholtz equation.
        The method is mainly based on a Krylov method, preconditioned by
        a special operator which represents a damped Helmholtz operator.
        The discretization of the preconditioning operator is then solved
        by one multigrid sweep. It can be shown that while the spectrum is
        bounded above by one, the smallest eigenvalue of the preconditioned
        system is of order $k^{-1}$. In this situation, the convergence of
        a Krylov method will be proportional to the frequency of the problem.
        Further convergence acceleration can be achieved if eigenvalues of
        order $k^{-1}$ are projected from the spectrum. This can be done
        by a projection operator, similar to but more stable than deflation.
        This projection operator has been the core of a new multilevel method,
        called multilevel Krylov method, proposed by Erlangga and Nabben
        only recently. Putting the preconditioned Helmholtz operator in this
        setting, a convergence which is independent of frequency can be obtained.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Erlangga_Ite.pdf}}


@ARTICLE{berkhout97eom,
  author = {A. J. Berkhout and D. J. Verschuur},
  title = {Estimation of multiple scattering by iterative inversion, Part {I}:
	Theoretical considerations},
  journal = {Geophysics},
  year = {1997},
  volume = {62},
  pages = {1586-1595},
  number = {5},
  abstract = {A review has been given of the surface-related multiple problem by
	making use of the so-called feedback model. From the resulting equations
	it has been concluded that the proposed solution does not require
	any properties of the subsurface. However, source-detector and reflectivity
	properties of the surface need be specified. Those properties have
	been quantified in a surface operator and this operator is estimated
	as part of the multiple removal problem. The surface-related multiple
	removal algorithm has been formulated in terms of a Neumann series
	and in terms of an iterative equation. The Neumann formulation requires
	a nonlinear optimization process for the surface operator; while
	the iterative formulation needs a number of linear optimizations.
	The iterative formulation also has the advantage that it can be integrated
	easily with another multiple removal method. An algorithm for the
	removal of internal multiples has been proposed as well. This algorithm
	is an extension of the surface-related method. Removal of internal
	multiples requires knowledge of the macro velocity model between
	the surface and the upper boundary of the multiple generating layer.
	In part II (also published in this issue) the success of the proposed
	algorithms has been demonstrated on numerical experiments and field
	data examples. {\copyright}1997 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/62/1586/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444261},
  date-added = {2008-05-07 18:38:50 -0700},
  date-modified = {2008-08-14 14:46:15 -0700},
  doi = {10.1190/1.1444261},
  issue = {5},
  keywords = {SRME},
  pdf = {http://link.aip.org/link/?GPY/62/1586/1},
  publisher = {SEG},
  url = {http://link.aip.org/link/?GPY/62/1586/1}
}

@BOOK{biondo063ds,
  title = {3-{D} seismic imaging},
  publisher = {SEG},
  year = {2006},
  author = {B. L. Biondi},
  number = {14},
  series = {Investigations in Geophysics},
  date-added = {2008-05-08 15:25:18 -0700},
  date-modified = {2008-05-20 19:45:00 -0700},
  issue = {14},
  keywords = {imaging}
}

@ARTICLE{cordoba78wpa,
  author = {A. C\'ordoba and C. Fefferman},
  title = {Wave packets and {F}ourier integral operators},
  journal = {Communications in Partial Differential Equations},
  year = {1978},
  volume = {3},
  pages = {979-1005},
  number = {11},
  bdsk-url-1 = {http://dx.doi.org/10.1080/03605307808820083},
  date-added = {2008-05-07 11:53:23 -0700},
  date-modified = {2008-05-20 11:48:08 -0700},
  doi = {10.1080/03605307808820083},
  issue = {11},
  keywords = {wave packets, FIO},
  publisher = {Taylor \& Francis}
}

@PHDTHESIS{candes98rta,
  author = {E. J. Cand\`es},
  title = {Ridgelets: theory and applications},
  school = {Stanford University},
  year = {1998},
  address = {Stanford, CA},
  bdsk-url-1 = {http://www.acm.caltech.edu/~emmanuel/papers/Thesis.ps.gz},
  date-added = {2008-05-27 18:24:11 -0700},
  date-modified = {2008-05-27 18:26:14 -0700},
  keywords = {ridgelet transform}
}

@ARTICLE{candes05tcr,
  author = {E. J. Cand\`es and L. Demanet},
  title = {The curvelet representation of wave propagators is optimally sparse},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2005},
  volume = {58},
  pages = {1472-1528},
  number = {11},
  abstract = {This paper argues that curvelets provide a powerful tool for representing
	very general linear symmetric systems of hyperbolic differential
	equations. Curvelets are a recently developed multiscale system [10,
	7] in which the elements are highly anisotropic at fine scales, with
	effective support shaped according to the parabolic scaling principle
	width ≈ length^2 at fine scales. We prove that for a wide class
	of linear hyperbolic differential equations, the curvelet representation
	of the solution operator is both optimally sparse and well organized.
	* It is sparse in the sense that the matrix entries decay nearly
	exponentially fast (i.e. faster than any negative polynomial), *
	and well-organized in the sense that the very few nonnegligible entries
	occur near a few shifted diagonals. Indeed, we show that the wave
	group maps each curvelet onto a sum of curvelet-like waveforms whose
	locations and orientations are obtained by following the different
	Hamiltonian flows---hence the diagonal shifts in the curvelet representation.
	A physical interpretation of this result is that curvelets may be
	viewed as coherent waveforms with enough frequency localization so
	that they behave like waves but at the same time, with enough spatial
	localization so that they simultaneously behave like particles. },
  bdsk-url-1 = {http://www.acm.caltech.edu/~emmanuel/papers/CurveletsWaves.pdf},
  date-added = {2008-05-07 11:10:43 -0700},
  date-modified = {2008-08-14 14:57:23 -0700},
  doi = {10.1002/cpa.20078},
  issue = {11},
  keywords = {curvelet transform, FIO},
  pdf = {http://www.acm.caltech.edu/~emmanuel/papers/CurveletsWaves.pdf}
}

@ARTICLE{candes06fdc,
  author = {E. J. Cand\`es and L. Demanet and D. L. Donoho and L. Ying},
  title = {Fast discrete curvelet transforms},
  journal = {Multiscale Modeling and Simulation},
  year = {2006},
  volume = {5},
  pages = {861-899},
  number = {3},
  abstract = {This paper describes two digital implementations of a new mathematical
	transform, namely, the second generation curvelet transform [12,
	10] in two and three dimensions. The first digital transformation
	is based on unequally-spaced fast Fourier transforms (USFFT) while
	the second is based on the wrapping of specially selected Fourier
	samples. The two implementations essentially differ by the choice
	of spatial grid used to translate curvelets at each scale and angle.
	Both digital transformations return a table of digital curvelet coefficients
	indexed by a scale parameter, an orientation parameter, and a spatial
	location parameter. And both implementations are fast in the sense
	that they run in O(n^2 log n) flops for n by n Cartesian arrays;
	in addition, they are also invertible, with rapid inversion algorithms
	of about the same complexity. Our digital transformations improve
	upon earlier implementations---based upon the first generation of
	curvelets---in the sense that they are conceptually simpler, faster
	and far less redundant. The software CurveLab, which implements both
	transforms presented in this paper, is available at http://www.curvelet.org.
	},
  bdsk-url-1 = {http://dx.doi.org/10.1137/05064182X},
  bdsk-url-2 = {http://www.acm.caltech.edu/~emmanuel/papers/FDCT.pdf},
  date-added = {2008-05-06 19:34:41 -0700},
  date-modified = {2008-08-14 14:58:30 -0700},
  doi = {10.1137/05064182X},
  issue = {3},
  keywords = {curvelet transform},
  pdf = {http://www.acm.caltech.edu/~emmanuel/papers/FDCT.pdf},
  publisher = {SIAM}
}

@INCOLLECTION{candes00cas,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {Curvelets: a surprisingly effective nonadaptive representation of
	objects with edges},
  booktitle = {Curve and surface fitting},
  publisher = {Vanderbilt University Press},
  year = {2000},
  editor = {A. Cohen, C. Rahut, and L. L. Schumaker},
  pages = {105-120},
  address = {Nashville, TN},
  abstract = {It is widely believed that to efficiently represent an otherwise smooth
	ob ject with discontinuities along edges, one must use an adaptive
	representation that in some sense `tracks' the shape of the discontinuity
	set. This folk-belief --- some would say folk-theorem --- is incorrect.
	At the very least, the possible quantitative advantage of such adaptation
	is vastly smaller than commonly believed. We have recently constructed
	a tight frame of curvelets which provides stable, efficient, and
	near-optimal representation of otherwise smooth ob jects having discontinuities
	along smooth curves. By applying naive thresholding to the curvelet
	transform of such an ob ject, one can form m-term approximations
	with rate of L2 approximation rivaling the rate obtainable by complex
	adaptive schemes which attempt to `track' the discontinuity set.
	In this article we explain the basic issues of efficient m-term approximation,
	the construction of efficient adaptive representation, the construction
	of the curvelet frame, and a crude analysis of the performance of
	curvelet schemes. },
  bdsk-url-1 = {http://www.acm.caltech.edu/~emmanuel/papers/Curvelet-SMStyle.pdf},
  date-added = {2008-05-26 17:48:55 -0700},
  date-modified = {2008-08-14 15:26:58 -0700},
  keywords = {curvelet transform}
}

@ARTICLE{candes05cct,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {Continuous curvelet transform: {I.} Resolution of the wavefront set},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2005},
  volume = {19},
  pages = {162-197},
  number = {2},
  month = {September},
  bdsk-url-1 = {http://dx.doi.org/10.1016/j.acha.2005.02.003},
  date-added = {2008-05-26 18:21:22 -0700},
  date-modified = {2008-05-26 18:23:57 -0700},
  issue = {2},
  keywords = {curvelet transform}
}

@ARTICLE{candes05cct1,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {Continuous curvelet transform: {II.} Discretization and frames},
  journal = {Applied and Computational Harmonic Analysis},
  year = {2005},
  volume = {19},
  pages = {198-222},
  number = {2},
  month = {September},
  bdsk-url-1 = {http://dx.doi.org/10.1016/j.acha.2005.02.004},
  date-added = {2008-05-26 18:23:17 -0700},
  date-modified = {2008-05-26 18:24:36 -0700},
  issue = {2},
  keywords = {curvelet transform}
}

@ARTICLE{candes04ntf,
  author = {E. J. Cand\`es and D. L. Donoho},
  title = {New tight frames of curvelets and optimal representations of objects
	with piecewise-{C}$^2$ singularities},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2004},
  volume = {57},
  pages = {219-266},
  number = {2},
  bdsk-url-1 = {http://dx.doi.org/10.1002/cpa.10116},
  bdsk-url-2 = {http://www.acm.caltech.edu/~emmanuel/papers/CurveEdges.pdf},
  date-added = {2008-05-07 11:47:59 -0700},
  date-modified = {2008-08-14 14:46:59 -0700},
  doi = {10.1002/cpa.10116},
  issue = {2},
  keywords = {curvelet transform},
  pdf = {http://www.acm.caltech.edu/~emmanuel/papers/CurveEdges.pdf}
}

@ARTICLE{chauris08sdm,
  author = {H. Chauris and T. Nguyen},
  title = {Seismic demigration/migration in the curvelet domain},
  journal = {Geophysics},
  year = {2008},
  volume = {73},
  pages = {S35-S46},
  number = {2},
  abstract = {Curvelets can represent local plane waves. They efficiently decompose
	seismic images and possibly imaging operators. We study how curvelets
	are distorted after demigration followed by migration in a different
	velocity model. We show that for small local velocity perturbations,
	the demigration/migration is reduced to a simple morphing of the
	initial curvelet. The derivation of the expected curvature of the
	curvelets shows that it is easier to sparsify the demigration/migration
	operator than the migration operator. An application on a 2D synthetic
	data set, generated in a smooth heterogeneous velocity model and
	with a complex reflectivity, demonstrates the usefulness of curvelets
	to predict what a migrated image would become in a locally different
	velocity model without the need for remigrating the full input data
	set. Curvelets are thus well suited to study the sensitivity of a
	prestack depth-migrated image with respect to the heterogeneous velocity
	model used for migration. {\copyright}2008 Society of Exploration
	Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/73/S35/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.2831933},
  date-added = {2008-05-07 14:48:33 -0700},
  date-modified = {2008-08-14 14:59:04 -0700},
  doi = {10.1190/1.2831933},
  issue = {2},
  keywords = {curvelet transform, imaging},
  pdf = {http://link.aip.org/link/?GPY/73/S35/1},
  publisher = {SEG}
}

@BOOK{claerbout92esa,
  title = {Earth soundings analysis: processing versus inversion},
  publisher = {Blackwell Scientific Publications},
  year = {1992},
  author = {J. F. Claerbout},
  address = {Boston},
  bdsk-url-1 = {http://sepwww.stanford.edu/sep/prof/pvi.pdf},
  date-added = {2008-05-06 19:27:28 -0700},
  date-modified = {2008-05-07 11:44:19 -0700},
  keywords = {PEF},
  pdf = {http://sepwww.stanford.edu/sep/prof/pvi.pdf}
}

@ARTICLE{claerbout71tau,
  author = {J. F. Claerbout},
  title = {Toward a unified theory of reflector mapping},
  journal = {Geophysics},
  year = {1971},
  volume = {36},
  pages = {467-481},
  number = {3},
  abstract = {Schemes for seismic mapping of reflectors in the presence of an arbitrary
	velocity model, dipping and curved reflectors, diffractions, ghosts,
	surface elevation variations, and multiple reflections are reviewed
	and reduced to a single formula involving up and downgoing waves.
	The mapping formula may be implemented without undue complexity by
	means of difference approximations to the relativistic Schroedinger
	equation. {\copyright}1971 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/36/467/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1440185},
  date-added = {2008-05-08 14:59:36 -0700},
  date-modified = {2008-08-14 14:59:35 -0700},
  doi = {10.1190/1.1440185},
  issue = {3},
  keywords = {WEM, imaging},
  pdf = {http://link.aip.org/link/?GPY/36/467/1},
  publisher = {SEG}
}

@ARTICLE{daubechies04ait,
  author = {I. Daubechies and M. Defrise and C. {De Mol}},
  title = {An iterative thresholding algorithm for linear inverse problems with
	a sparsity constraint},
  journal = {Communications on Pure and Applied Mathematics},
  year = {2004},
  volume = {57},
  pages = {1413-1457},
  number = {11},
  abstract = {We consider linear inverse problems where the solution is assumed
	to have a sparse expansion on an arbitrary preassigned orthonormal
	basis. We prove that replacing the usual quadratic regularizing penalties
	by weighted p-penalties on the coefficients of such expansions, with
	1 p 2, still regularizes the problem. Use of such p-penalized problems
	with p < 2 is often advocated when one expects the underlying ideal
	noiseless solution to have a sparse expansion with respect to the
	basis under consideration. To compute the corresponding regularized
	solutions, we analyze an iterative algorithm that amounts to a Landweber
	iteration with thresholding (or nonlinear shrinkage) applied at each
	iteration step. We prove that this algorithm converges in norm. {\copyright}
	2004 Wiley Periodicals, Inc.},
  bdsk-url-1 = {http://dx.doi.org/10.1002/cpa.20042},
  date-added = {2008-05-20 13:58:17 -0700},
  date-modified = {2008-08-14 15:01:17 -0700},
  issue = {11},
  pdf = {http://dx.doi.org/10.1002/cpa.20042},
  refer1 = {10.1002/cpa.20042}
}

@ARTICLE{do2002can,
  author = {M. N. Do and M. Vetterli},
  title = {Contourlets: a new directional multiresolution image representation},
  journal = {Proceedings. 2002 International Conference on Image Processing.},
  year = {2002},
  volume = {1},
  abstract = {We propose a new scheme, named contourlet, that provides a flexible
	multiresolution, local and directional image expansion. The contourlet
	transform is realized efficiently via a double iterated filter bank
	structure. Furthermore, it can be designed to satisfy the anisotropy
	scaling relation for curves, and thus offers a fast and structured
	curvelet-like decomposition. As a result, the contourlet transform
	provides a sparse representation for two-dimensional piecewise smooth
	signals resembling images. Finally, we show some numerical experiments
	demonstrating the potential of contourlets in several image processing
	tasks.},
  bdsk-url-1 = {http://dx.doi.org/10.1109/ICIP.2002.1038034},
  date-added = {2008-05-07 11:58:00 -0700},
  date-modified = {2008-08-14 15:01:55 -0700},
  doi = {10.1109/ICIP.2002.1038034},
  keywords = {contourlet transform}
}

@TECHREPORT{donoho99dct,
  author = {D. L. Donoho and M. R. Duncan},
  title = {Digital curvelet transform: strategy, implementation, and experiments},
  institution = {Stanford Statistics Department},
  year = {1999},
  month = {November},
  bdsk-url-1 = {http://citeseer.ist.psu.edu/rd/44392127,300178,1,0.25,Download/http://citeseer.ist.psu.edu/cache/papers/cs/15527/http:zSzzSzwww-stat.stanford.eduzSz~donohozSzReportszSz1999zSzDCvT.pdf/donoho99digital.pdf},
  date-added = {2008-05-26 17:33:51 -0700},
  date-modified = {2008-05-26 17:35:32 -0700},
  keywords = {curvelet transform}
}

@ARTICLE{douma07los,
  author = {H. Douma and M. V. de Hoop},
  title = {Leading-order seismic imaging using curvelets},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  pages = {S231-S248},
  number = {6},
  abstract = {Curvelets are plausible candidates for simultaneous compression of
	seismic data, their images, and the imaging operator itself. We show
	that with curvelets, the leading-order approximation (in angular
	frequency, horizontal wavenumber, and migrated location) to common-offset
	(CO) Kirchhoff depth migration becomes a simple transformation of
	coordinates of curvelets in the data, combined with amplitude scaling.
	This transformation is calculated using map migration, which employs
	the local slopes from the curvelet decomposition of the data. Because
	the data can be compressed using curvelets, the transformation needs
	to be calculated for relatively few curvelets only. Numerical examples
	for homogeneous media show that using the leading-order approximation
	only provides a good approximation to CO migration for moderate propagation
	times. As the traveltime increases and rays diverge beyond the spatial
	support of a curvelet; however, the leading-order approximation is
	no longer accurate enough. This shows the need for correction beyond
	leading order, even for homogeneous media. {\copyright}2007 Society
	of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/72/S231/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.2785047},
  date-added = {2008-05-07 14:35:47 -0700},
  date-modified = {2008-08-14 15:02:25 -0700},
  doi = {10.1190/1.2785047},
  issue = {6},
  keywords = {curvelet transform, imaging},
  pdf = {http://link.aip.org/link/?GPY/72/S231/1},
  publisher = {SEG}
}

@INCOLLECTION{feichtinger94tap,
  author = {H. G. Feichtinger and K. Grochenig},
  title = {Theory and practice of irregular sampling},
  booktitle = {Wavelets: mathematics and applications},
  publisher = {CRC Press},
  year = {1994},
  editor = {J. J. Benedetto and M. Frazier},
  series = {Studies in Advanced Mathematics},
  chapter = {8},
  pages = {305-363},
  address = {Boca Raton, FL},
  bdsk-url-1 = {http://www.univie.ac.at/nuhag-php/bibtex/open_files/fegr94_fgthpra.pdf},
  date-added = {2008-05-20 17:10:18 -0700},
  date-modified = {2008-05-20 17:24:38 -0700},
  keywords = {sampling},
  pdf = {http://www.univie.ac.at/nuhag-php/bibtex/open_files/fegr94_fgthpra.pdf}
}

@MISC{fenelon08msc,
  author = {Lloyd Fenelon},
  title = {Nonequispaced discrete curvelet transform for seismic data reconstruction},
  howpublished = {BSc thesis, Ecole Nationale Superieure De Physique de Strasbourg},
  month = {August},
  year = {2008},
  abstract = {Physical constraints during seismic acquisitions lead to incomplete
	seismic datasets. Curvelet Reconstruction with Sparsity promoting
	Inversion (CRSI) is one of the most efficient interpolation method
	available to recover complete datasets from data with missing traces.
	The method uses in its definition the curvelet transform which is
	well suited to process seismic data. However, its main shortcoming
	is to not be able to provide an accurate result if the data are acquired
	at irregular positions. This come from the curvelet transform implementation
	which cannot handle this type of data. In this thesis the implementation
	of the curvelet transform is modified to offer the possibility to
	CRSI to give better representation of seismic data for high quality
	seismic imaging. },
  bdsk-url-1 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/fenelon08msc.pdf},
  date-added = {2008-09-03 16:18:08 -0700},
  date-modified = {2008-09-03 16:25:10 -0700},
  keywords = {SLIM, BSc},
  pdf = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/fenelon08msc.pdf}
}

@MISC{fomel07mos,
  author = {S. Fomel and P. Sava},
  title = {{MADAGASCAR}: open-source software package for geophysical data processing
	and reproducible numerical experiments},
  year = {2007},
  abstract = {is an open-source software package for geophysical data analysis and
	reproducible numerical experiments. Its mission is to provide -a
	convenient and powerful environment -a convenient technology transfer
	tool for researchers working with digital image and data processing.
	The technology developed using the Madagascar project management
	system is transferred in the form of recorded processing histories,
	which become "computational recipes" to be verified, exchanged, and
	modified by users of the system.},
  bdsk-url-1 = {http://rsf.sf.net},
  date-added = {2008-06-26 15:31:10 -0700},
  date-modified = {2008-08-14 15:31:44 -0700},
  keywords = {software},
  url = {http://rsf.sf.net}
}

@ARTICLE{guo07osm,
  author = {K. Guo and D. Labate},
  title = {Optimally sparse multidimensional representation using shearlets},
  journal = {Journal of Mathematical Analysis},
  year = {2007},
  volume = {39},
  pages = {298-318},
  number = {1},
  bdsk-url-1 = {http://www4.ncsu.edu/~dlabate/shear_GL.pdf},
  bdsk-url-2 = {http://dx.doi.org/10.1137/060649781},
  date-added = {2008-05-07 12:03:03 -0700},
  date-modified = {2008-05-08 10:28:30 -0700},
  doi = {10.1137/060649781},
  issue = {1},
  keywords = {shearlet transform},
  pdf = {http://www4.ncsu.edu/~dlabate/shear_GL.pdf},
  publisher = {SIAM}
}

@ARTICLE{hampson86ivs,
  author = {D. Hampson},
  title = {Inverse Velocity Stacking for Multiple Elimination},
  journal = {Journal of the Canadian Society of Exploration Geophysicists},
  year = {1986},
  volume = {22},
  pages = {44-45},
  number = {1},
  bdsk-url-1 = {http://www.cseg.ca/publications/journal/1986_12/1986_Hampson_D_inverse_velocity_stacking.pdf},
  date-added = {2008-05-06 19:09:45 -0700},
  date-modified = {2008-05-07 11:44:52 -0700},
  issue = {1},
  keywords = {Radon transform},
  pdf = {http://www.cseg.ca/publications/journal/1986_12/1986_Hampson_D_inverse_velocity_stacking.pdf},
  publisher = {CSEG}
}

@ARTICLE{hindriks00ro3,
  author = {K. Hindriks and A. J. W. Duijndam},
  title = {Reconstruction of {3-D} seismic signals irregularly sampled along
	two spatial coordinates},
  journal = {Geophysics},
  year = {2000},
  volume = {65},
  pages = {253-263},
  number = {1},
  abstract = {Seismic signals are often irregularly sampled along spatial coordinates,
	leading to suboptimal processing and imaging results. Least-squares
	estimation of Fourier components is used for the reconstruction of
	band-limited seismic signals that are irregularly sampled along two
	spatial coordinates. A simple and efficient diagonal weighting scheme,
	based on the areas surrounding the spatial samples, takes the properties
	of the noise (signal outside the bandwidth) into account in an approximate
	sense. Diagonal stabilization based on the energies of the signal
	and the noise ensures robust estimation. Reconstruction by temporal
	frequency component allows the specification of varying bandwidth
	in two dimensions, depending on the minimum apparent velocity. This
	parameterization improves the reconstruction capability for lower
	temporal frequencies. The shape of the spatial aperture affects the
	method of sampling the Fourier domain. Taking into account this property,
	a larger bandwidth can be recovered. The properties of the least-squares
	estimator allow a very efficient implementation which, when using
	a conjugate gradient algorithm, requires a modest number of 2-D fast
	Fourier transforms per temporal frequency. The method shows signicant
	improvement over the conventionally used binning and stacking method
	on both synthetic and real data. The method can be applied to any
	subset of seismic data with two varying spatial coordinates. {\copyright}2000
	Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/65/253/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444716},
  date-added = {2008-05-20 16:12:37 -0700},
  date-modified = {2008-08-14 15:05:01 -0700},
  doi = {10.1190/1.1444716},
  issue = {1},
  keywords = {reconstruction},
  pdf = {http://link.aip.org/link/?GPY/65/253/1},
  publisher = {SEG}
}

@PHDTHESIS{kunis06nff,
  author = {S. Kunis},
  title = {Nonequispaced {FFT}: generalisation and inversion},
  school = {L\"ubeck university},
  year = {2006},
  bdsk-url-1 = {http://www-user.tu-chemnitz.de/~skunis/paper/KunisDiss.pdf},
  date-added = {2008-05-07 18:51:16 -0700},
  date-modified = {2008-05-20 11:49:04 -0700},
  keywords = {NFFT},
  pdf = {http://www-user.tu-chemnitz.de/~skunis/paper/KunisDiss.pdf}
}

@ARTICLE{lu07mdf,
  author = {Y. M. Lu and M. N. Do},
  title = {Multidimensional directional filter banks and surfacelets},
  journal = {IEEE Transactions on Image Processing},
  year = {2007},
  volume = {16},
  pages = {918-931},
  number = {4},
  month = {April},
  abstract = {In 1992, Bamberger and Smith proposed the directional filter bank
	(DFB) for an efficient directional decomposition of 2-D signals.
	Due to the nonseparable nature of the system, extending the DFB to
	higher dimensions while still retaining its attractive features is
	a challenging and previously unsolved problem. We propose a new family
	of filter banks, named NDFB, that can achieve the directional decomposition
	of arbitrary N-dimensional (Nges2) signals with a simple and efficient
	tree-structured construction. In 3-D, the ideal passbands of the
	proposed NDFB are rectangular-based pyramids radiating out from the
	origin at different orientations and tiling the entire frequency
	space. The proposed NDFB achieves perfect reconstruction via an iterated
	filter bank with a redundancy factor of N in N-D. The angular resolution
	of the proposed NDFB can be iteratively refined by invoking more
	levels of decomposition through a simple expansion rule. By combining
	the NDFB with a new multiscale pyramid, we propose the surfacelet
	transform, which can be used to efficiently capture and represent
	surface-like singularities in multidimensional data},
  bdsk-url-1 = {http://dx.doi.org/10.1109/TIP.2007.891785},
  date-added = {2008-05-07 12:19:48 -0700},
  date-modified = {2008-08-14 15:05:31 -0700},
  doi = {10.1109/TIP.2007.891785},
  issn = {1057-7149},
  issue = {4},
  keywords = {surfacelet transform},
  publisher = {IEEE}
}

@BOOK{mallat99awt,
  title = {A Wavelet Tour of Signal Processing, Second Edition},
  publisher = {Academic Press},
  year = {1999},
  author = {S. Mallat},
  month = {September},
  date-added = {2008-05-22 16:32:31 -0700},
  date-modified = {2008-05-22 16:33:57 -0700},
  howpublished = {Hardcover},
  isbn = {012466606X},
  keywords = {wavelet transform}
}

@CONFERENCE{morton98fsr,
  author = {S. A. Morton and C. C. Ober},
  title = {Faster shot-record depth migrations using phase encoding},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {1998},
  volume = {17},
  number = {1},
  pages = {1131-1134},
  publisher = {SEG},
  bdsk-url-1 = {http://link.aip.org/link/?SGA/17/1131/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1820088},
  date-added = {2008-05-27 16:44:01 -0700},
  date-modified = {2008-05-27 16:45:21 -0700},
  doi = {10.1190/1.1820088},
  issue = {1},
  pdf = {http://link.aip.org/link/?SGA/17/1131/1}
}

@ARTICLE{paige82lsq,
  author = {C. C. Paige and M. A. Saunders},
  title = {{LSQR}: an algorithm for sparse linear equations and sparse least
	squares},
  journal = {Transactions on Mathematical Software},
  year = {1982},
  volume = {8},
  pages = {43-71},
  number = {1},
  address = {New York, NY, USA},
  bdsk-url-1 = {http://doi.acm.org/10.1145/355984.355989},
  date-added = {2008-05-20 14:00:44 -0700},
  date-modified = {2008-05-20 19:47:37 -0700},
  doi = {http://doi.acm.org/10.1145/355984.355989},
  issn = {0098-3500},
  issue = {1},
  keywords = {LSQR},
  publisher = {ACM}
}

@INCOLLECTION{potts01mst,
  author = {D. Potts and G. Steidl and M. Tasche},
  title = {Fast {F}ourier transforms for nonequispaced data: a tutorial},
  booktitle = {Modern sampling theory: mathematics and applications},
  publisher = {Birkhauser},
  year = {2001},
  editor = {J. J. Benedetto and P. Ferreira},
  chapter = {12},
  pages = {249-274},
  abstract = {In this section, we consider approximate methods for the fast computiation
	of multivariate discrete Fourier transforms for nonequispaced data
	(NDFT) in the time domain and in the frequency domain. In particular,
	we are interested in the approximation error as function of arithmetic
	complexity of the algorithm. We discuss the robustness of NDFT-algorithms
	with respect to roundoff errors and apply NDFT-algorithms for the
	fast computation of Bessel transforms.},
  bdsk-url-1 = {http://www.tu-chemnitz.de/~potts/paper/ndft.pdf},
  date-added = {2008-05-07 18:44:29 -0700},
  date-modified = {2008-08-14 15:28:37 -0700},
  keywords = {NFFT},
  pdf = {http://www.tu-chemnitz.de/~potts/paper/ndft.pdf}
}

@ARTICLE{romero00peo,
  author = {L. A. Romero and D. C. Ghiglia and C. C. Ober and S. A. Morton},
  title = {Phase encoding of shot records in prestack migration},
  journal = {Geophysics},
  year = {2000},
  volume = {65},
  pages = {426-436},
  number = {2},
  abstract = {Frequency-domain shot-record migration can produce higher quality
	images than Kirchhoff migration but typically at a greater cost.
	The computing cost of shot-record migration is the product of the
	number of shots in the survey and the expense of each individual
	migration. Many attempts to reduce this cost have focused on the
	speed of the individual migrations, trying to achieve a better trade-off
	between accuracy and speed. Another approach is to reduce the number
	of migrations. We investigate the simultaneous migration of shot
	records using frequency-domain shot-record migration algorithms.
	The difficulty with this approach is the production of so-called
	crossterms between unrelated shot and receiver wavefields, which
	generate unwanted artifacts or noise in the final image. To reduce
	these artifacts and obtain an image comparable in quality to the
	single-shot-per-migration result, we have introduced a process called
	phase encoding, which shifts or disperses these crossterms. The process
	of phase encoding thus allows one to trade S/N ratio for the speed
	of migrating the entire survey. Several encoding functions and two
	application strategies have been tested. The first strategy, combining
	multiple shots per migration and using each shot only once, reduces
	computation in direct relation to the number of shots combined. The
	second strategy, performing multiple migrations of all the shots
	in the survey, provides a means to reduce the crossterm noise by
	stacking the resulting images. The additional noise in both strategies
	may be tolerated if it is no stronger than the inherent seismic noise
	in the migrated image and if the final image is achieved with less
	cost. {\copyright}2000 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/65/426/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444737},
  date-added = {2008-05-27 16:42:50 -0700},
  date-modified = {2008-08-14 15:07:08 -0700},
  doi = {10.1190/1.1444737},
  issue = {2},
  pdf = {http://link.aip.org/link/?GPY/65/426/1},
  publisher = {SEG}
}

@ARTICLE{sacchi98iae,
  author = {M. D. Sacchi and T. J. Ulrych and C. J. Walker},
  title = {Interpolation and extrapolation using a high-resolution discrete
	{F}ourier transform},
  journal = {IEEE Transactions on Signal Processing},
  year = {1998},
  volume = {46},
  pages = {31-38},
  number = {1},
  abstract = {We present an iterative nonparametric approach to spectral estimation
	that is particularly suitable for estimation of line spectra. This
	approach minimizes a cost function derived from Bayes' theorem. The
	method is suitable for line spectra since a ``long tailed'' distribution
	is used to model the prior distribution of spectral amplitudes. An
	important aspect of this method is that since the data themselves
	are used as constraints, phase information can also be recovered
	and used to extend the data outside the original window. The objective
	function is formulated in terms of hyperpa- rameters that control
	the degree of fit and spectral resolution. Noise rejection can also
	be achieved by truncating the number of iterations. Spectral resolution
	and extrapolation length are controlled by a single parameter. When
	this parameter is large compared with the spectral powers, the algorithm
	leads to zero extrapolation of the data, and the estimated Fourier
	transform yields the periodogram. When the data are sampled at a
	constant rate, the algorithm uses one Levinson recursion per iteration.
	For irregular sampling (unevenly sampled and/or gapped data), the
	algorithm uses one Cholesky decomposition per iteration. The performance
	of the algorithm is illustrated with three different problems that
	frequently arise in geophysical data processing: 1) harmonic retrieval
	from a time series contaminated with noise; 2) linear event detection
	from a finite aperture array of receivers [which, in fact, is an
	extension of 1)], 3) interpolation/extrapolation of gapped data.
	The performance of the algorithm as a spectral estimator is tested
	with the Kay and Marple data set. It is shown that the achieved resolution
	is comparable with parametric methods but with more accurate representation
	of the relative power in the spectral lines. },
  bdsk-url-1 = {http://www-geo.phys.ualberta.ca/saig/papers/Sacchi_Ulrych_Walker_IEEE_98.pdf},
  date-added = {2008-05-06 19:18:50 -0700},
  date-modified = {2008-08-14 15:08:37 -0700},
  doi = {10.1109/78.651165},
  issue = {1},
  keywords = {Fourier transform, reconstruction},
  pdf = {http://www-geo.phys.ualberta.ca/saig/papers/Sacchi_Ulrych_Walker_IEEE_98.pdf},
  publisher = {IEEE}
}

@PHDTHESIS{schonewille00phd,
  author = {M. A. Schonewille},
  title = {Fourier reconstruction of irregularly sampled seismic data},
  school = {Delft University of Technology},
  year = {2000},
  address = {Delft, The Netherlands},
  month = {November},
  date-added = {2008-05-06 19:03:35 -0700},
  date-modified = {2008-05-09 14:43:57 -0700},
  keywords = {Fourier transform, reconstruction},
  rating = {0},
  read = {Yes}
}

@ARTICLE{smith98ahs,
  author = {H. Smith},
  title = {A Hardy space for {F}ourier integral operators},
  journal = {Journal of Geometric Analysis},
  year = {1998},
  volume = {8},
  pages = {629-653},
  number = {4},
  date-added = {2008-05-07 12:25:03 -0700},
  date-modified = {2008-08-14 15:09:47 -0700},
  issue = {4},
  keywords = {FIO}
}

@BOOK{snieder93giu,
  title = {Global inversions using normal mode and long-period surface waves},
  publisher = {Chapman and Hall},
  year = {1993},
  author = {R. Snieder},
  date-added = {2008-05-20 17:16:42 -0700},
  date-modified = {2008-05-20 17:19:44 -0700},
  keywords = {sampling}
}

@ARTICLE{spitz91sti,
  author = {S. Spitz},
  title = {Seismic trace interpolation in the {FX} domain},
  journal = {Geophysics},
  year = {1991},
  volume = {56},
  pages = {785-794},
  number = {6},
  abstract = {Interpolation of seismic traces is an effective means of improving
	migration when the data set exhibits spatial aliasing. A major difficulty
	of standard interpolation methods is that they depend on the degree
	of reliability with which the various geological events can be separated.
	In this respect, a multichannel interpolation method is described
	which requires neither a priori knowledge of the directions of lateral
	coherence of the events, nor estimation of these directions. The
	method is based on the fact that linear events present in a section
	made of equally spaced traces may be interpolated exactly, regardless
	of the original spatial interval, without any attempt to determine
	their true dips. The predictability of linear events in the f-x domain
	allows the missing traces to be expressed as the output of a linear
	system, the input of which consists of the recorded traces. The interpolation
	operator is obtained by solving a set of linear equations whose coefficients
	depend only on the spectrum of the spatial prediction filter defined
	by the recorded traces. Synthetic examples show that this method
	is insensitive to random noise and that it correctly handles curvatures
	and lateral amplitude variations. Assessment of the method with a
	real data set shows that the interpolation yields an improved migrated
	section. {\copyright}1991 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://dx.doi.org/10.1190/1.1443096},
  date-added = {2008-05-06 19:29:12 -0700},
  date-modified = {2008-08-14 15:18:16 -0700},
  doi = {10.1190/1.1443096},
  issue = {6},
  keywords = {PEF},
  publisher = {SEG}
}

@ARTICLE{starck02tct,
  author = {J.-L. Starck and E. J. Cand\`es and D. L. Donoho},
  title = {The curvelet transform for image denoising},
  journal = {IEEE Transactions on Image Processing},
  year = {2002},
  volume = {11},
  pages = {670-684},
  number = {6},
  month = {June},
  abstract = {We describe approximate digital implementations of two new mathematical
	transforms, namely, the ridgelet transform and the curvelet transform.
	Our implementations offer exact reconstruction, stability against
	perturbations, ease of implementation, and low computational complexity.
	A central tool is Fourier-domain computation of an approximate digital
	Radon transform. We introduce a very simple interpolation in the
	Fourier space which takes Cartesian samples and yields samples on
	a rectopolar grid, which is a pseudo-polar sampling set based on
	a concentric squares geometry. Despite the crudeness of our interpolation,
	the visual performance is surprisingly good. Our ridgelet transform
	applies to the Radon transform a special overcomplete wavelet pyramid
	whose wavelets have compact support in the frequency domain. Our
	curvelet transform uses our ridgelet transform as a component step,
	and implements curvelet subbands using a filter bank of a&grave;
	trous wavelet filters. Our philosophy throughout is that transforms
	should be overcomplete, rather than critically sampled. We apply
	these digital transforms to the denoising of some standard images
	embedded in white noise. In the tests reported here, simple thresholding
	of the curvelet coefficients is very competitive with "state of the
	art" techniques based on wavelets, including thresholding of decimated
	or undecimated wavelet transforms and also including tree-based Bayesian
	posterior mean methods. Moreover, the curvelet reconstructions exhibit
	higher perceptual quality than wavelet-based reconstructions, offering
	visually sharper images and, in particular, higher quality recovery
	of edges and of faint linear and curvilinear features. Existing theory
	for curvelet and ridgelet transforms suggests that these new approaches
	can outperform wavelet methods in certain image reconstruction problems.
	The empirical results reported here are in encouraging agreement},
  bdsk-url-1 = {http://dx.doi.org/10.1109/TIP.2002.1014998},
  bdsk-url-2 = {http://ieeexplore.ieee.org/iel5/83/21845/01014998.pdf},
  date-added = {2008-05-26 17:38:14 -0700},
  date-modified = {2008-08-14 15:19:16 -0700},
  doi = {10.1109/TIP.2002.1014998},
  issn = {1057-7149},
  issue = {6},
  keywords = {curvelet transform},
  publisher = {IEEE}
}

@ARTICLE{symes07rtm,
  author = {W. W. Symes},
  title = {Reverse time migration with optimal checkpointing},
  journal = {Geophysics},
  year = {2007},
  volume = {72},
  pages = {SM213-SM221},
  number = {5},
  abstract = {Reverse time migration (RTM) requires that fields computed in forward
	time be accessed in reverse order. Such out-of-order access, to recursively
	computed fields, requires that some part of the recursion history
	be stored (checkpointed), with the remainder computed by repeating
	parts of the forward computation. Optimal checkpointing algorithms
	choose checkpoints in such a way that the total storage is minimized
	for a prescribed level of excess computation, or vice versa. Optimal
	checkpointing dramatically reduces the storage required by RTM, compared
	to that needed for nonoptimal implementations, at the price of a
	small increase in computation. This paper describes optimal checkpointing
	in a form which applies both to RTM and other applications of the
	adjoint state method, such as construction of velocity updates from
	prestack wave equation migration. {\copyright}2007 Society of Exploration
	Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/72/SM213/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.2742686},
  date-added = {2008-05-08 14:42:11 -0700},
  date-modified = {2008-08-14 15:19:43 -0700},
  doi = {10.1190/1.2742686},
  issue = {5},
  keywords = {RTM, imaging},
  pdf = {http://link.aip.org/link/?GPY/72/SM213/1},
  publisher = {SEG}
}

@ARTICLE{thorson85vsa,
  author = {J. R. Thorson and J. F. Claerbout},
  title = {Velocity-stack and slant-stack stochastic inversion},
  journal = {Geophysics},
  year = {1985},
  volume = {50},
  pages = {2727-2741},
  number = {12},
  abstract = {Normal moveout (NMO) and stacking, an important step in analysis of
	reflection seismic data, involves summation of seismic data over
	paths represented by a family of hyperbolic curves. This summation
	process is a linear transformation and maps the data into what might
	be called a velocity space: a two-dimensional set of points indexed
	by time and velocity. Examination of data in velocity space is used
	for analysis of subsurface velocities and filtering of undesired
	coherent events (e.g., multiples), but the filtering step is useful
	only if an approximate inverse to the NMO and stack operation is
	available. One way to effect velocity filtering is to use the operator
	LT (defined as NMO and stacking) and its adjoint L as a transform
	pair, but this leads to unacceptable filtered output. Designing a
	better estimated inverse to L than LT is a generalization of the
	inversion problem of computerized tomography: deconvolving out the
	point-spread function after back projection. The inversion process
	is complicated by missing data, because surface seismic data are
	recorded only within a finite spatial aperture on the Earth's surface.
	Our approach to solving the problem of an ill-conditioned or nonunique
	inverse L--1, brought on by missing data, is to design a stochastic
	inverse to L. Starting from a maximum a posteriori (MAP) estimator,
	a system of equations can be set up in which a priori information
	is incorporated into a sparseness measure: the output of the stochastic
	inverse is forced to be locally focused, in order to obtain the best
	possible resolution in velocity space. The size of the resulting
	nonlinear system of equations is immense, but using a few iterations
	with a gradient descent algorithm is adequate to obtain a reasonable
	solution. This theory may also be applied to other large, sparse
	linear operators. The stochastic inverse of the slant-stack operator
	(a particular form of the Radon transform), can be developed in a
	parallel manner, and will yield an accurate slant-stack inverse pair.
	{\copyright}1985 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://dx.doi.org/10.1190/1.1441893},
  date-added = {2008-05-06 19:06:15 -0700},
  date-modified = {2008-08-14 15:20:19 -0700},
  doi = {10.1190/1.1441893},
  issue = {12},
  keywords = {Radon transform},
  publisher = {SEG}
}

@ARTICLE{trad03lvo,
  author = {D. Trad and T. J. Ulrych and M. D. Sacchi},
  title = {Latest views of the sparse {R}adon transform},
  journal = {Geophysics},
  year = {2003},
  volume = {68},
  pages = {386-399},
  number = {1},
  abstract = {The Radon transform (RT) suffers from the typical problems of loss
	of resolution and aliasing that arise as a consequence of incomplete
	information, including limited aperture and discretization. Sparseness
	in the Radon domain is a valid and useful criterion for supplying
	this missing information, equivalent somehow to assuming smooth amplitude
	variation in the transition between known and unknown (missing) data.
	Applying this constraint while honoring the data can become a serious
	challenge for routine seismic processing because of the very limited
	processing time available, in general, per common midpoint. To develop
	methods that are robust, easy to use and flexible to adapt to different
	problems we have to pay attention to a variety of algorithms, operator
	design, and estimation of the hyperparameters that are responsible
	for the regularization of the solution.In this paper, we discuss
	fast implementations for several varieties of RT in the time and
	frequency domains. An iterative conjugate gradient algorithm with
	fast Fourier transform multiplication is used in all cases. To preserve
	the important property of iterative subspace methods of regularizing
	the solution by the number of iterations, the model weights are incorporated
	into the operators. This turns out to be of particular importance,
	and it can be understood in terms of the singular vectors of the
	weighted transform. The iterative algorithm is stopped according
	to a general cross validation criterion for subspaces. We apply this
	idea to several known implementations and compare results in order
	to better understand differences between, and merits of, these algorithms.
	{\copyright}2003 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/68/386/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1543224},
  date-added = {2008-05-07 19:03:39 -0700},
  date-modified = {2008-08-14 15:20:56 -0700},
  doi = {10.1190/1.1543224},
  issue = {1},
  keywords = {Radon transform},
  pdf = {http://link.aip.org/link/?GPY/68/386/1},
  publisher = {SEG}
}

@ARTICLE{verschuur97eom,
  author = {D. J. Verschuur and A. J. Berkhout},
  title = {Estimation of multiple scattering by iterative inversion, Part {II}:
	Practical aspects and examples},
  journal = {Geophysics},
  year = {1997},
  volume = {62},
  pages = {1596-1611},
  number = {5},
  abstract = {A surface-related multiple-elimination method can be formulated as
	an iterative procedure: the output of one iteration step is used
	as input for the next iteration step (part I of this paper). In this
	paper (part II) it is shown that the procedure can be made very efficient
	if a good initial estimate of the multiple-free data set can be provided
	in the first iteration, and in many situations, the Radon-based multiple-elimination
	method may provide such an estimate. It is also shown that for each
	iteration, the inverse source wavelet can be accurately estimated
	by a linear (least-squares) inversion process. Optionally, source
	and detector variations and directivity effects can be included,
	although the examples are given without these options. The iterative
	multiple elimination process, together with the source wavelet estimation,
	are illustrated with numerical experiments as well as with field
	data examples. The results show that the surface-related multiple-elimination
	process is very effective in time gates where the moveout properties
	of primaries and multiples are very similar (generally deep data),
	as well as for situations with a complex multiple-generating system.
	{\copyright}1997 Society of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/62/1596/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1444262},
  date-added = {2008-05-07 18:40:45 -0700},
  date-modified = {2008-08-14 15:21:18 -0700},
  doi = {10.1190/1.1444262},
  issue = {5},
  keywords = {SRME},
  pdf = {http://link.aip.org/link/?GPY/62/1596/1},
  publisher = {SEG}
}

@ARTICLE{xu05aft,
  author = {S. Xu and Y. Zhang and D. Pham and G. Lambar\'{e}},
  title = {Antileakage {F}ourier transform for seismic data regularization},
  journal = {Geophysics},
  year = {2005},
  volume = {70},
  pages = {V87-V95},
  number = {4},
  abstract = {Seismic data regularization, which spatially transforms irregularly
	sampled acquired data to regularly sampled data, is a long-standing
	problem in seismic data processing. Data regularization can be implemented
	using Fourier theory by using a method that estimates the spatial
	frequency content on an irregularly sampled grid. The data can then
	be reconstructed on any desired grid. Difficulties arise from the
	nonorthogonality of the global Fourier basis functions on an irregular
	grid, which results in the problem of "spectral leakage": energy
	from one Fourier coefficient leaks onto others. We investigate the
	nonorthogonality of the Fourier basis on an irregularly sampled grid
	and propose a technique called "antileakage Fourier transform" to
	overcome the spectral leakage. In the antileakage Fourier transform,
	we first solve for the most energetic Fourier coefficient, assuming
	that it causes the most severe leakage. To attenuate all aliases
	and the leakage of this component onto other Fourier coefficients,
	the data component corresponding to this most energetic Fourier coefficient
	is subtracted from the original input on the irregular grid. We then
	use this new input to solve for the next Fourier coefficient, repeating
	the procedure until all Fourier coefficients are estimated. This
	procedure is equivalent to "reorthogonalizing" the global Fourier
	basis on an irregularly sampled grid. We demonstrate the robustness
	and effectiveness of this technique with successful applications
	to both synthetic and real data examples. {\copyright}2005 Society
	of Exploration Geophysicists},
  bdsk-url-1 = {http://link.aip.org/link/?GPY/70/V87/1},
  bdsk-url-2 = {http://dx.doi.org/10.1190/1.1993713},
  date-added = {2008-05-09 17:43:47 -0700},
  date-modified = {2008-08-14 15:21:45 -0700},
  doi = {10.1190/1.1993713},
  issue = {4},
  keywords = {Fourier transform, reconstruction},
  pdf = {http://link.aip.org/link/?GPY/70/V87/1},
  publisher = {SEG}
}

@ARTICLE{ying053dd,
  author = {L. Ying and L. Demanet and E. J. Cand\`es},
  title = {{3-D} discrete curvelet transform},
  journal = {Proceedings SPIE wavelets XI, San Diego},
  year = {2005},
  volume = {5914},
  pages = {344-354},
  month = {January},
  abstract = {In this paper, we present the first 3D discrete curvelet transform.
	This transform is an extension to the 2D transform described in Candes
	et al..1 The resulting curvelet frame preserves the important properties,
	such as parabolic scaling, tightness and sparse representation for
	singularities of codimension one. We describe three different implementations:
	in-core, out-of-core and MPI-based parallel implementations. Numerical
	results verify the desired properties of the 3D curvelets and demonstrate
	the efficiency of our implementations. },
  bdsk-url-1 = {http://dx.doi.org/10.1117/12.616205},
  date-added = {2008-05-07 14:14:59 -0700},
  date-modified = {2008-08-14 15:21:59 -0700},
  doi = {10.1117/12.616205},
  keywords = {curvelet transform}
}

@PHDTHESIS{zwartjes05phd,
  author = {P. M. Zwartjes},
  title = {Fourier reconstruction with sparse inversion},
  school = {Delft University of Technology},
  year = {2005},
  address = {Delft, The Netherlands},
  month = {December},
  date-added = {2008-05-06 18:58:35 -0700},
  date-modified = {2008-05-09 14:44:04 -0700},
  keywords = {Fourier transform, reconstruction},
  rating = {0},
  read = {Yes}
}

