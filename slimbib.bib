%% This BibTeX bibliography file was created using BibDesk.
%% http://www.cs.ucsd.edu/~mmccrack/bibdesk.html


%% Created for Felix Herrmann at 2010-05-19 14:21:39 -0700 


%% Saved with string encoding Unicode (UTF-8) 
@conference{herrmann08gbu,
	Author = {Felix J. Herrmann},
	Booktitle = {SEG},
	Date-Added = {2008-11-17 11:16:00 -0700},
	Date-Modified = {2008-11-17 11:16:00 -0700},
	Keywords = {SLIM, SEG, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08gbu.pdf},
	Title = {Seismic noise: the good, the bad, & the ugly},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08gbu.pdf}}


@conference{frijlink10cos,
Abstract = {In recent years, data-driven multiple prediction methods
and wavefield extrapolation methods have proven to be powerful methods
to attenuate multiples from data acquired in complex 3-D geologic
environments. These methods make use of a two-stage approach, where
first the multiples (surface-related and / or internal) multiples are
predicted before they are subtracted from the original input data in
an adaptively. The quality of these predicted multiples often raises
high expectations for the adaptive subtraction techniques, but for
various reasons these expectations are not always met in practice.
Standard adaptive subtraction methods use the well-known minimum
energy criterion, stating that the total energy after optimal multiple
attenuation should be minimal. When primaries and multiples interfere
, the minimum energy criterion is no longer appropriate. Also, when
multiples of different orders interfere, adaptive energy minimization
will lead to a compromise between different amplitudes corrections for
the different orders of multiples. This paper investigates the
performance of two multiple subtraction schemes for a real data set
that exhibits both interference problems. Results from an adaptive
subtraction in the real curvelet domain, separating primaries and
multiples, are compared to those obtained using a more conventional
adaptive subtraction method in the spatial domain.},
Author = {M.O. Frijlink and R. Shahidi and F.J. Herrmann and R.G. van Borselen},
Keywords = {EAGE},
Organization = {EAGE},
Publisher = {EAGE},
Title = {Comparison of Standard Adaptive Subtraction and Primary-multiple Separation in the Curvelet Domain},
Date-Added = {2010-05-08 15:10:48 -0700},
Year = {2010}}


@conference{herrman10rss,
	Abstract = { Seismic exploration relies on the collection of massive data
   volumes that are subsequently mined for information during seismic
   processing. While this approach has been extremely successful in
   the past, the current trend towards higher quality images in
   increasingly complicated regions continues to reveal fundamental
   shortcomings in our workflows for high-dimensional data
   volumes. Two causes can be identified.. First, there is the
   so-called ``curse of dimensionality'' exemplified by Nyquist's
   sampling criterion, which puts disproportionate strain on current
   acquisition and processing systems as the size and desired
   resolution of our survey areas continues to increase. Secondly,
   there is the recent ``departure from Moore's law'' that forces us
   to lower our expectations to compute ourselves out of this curse of
   dimensionality. In this paper, we offer a way out of this situation
   by a deliberate randomized subsampling combined with
   structure-exploiting transform-domain sparsity promotion. Our
   approach is successful because it reduces the size of seismic data
   volumes without loss of information. As such we end up with a new
   technology where the costs of acquisition and processing are no
   longer dictated by the size of the acquisition but by the
   transform-domain sparsity of the end-product.},
	Author = {F. J. Herrmann},
	Date-Added = {2010-01-25 17:11:23 -0800},
	Date-Modified = {2010-01-25 17:24:07 -0800},
	Keywords = {EAGE},
	Organization = {EAGE},
	Publisher = {EAGE},
	Title = {Randomized sampling strategies},
	Year = {2010}}


@conference{herrman10rdr,
	Abstract = {Full-waveform inversion relies on the collection of large
   multi-experiment data volumes in combination with a sophisticated
   back-end to create high-fidelity inversion results. While
   improvements in acquisition and inversion have been extremely
   successful, the current trend of incessantly pushing for higher
   quality models in increasingly complicated regions of the Earth
   continues to reveal fundamental shortcomings in our ability to
   handle the ever increasing problem size numerically. Two causes
   can be identified as the main culprits responsible for this
   barrier. First, there is the so-called ``curse of
   dimensionality'' exemplified by Nyquist's sampling criterion,
   which puts disproportionate strain on current acquisition and
   processing systems as the size and desired resolution of our
   survey areas continues to increase. Secondly, there is the recent
   ``departure from Moore's law'' that forces us to lower our
   expectations to compute ourselves out of this. In this paper, we
   address this situation by randomized dimensionality reduction,
   which we adapt from the field of compressive sensing. In this
   approach, we combine deliberate randomized subsampling with
   structure-exploiting transform-domain sparsity promotion. Our
   approach is successful because it reduces the size of seismic
   data volumes without loss of information. With this reduction, we
   compute Newton-like updates at the cost of roughly one gradient
   update for the fully-sampled wavefield.},
	Author = {F. J. Herrmann and Xiang Li},
	Date-Added = {2010-01-25 17:14:37 -0800},
	Date-Modified = {2010-01-25 17:24:11 -0800},
	Keywords = {EAGE},
	Organization = {EAGE},
	Publisher = {EAGE},
	Title = {Randomized dimensionality reduction for full-waveform inversion},
	Year = {2010}}



@techreport{tang09dtr,
	Abstract = {The tasks of sampling, compression and
                  reconstruction are very common and often necessary
                  in seismic data processing due to the large size of
                  seismic data. Curvelet-based Recovery by
                  Sparsity-promoting Inversion, motivated by the newly
                  developed theory of compressive sensing, is among
                  the best recovery strategies for seismic data. The
                  incomplete data input to this curvelet-based
                  recovery is determined by randomized sampling of the
                  original complete data. Unlike usual regular
                  undersampling, randomized sampling can convert
                  aliases to easy-to-eliminate noise, thus
                  facilitating the process of reconstruction of the
                  complete data from the incomplete data. Randomized
                  sampling methods such as jittered sampling have been
                  developed in the past that are suitable for
                  curvelet-based recovery, however most have only been
                  applied to sampling in one dimension. Considering
                  that seismic datasets are usually higher dimensional
                  and extremely large, in the present paper, we extend
                  the 1D version of jittered sampling to two
                  dimensions, both with underlying Cartesian and
                  hexagonal grids. We also study separable and
                  non-separable two dimensional jittered sampling, the
                  former referring to the Kronecker product of two
                  one-dimensional jittered samplings. These different
                  categories of jittered sampling are compared against
                  one another in terms of signal-to-noise ratio and
                  visual quality, from which we find that jittered
                  hexagonal sampling is better than jittered Cartesian
                  sampling, while fully non-separable jittered
                  sampling is better than separable sampling. Because
                  in the image processing and computer graphics
                  literature, sampling patterns with blue-noise
                  spectra are found to be ideal to avoid aliasing, we
                  also introduce two other randomized sampling
                  methods, possessing sampling spectra with beneficial
                  blue noise characteristics, Poisson Disk sampling
                  and Farthest Point sampling. We compare these
                  methods, and apply the introduced sampling
                  methodologies to higher dimensional curvelet-based
                  reconstruction. These sampling schemes are shown to
                  lead to better results from CRSI compared to the
                  other more traditional sampling protocols,
                  e.g. regular subsampling.  },
	Annote = {In revision.},
	Author = {Gang Tang and Reza Shahidi and Jianwei Ma},
	Date-Added = {2010-05-19 14:19:48 -0700},
	Date-Modified = {2010-05-19 14:21:37 -0700},
	Institution = {UBC Earth and Ocean Sciences Department},
	Number = {TR-2009-03},
	Title = { Design of two-dimensional randomized sampling schemes for curvelet-based sparsity-promoting seismic data recovery},
	Year = {2009},
	PDF = {http://slim.eos.ubc.ca/Publications/Private/Journals/tang09dtr.pdf},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Journals/tang09dtr.pdf}}

@techreport{herrmann2010rsa,
	Abstract = {Many seismic exploration techniques rely on the
                  collection of massive data volumes that are
                  subsequently mined for information during
                  processing. While this approach has been extremely
                  successful in the past, current efforts toward
                  higher- resolution images in increasingly
                  complicated regions of the Earth continue to reveal
                  fundamental shortcomings in our workflows. Chiefly
                  amongst these is the so-called ``curse of
                  dimensionality'' exemplified by Nyquist's sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. In this paper, we offer an alternative
                  sampling method leveraging recent insights from
                  compressive sensing towards seismic acquisition and
                  processing for data that are traditionally
                  considered to be undersampled. The main outcome of
                  this approach is a new technology where acquisition
                  and processing related costs are no longer
                  determined by overly stringent sampling criteria,
                  such as Nyquist. At the heart of our approach lies
                  randomized incoherent sampling that breaks
                  subsampling related interferences by turning them
                  into harmless noise, which we subsequently remove by
                  promoting transform-domain sparsity. Now, costs no
                  longer grow significantly with resolution and
                  dimensionality of the survey area, but instead
                  depend on transform-domain sparsity only. Our
                  contribution is twofold. First, we demonstrate by
                  means of carefully designed numerical experiments
                  that compressive sensing can successfully be
                  adapted to seismic exploration. Second, we show that
                  accurate recovery can be accomplished for
                  compressively sampled data volumes sizes that exceed
                  the size of conventional transform-domain data
                  volumes by only a small factor. Because compressive
                  sensing combines transformation and encoding by a
                  single linear encoding step, this technology is
                  directly applicable to acquisition and to
                  dimensionality reduction during processing. In
                  either case, sampling, storage, and processing costs
                  scale with transform-domain sparsity. We illustrate
                  this principle by means of number of case studies.
                  },
	Annote = {Accepted for publication},
	Author = {Felix J. Herrmann},
	Date-Added = {2010-05-19 14:16:26 -0700},
	Date-Modified = {2010-05-19 14:18:20 -0700},
	Institution = {UBC Earth and Ocean Sciences Department},
	Number = {TR-2010-1},
	Title = {Randomized sampling and sparsity: getting more information from fewer samples},
	Year = {2010},
	PDF = {http://slim.eos.ubc.ca/index.php?module=articles&func=display&catid=18&aid=317},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/index.php?module=articles&func=display&catid=18&aid=317}}

@techreport{hennenfent2010nct,
	Abstract = {Seismic data are typically irregularly sampled
                  along spatial axes. This irregular sampling may
                  adversely affect some key steps, e.g., multiple
                  prediction/attenuation or imaging, in the processing
                  workflow. To overcome this problem almost every
                  large dataset is regularized and/or
                  interpolated. Our contribution is twofold. Firstly,
                  we extend our earlier work on the nonequispaced fast
                  discrete curvelet transform (NFDCT) and introduce a
                  second generation of the transform. This new
                  generation differs from the previous one by the
                  approach taken to compute accurate curvelet
                  coefficients from irregularly sampled data. The
                  first generation relies on accurate Fourier
                  coefficients obtained by an $\ell_2$-regularized
                  inversion of the nonequispaced fast Fourier
                  transform, while the second is based on a direct,
                  $\ell_1$ -regularized inversion of the operator that
                  links curvelet coefficients to irregular data. Also,
                  by construction, the NFDCT second generation is
                  lossless, unlike the NFDCT first generation. This
                  property is particularly attractive for processing
                  irregularly sampled seismic data in the curvelet
                  domain and bringing them back to their irregular
                  recording locations with high fidelity. Secondly, we
                  combine the NFDCT second generation with the
                  standard fast discrete curvelet transform (FDCT) to
                  form a new curvelet-based method, coined
                  nonequispaced curvelet reconstruction with
                  sparsity-promoting inversion (NCRSI), for the
                  regularization and interpolation of irregularly
                  sampled data. We demonstrate that, for a pure
                  regularization problem, the reconstruction is very
                  accurate. The signal-to-reconstruction error ratio
                  is, in our example, above 40 dB. We also conduct
                  combined interpolation and regularization
                  experiments. The reconstructions for synthetic data
                  are accurate, particularly when the recording
                  locations are optimally jittered. The reconstruction
                  in our real data example shows amplitudes along the
                  main wavefronts smoothly varying with no obvious
                  acquisition imprint; a result very competitive with
                  results from other reconstruction methods overall.
                  },
	Annote = {In revision},
	Author = {Gilles Hennenfent and Lloyd Fenelon and Felix J. Herrmann},
	Date-Added = {2010-05-19 13:24:59 -0700},
	Date-Modified = {2010-05-19 13:27:03 -0700},
	Institution = {UBC Earth and Ocean Sciences Department},
	Number = {TR-2010-2},
	Title = {Nonequispaced curvelet transform for seismic data reconstruction: a sparsity-promoting approach},
	Year = {2010},
	PDF = {http://slim.eos.ubc.ca/Publications/Private/Journals/hennenfent2010nct.pdf},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Journals/hennenfent2010nct.pdf}}

@conference{herrmann10rds,
	Abstract = {Full-waveform inversion relies on the collection
                  of large multi-experiment data volumes in
                  combination with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth continues to reveal
                  fundamental shortcomings in our ability to handle
                  the ever increasing problem size numerically. Two
                  causes can be identified as the main culprits
                  responsible for this barrier. First, there is the
                  so-called ``curse of dimensionality'' exemplified by
                  Nyquist's sampling criterion, which puts
                  disproportionate strain on current acquisition and
                  processing systems as the size and desired
                  resolution of our survey areas continues to
                  increase. Secondly, there is the recent ``departure
                  from Moore's law'' that forces us to lower our
                  expectations to compute ourselves out of this. In
                  this paper, we address this situation by randomized
                  dimensionality reduction, which we adapt from the
                  field of compressive sensing. In this approach, we
                  combine deliberate randomized subsampling with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. With this reduction, we compute
                  Newton-like updates at the cost of roughly one
                  gradient update for the fully-sampled wavefield.},
	Author = {F. J. Herrmann and Xiang Li},
	Date-Added = {2010-01-25 17:14:37 -0800},
	Date-Modified = {2010-01-25 17:24:11 -0800},
	Keywords = {EAGE},
	Organization = {EAGE},
	Publisher = {EAGE},
	Title = {Randomized dimensionality reduction for full-waveform inversion},
	Year = {2010}}

@conference{herrmann10rss,
	Abstract = { Seismic exploration relies on the collection of
                  massive data volumes that are subsequently mined for
                  information during seismic processing. While this
                  approach has been extremely successful in the past,
                  the current trend towards higher quality images in
                  increasingly complicated regions continues to reveal
                  fundamental shortcomings in our workflows for
                  high-dimensional data volumes. Two causes can be
                  identified.. First, there is the so-called ``curse
                  of dimensionality'' exemplified by Nyquist's
                  sampling criterion, which puts disproportionate
                  strain on current acquisition and processing systems
                  as the size and desired resolution of our survey
                  areas continues to increase. Secondly, there is the
                  recent ``departure from Moore's law'' that forces us
                  to lower our expectations to compute ourselves out
                  of this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  randomized subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. As such we end up with a new
                  technology where the costs of acquisition and
                  processing are no longer dictated by the size of the
                  acquisition but by the transform-domain sparsity of
                  the end-product.},
	Author = {F. J. Herrmann},
	Date-Added = {2010-01-25 17:11:23 -0800},
	Date-Modified = {2010-01-25 17:24:07 -0800},
	Keywords = {EAGE},
	Organization = {EAGE},
	Publisher = {EAGE},
	Title = {Randomized sampling strategies},
	Year = {2010}}

@conference{lin10seo,
	Abstract = {Recent works on surface-related multiple removal
                  include a direct estimation method proposed by van
                  Groenestijn and Verschuur (2009), where under a
                  sparsity assumption the primary impulse response is
                  determined directly from a data-driven wavefield
                  inversion process called Estimation of Primaries by
                  Sparse Inversion (EPSI). The authors have shown that
                  this approach is superior to traditional
                  estimation subtraction processes such as SRME on
                  shallow bottom marine data, where by expanding the
                  model to simultaneously invert for the near-offset
                  traces, which are not directly available in most
                  situation but are observable in the data multiples,
                  a large improvement over Radon interpolation is
                  demonstrated.  One of the major roadblocks to the
                  widespread adoption of EPSI is that one must have
                  precise knowledge of a time-window that contains
                  multiple-free primaries during each update. There is
                  some anecdotal evidence that the inversion result is
                  unstable under errors in the time-window length, a
                  behavior that runs contrary to the strengths of
                  EPSI and diminishes its effectiveness for
                  shallow-bottom marine data where multiples are
                  closely spaced. Moreover, due to the nuances
                  involved in regularizing the model impulse response
                  in the inverse problem, the EPSI approach has an
                  additional number of inversion parameters to choose
                  and often also does not often lead to a stable
                  solution under perturbations to these parameters.
                  We show that the specific sparsity constraint on the
                  EPSI updates lead to an inherently intractable
                  problem, and that the time-window and other
                  inversion variables arise as additional
                  regularizations on the unknown towards a meaningful
                  solution. We furthermore suggest a way to remove
                  almost all of these parameters via a L0 to L1
                  convexification, which stabilizes the inversion
                  while preserving the crucial sparsity assumption in
                  the primary impulse response model.},
	Author = {T.T.Y. Lin and F. J. Herrmann},
	Date-Added = {2010-01-25 11:44:30 -0800},
	Date-Modified = {2010-01-25 11:46:09 -0800},
	Keywords = {EAGE},
	Organization = {EAGE},
	Publisher = {EAGE},
	Title = {Stabilization of estimation of primaries via sparse inversion},
	Year = {2010}}

@conference{johnson10eop,
	Abstract = {Accurate removal of surface related multiples is a
                  key step in seismic data processing. The industry
                  standard for removing multiples is SRME, which
                  involves convolving the data with itself to predict
                  the multiples, followed by an adaptive subtraction
                  procedure to recover the primaries (Verschuur and
                  Berkhout, 1997). Other methods involve
                  multidimensional division of the up-going and
                  down-going wavefields (Amundsen, 2001). However,
                  this approach may suffer from stability
                  problems. With the introduction of the ``estimation
                  of primaries by sparse inversion''(EPSI), van
                  Groenestijn and Verschuur (2009) recentely
                  reformulated SRME to jointly estimate the
                  surface-free impulse response and the source
                  signature directly from the data. The advantage of
                  EPSI is that it recovers the primary response
                  directly, and does not require a second processing
                  step for the subtraction of estimated multiples from
                  the original data. However, because it estimates
                  both the primary impulse response and source
                  signature from the data EPSI must be regularized.
                  Motivated by recent successful application of the
                  curvelet transform in seismic data processing
                  (Herrmann et al., 2007), we formulate EPSI as a
                  bi-convex optimization problem that seeks sparsity
                  on the surface-free Green's function and
                  Fourier-domain smoothness on the source wavelet. Our
                  main contribution compared to previous work (Lin and
                  Herrmann, 2009), and the contribution of that
                  author to the proceedings of this meeting(Lin and
                  Herrmann, 2010), is that we employ the physical
                  principle of as source-receiver reciprocity to
                  improve the inversion.},
	Author = {J.G.A. Johnson and T.T.Y. Lin and F. J. Herrmann},
	Date-Added = {2010-01-25 11:41:50 -0800},
	Date-Modified = {2010-01-25 11:44:04 -0800},
	Keywords = {EAGE},
	Organization = {EAGE},
	Publisher = {EAGE},
	Title = {Estimation of primaries via sparse inversion with reciprocity},
	Year = {2010}}

@mastersthesis{alhashim09msc,
	Abstract = {The process of obtaining high quality seismic
                  images is very challenging when exploring new areas
                  that have high complexities. The to be processed
                  seismic data comes from the field noisy and commonly
                  incomplete. Recently, major advances were
                  accomplished in the area of coherent noise removal,
                  for example, Surface Related Multiple Elimination
                  (SRME).  Predictive multiple elimination methods,
                  such as SRME, consist of two steps: The first step
                  is the prediction step, in this step multiples are
                  predicted from the seismic data. The second step
                  is the separation step in which primary reflection
                  and surface related multiples are separated, this
                  involves predicted multiples from the first step to
                  be ''matched'' with the true multiples in the data
                  and eventually removed . A recent robust Bayesian
                  wavefield separation method have been recently
                  introduced to improve on the separation by matching
                  methods. This method utilizes the effectiveness of
                  using the multi scale and multi angular curvelet
                  transform in processing seismic images. The method
                  produced excellent results and improved multiple
                  removal. A considerable problem in the seismic
                  processing field is the fact that seismic data are
                  large and require a correspondingly large memory
                  size and processing time. The fact that curvelets
                  are redundant also increases the need for large
                  memory to process seismic data.  In this thesis we
                  propose a parallel approach based windowing operator
                  that divides large seismic data into smaller more
                  managable datasets that can fit in memory so that it
                  is possible to apply the Bayesian separation pro-
                  cess in parallel with minimal harm to the image
                  quality and data integrity.  However, by dividing
                  the data, we introduce discontinuities. We take
                  these discontinuities into account and compare two
                  ways that different windows may communicate. The
                  first method is to communicate edge information at
                  only two steps, namely, data scattering and
                  gathering processes while applying the multiple
                  separation on each window separately. The second
                  method is to define our windowing operator as a
                  global operator, which exchanges window edge
                  information at each forward and inverse curvelet
                  transform. We discuss the trade off between the two
                  methods trying to minimize complexity and I/O time
                  spent in the process.  We test our windowing
                  operator on a seismic denoising problem and then
                  apply the windowing operator on our sparse-domain
                  Bayesian primary- multiple separation.  },
	Author = {F. Alhashim},
	Date-Added = {2009-09-30 14:53:59 -0700},
	Date-Modified = {2009-09-30 14:55:45 -0700},
	Keywords = {MSc},
	School = {University of British Columbia},
	Title = {Seismic Data Processing with the Parallel Windowed Curvelet Transform Seismic Data Processing with the Parallel Windowed Curvelet Transform},
	talkurl={http://slim.eos.ubc.ca/Publications/public/presentations/2009/alhashim09sdp1.pdf},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/public/theses/2009/alhashim09sdp.pdf}}

@inproceedings{Vandenberg09ocf,
	Abstract = {An optimization algorithm for minimizing a smooth
                  function over a convex set is described. Each
                  iteration of the method computes a descent direction
                  by minimizing, over the original constraints, a
                  diagonal-plus low-rank quadratic approximation to
                  the function. The quadratic approximation is
                  constructed using a limited-memory quasi-Newton
                  update. The method is suitable for large-scale
                  problems where evaluation of the function is
                  substantially more expensive than projection onto
                  the constraint set. Numerical experiments on
                  one-norm regularized test problems indicate that the
                  proposed method is competitive with state-of-the-art
                  methods such as bound-constrained L-BFGS and
                  orthant-wise descent. We further show that the
                  method generalizes to a wide class of problems, and
                  substantially improves on state-of-the-art methods
                  for problems such as learning the structure of
                  Gaussian graphical models (involving
                  positive-definite matrix constraints) and Markov
                  random fields (involving second-order cone
                  constraints).},
	Author = {E. van den Berg and M. P. Friedlander and K. Murphy},
	Booktitle = {JMLR Workshop and Conference Proceedings Volume 5: AISTATS 2009},
	Date-Added = {2009-07-07 13:54:00 -0700},
	Date-Modified = {2009-07-07 13:56:09 -0700},
	Title = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm},
	Year = {2009},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/~mpf/public/group.pdf}}

@mastersthesis{Kumar09msc,
	Abstract = {Curvelets are a recently introduced transform
                  domain that belongs to a family of multiscale and
                  also multidirectional data expansions. As such,
                  curvelets can be applied to resolution of the issues
                  of complicated seismic wavefronts. We make use of
                  this multiscale, multidirectional and hence
                  sparsifying ability of the curvelet transform to
                  suppress incoherent noise from crustal data where
                  the signal-to-noise ratio is low and to develop an
                  improved deconvolution procedure. Incoherent noise
                  present in seismic reflection data corrupts the
                  quality of the signal and can often lead to
                  misinterpretation. The curvelet domain lends itself
                  particularly well for denoising because coherent
                  seismic energy maps to a relatively small number of
                  significant curvelet coefficents.},
	Author = {V. Kumar},
	Date-Added = {2009-07-07 13:51:14 -0700},
	Date-Modified = {2009-07-07 13:52:39 -0700},
	Keywords = {MSc},
	School = {University of British Columbia},
	Title = {Incoherent noise suppression and deconvolution using curvelet-domain sparsity},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2009/kumar09ins.pdf}}

@article{herrmann09cbm,
	Abstract = {The extremely large size of typical seismic
                  imaging problems has been one of the major stumbling
                  blocks for iterative techniques to attain accurate
                  migration amplitudes. These iterative methods are
                  important because they complement theoretical
                  approaches that are hampered by difficulties to
                  control problems such as finite-acquisition
                  aperture, source-receiver frequency response, and
                  directivity. To solve these problems, we apply
                  preconditioning, which significantly improves
                  convergence of least-squares migration. We discuss
                  different levels of preconditioning that range from
                  corrections for the order of the migration operator
                  to corrections for spherical spreading, and position
                  and reflector-dip dependent amplitude errors. While
                  the first two corrections correspond to simple
                  scalings in the Fourier and physical domain, the
                  third correction requires phase-space (space spanned
                  by location and dip) scaling, which we carry out
                  with curvelets. We show that our combined
                  preconditioner leads to a significant improvement of
                  the convergence of least-squares `wave-equation'
                  migration on a line from the SEG AA' salt model.},
	Author = {F. J. Herrmann and C. R. Brown and Y. A. Erlangga and P. P. Moghaddam},
	Date-Added = {2009-07-03 13:10:26 -0700},
	Date-Modified = {2009-07-03 13:13:32 -0700},
	Journal = {Geophysics},
	Keywords = {migration},
	Pages = {A41},
	Title = {Curvelet-based migration preconditioning and scaling},
	Volume = {74},
	Year = {2009},
	PDF = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann08cmp-r.pdf},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann08cmp-r.pdf}}

@article{herrmann09csf,
	Abstract = {The fact that computational complexity of
                  wavefield simulation is proportional to the size of
                  the discretized model and acquisition geometry, and
                  not to the complexity of the simulated wavefield, is
                  a major impediment within seismic imaging. By
                  turning simulation into a compressive sensing
                  problem---where simulated data is recovered from a
                  relatively small number of independent simultaneous
                  sources---we remove this impediment by showing that
                  compressively sampling a simulation is equivalent to
                  compressively sampling the sources, followed by
                  solving a reduced system. As in compressive sensing,
                  this allows for a reduction in sampling rate and
                  hence in simulation costs. We demonstrate this
                  principle for the time-harmonic Helmholtz
                  solver. The solution is computed by inverting the
                  reduced system, followed by a recovery of the full
                  wavefield with a sparsity promoting
                  program. Depending on the wavefield's sparsity, this
                  approach can lead to significant cost reductions, in
                  particular when combined with the implicit
                  preconditioned Helmholtz solver, which is known to
                  converge even for decreasing mesh sizes and
                  increasing angular frequencies. These properties
                  make our scheme a viable alternative to explicit
                  time-domain finite-differences.},
	Author = {F. J. Herrmann and Y. A. Erlangga and T.T.Y. Lin},
	Date-Added = {2009-07-03 13:04:56 -0700},
	Date-Modified = {2009-07-03 13:10:18 -0700},
	Journal = {Geophysics},
	Keywords = {full-waveform},
	Pages = {A35},
	Title = {Compressive simultaneous full-waveform simulation},
	Volume = {74},
	Year = {2009},
	PDF = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08csf-r.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg09/herrmann09segcsf.pdf};
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08csf-r.pdf}}

@conference{herrmann09sns,
	Abstract = {Seismic exploration relies on the collection of
                  massive data volumes that are subsequently mined for
                  information during seismic processing. While this
                  approach has been extremely successful in the past,
                  the current trend of incessantly pushing for higher
                  quality images in increasingly complicated regions
                  of the Earth continues to reveal fundamental
                  shortcomings in our workflows to handle massive
                  high-dimensional data volumes. Two causes can be
                  identified as the main culprits responsible for this
                  barrier. First, there is the so-called ``curse of
                  dimensionality'' exemplified by Nyquist's sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase. Secondly, there is the recent
                  ``departure from Moore's law'' that forces us to
                  lower our expectations to compute ourselves out of
                  this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  \emph{randomized} subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. Because of this size reduction
                  both impediments are removed and we end up with a
                  new technology where the costs of acquisition and
                  processing are no longer dictated by the \emph{size
                  of the acquisition} but by the transform-domain
                  \emph{sparsity} of the end-product after
                  processing.},
	Author = {F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:23:30 -0700},
	Date-Modified = {2009-10-05 13:29:02 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {3410-3415},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09segsss.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/herrmann09segsns.pdf};
	Title = {Sub-Nyquist sampling and sparsity: getting more information from fewer samples},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09segsss.pdf}}

@conference{tang09hdb,
	Abstract = {In combination with compressive sensing, a
                  successful reconstruction scheme called
                  Curvelet-based Recovery by Sparsity-promoting
                  Inversion (CRSI) has been developed, and has proven
                  to be useful for seismic data processing. One of the
                  most important issues for CRSI is the sampling
                  scheme, which can greatly affect the quality of
                  reconstruction. Unlike usual regular undersampling,
                  stochastic sampling can convert aliases to
                  easy-to-eliminate noise. Some stochastic sampling
                  methods have been developed for CRSI, e.g. jittered
                  sampling, however most have only been applied to 1D
                  sampling along a line. Seismic datasets are usually
                  higher dimensional and very large, thus it is
                  desirable and often necessary to develop higher
                  dimensional sampling methods to deal with these
                  data. For dimensions higher than one, few results
                  have been reported, except uniform random sampling,
                  which does not perform well. In the present paper,
                  we explore 2D sampling methodologies for
                  curvelet-based reconstruction, possessing sampling
                  spectra with blue noise characteristics, such as
                  Poisson Disk sampling, Farthest Point Sampling, and
                  the 2D extension of jittered sampling. These
                  sampling methods are shown to lead to better
                  recovery and results are compared to the other more
                  traditional sampling protocols.},
	Author = {G. Tang and R. Shahidi and F. J. Herrmann and J. Ma},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:21:51 -0700},
	Date-Modified = {2009-10-05 13:30:59 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {191-195},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/tang09seghdb.pdf},
	Title = {Higher dimensional blue-noise sampling schemes for curvelet-based seismic data recovery},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/tang09seghdb.pdf}}

@conference{yan09gpb,
	Abstract = {The removal of groundroll in land based seismic
                  data is a critical step for seismic imaging. In this
                  paper, we introduce a work flow to predict the
                  groundroll by interferometry and then separate the
                  groundroll in the curvelet domain. Thus workflow is
                  similar to the workflow of surface-related multiple
                  elimination (SRME). By exploiting the adaptability
                  and sparsity of curvelets, we are able to
                  significantly improve the separation of groundroll
                  in comparison to results yielded by frequency-domain
                  adaptive subtraction methods. We provide synthetic
                  data example to illustrate our claim.},
	Author = {J. Yan and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:19:18 -0700},
	Date-Modified = {2009-10-05 13:32:36 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {3297-3301},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09seggrp.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/yan09seggpi.pdf},
	Title = {Groundroll prediction by interferometry and separation by curvelet-domain matched filtering},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09seggrp.pdf}}

@conference{kumar09ins,
	Abstract = {The separation of signal and noise is a key issue
                  in seismic data processing. By noise we refer to the
                  incoherent noise that is present in the data. We use
                  the recently introduced multiscale and
                  multidirectional curvelet transform for suppression
                  of random noise. The curvelet transform decomposes
                  data into directional plane waves that are local in
                  nature. The coherent features of the data occupy the
                  large coefficients in the curvelet domain, whereas
                  the incoherent noise lives in the small
                  coefficients. In other words, signal and noise have
                  minimal overlap in the curvelet domain. This gives
                  us a chance to use curvelets to suppress noise
                  present in data.},
	Author = {V. Kumar and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:18:02 -0700},
	Date-Modified = {2009-10-05 13:33:58 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {3356-3360},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09segins.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/kumar09segins.pdf},
	Title = {Incoherent noise suppression with curvelet-domain sparsity},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09segins.pdf}}

@conference{erlangga09swi,
	Abstract = {This abstract discusses an implicit implementation
                  of the Gauss-Newton method, used for the
                  frequency-domain full-waveform inversion, where the
                  inverse of the Hessian for the update is never
                  formed explicitly. Instead, the inverse of the
                  Hessian is computed approximately by a conjugate
                  gradient (CG) method, which only requires the action
                  of the Hessian on the CG search direction. This
                  procedure avoids an excessive computer storage,
                  usually needed for storing the Hessian, at the
                  expense of extra computational work in CG. An
                  effective preconditioner for the Hessian is
                  important to improve the convergence of CG, and
                  hence to reduce the overall computational work.},
	Author = {Y. A. Erlangga and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:16:22 -0700},
	Date-Modified = {2009-10-05 13:36:27 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {2357-2361},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09segswi.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg09/erlangga09fwi.pdf},
	Title = {Seismic waveform inversion with Gauss-Newton-Krylov method},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09segswi.pdf}}

@conference{sahidi09cdm,
	Abstract = {In Herrmann et al. (2008), it is shown that
                  zero-order pseudodifferential operators, which model
                  the migration-demigration operator and the operator
                  mapping the predicted multiples to the true
                  multiples, can be represented by a diagonal
                  weighting in the curvelet domain. In that paper, a
                  smoothness constraint was introduced in the phase
                  space of the operator in order to regularize the
                  solution to make it unique. In this paper, we use
                  recent results in Demanet and Ying (2008) on the
                  discrete symbol calculus to impose a further
                  smoothness constraint, this time in the frequency
                  domain. It is found that with this additional
                  constraint, faster convergence is realized. Results
                  on a synthetic pseudodifferential operator as well
                  as on an example of primary-multiple separation in
                  seismic data are included, comparing the model with
                  and without the new smoothness constraint, from
                  which it is found that results of improved quality
                  are also obtained.},
	Author = {R. Shahidi and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:14:41 -0700},
	Date-Modified = {2009-10-05 13:37:33 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {3645},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/shahidi09segcmf.pdf},
	Title = {Curvelet-domain matched filtering with frequency-domain regularization},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/shahidi09segcmf.pdf}}

@conference{lin09ucs,
	Abstract = {The central promise of simultaneous acquisition is
                  a vastly improved crew efficiency during acquisition
                  at the cost of additional post-processing to obtain
                  conventional source-separated data volumes. Using
                  recent theories from the field of compressive
                  sensing, we present a way to systematically model
                  the effects of simultaneous acquisition. Our
                  formulation form a new framework in the study of
                  acquisition design and naturally leads to an
                  inversion-based approach for the separation of shot
                  records. Furthermore, we show how other
                  inversion-based methods, such as a recently proposed
                  method from van Groenestijn and Verschuur (2009) for
                  primary estimation, can be processed together with
                  the demultiplexing problem to achieve a better
                  result compared to a separate treatment of these
                  problems.},
	Author = {T.T.Y. Lin and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:13:09 -0700},
	Date-Modified = {2009-10-05 13:40:51 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {3113-3117},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09segucf.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg09/lin09segucs.pdf},
	Title = {Unified compressive sensing framework for simultaneous acquisition with primary estimation},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09segucf.pdf}}

@conference{Lin09csf,
	Abstract = {The fact that the computational complexity of
                  wavefield simulation is proportional to the size of
                  the discretized model and acquisition geometry, and
                  not to the complexity of the simulated wavefield, is
                  a major impediment within seismic imaging. By
                  turning simulation into a compressive sensing
                  problem--where simulated data is recovered from a
                  relatively small number of independent simultaneous
                  sources--we remove this impediment by showing that
                  compressively sampling a simulation is equivalent to
                  compressively sampling the sources, followed by
                  solving a reduced system. As in compressive sensing,
                  this allows for a reduction in sampling rate and
                  hence in simulation costs. We demonstrate this
                  principle for the time-harmonic Helmholtz
                  solver. The solution is computed by inverting the
                  reduced system, followed by a recovery of the full
                  wavefield with a sparsity promoting
                  program. Depending on the wavefield's sparsity, this
                  approach can lead to a significant cost reduction,
                  in particular when combined with the implicit
                  preconditioned Helmholtz solver, which is known to
                  converge even for decreasing mesh sizes and
                  increasing angular frequencies. These properties
                  make our scheme a viable alternative to explicit
                  time-domain finite-difference.},
	Author = {T.T.Y. Lin and F. J. Herrmannand Y. A. Erlangga},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:11:16 -0700},
	Date-Modified = {2009-10-05 14:04:04 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {2577},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09segcss.pdf},
	Publisher = {SEG},
	Title = {Compressive simultaneous full-waveform simulation},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09segcss.pdf}}

@conference{herrmann09cib,
	Abstract = {Migration relies on multi-dimensional correlations
                  between source- and residual wavefields. These
                  multi-dimensional correlations are computationally
                  expensive because they involve operations with
                  explicit and full matrices that contain both
                  wavefields. By leveraging recent insights from
                  compressive sampling, we present an alternative
                  method where linear correlation-based imaging is
                  replaced by imaging via multidimensional
                  deconvolutions of compressibly sampled
                  wavefields. Even though this approach goes at the
                  expense of having to solve a sparsity-promotion
                  recovery program for the image, our wavefield
                  inversion approach has the advantage of reducing the
                  system size in accordance to transform-domain
                  sparsity of the image. Because seismic images also
                  exhibit a focusing of the energy towards zero
                  offset, the compressive-wavefield inversion itself
                  is carried out using a recent extension of one-norm
                  solver technology towards matrix-valued
                  problems. These so-called hybrid $(1,\,2)$-norm
                  solvers allow us to penalize pre-stack energy away
                  from zero offset while exploiting joint sparsity
                  amongst near-offset images. Contrary to earlier work
                  to reduce modeling and imaging costs through random
                  phase-encoded sources, our method compressively
                  samples wavefields in model space. This approach has
                  several advantages amongst which improved
                  system-size reduction, and more flexibility during
                  subsequent inversions for subsurface properties.},
	Author = {F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:09:07 -0700},
	Date-Modified = {2009-10-05 13:41:46 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {2337-2341},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09segciw.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/herrmann09segciw.pdf},
	Publisher = {SEG},
	Title = {Compressive imaging by wavefield inversion with group sparsity},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09segciw.pdf}}

@conference{herrmann09rpl,
	Abstract = {By combining Percolation models with lithological
                  smoothing, we arrive at method for upscaling rock
                  elastic constants that preserves reflections. In
                  this approach, the Percolation model predicts sharp
                  onsets in the elastic moduli of sand-shale mixtures
                  when the shales reach a critical volume fraction. At
                  that point, the shale inclusions form a connected
                  cluster, and the macroscopic rock properties change
                  with the power-law growth of the cluster. This
                  switch-like nonlinearity preserves singularities,
                  and hence reflections, even if no sharp transition
                  exists in the lithology or if they are smoothed out
                  using standard upscaling procedures.},
	Author = {F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2009-06-23 16:07:07 -0700},
	Date-Modified = {2009-10-05 13:44:52 -0700},
	Keywords = {SEG},
	Number = {1},
	Organization = {SEG},
	Pages = {3466-3470},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09segrlu.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/herrmann09segrpu.pdf},
	Publisher = {SEG},
	Title = {Reflector-preserved lithological upscaling},
	Volume = {28},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09segrlu.pdf}}

@conference{saab09asn,
	Abstract = {In this note, we summarize the results we recently
                  proved in\cite{SY08} on the theoretical performance
                  guarantees of the decoders $\Delta_p$. These
                  decoders rely on $\ell^p$ minimization with $p \in
                  (0,1)$ to recover estimates of sparse and
                  compressible signals from incomplete and inaccurate
                  measurements. Our guarantees generalize the results
                  of \cite{CRT05} and \cite{Wojtaszczyk08} about
                  decoding by $\ell_p$ minimization with $p=1$, to the
                  setting where $p \in (0,1)$ and are obtained under
                  weaker sufficient conditions. We also present novel
                  extensions of our results in \cite{SY08} that follow
                  from the recent work of DeVore et al. in
                  \cite{DPW08}. Finally, we show some insightful
                  numerical experiments displaying the trade-off in
                  the choice of $p \in (0,1]$ depending on certain
                  properties of the input signal.},
	Author = {R. Saab and O. Yilmaz},
	Date-Added = {2009-06-23 16:04:31 -0700},
	Date-Modified = {2009-06-23 16:06:19 -0700},
	Organization = {SAMPTA},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/SY09sampta.pdf},
	Title = {A short note on non-convex compressed sensing},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/SY09sampta.pdf}}

@conference{Herrmann09cws,
	Abstract = {Full-waveform inversion's high demand on
                  computational resources forms, along with the
                  non-uniqueness problem, the major impediment
                  withstanding its widespread use on industrial-size
                  datasets. Turning modeling and inversion into a
                  compressive sensing problem---where simulated data
                  are recovered from a relatively small number of
                  independent simultaneous sources---can effectively
                  mitigate this high-cost impediment. The key is in
                  showing that we can design a sub-sampling operator
                  that commutes with the time-harmonic Helmholtz
                  system. As in compressive sensing, this leads to a
                  reduction in simulation cost. Moreover, this
                  reduction is commensurate with the transform-domain
                  sparsity of the solution, implying that
                  computational costs are no longer determined by the
                  size of the discretization but by transform-domain
                  sparsity of the solution of the CS problem which
                  forms our data. The combination of this sub-sampling
                  strategy with our recent work on implicit solvers
                  for the Helmholtz equation provides a viable
                  alternative to full-waveform inversion schemes based
                  on explicit finite-difference methods.},
	Author = {F. J. Herrmann and Y. A. Erlangga and T.T.Y. Lin},
	Date-Added = {2009-06-23 16:02:17 -0700},
	Date-Modified = {2009-06-23 16:04:25 -0700},
	Keywords = {SAMPTA},
	Organization = {SAMPTA},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/herrmann09sampta.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/SAMPTA2009.pdf},
	Title = {Compressive-waveefield simulations},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/herrmann09sampta.pdf}}

@conference{lin09dsa,
	Author = {T.T.Y. Lin and F. J. Herrmann},
	ABSTRACT = {The goal of this paper is in designing a functional simultaneous
acquisition scheme by applying the principles of compressive sensing. By framing the
acquisition in a compressive sensing setting we immediately gain insight into not
only how to choose the source signature and shot patterns, but also in how well we
can hope to demultiplex the data when given a set amount of reduction in the number
of sweeps. The principles of compressive sensing dictates that the quality of the
demultiplexed data is closely related to the transform-domain sparsity of the
solution. This means that, given an estimate in the complexity of the expectant data
wavefield, it is possible to controllably reduce the number of shots that needs to
be recorded in the field. We show a proof of concept by introducing an acquisition
compatible with compressive sensing based on randomly phase-encoded vibroseis
sweeps.},
	Date-Added = {2009-06-23 16:00:19 -0700},
	Date-Modified = {2009-06-23 16:02:03 -0700},
	Keywords = {EAGE},
	Organization = {EAGE},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/CompAq.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/lin2009eagedsa.pdf}
	Publisher = {EAGE},
	Title = {Designing simultaneous acquisitions with compressive sensing},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/CompAq.pdf}}

@conference{Herrmann09csa,
	Abstract = {With the recent resurgence of full-waveform
                  inversion, the computational cost of solving forward
                  modeling problems has become---aside from issues
                  with non-uniqueness---one of the major impediments
                  withstanding successful application of this
                  technology to industry-size data volumes. To
                  overcome this impediment, we argue that further
                  improvements in this area will depend on a problem
                  formulation with a computational complexity that is
                  no longer strictly determined by the size of the
                  discretization but by transform-domain sparsity of
                  its solution. In this new paradigm, we bring
                  computational costs in par with our ability to
                  compress seismic data and images. This premise is
                  related to two recent developments.  First, there is
                  the new field of compressive sensing (CS in short
                  throughout the paper, Cand`es et al., 2006; Donoho,
                  2006)---where the argument is made, and rigorously
                  proven, that compressible signals can be recovered
                  from severely sub-Nyquist sampling by solving a
                  sparsity promoting program. Second, there is in the
                  seismic community the recent resurgence of
                  simultaneous-source acquisition (Beasley, 2008;
                  Krohn and Neelamani, 2008; Herrmann et al., 2009;
                  Berkhout, 2008; Neelamani et al., 2008), and
                  continuing efforts to reduce the cost of seismic
                  modeling, imaging, and inversion through phase
                  encoding of simultaneous sources (Morton and Ober,
                  1998; Romero et al., 2000; Krohn and Neelamani,
                  2008; Herrmann et al., 2009), removal of subsets of
                  angular frequencies (Sirgue and Pratt, 2004; Mulder
                  and Plessix, 2004; Lin et al., 2008) or plane waves
                  (Vigh and Starr, 2008). By using CS principles, we
                  remove sub-sampling interferences asocciated with
                  these approaches through a combination of exploiting
                  transform-domain sparsity, properties of certain
                  sub-sampling schemes, and the existence of sparsity
                  promoting solvers.},
	Author = {F. J. Herrmann and Y. A. Erlangga and T.T.Y. Lin},
	Date-Added = {2009-06-23 15:57:24 -0700},
	Date-Modified = {2009-06-23 15:59:58 -0700},
	Keywords = {EAGE},
	Organization = {EAGE},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/herrmann09eage.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09eagecs.pdf},
	Publisher = {EAGE},
	Title = {Compressive sensing applied to full-waveform inversion},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/herrmann09eage.pdf}}

@conference{Erlangga09mwi,
	Abstract = {From the measured seismic data, the location and
                  the amplitude of reflectors can be determined via a
                  migration algorithm. Classically, following
                  Claerbout's imaging principle [2], a reflector is
                  located at the position where the source's
                  forward-propagated wavefield correlates with the
                  backward-propagated wavefield of the receiver
                  data. Lailly and Tarantola later showed that this
                  imaging principle is an instance of inverse
                  problems, with the associated migration operator
                  formulated via a least-squares functional; see [6,
                  12, 13]. Furthermore, they showed that the migrated
                  image is associated with the gradient of this
                  functional with respect to the image. If the
                  solution of the least-squares functional is done
                  iteratively, the correlation-based image coincides
                  up to a constant with the first iteration of a
                  gradient method. In practice, this migration is done
                  either in the time domain or in the frequency
                  domain.  In the frequency-domain migration, the main
                  bottleneck thus far, which renders its full
                  implementation to large scale problems, is the lack
                  of efficient solvers for computing
                  wavefields. Robust direct methods easily run into
                  excessive memory requirements as the size of the
                  problem increases. On the other hand, iterative
                  methods, which are less demanding in terms of
                  memory, suffered from lack of convergence.  During
                  the past years, however, progress has been made in
                  the development of an efficient iterative method [4,
                  3] for the frequency-domain wavefield
                  computations. In this paper, we will show the
                  significance of this method (called MKMG) in the
                  context of the frequency-domain migration, where
                  multi-shot-frequency wavefields (of order of 10,000
                  related wavefields) need to be computed.},
	Author = {Y. A. Erlangga and F. J. Herrmann},
	Date-Added = {2009-06-23 15:51:04 -0700},
	Date-Modified = {2009-06-23 15:56:25 -0700},
	Keywords = {EAGE migration},
	Organization = {EAGE},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/erlangga2009.pdf},
	talkurl={http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/erlanggaEAGE2009.pdf},
	Publisher = {EAGE},
	Title = {Migration with implicit solvers for the time-harmonic Helmholtz equation},
	Year = {2009},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/erlangga2009.pdf}}

@conference{vandenberg08ocf,
	Abstract = {An optimization algorithm for minimizing a smooth
                  function over a convex set is described.  Each
                  iteration of the method computes a descent direction
                  by minimizing, over the original constraints, a
                  diagonal-plus low-rank quadratic approximation to
                  the function. The quadratic approximation is
                  constructed using a limited-memory quasi-Newton
                  update. The method is suitable for large-scale
                  problems where evaluation of the function is
                  substan- tially more expensive than projection onto
                  the constraint set. Numerical experiments on one-
                  norm regularized test problems indicate that the
                  proposed method is competitve with state- of-the-art
                  methods such as bound-constrained L-BFGS and
                  orthant-wise descent. We further show that the
                  method generalizes to a wide class of problems, and
                  substantially improves on state-of-the-art methods
                  for problems such as learning the structure of
                  Gaussian graphi- cal models (involving
                  positive-denite matrix constraints) and Markov
                  random elds (in- volving second-order cone
                  constraints).},
	Author = {E. {van den} Berg, and M. Schmidt, and M. P. Friedlander, and K. Murphy},
	Date-Added = {2009-01-29 17:02:44 -0800},
	Date-Modified = {2009-01-29 17:13:16 -0800},
	Keywords = {SLIM},
	Month = {April},
	Pdf = {http://www.cs.ubc.ca/~mpf/public/group.pdf},
	Series = {Twelfth International Conference on Artificial Intelligence and Statistics},
	Title = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm},
	Volume = {12},
	Year = {2009}}

@conference{Herrmann08cdm3,
	Abstract = {Matching seismic wavefields lies at the heart of
                  seismic processing whether one is adaptively
                  subtracting multiples predictions or groundroll. In
                  both cases, the predictions are matched to the
                  actual to-be-separated wavefield components in the
                  observed data. The success of these wavefield
                  matching procedures depends on our ability to (i)
                  control possible overfitting, which may lead to
                  accidental removal of primary energy, (ii) handle
                  data with nonunique dips, and (iii) apply wavefield
                  separation after matching stably. In this paper, we
                  show that the curvelet transform allows us to
                  address these issues by imposing smoothness in phase
                  space, by using their capability to handle
                  conflicting dips, and by leveraging their ability to
                  represent seismic data sparsely.},
	Author = {F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-11-21 14:54:57 -0800},
	Date-Modified = {2008-11-21 14:59:01 -0800},
	Keywords = {SLIM, SEG},
	Month = {November},
	Number = {1},
	Pages = {3643-3647},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08segws.pdf},
	Publisher = {SEG},
	Title = {Curvelet-domain matched filtering},
	Volume = {27},
	Year = {2008}}

@conference{vandenberg08ocf,
	Abstract = {An optimization algorithm for minimizing a smooth
                  function over a convex set is described.  Each
                  iteration of the method computes a descent direction
                  by minimizing, over the original constraints, a
                  diagonal-plus low-rank quadratic approximation to
                  the function. The quadratic approximation is
                  constructed using a limited-memory quasi-Newton
                  update. The method is suitable for large-scale
                  problems where evaluation of the function is
                  substan- tially more expensive than projection onto
                  the constraint set. Numerical experiments on one-
                  norm regularized test problems indicate that the
                  proposed method is competitve with state- of-the-art
                  methods such as bound-constrained L-BFGS and
                  orthant-wise descent. We further show that the
                  method generalizes to a wide class of problems, and
                  substantially improves on state-of-the-art methods
                  for problems such as learning the structure of
                  Gaussian graphi- cal models (involving
                  positive-definite matrix constraints) and Markov
                  random fields (in- volving second-order cone
                  constraints).},
	Author = {E. {van den} Berg, and M. Schmidt, and M. P. Friedlander, and K. Murphy},
	Date-Added = {2009-01-29 17:16:34 -0800},
	Date-Modified = {2009-01-29 17:16:34 -0800},
	Keywords = {SLIM},
	Month = {April},
	Pdf = {http://www.cs.ubc.ca/~mpf/public/group.pdf},
	Series = {Twelfth International Conference on Artificial Intelligence and Statistics},
	Title = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm},
	Volume = {12},
	Year = {2009}}

@article{saab08srb,
	Abstract = {In this note, we address the theoretical
                  properties of $\Delta_p$, a class of compressed
                  sensing decoders that rely on $l^p$ minimization
                  with $p \in (0, 1)$ to recover estimates of sparse
                  and compressible signals from incomplete and
                  inaccurate measurements. In particular, we extend
                  the results of Cand`es, Romberg and Tao [3] and
                  Wojtaszczyk [30] regarding the decoder $\Delta_1$,
                  based on $\ell^1$ minimization, to $\Delta p$ with
                  $p \in (0, 1)$. Our results are two-fold. First, we
                  show that under certain sufficient conditions that
                  are weaker than the analogous sufficient conditions
                  for $\Delta_1$ the decoders $\Delta_p$ are robust to
                  noise and stable in the sense that they are $(2, p)$
                  instance optimal. Second, we extend the results of
                  Wojtaszczyk to show that, like $\Delta_1$, the
                  decoders $\Delta_p$ are (2, 2) instance optimal in
                  probability provided the measurement matrix is drawn
                  from an appropriate distribution. While the
                  extension of the results of [3] to the setting where
                  $p \in (0, 1)$ is straightforward, the extension of
                  the instance optimality in probability result of
                  [30] is non-trivial. In particular, we need to prove
                  that the $LQ_1$ property, introduced in [30], and
                  shown to hold for Gaussian matrices and matrices
                  whose columns are drawn uniformly from the sphere,
                  generalizes to an $LQ_p$ property for the same
                  classes of matrices. Our proof is based on a result
                  by Gordon and Kalton [18] about the Banach-Mazur
                  distances of p-convex bodies to their convex hulls.},
	Author = {Rayan Saab and \ozgur},
	Journal = {Applied and Computational Harmonic Analysis},
	Keywords = {non-convex},
	pdf = {http://arxiv.org/pdf/0809.0745v1},
	url = {http://dx.doi.org/10.1016/j.acha.2009.08.002},
	Title = {Sparse Recovery by Non-Convex Optimization - Instance Optimality},
	Year = {2009}}

@misc{fenelon08msc,
	Abstract = {Physical constraints during seismic acquisitions
                  lead to incomplete seismic datasets. Curvelet
                  Reconstruction with Sparsity promoting Inversion
                  (CRSI) is one of the most efficient interpolation
                  method available to recover complete datasets from
                  data with missing traces. The method uses in its
                  definition the curvelet transform which is well
                  suited to process seismic data.  However, its main
                  shortcoming is to not be able to provide an accurate
                  result if the data are acquired at irregular
                  positions. This come from the curvelet transform
                  implementation which cannot handle this type of
                  data. In this thesis the implementation of the
                  curvelet transform is modified to offer the
                  possibility to CRSI to give better representation of
                  seismic data for high quality seismic imaging.  },
	Author = {Lloyd Fenelon},
	Date-Added = {2008-09-03 16:18:08 -0700},
	Date-Modified = {2008-09-03 16:25:10 -0700},
	Howpublished = {BSc thesis, Ecole Nationale Superieure De Physique de Strasbourg},
	Keywords = {SLIM, BSc},
	Month = {August},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/fenelon08msc.pdf},
	Title = {Nonequispaced discrete curvelet transform for seismic data reconstruction},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/fenelon08msc.pdf}}

@misc{lin08cco,
	Abstract = {In this work an explicit algorithm for the
                  extrapolation of one-way wavefields is proposed
                  which combines recent developments in information
                  theory and theoretical signal processing with the
                  physics of wave propagation. Because of excessive
                  memory requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from ``compressed sensing'', we are able
                  to formulate the (inverse) wavefield extrapolation
                  problem on small subsets of the data volume, thereby
                  reducing the size of the operators. Compressed
                  sensing entails a new paradigm for signal recovery
                  that provides conditions under which signals can be
                  recovered from incomplete samplings by
                  \emph{nonlinear} recovery methods that promote
                  sparsity of the to-be-recovered signal. According to
                  this theory, signals can successfully be recovered
                  when the measurement basis is \emph{incoherent} with
                  the representation in which the wavefield is
                  sparse. In this new approach, the eigenfunctions of
                  the Helmholtz operator are recognized as a basis
                  that is incoherent with sparsity transforms that are
                  known to compress seismic wavefields. By casting the
                  wavefield extrapolation problem in this framework,
                  wavefields can successfully be extrapolated in the
                  modal domain, despite evanescent wave modes. The
                  degree to which the wavefield can be recovered
                  depends on the number of missing (evanescent) wave
                  modes and on the complexity of the wavefield. A
                  proof of principle for the ``compressed sensing''
                  method is given for inverse wavefield extrapolation
                  in 2-D. The results show that our method is stable,
                  has reduced dip limitations and handles evanescent
                  waves in inverse extrapolation.},
	Author = {Tim Lin},
	Howpublished = {BSc Honours thesis, The University of British Columbia},
	Keywords = {SLIM, BSc},
	Month = {April},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/timlin08bsch.pdf},
	Title = {Compressed computation of large-scale wavefield extrapolation in inhomogeneous medium},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/timlin08bsch.pdf}}

@mastersthesis{lebed08msc,
	Abstract = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last thirty years
                  many transforms such as wavelets, curvelets,
                  contourlets, surfacelets, shear- lets, and many
                  other types of 'x-lets' have been developed. Such
                  transform were leveraged to resolve this issue of
                  sparse representations. In this work we compare the
                  properties of four of these commonly used
                  transforms, namely the shift-invariant wavelets,
                  complex wavelets, curvelets and surfacelets. We also
                  explore the performance of these transforms for the
                  problem of recov- ering seismic wavefields from
                  incomplete measurements.  },
	Author = {E. Lebed},
	Date-Added = {2008-08-29 13:16:58 -0700},
	Date-Modified = {2008-08-29 13:19:15 -0700},
	Keywords = {SLIM, MSc},
	Month = {August},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/lebed08msc.pdf},
	School = {The University of British Columbia},
	Title = {Sparse Signal Recovery in a Transform Domain},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/lebed08msc.pdf}}

@techreport{Erlangga08oam,
	Author = {Y. A. Erlangga and R. Nabben},
	Date-Modified = {2008-11-20 18:02:38 -0800},
	Institution = {UBC Earth and Ocean Sciences Department},
	Journal = {Elec. Trans. Numer. Anal.},
	Keywords = {SLIM},
	Number = {TR-2008-2},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Journals/Yogi/erlangga-nabben-helmholtz.pdf},
	Title = {On a multilevel projection Krylov method for the Helmholtz equation preconditioned by shifted Laplacian},
	Volume = {submitted},
	Year = {2008}}

@conference{herrmann03oiw,
	Author = {F. J. Herrmann},
	Booktitle = {2003 SEG},
	Date-Added = {2008-08-28 17:37:04 -0700},
	Date-Modified = {2008-08-28 17:38:23 -0700},
	Keywords = {Presentation, SLIM, SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg03/herrmann03oiw.pdf},
	Title = {Optimal Imaging Curvelets},
	Year = {2003}}

@conference{herrmann04cdm1,
	Author = {F. J. Herrmann},
	Booktitle = {SEG 2004},
	Date-Added = {2008-08-28 17:35:45 -0700},
	Date-Modified = {2008-08-28 17:36:43 -0700},
	Keywords = {Presentation,SLIM},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg04/herrmann04cdm.pdf},
	Title = {Curvelet-domain multiple elimination with sparseness constraints},
	Year = {2004}}

@conference{Herrmann04cbn1,
	Author = {F. J. Herrmann},
	Booktitle = {SEG 2004},
	Date-Added = {2008-08-28 17:34:20 -0700},
	Date-Modified = {2008-08-28 17:35:35 -0700},
	Keywords = {Presentation, SLIM, SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg04/herrmann04cbn.pdf},
	Title = {Curvelet-based non-linear adaptive subtraction with sparseness constraints},
	Year = {2004}}

@conference{hennenfent04tta1,
	Author = {G. Hennenfent},
	Booktitle = {SEG 2004},
	Date-Added = {2008-08-28 17:32:03 -0700},
	Date-Modified = {2008-08-28 17:33:58 -0700},
	Keywords = {SLIM, Presentation, SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg04/hennenfent04tta.pdf},
	Title = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by curvelet and wavelet transforms},
	Year = {2004}}

@conference{beyreuther04cdo1,
	Author = {M. Beyreuther},
	Booktitle = {2004 EAGE},
	Date-Added = {2008-08-28 17:22:15 -0700},
	Date-Modified = {2008-08-28 17:26:13 -0700},
	Keywords = {SLIM, EAGE, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage04/beyreuther04cdo.pdf},
	Title = {Curvelet denoising of 4-D seismic},
	Year = {2004}}

@misc{slimp08,
	Abstract = {SLIMpy is a Python interface that exposes the
                  functionality of seismic data processing packages,
                  such as MADAGASCAR, through operator
                  overloading. SLIMpy provides a concrete
                  coordinate-free implementation of classes for
                  out-of-core linear (implicit matrix-vector), and
                  element-wise operations, including calculation of
                  norms and other basic vector operations. The library
                  is intended to provide the user with an abstract
                  scripting language to program iterative algorithms
                  from numerical linear algebra. These algorithms
                  require repeated evaluation of operators that were
                  initially designed to be run as part of
                  batch-oriented processing flows. The current
                  implementation supports a plugin for Madagascar's
                  out-of-core UNIX pipe-based applications and is
                  extenable to pipe- based collections of programs
                  such as Seismic Un*x, SEPLib, and FreeUSP. To
                  optimize performance, SLIMpy uses an Abstract Syntax
                  Tree that parses the algorithm and optimizes the
                  pipes.}}

@conference{friedlander08asa,
	Author = {M. P. Friedlander},
	Booktitle = {SIAM Optimization},
	Date-Added = {2008-08-28 14:34:58 -0700},
	Date-Modified = {2008-08-28 14:36:37 -0700},
	Keywords = {SLIM,Presentation},
	Month = {May},
	Organization = {SIAM Optimization},
	Talkurl = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf},
	Title = {Active-set Approaches to Basis Pursuit Denoising},
	Year = {2008}}

@conference{friedlander08asm,
	Abstract = {Many imaging and compressed sensing applications
                  seek sparse solutions to large under-determined
                  least-squares problems.  The basis pursuit (BP)
                  approach minimizes the 1-norm of the solution, and
                  the BP denoising (BPDN) approach balances it against
                  the least-squares fit.  The duals of these problems
                  are conventional linear and quadratic programs.  We
                  introduce a modified parameterization of the BPDN
                  problem and explore the effectiveness of active-set
                  methods for solving its dual.  Our basic algorithm
                  for the BP dual unifies several existing algorithms
                  and is applicable to large-scale examples.},
	Author = {M. P. Friedlander and M. A. Saunders},
	Booktitle = {West Coast Opitmization Meeting (WCOM)},
	Month = {September},
	Talkurl = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf},
	Title = {Active-set methods for basis pursuit},
	Year = {2008}}

@unpublished{friedlander07ero1,
	Author = {M. P. Friedlander and P. Tseng},
	Date-Added = {2008-08-28 14:31:38 -0700},
	Date-Modified = {2008-08-28 14:32:15 -0700},
	Keywords = {SLIM,Presentation},
	Month = {October},
	Title = {Exact Regularization of Convex Programs},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/~mpf/public/mpf07caltech.pdf}}

@unpublished{friedlander07ipo,
	Author = {M. P. Friedlander},
	Date-Added = {2008-08-28 14:29:44 -0700},
	Date-Modified = {2008-08-28 14:31:09 -0700},
	Keywords = {SLIM,Presentations},
	Month = {November},
	Note = {Arizona},
	Talkurl = {http://www.cs.ubc.ca/~mpf/public/mpf07ams.pdf},
	Title = {In Pursuit of a Root},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/~mpf/public/mpf07arizona.pdf}}

@conference{vandenberg07ipo1,
	Author = {E. van den Berg and M. P. Friedlander},
	Booktitle = {2007 Von Neumann Symposium},
	Date-Added = {2008-08-28 14:25:37 -0700},
	Date-Modified = {2008-08-28 14:28:11 -0700},
	Keywords = {SLIM,minimization, Presentation},
	Talkurl = {http://www.cs.ubc.ca/~mpf/public/mpf07ams.pdf},
	Title = {In Pursuit of a Root},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/~mpf/public/mpf07ams.pdf}}

@conference{friedlander09a,
	Address = {Hanoi, Vietnam},
	Author = {M. P. Friedlander},
	Booktitle = {2009 High Performance Scientific Computing Conference},
	Keywords = {SLIM,minimization, Presentation},
	Title = {Computing sparse and group-sparse approximations},
	Year = {2009}}

@conference{friedlander09b,
	Address = {Northwestern University},
	Author = {M. P. Friedlander},
	Booktitle = {IEMS Colloquim Speaker},
	Keywords = {SLIM,minimization, Presentation},
	Title = {Algorithms for large-scale sparse reconstruction},
	Year = {2009}}

@conference{friedlander09b,
	Address = {University of British Columbia},
	Author = {E. van den Berg and M. P. Friedlander},
	Booktitle = {SCAIM Seminar},
	Keywords = {SLIM,minimization, Presentation},
	Title = {Spot: A linear-operator toolbox for Matlab},
	Year = {2009}}

@articletechreport{vandendberg07ipo,
	Abstract = {The basis pursuit technique is used to find a
                  minimum one-norm solution of an un- derdetermined
                  least-squares problem. Basis pursuit denoise fits
                  the least-squares problem only approximately, and a
                  single parameter determines a curve that traces the
                  trade-off between the least-squares fit and the
                  one-norm of the solution. We show that the function
                  that describes this curve is convex and continuously
                  differ- entiable over all points of interest. The
                  dual solution of a least-squares problem with an
                  explicit one-norm constraint gives function and
                  derivative information needed for a root-finding
                  method. As a result, we can compute arbitrary points
                  on this curve. Numerical experiments demonstrate
                  that our method, which relies on only matrix-vector
                  operations, scales well to large problems. },
	Author = {E. van den Berg and M. P. Friedlander},
	Date-Added = {2008-08-28 14:21:37 -0700},
	Date-Modified = {2008-08-28 14:23:12 -0700},
	Institution = {UBC},
	Keywords = {SLIM,SPGL1},
	Month = {June},
	Title = {In Pursuit of a Root},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/~mpf/downloads/BergFrie07.pdf}}

@article{friedlander07ero,
	Abstract = {The regularization of a convex program is exact if
                  all solutions of the regularized problem are also
                  solutions of the original problem for all values of
                  the regularization parameter below some positive
                  threshold. For a general convex program, we show
                  that the regularization is exact if and only if a
                  certain selection problem has a Lagrange
                  multiplier. Moreover, the regularization parameter
                  threshold is inversely related to the Lagrange
                  multiplier. We use this result to generalize an
                  exact regularization result of Ferris and
                  Mangasarian [Appl. Math. Optim., 23(1991),
                  pp. 266--273] involving a linearized selection
                  problem. We also use it to derive necessary and
                  sufficient conditions for exact penalization,
                  similar to those obtained by Bertsekas
                  [Math. Programming, 9(1975), pp. 87--99] and by
                  Bertsekas, Nedi , Ozdaglar [Convex Analysis and
                  Optimization, Athena Scientific, Belmont, MA, 2003].
                  When the regularization is not exact, we derive
                  error bounds on the distance from the regularized
                  solution to the original solution set. We also show
                  that existence of a ``weak sharp minimum'' is in
                  some sense close to being necessary for exact
                  regularization. We illustrate the main result with
                  numerical experiments on the l1 regularization of
                  benchmark (degenerate) linear programs and
                  semidefinite/second-order cone programs. The
                  experiments demonstrate the usefulness of l1
                  regularization in finding sparse solutions.},
	Author = {M. P. Friedlander and P. Tseng},
	Date-Added = {2008-08-28 14:16:25 -0700},
	Date-Modified = {2008-08-28 14:18:56 -0700},
	Doi = {10.1137/060675320},
	Journal = {SIAM J. Optim},
	Keywords = {SLIM},
	Number = {4},
	Pages = {1326-1350},
	Title = {Exact Regularization of Convex Programs},
	Volume = {18},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/~mpf/downloads/FrieTsen07.pdf}}

@article{friedlander07dtd,
	Author = {M. P. Friedlander and M. A. Saunders},
	Date-Added = {2008-08-28 14:10:19 -0700},
	Date-Modified = {2008-08-28 14:20:29 -0700},
	Doi = {10.1214/009053607000000479},
	Journal = {The Annals of Statistics},
	Keywords = {statistics, dantzig, SLIM},
	Number = {6},
	Pages = {2385-2391},
	Title = {Discussion: The Dantzig Selector: Statistical estimation when p is much larger then n},
	Volume = {35},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/~mpf/downloads/FriedlanderSaunders08.pdf}}

@techreport{BergFriedlander:2010,
	Abstract = {The use of convex optimization for the recovery of
                  sparse signals from incomplete or compressed data is
                  now common practice. Motivated by the success of
                  basis pursuit in recovering sparse vectors, new
                  formulations have been proposed that take advantage
                  of different types of sparsity. In this paper we
                  propose an efficient algorithm for solving a general
                  class of sparsifying formulations. For several
                  common types of sparsity we provide applications,
                  along with details on how to apply the algorithm,
                 a nd experimental results.},
	Address = {University of British Columbia, Vancouver},
	Author = {E. van den Berg and M. P. Friedlander},
	Institution = {Department of Computer Science},
	Month = {January},
	Number = {TR-2010-02},
	Title = {Sparse optimization with least-squares constraints},
	Type = {Tech. Rep.},
	Year = 2010}

@techreport{BergFriedlander:2009,
	Abstract = {The joint-sparse recovery problem aims to recover, from
                  sets of compressed measurements, unknown sparse
                  matrices with nonzero entries restricted to a subset
                  of rows. This is an extension of the
                  single-measurement-vector (SMV) problem widely
                  studied in compressed sensing. We analyze the
                  recovery properties for two types of recovery
                  algorithms. First, we show that recovery using
                  sum-of-norm minimization cannot exceed the uniform
                  recovery rate of sequential SMV using L1
                  minimization, and that there are problems that can
                  be solved with one approach but not with the
                  other. Second, we analyze the performance of the
                  ReMBo algorithm [M. Mishali and Y. Eldar, IEEE
                  Trans. Sig. Proc., 56 (2008)] in combination with L1
                  minimization, and show how recovery improves as more
                  measurements are taken. From this analysis it
                  follows that having more measurements than number of
                  nonzero rows does not improve the potential
                  theoretical recovery rate.},
	Address = {University of British Columbia, Vancouver},
	Author = {E. van den Berg and M. P. Friedlander},
	Institution = {Department of Computer Science},
	Month = {September},
	Note = {to appear in {\it IEEE Trans. Info. Th.}},
	Number = {TR-2009-7},
	Title = {Theoretical and empirical results for recovery from multiple measurements},
	Type = {Tech. Rep.},
	Year = 2009}

@techreport{herrmann08cdm1,
	Abstract = {Matching seismic wavefields and images lies at the
                  heart of many pre-/post-processing steps part of
                  seismic imaging---whether one is matching predicted
                  wavefield components, such as multiples, to the
                  actual to-be-separated wavefield components present
                  in the data or whether one is aiming to restore
                  migration amplitudes by scaling, using an
                  image-to-remigrated-image matching procedure to
                  calculate the scaling coefficients. The success of
                  these wavefield matching procedures depends on our
                  ability to (i) control possible overfitting, which
                  may lead to accidental removal of energy or to
                  inaccurate image-amplitude corrections, (ii) handle
                  data or images with nonunique dips, and (iii) apply
                  subsequent wavefield separations or migraton
                  amplitude corrections stably. In this paper, we show
                  that the curvelet transform allows us to address all
                  these issues by imposing smoothness in phase space,
                  by using their capability to handle conflicting
                  dips, and by leveraging their ability to represent
                  seismic data and images sparsely. This latter
                  property renders curvelet-domain sparsity promotion
                  an effective prior.},
	Author = {F. J. Herrmann and P. P. Moghaddam and D. Wang},
	Date-Added = {2008-08-14 13:46:26 -0700},
	Date-Modified = {2008-08-14 16:34:56 -0700},
	Institution = {UBC Earth and Ocean Sciences Department},
	Keywords = {SLIM},
	Month = {August},
	Number = {TR-2008-6},
	Pdf = {http://slim.eos.ubc.ca/Publications//Public/Conferences/SEG/2008/herrmann08segmat.pdf},
	Title = {Curvelet-domain matched filtering},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications//Public/Conferences/SEG/2008/herrmann08segmat.pdf}}

@article{BergFriedlander:2008,
	Abstract = {The basis pursuit problem seeks a minimum one-norm
                  solution of an underdetermined least-squares
                  problem. Basis pursuit denoise (BPDN) fits the
                  least-squares problem only approximately, and a
                  single parameter determines a curve that traces the
                  optimal trade-off between the least-squares fit and
                  the one-norm of the solution. We prove that this
                  curve is convex and continuously differentiable over
                  all points of interest, and show that it gives an
                  explicit relationship to two other optimization
                  problems closely related to BPDN.  We describe a
                  root-finding algorithm for finding arbitrary points
                  on this curve; the algorithm is suitable for
                  problems that are large scale and for those that are
                  in the complex domain. At each iteration, a spectral
                  gradient-projection method approximately minimizes a
                  least-squares problem with an explicit one-norm
                  constraint. Only matrix-vector operations are
                  required. The primal-dual solution of this problem
                  gives function and derivative information needed for
                  the root-finding method.  Numerical experiments on a
                  comprehensive set of test problems demonstrate that
                  the method scales well to large problems.},
				  Author = {E. van den Berg and M. P. Friedlander},
				  Title = {Probing the Pareto frontier for basis pursuit solutions},
				  publisher = {SIAM},
				  year = {2008},
				  journal = {SIAM Journal on Scientific Computing},
				  volume = {31},
				  number = {2},
				  pages = {890-912},
				  keywords = {basis pursuit, convex program, duality, root-finding,
				              Newton's method, projected gradient,
				              one-norm regularization, sparse solutions
				             },
				  pdf = {http://link.aip.org/link/?SCE/31/890},
				  doi = {10.1137/080714488}				 }


@inproceedings{Herrmann2003SPIE,
	Abstract = { Seismic imaging commits itself to locating
                  singularities in the elastic properties of the
                  Earth's subsurface.  Using the high-frequency
                  ray-Born approximation for scattering from
                  non-intersecting smooth interfaces, seismic data can
                  be represented by a generalized Radon transform
                  mapping the singularities in the medium to seismic
                  data. Even though seismic data are bandwidth
                  limited, signatures of the singularities in the
                  medium carry through this transform and its inverse
                  and this mapping property presents us with the
                  possibility to develop new imaging techniques that
                  preserve and characterize the singularities from
                  incomplete, bandwidth-limited and noisy data. In
                  this paper we propose a non-adaptive
                  Curvelet/Contourlet technique to image and preserve
                  the singularities and a data-adaptive Matching
                  Pursuit method to characterize these imaged
                  singularities by Multi-fractional Splines. This
                  first technique borrows from the ideas within the
                  Wavelet-Vaguelette/Quasi-SVD approach. We use the
                  almost diagonalization of the scattering operator to
                  approximately compensate for (i) the coloring of the
                  noise and hence facilitate estimation; (ii) the
                  normal operator itself. Results of applying these
                  techniques to seismic imaging are encouraging
                  although many open fundamental questions remain. },
	Author = {F. J. Herrmann},
	Booktitle = {Proceedings of SPIE Technical Conference on Wavelets: Applications in Signal and Image Processing X},
	Editor = {M. Unser and A. Aldroubi},
	Keywords = {SLIM},
	Opturl = {http://www.eos.ubc.ca/~felix/Preprint/SPIE03DEF.pdf},
	Organization = {SPIE},
	Pages = {240-258},
	Pdf = {http://www.eos.ubc.ca/~felix/Preprint/SPIE03DEF.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/herrmann03msa.pdf},
	Title = {Multifractional splines: application to seismic imaging},
	Volume = {5207},
	Year = {2003}}

@misc{slimpy08,
	Abstract = {SLIMpy is a Python interface that exposes the
                  functionality of seismic data processing packages,
                  such as MADAGASCAR, through oper ator
                  overloading. SLIMpy provides a concrete
                  coordinate-free implementation of classes for
                  out-of-core linear (implicit matrix-vector), and
                  element -wise operations, including calculation of
                  norms and other basic vector operations. The library
                  is intended to provide the user with an abstract sc
                  ripting language to program iterative algorithms
                  from numerical linear algebra. These algorithms
                  require repeated evaluation of operators that were
                  initially designed to be run as part of
                  batch-oriented processing flows. The current
                  implementation supports a plugin for Madagascar's
                  out-of-core UNIX pipe-based applications and is
                  extenable to pipe-based collections of programs such
                  as Seismic Un*x, SEPLib, and FreeUSP. To optimize
                  perform ance, SLIMpy uses an Abstract Syntax Tree
                  that parses the algorithm and optimizes the pipes.},
	Author = {S. Ross-Ross and H. Modzeleweski and F. J. Herrmann},
	Month = {July},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Sean_Sli.pdf},
	Title = {{SLIMPY}: a python interface for unix-pipe based coordinate-free scientific computing},
	Url = {http://slim.eos.ubc.ca/SLIMpy/},
	Year = 2008,
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/SLIMpy/}}

@misc{spgl107,
	Abstract = {\spgl{} is a Matlab solver for large-scale
                  one-norm regularized least squares. It is designed
                  to solve any of the following three problems; Basis
                  pursuit denoise (BPDN):
                  \begin{equation}\label{BPs}\tag{\BPs} \minimize{x}
                  \quad \Vert x\Vert_1 \quad \text{subject to} \quad
                  \Vert Ax-b\Vert_2\le\sigma; \end{equation} Basis
                  pursuit: \begin{equation}\tag{\BP} \minimize{x}
                  \quad \Vert x\Vert_1 \quad \text{subject to} \quad
                  Ax=b; \end{equation} and Lasso:
                  \begin{equation}\tag{\LSt} \minimize{x} \quad \Vert
                  Ax-b\Vert_2 \quad \text{subject to} \quad \Vert
                  x\Vert_1 \le \tau. \end{equation} \spgl{} relies
                  only on matrix-vector operations $Ax$ and $A^Ty$ and
                  accepts both explicit matrices and functions that
                  evaluate these products. \spgl{} is suitable for
                  problems that are in the complex domain.  \\ The
                  multiple measurement vectors (MMV ) version of
                  \ref{BPs} solves \begin{equation}\tag{\MMVs}
                  \minimize{X} \quad \Vert X\Vert_{1,2} \quad
                  \text{subject to} \quad \Vert AX - B\Vert_F \leq
                  \sigma; \end{equation} with $\Vert\cdot\Vert_F$ the
                  Frobenius norm and the mixed $(p,q)$-norm applied to
                  $m\times n$ matrix $X$, defined as $$ \Vert
                  X\Vert_{p,q} := \left(\sum_{i=1}^m \Vert
                  X_i^T\Vert_q^p\right)^{1/p}, $$ where $X_i$ denotes
                  the $i$-th row of $X$. When weights are given, they
                  apply to each row of $X$.
                  \vspace{12pt}\noindent\dotfill\vspace{12pt} In
                  group-sparse BPDN each entry of $x$ is assigned to a
                  group. Denoting by $\Lambda_i$ the indices of group
                  $i$, the group-sparse BPDN problem, for $\sigma \geq
                  0$, is given by: \begin{equation}\tag{\GRPs}
                  \minimize{x} \quad \sum_i \Vert x_{\Lambda_i}\Vert_2
                  \quad \text{subject to} \quad \Vert Ax - b\Vert_2
                  \leq \sigma.  \end{equation} It is assumed that the
                  $\Lambda_i$ are disjoint and that their union gives
                  the set $\{1,\ldots,n\}$. When weights are given,
                  they apply to each group.},
	Author = {E. {van den} Berg and M. P. Friedlander},
	Keywords = {SLIM},
	Month = {June},
	Title = {{SPGL1}: A solver for large-scale sparse reconstruction},
	Url = {http://www.cs.ubc.ca/labs/scl/spgl1},
	Year = 2007,
	Bdsk-Url-1 = {http://www.cs.ubc.ca/labs/scl/spgl1}}

@manual{hennenfent08rap,
	Abstract = {repro is a Python package for automating
                  reproducible research in scientific computing. repro
                  works in combination with SCons, a next-generation
                  build tool. The package is freely available over the
                  Internet. Downloading and installation instructions
                  are provided in this gui de. The repro package is
                  documented in various ways (many comments in source
                  code, this guide---written using repro itself!---and
                  a reference guide ). In this user's guide, we
                  present a few pedagogical examples that uses Matlab,
                  Python, Seismic Unix (SU), and Madagascar. We also
                  include demo pa pers. These papers are written in
                  \LaTeX\ and compiled using repro. The figures they
                  contain are automatically generated from the source
                  codes prov ided. In that sense, the demo papers are
                  a model of self-contained documents that are fully
                  reproducible.  The repro package is largely inspired
                  by some parts of Madagascar, a geophysical software
                  package for reproducible research. However, the
                  repro package is intended for a broad audience co
                  ming from a wide spectrum of interest areas. },
	Author = {G. Hennenfent and S. Ross-Ross},
	Date-Added = {2008-08-28 11:50:31 -0700},
	Date-Modified = {2008-08-28 12:14:13 -0700},
	Institution = {The University of British Columbia},
	Keywords = {SLIM},
	Month = {August},
	Organization = {Seismic Laboratory for Imaging and Modeling},
	Title = {TR2008-7 Repro: a Python package for automating reproducible research in scientific computing},
	Type = {software package},
	Year = {2008}}

@misc{sparco07,
	Abstract = {Sparco is a suite of problems for testing and
                  benchmarking algorithms for sparse signal
                  reconstruction. It is also an environment for
                  creating new test problems, and a suite of standard
                  linear operators is provided from which new problems
                  can be assembled. Sparco is implement ed entirely in
                  Matlab and is self contained. (A few optional test
                  problems are based on the CurveLab toolbox, which
                  can be installed separately.)  At the core of the
                  sparse recovery problem is the linear system
                  $Ax+r=b$, where $A$ is an $m$-by-$n$ linear operator
                  and the $m$-vector $b$ is the observed signal. The
                  goal is to find a sparse $n$-vector $x$ such that
                  $r$ is small in norm.},
	Author = {E. van den Berg and M. P. Friedlander},
	Keywords = {SLIM},
	Month = {October},
	Pdf = {http://www.cs.ubc.ca/labs/scl/sparco/downloads.php?filename=sparco.pdf},
	Title = {SPARCO: A toolbox for testing sparse reconstruction algorithms},
	Url = {http://www.cs.ubc.ca/labs/scl/sparco/},
	Year = 2007,
	Bdsk-Url-1 = {http://www.cs.ubc.ca/labs/scl/sparco/}}

@conference{herrmann08cmf,
	Abstract = { Matching seismic wavefields lies at the heart of
                  seismic processing whether one is adaptively
                  subtracting multiples predictions or groundroll. In
                  both cases, the predictions are matched to the
                  actual to-be-separated wavefield components in the
                  observed data. The success of these wavefield
                  matching procedures depends on our ability to (i)
                  control possible overfitting, which may lead to
                  accidental removal of primary energy, (ii) handle
                  data with nonunique dips, and (iii) apply wavefield
                  separation after matching stably. In this paper, we
                  show that the curvelet transform allows us to
                  address these issues by imposing smoothness in phase
                  space, by using their capability to handle
                  conflicting dips, and by leveraging their ability to
                  represent seismic data sparsely.},
	Author = {F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Issue = {1},
	Keywords = {SLIM},
	Month = {November},
	Number = {1},
	Optdate-Added = {2008-08-14 13:17:16 -0700},
	Optdate-Modified = {2008-08-14 16:34:43 -0700},
	Pdf = {http://slim.eos.ubc.ca/Publications//Public/Conferences/SEG/2008/herrmann08segws.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08cmf.pdf},
	Publisher = {SEG Workshop},
	Title = {Curvelet-domain matched filtering},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications//Public/Conferences/SEG/2008/herrmann08segwav.pdf}}

@conference{hennenfent07jdn,
	Abstract = {In this talk, we turn the interpolation problem of
                  coarsely-sampled data into a denoising problem. From
                  this point of view, we illustrate the benefit of
                  random sampling at sub-Nyquist rate over regular
                  sampling at the same rate. We show that, using
                  nonlinear sparsity-promoting optimization, coarse
                  random sampling may actually lead to significantly
                  better wavefield reconstruction than equivalent
                  regularly sampled data.},
	Author = {G. Hennenfent},
	Booktitle = {SINBAD 2007},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Just denoise. Nonlinear recovery from randomly sampled data},
	Year = {2007}}

@conference{herrmann07rdi2,
	Abstract = {In this talk, we present a novel primary-multiple
                  separation scheme which makes use of the sparsity of
                  both primaries and multiples in a transform domain,
                  such as the curvelet transform, to provide estimates
                  of each. The proposed algorithm utilizes seismic
                  data as well as the output of a preliminary step
                  that provides (possibly) erroneous predictions of
                  the multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm. Time permitted, we will also briefly
                  discuss a propasal for adaptive curvelet-domain
                  matched filtering. This is joint work with Deli
                  Wang, Rayan Saaba, \ozgur Yilmaz and Eric
                  Verschuur.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2007},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Recent developments in primary-multiple separation},
	Year = {2007}}

@conference{yarham07nsw,
	Abstract = {Removal of surface waves is an integral step in
                  seismic processing. There are many standard
                  techniques for removal of this type of coherent
                  noise, such as f-k filtering, but these methods are
                  not always effective.  One of the common problems
                  with removal of surface waves is that they tend to
                  be aliased in the frequency domain.  This can make
                  removal difficult and affect the frequency content
                  of the reflector signals, as this signals will not
                  be completely separated. As seen in (Hennenfent,
                  G. and F. Herrmann, 2006, Application of stable
                  signal recovery to seismic interpolation)
                  interpolation can be used effectively to resample
                  the seismic record thus dealiasing the surface
                  waves.  This separates the signals in the frequency
                  domain allowing for a more precise and complete
                  removal.  The use of this technique with curvelet
                  based surface wave predictions and an iterative L1
                  separation scheme can be used to remove surface
                  waves from shot records more completely that with
                  standard techniques.},
	Author = {C. Yarham},
	Booktitle = {SINBAD 2007},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Nonlinear surface wave prediction and separation},
	Year = {2007}}

@conference{sastry07nor,
	Abstract = {Seismic traces are sampled irregularly and
                  insufficiently due to practical and economical
                  limitations. The use of such data in seismic imaging
                  results in image artifacts and poor spatial
                  resolution. Therefore, before being used, the
                  measurements are to be interpolated onto a regular
                  grid. One of the methods achieving this objective is
                  based on the Fourier reconstruction, which deals
                  with the under-determined system of equations. The
                  recent pursuit techniques (namely, basis pursuit,
                  matching pursuit etc) admit certain promising
                  features such as faster and simpler implementation
                  even in large scale settings.  The presentation
                  discusses the application of the pursuit algorithms
                  to the Fourier-based interpolation problem for the
                  signals that have sparse Fourier spectra.  In
                  particular, the objective of the presentation
                  includes: 1).  studying the performance of the
                  algorithm if, and how far, the measurement
                  coordinates can be shifted from uniform distribution
                  on the continuous interval.  2).  studying what
                  could be the allowable misplacement in the
                  measurement coordinates that does not alter the
                  quality of the reconstruction process},
	Author = {C. Sastry},
	Booktitle = {SINBAD 2007},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Norm-one recovery from irregular sampled data},
	Year = {2007}}

@conference{hennenfent07rii,
	Abstract = {During this talk, an overview is given on our work
                  on norm-one solvers as part of the DNOISE
                  project. Gilles will explain the ins and outs of our
                  iterative thresholding solver based on log cooling
                  while Felix will present the work of Michael
                  Friedlander "A Newton root-finding algorithms for
                  large-scale basis pursuit denoise". Both approaches
                  involve the solution of the basis pursuit problem
                  that seeks a minimum one-norm solution of an
                  underdetermined least-squares problem. Basis pursuit
                  denoise (BPDN) fits the least-squares problem only
                  approximately, and a single parameter determines a
                  curve that traces the trade-off between the
                  least-squares fit and the one-norm of the solution.
                  In the work of Friedlander, it is shown show that
                  the function that describes this curve is convex and
                  continuously differentiable over all points of
                  interest. They describe an efficient procedure for
                  evaluating this function and its derivatives. As a
                  result, they can compute arbitrary points on this
                  curve. Their method is suitable for large-scale
                  problems. Only matrix-vector operations are
                  required. This is joint work with Ewout van der Berg
                  and Michael P. Friedlander},
	Author = {G. Hennenfent and F. J. Herrmann},
	Booktitle = {SINBAD 2007},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Recent insights in $L_1$ solvers},
	Year = {2007}}

@conference{herrmann07sia2,
	Abstract = {In this talk, we recover the amplitude of a
                  seismic image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time 'wave-equation' migration code
                  simulating the acoustic wave equation on the SEG-AA
                  salt model. This is joint work with Peyman Moghaddam
                  and Chris Stolk (University of Twente)},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2007},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Seismic image amplitude recovery},
	Year = {2007}}

@conference{herrmann07frw,
	Abstract = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach. During the first step, the
                  recently developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation. This is joint work
                  with Deli Wang and Gilles Hennenfent.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2007},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Focused recovery with the curvelet transform},
	Year = {2007}}

@conference{herrmann07cwe,
	Abstract = {An explicit algorithm for the extrapolation of
                  one-way wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from ``compressed sensing'', we are able
                  to formulate the (inverse) wavefield extrapolation
                  problem on small subsets of the data volume, thereby
                  reducing the size of the operators. According to
                  compressed sensing theory, signals can successfully
                  be recovered from an incomplete set of measurements
                  when the measurement basis is incoherent with the
                  representation in which the wavefield is sparse. In
                  this new approach, the eigenfunctions of the
                  Helmholtz operator are recognized as a basis that is
                  incoherent with curvelets that are known to compress
                  seismic wavefields. By casting the wavefield
                  extrapolation problem in this framework, wavefields
                  can successfully be extrapolated in the modal domain
                  via a computationally cheaper operation. A proof of
                  principle for the ``compressed sensing'' method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. This is joint
                  work with Tim Lin.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2007},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Compressed wavefield extrapolation},
	Year = {2007}}

@conference{maysami07src2,
	Abstract = {Seismic transitions of the subsurface are
                  typically considered as zero-order singularities
                  (step functions). According to this model, the
                  conventional deconvolution problem aims at
                  recovering the seismic reflectivity as a sparse
                  spike train. However, recent multiscale analysis on
                  sedimentary records revealed the existence of
                  accumulations of varying order singularities in the
                  subsurface, which give rise to fractional-order
                  discontinuities. This observation not only calls for
                  a richer class of seismic reflection waveforms, but
                  it also requires a different methodology to detect
                  and characterize these reflection events. For
                  instance, the assumptions underlying conventional
                  deconvolution no longer hold. Because of the
                  bandwidth limitation of seismic data, multiscale
                  analysis methods based on the decay rate of wavelet
                  coefficients may yield ambiguous results. We avoid
                  this problem by formulating the estimation of the
                  singularity orders by a parametric nonlinear
                  inversion method.},
	Author = {M. Maysami},
	Booktitle = {SINBAD 2007},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Seismic reflector characterization by detection-estimation},
	Year = {2007}}

@conference{herrmann06om,
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/1-Felix1.pdf},
	Title = {Opening meeting},
	Year = {2006}}

@conference{hennenfent06apo,
	Abstract = {During this presentation an introduction will be
                  given on the method of stable recovery from noisy
                  and incomplete data. Strong recovery conditions that
                  guarantee the recovery for arbitrary acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented.},
	Author = {G. Hennenfent},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/2-Gilles1.pdf},
	Title = {A primer on stable signal recovery},
	Year = {2006}}

@conference{herrmann06sra,
	Abstract = {During this presentation an overview will be given
                  on how seismic data regularization and separation
                  problems can be cast into the framework of stable
                  signal recovery. It is shown that the successful
                  solution of these two problems depends on the
                  existence of signal expansions that are
                  compressible. Preliminary examples will be shown.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/3-Felix2.pdf},
	Title = {Stable recovery and separation of seismic data},
	Year = {2006}}

@conference{herrmann06apo,
	Abstract = {During this presentation an overview will be given
                  on the different sparsity transforms that are used
                  at SLIM. Emphasis will be on two directional and
                  multiscale wavelet transforms, namely the curvelet
                  and the recently introduced wave-atom
                  transforms. The main properties of these transforms
                  will be listed and their performance on seismic data
                  will be discussed.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/4-Felix3.pdf},
	Title = {A primer on sparsity transforms: curvelets and wave atoms},
	Year = {2006}}

@conference{hennenfent06ros,
	Abstract = {We propose a method for seismic data interpolation
                  based on 1) the reformulation of the problem as a
                  stable signal recovery problem and 2) the fact that
                  seismic data is sparsely represented by
                  curvelets. This method does not require information
                  on the seismic velocities. Most importantly, this
                  formulation potentially leads to an explicit
                  recovery condition. We also propose a large-scale
                  problem solver for the l1-regularization
                  minimization involved in the recovery and
                  successfully illustrate the performance of our
                  algorithm on 2D synthetic and real examples.},
	Author = {G. Hennenfent},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/5-Gilles2.pdf},
	Title = {Recovery of seismic data: practical considerations},
	Year = {2006}}

@conference{herrmann06apow,
	Abstract = {During this presentation an introduction will be
                  given on the method of stable recovery from noisy
                  and incomplete data. Weak recovery conditions that
                  guarantee the recovery for typical acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented. The advantage of these
                  weak conditions is that they are less pessimistic
                  and `verifiable' or very large-scale acquisition
                  geometries.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/6-Felix4.pdf},
	Title = {A primer on weak conditions for stable recovery},
	Year = {2006}}

@conference{yarham06apo,
	Abstract = {During this presentation an introduction will be
                  given on morphological component analysis to
                  separate signal components. Main concepts such as
                  the sparck and block solvers will be introduced.},
	Author = {C. Yarham},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/7-Carson1.pdf},
	Title = {A primer on morphological component analysis},
	Year = {2006}}

@conference{herrmann06pms,
	Abstract = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, during which
                  multiples are predicted from seismic data, and a
                  primary-multiple separation step, during which the
                  predicted multiples are 'matched' with the true
                  multiples in the data and subsequently removed. The
                  last step is crucial in practice: an incorrect
                  separation will cause residual multiple energy in
                  the result or may lead to a distortion of the
                  primaries, or both. To reduce these adverse effects,
                  a new transformed-domain method is proposed where
                  primaries and multiples are separated rather than
                  matched. This separation is carried out on the basis
                  of differences in the multiscale and
                  multidirectional characteristics of these two signal
                  components. Our method uses the curvelet transform,
                  which maps multidimensional data volumes into almost
                  orthogonal localized multidimensional prototype
                  waveforms that vary in directional and
                  spatio-temporal content. Primaries-only and
                  multiples-only signal components are recovered from
                  the total data volume by a nonlinear optimization
                  scheme that is stable under noisy input data. During
                  the optimization, the two signal components are
                  separated by enhancing sparseness (through weighted
                  l1-norms) in the transformed domain subject to
                  fitting the observed data as the sum of the
                  separated components to within a user-defined
                  tolerance level. Whenever the prediction for the two
                  signal components in the transformed domain
                  correlate, the recovery is suppressed while for
                  regions where the correlation is small the method
                  seeks the sparsest set of coefficients that
                  represent each signal component. Our algorithm does
                  not seek a matched filter and as such it differs
                  fundamentally from traditional adaptive subtraction
                  methods. The method derives its stability from the
                  sparseness obtained by a non-parametric multiscale
                  and multidirectional overcomplete signal
                  representation. This sparsity serves as prior
                  information and allows for a Bayesian interpretation
                  of our method during which the log-likelihood
                  function is minimized while the two signal
                  components are assumed to be given by a
                  superposition of prototype waveforms, drawn
                  independently from a probability function that is
                  weighted by the predicted primaries and
                  multiples. In this paper, the predictions are based
                  on the data-driven surface-related multiple
                  elimination (SRME) method.  Synthetic and field data
                  examples show a clean separation leading to a
                  considerable improvement in multiple suppression
                  compared to the conventional method of adaptive
                  matched filtering. This improved separation
                  translates into an improved stack.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/8-Felix5.pdf},
	Title = {Primary-multiple separation by curvelet frames},
	Year = {2006}}

@conference{yarham06grr,
	Abstract = {We have effectively identified and removed ground
                  roll through a twostep process. The first step is to
                  identify the major components of the ground roll
                  through various methods including multiscale
                  separation, directional or frequency filtering or by
                  any other method that identifies the ground
                  roll. Given this estimate for ground roll, the
                  recorded signal is separated during the second step
                  through a block-coordinate relaxation method that
                  seeks the sparsest set for weighted curvelet
                  coefficients of the ground roll and the sought-after
                  reflectivity. The combination of these two methods
                  allows us to separate out the ground roll signal
                  while preserving the reflector information. Since
                  our method is iterative, we have control of the
                  separation process. We successfully tested our
                  algorithm on a real data set with a complex ground
                  roll and reflector structure.},
	Author = {C. Yarham},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/9-Carson2.pdf},
	Title = {Ground-roll removal},
	Year = {2006}}

@conference{hennenfent06tnf,
	Abstract = {The authors present an extension of the fast
                  discrete curvelet transform (FDCT) to nonuniformly
                  sampled data. This extension not only restores
                  curvelet compression rates for nonuniformly sampled
                  data but also removes noise and maps the data to a
                  regular grid.},
	Author = {G. Hennenfent},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/10-Gilles3.pdf},
	Title = {The Nonuniform Fast Discrete Curvelet Transform (NFDCT)},
	Year = {2006}}

@conference{sastry07rfu,
	Author = {C. Sastry},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/11-Sastry1.pdf},
	Title = {Recovery from unstructured data},
	Year = {2006}}

@conference{maysami06rro,
	Abstract = {One of the important steps in seismic imaging is
                  to provide suitable information about
                  boundaries. Sharp variation of physical properties
                  at a layer boundary cause reflection the
                  wavefield. In previous work done by C. M. Dupuis,
                  seismic signal characterization is divided into two
                  steps: detection and estimation. In the detection
                  phase, the goal is to find all singularities in a
                  seismic section regardless of their order and then
                  to categorize the data to different events by
                  windowing each singularity. In the estimation step,
                  we determine the order of singularity more precisely
                  by using a rough estimate based on the detection
                  phase. Traditionally, a redundant dictionary method
                  is employed for the detection part. However, we
                  attempt to instead use a new L1-solver developed by
                  D.L. Donoho: the Stagewise Orthogonal Matching
                  Pursuit (StOMP). It approximates the solution to
                  inverse problems while promoting the sparsity in the
                  solution vector. This algorithm will allow us to
                  experimentally confirm the recent analysis by
                  S. Mallat on spiky deconvolution limits, which
                  imposes a required minimum distance between
                  spikes. This required minimum distance between
                  different spikes is dependent on the number of
                  spikes as well as the width of the chosen source
                  wavelet used in convolution with the train. These
                  results allow for the design of more robust and
                  accurate detection schemes for seismic signal
                  characterization.},
	Author = {M. Maysami},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/12-Moe1.pdf},
	Title = {Recent results on seismic deconvolution},
	Year = {2006}}

@conference{moghaddam06sac,
	Abstract = {During this presentation, the importance of
                  sparsity and continuity enhancing energy norms is
                  emphasized for seismic imaging and inversion. The
                  continuity promoting energy norm is justified by the
                  apparent smoothness of reflectors in the direction
                  along and the oscillatory behavior across the
                  interfaces. This energy norm is called anisotropic
                  diffusion and will be defined
                  mathematically. Denoising examples will be given
                  during which seismic images are recovered from the
                  noise by a joint norm-one and continuity promoting
                  minimization.},
	Author = {P. Moghaddam},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/13-Peyman1.pdf},
	Title = {Sparsity- and continuity-promoting norms for seismic images},
	Year = {2006}}

@conference{herrmann06sac,
	Abstract = {A nonlinear singularity-preserving solution to
                  seismic image recovery with sparseness and
                  continuity constraints is proposed. The method
                  explicitly explores the curvelet transform as a
                  directional frame expansion that, by virtue of its
                  sparsity on seismic images and its invariance under
                  the Hessian of the linearized imaging problem,
                  allows for a stable recovery of the migration
                  amplitudes from noisy data. The method corresponds
                  to a preconditioning that corrects the amplitudes
                  during a post-processing step.  The solution is
                  formulated as a nonlinear optimization problem where
                  sparsity in the curvelet domain as well as
                  continuity along the imaged reflectors are jointly
                  promoted. To enhance sparsity, the l1-norm on the
                  curvelet coefficients is minimized while continuity
                  is promoted by minimizing an anisotropic diffusion
                  norm on the image. The performance of the recovery
                  scheme is evaluated with 'wave-equation' migration
                  code on a synthetic dataset. This is joint work with
                  Peyman Moghaddam.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/14-Felix6.pdf},
	Title = {Sparsity- and continuity-promoting seismic image recovery with curvelets},
	Year = {2006}}

@conference{moghaddam06ioa,
	Abstract = {In this presentation, the normal
                  (demigation-migration) operator is studied in terms
                  of a pseudo-differential operator. The invariance of
                  curvelets under this operator and their sparsity on
                  the seismic images is used to precondition the
                  migration operator. A brief overview will be given
                  on some of the theory from micro-local analysis
                  which proofs that curvelets remain approximately
                  invariant under the operator. The proper setting for
                  which a diagonal approximation in the curvelet
                  domain is accurate is discussed together with
                  different methods that estimate this diagonal from
                  of-the-shelf migration operators. This is joint work
                  with Chris Stolk.},
	Author = {P. Moghaddam},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/15-Peyman2.pdf},
	Title = {Imaging operator approximation using Curvelets},
	Year = {2006}}

@conference{herrmann06mpf,
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/16-Felix7.pdf},
	Title = {Multiple prediction from incomplete data},
	Year = {2006}}

@conference{lin06ci,
	Abstract = {In 1998 Grimbergen et. al. introduced a new method
                  for computing wavefield propagation which improved
                  on the previously employed local explicit operator
                  method in that it exhibited no dip limitation,
                  accurately handled laterally varying background
                  ground velocity models, and is unconditionally
                  stable. These desirable properties are mainly
                  attributed to bringing the propagation problem into
                  an eigenvector basis that diagonalizes the
                  propagation operators. This modal-transform method,
                  however, requires at each depth-level the solution
                  of a large-scale sparse eigenvalue problem to
                  compute the square-root of the Helmholtz
                  operator. By using recent results from compressed
                  sensing, we hope to reduce these computational costs
                  that typically involve the synthesizes of the
                  imaging operators and the cost of matrix-vector
                  products. To reduce these costs, we compress the
                  extrapolation operators by using only a fraction of
                  the positive eigenvalues and temporal
                  frequencies. This reduction not only leads to
                  smaller matrices but also to reduced synthesis
                  costs. These reductions go at the expense of solving
                  a recovery problem from incomplete data. During the
                  presentation, we show that wavefields can accurately
                  be extrapolated with a compressed operators and
                  competitive costs.},
	Author = {T. Lin},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/17-Tim1.pdf},
	Title = {Compressed imaging},
	Year = {2006}}

@conference{thomson06lss,
	Abstract = {We propose using overlapping, tapered windows to
                  process seismic data in parallel.  This method
                  consists of numerically tight linear operators and
                  adjoints that are suitable for use in iterative
                  algorithms.  This method is also highly scalable and
                  makes parallelprocessing of large seismic data sets
                  feasible.  We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we have applied to a seismic data
                  interpolation algorithm. Some preliminary results
                  will be shown.  Henryk Modzeleweski: Design and
                  specifications for SLIMPy's software framework The
                  SLIM group is actively developing software for
                  seismic imaging. This talk will give a general
                  overview of the software development philosophy
                  adopted by SLIM. The covered topics will include: 1)
                  adopting Python for object-oriented programming, 2)
                  including parallelism into the algorithms used in
                  seismic imaging/modeling, 3) in-house algorithms for
                  seismic imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session ``SINBAD Software
                  releases".},
	Author = {D. Thomson},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/18-Darren1.pdf},
	Title = {Large-scale seismic data recovery by the Parallel Windowed Curvelet Transform},
	Year = {2006}}

@conference{modzelewski06das,
	Author = {H. Modzelewski},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/19-Henryk1.pdf},
	Title = {Design and specifications for SLIM's software framework},
	Year = {2006}}

@conference{thomson06ppe,
	Abstract = {The parallel extensions to the SLIMpy environment
                  enable pipe-based processing of large data sets in
                  an MPI-based parallel environment. Parallel
                  processing can be done by straightforward slicing of
                  data, or by using an overlapping domain
                  decomposition that requires communication between
                  different processors.  The principal aim of the
                  parallel extensions is to leave abstract numerical
                  algorithms (ANA's) and applications programmed for
                  use in SLIMpy untouched when moving to parallel
                  processing.  The object-oriented functionality of
                  Python makes this possible.},
	Author = {D. Thomson},
	Booktitle = {SINBAD 2006},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/21-Darren2.pdf},
	Title = {(P)SLIMPy: parallel extension},
	Year = {2006}}

@conference{vandenberg08esr,
	Author = {E. van den Berg},
	Booktitle = {IAM},
	Date-Added = {2008-08-26 15:44:44 -0700},
	Date-Modified = {2008-08-26 15:45:58 -0700},
	Keywords = {SLIM, IAM, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf},
	Title = {Exact sparse reconstruction and neighbourly polytopes},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf}}

@conference{herrmann08csa,
	Abstract = {Seismic data processing and imaging are firmly
                  rooted in the well-established paradigm of regular
                  Nyquist sampling. Faced with a typical uncooperative
                  environment, practitioners of seismic data
                  acquisition make all efforts to comply to this
                  theory by creating regularly-sampled seismic-data
                  volumes that are suitable for Fourier-based
                  processing flows. The current advent of new
                  alternative transform domains--- such as the
                  sparsifying curvelet domain, where seismic data is
                  decomposed into localized, multiscale and
                  multidirectional plane waves--- opens the
                  possibility to change this paradigm by no longer
                  combating sampling irregularity but by embracing
                  it. During this talk, we show that as long as
                  seismic data volumes permit a compressible
                  representation---i.e., data can be represented as a
                  superposition of relatively few number of elementary
                  waveforms--- Nyquist sampling is unnecessary
                  pessimistic. So far, nothing new, we all know from
                  the work on Fourier- or other transform-based
                  seismic-data regularization methodologies that
                  wavefields can be recovered accurately from
                  sub-Nyquist samplings through some sort of
                  optimization procedure. What is new, however, are
                  recent insights from the field of "compressive
                  sampling", which dictate the conditions that
                  guarantee or, at least, in practice provide
                  conditions that favor sparsity-promoting recovery
                  from sub-Nyquist sampling. Random sub-sampling, or
                  to be more precise, jitter sub-sampling creates
                  favorable conditions for curvelet-based recovery. We
                  explain this phenomenon by arguing that this type of
                  sampling leads to noisy data, hence our slogan
                  "Simply denoise: wavefield reconstruction via
                  jittered undersampling", where we bank on separating
                  incoherent sub-sampling noise with curvelet-domain
                  sparsity promotion. During our presentation, we
                  introduce you to what curvelets are, why random
                  jitter sampling is important and why this opens a
                  pathway towards a new paradigm of curvelet-domain
                  seismic data processing. Our claims will be
                  supported by examples on synthetic and field
                  data. This is joint work with Gilles Hennenfent,
                  PhD. student at SLIM. },
	Author = {F. J. Herrmann},
	Booktitle = {ION},
	Date-Added = {2008-08-26 15:42:38 -0700},
	Date-Modified = {2008-08-26 15:43:48 -0700},
	Keywords = {SLIM, ION, Presentation},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07jitter.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08ion_pres.pdf},
	Title = {Compressive sampling: a new paradigm for seismic data acquistion and processing?},
	Url = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07jitter/paper_html},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08ion_pres.pdf}}

@conference{herrmann07smc,
	Author = {F. J. Herrmann},
	Booktitle = {Cyber},
	Date-Added = {2008-08-26 15:41:05 -0700},
	Date-Modified = {2008-08-26 15:42:07 -0700},
	Keywords = {SLIM, Cyber, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07Cyber.pdf},
	Title = {Seismology meets compressive sampling},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07Cyber.pdf}}

@conference{herrmann07csm,
	Author = {F. J. Herrmann},
	Booktitle = {PIMS},
	Date-Added = {2008-08-26 15:39:14 -0700},
	Date-Modified = {2008-08-26 15:39:52 -0700},
	Keywords = {SLIM, PIMS, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07PIMS.pdf},
	Title = {Compressive sampling meets seismic imaging},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07PIMS.pdf}}

@conference{herrmann07sdp1,
	Abstract = {In this abstract, we present a nonlinear
                  curvelet-based sparsity-promoting formulation of a
                  seismic processing flow, consisting of the following
                  steps: seismic data regularization and the
                  restoration of migration amplitudes. We show that
                  the curvelet's wavefront detection capability and
                  invariance under the migration-demigration operator
                  lead to a formulation that is stable under noise and
                  missing data.},
	Author = {F. J. Herrmann},
	Booktitle = {SEG 2007},
	Date-Added = {2008-08-26 15:37:27 -0700},
	Date-Modified = {2008-08-26 15:38:09 -0700},
	Keywords = {SLIM, SEG, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07SEGPROC.pdf},
	Title = {Seismic data processing with curvelets: a multiscale and nonlinear approach},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07SEGPROC.pdf}}

@conference{herrmann07mpf1,
	Abstract = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach. During the first step, the
                  recenly developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation.},
	Author = {F. J. Herrmann},
	Booktitle = {SEG 2007},
	Date-Added = {2008-08-26 15:32:29 -0700},
	Date-Modified = {2008-08-26 15:36:49 -0700},
	Keywords = {SLIM, SEG, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07SEGPRED.pdf},
	Title = {Multiple prediction from incomplete data with the focused curvelet transform},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07SEGPRED.pdf}}

@article{vandenberg08gsv,
	Abstract = {We present an efficient spectral
                  projected-gradient algorithm for optimization
                  subject to a group one-norm constraint. Our approach
                  is based on a novel linear-time algorithm for
                  Euclidean projection onto the one- and group
                  one-norm constraints. Numerical experiments on large
                  data sets suggest that the proposed method is
                  substantially more efficient and scalable than
                  existing methods.},
	Author = {E. van den Berg and M. Schmidt and M. P. Friedlander and K. Murphy},
	Keywords = {SLIM},
	Pdf = {http://www.optimization-online.org/DB_FILE/2008/07/2056.pdf},
	Title = {Group sparsity via linear-time projection},
	Year = {2008}}

@conference{lin07cwe2,
	Abstract = {An \emph{explicit} algorithm for the extrapolation
                  of one-way wavefields is proposed which combines
                  recent developments in information theory and
                  theoretical signal processing with the physics of
                  wave propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in
                  {3-D}. By using ideas from ``\emph{compressed
                  sensing}'', we are able to formulate the (inverse)
                  wavefield extrapolation problem on small subsets of
                  the data volume{,} thereby reducing the size of the
                  operators. According to compressed sensing theory,
                  signals can successfully be recovered from an
                  imcomplete set of measurements when the measurement
                  basis is \emph{incoherent} with the representation
                  in which the wavefield is sparse. In this new
                  approach, the eigenfunctions of the Helmholtz
                  operator are recognized as a basis that is
                  incoherent with curvelets that are known to compress
                  seismic wavefields. By casting the wavefield
                  extrapolation problem in this framework, wavefields
                  can successfully be extrapolated in the modal domain
                  via a computationally cheaper operatoion. A proof of
                  principle for the ``compressed sensing'' method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator.},
	Author = {T. T. Y. Lin},
	Booktitle = {SEG 2007},
	Date-Added = {2008-08-26 15:31:03 -0700},
	Date-Modified = {2008-08-26 15:36:23 -0700},
	Keywords = {SLIM},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/lin07SEG.pdf},
	Title = {Compressed wavefield extrapolation with curvelets},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/lin07SEG.pdf}}

@conference{herrmann07pti,
	Abstract = {In this paper, two different applications of phase
                  transitions to exploration seismology will be
                  discussed. The first application concerns a phase
                  diagram ruling the recovery conditions for seismic
                  data volumes from incomplete and noisy data while
                  the second phase transition describes the behavior
                  of bi-compositional mixtures as a function of the
                  volume fraction. In both cases, the phase
                  transitions are the result of randomness in large
                  system of equations in combination with
                  nonlinearity. The seismic recovery problem from
                  incomplete data involves the inversion of a
                  rectangular matrix. Recent results from the field of
                  "compressive sensing" provide the conditions for a
                  successful recovery of functions that are sparse in
                  some basis (wavelet) or frame (curvelet)
                  representation, by means of a sparsity
                  ($\ell_1$-norm) promoting nonlinear program. The
                  conditions for a successful recovery depend on a
                  certain randomness of the matrix and on two
                  parameters that express the matrix' aspect ratio and
                  the ratio of the number of nonzero entries in the
                  coefficient vector for the sparse signal
                  representation over the number of measurements. It
                  appears that the ensemble average for the success
                  rate for the recovery of the sparse transformed data
                  vector by a nonlinear sparsity promoting program,
                  can be described by a phase transition, demarcating
                  the regions for the two ratios for which recovery of
                  the sparse entries is likely to be successful or
                  likely to fail. Consistent with other phase
                  transition phenomena, the larger the system the
                  sharper the transition. The randomness in this
                  example is related to the construction of the
                  matrix, which for the recovery of spike trains
                  corresponds to the randomly restricted Fourier
                  matrix. It is shown, that these ideas can be
                  extended to the curvelet recovery by
                  sparsity-promoting inversion (CRSI) . The second
                  application of phase transitions in exploration
                  seismology concerns the upscaling problem. To
                  counter the intrinsic smoothing of singularities by
                  conventional equivalent medium upscaling theory, a
                  percolation-based nonlinear switch model is
                  proposed. In this model, the transport properties of
                  bi-compositional mixture models for rocks undergo a
                  sudden change in the macroscopic transport
                  properties as soon as the volume fraction of the
                  stronger material reaches a critical point. At this
                  critical point, the stronger material forms a
                  connected cluster, which leads to the creation of a
                  cusp-like singularity in the elastic moduli, which
                  in turn give rise to specular reflections. In this
                  model, the reflectivity is no longer explicitly due
                  to singularities in the rocks composition. Instead,
                  singularities are created whenever the volume
                  fraction exceeds the critical point. We will show
                  that this concept can be used for a
                  singularity-preserved lithological upscaling.},
	Author = {F. J. Herrmann},
	Booktitle = {COIP},
	Date-Added = {2008-08-26 15:29:22 -0700},
	Date-Modified = {2008-08-26 15:35:51 -0700},
	Keywords = {SLIM, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07COIP.pdf},
	Title = {Phase transitions in explorations seismology: statistical mechanics meets information theory},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07COIP.pdf}}

@conference{herrmann07csi,
	Abstract = {Seismic imaging involves the solution of an
                  inverse-scattering problem during which the energy
                  of (extremely) large data volumes is collapsed onto
                  the Earth's reflectors. We show how the ideas from
                  'compressive sampling' can alleviate this task by
                  exploiting the curvelet transform's 'wavefront-set
                  detection' capability and 'invariance' property
                  under wave propagation. First, a wavelet-vaguellete
                  technique is reviewed, where seismic amplitudes are
                  recovered from complete data by diagonalizing the
                  Gramm matrix of the linearized scattering
                  problem. Next, we show how the recovery of seismic
                  wavefields from incomplete data can be cast into a
                  compressive sampling problem, followed by a proposal
                  to compress wavefield extrapolation operators via
                  compressive sampling in the modal domain. During the
                  latter approach, we explicitly exploit the mutual
                  incoherence between the eigenfunctions of the
                  Helmholtz operator and the curvelet frame elements
                  that compress the extrapolated wavefield. This is
                  joint work with Gilles Hennenfent, Peyman Moghaddam,
                  Tim Lin, Chris Stolk and Deli Wang.},
	Author = {F. J. Herrmann},
	Booktitle = {AMS Von Neumann},
	Date-Added = {2008-08-26 15:27:49 -0700},
	Date-Modified = {2008-08-26 15:34:54 -0700},
	Keywords = {SLIM, AMS Von Neumann, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07Neumann.pdf},
	Title = {Compressive seismic imaging},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07Neumann.pdf}}

@conference{herrmann07sit,
	Abstract = {Inverse problems in (exploration) seismology are
                  known for their large to very large scale. For
                  instance, certain sparsity-promoting inversion
                  techniques involve vectors that easily exceed 230
                  unknowns while seismic imaging involves the
                  construction and application of matrix-free
                  discretized operators where single matrix-vector
                  evaluations may require hours, days or even weeks on
                  large compute clusters. For these reasons, software
                  development in this field has remained the domain of
                  highly technical codes programmed in low-level
                  languages with little eye for easy development, code
                  reuse and integration with (nonlinear) programs that
                  solve inverse problems. Following ideas from the
                  Symes' Rice Vector Library and Bartlett's C++
                  object-oriented interface, Thyra, and
                  Reduction/Transformation operators (both part of the
                  Trilinos software package), we developed a
                  software-development environment based on
                  overloading. This environment provides a pathway
                  from in-core prototype development to out-of-core
                  and MPI 'production' code with a high level of code
                  reuse. This code reuse is accomplished by
                  integrating the out-of-core and MPI functionality
                  into the dynamic object-oriented programming
                  language Python. This integration is implemented
                  through operator overloading and allows for the
                  development of a coordinate-free solver framework
                  that (i) promotes code reuse; (ii) analyses the
                  statements in an abstract syntax tree and (iii)
                  generates executable statements. In the current
                  implementation, we developed an interface to
                  generate executable statements for the out-of-core
                  unix-pipe based (seismic) processing package
                  RSF-Madagascar (rsf.sf.net). The modular design
                  allows for interfaces to other seismic processing
                  packages and to in-core Python packages such as
                  numpy. So far, the implementation overloads linear
                  operators and elementwise reduction/transformation
                  operators. We are planning extensions towards
                  nonlinear operators and integration with existing
                  (parallel) solver frameworks such as Trilinos.},
	Author = {F. J. Herrmann},
	Booktitle = {AIP},
	Date-Added = {2008-08-26 15:26:42 -0700},
	Date-Modified = {2008-08-26 15:34:28 -0700},
	Keywords = {SLIM, AIP, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07AIP2.pdf},
	Title = {Seismic inversion through operator overloading},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07AIP2.pdf}}

@conference{herrmann07ssd,
	Abstract = {In this talk, directional frames, known as
                  curvelets, are used to recover seismic data and
                  images from noisy and incomplete data. Sparsity and
                  invariance properties of curvelets are exploited to
                  formulate the recovery by a `1-norm promoting
                  program. It is shown that our data recovery approach
                  is closely linked to the recent theory of
                  ``compressive sensing'' and can be seen as a first
                  step towards a nonlinear sampling theory for
                  wavefields. The second problem that will be
                  discussed concerns the recovery of the amplitudes of
                  seismic images in clutter. There, the invariance of
                  curvelets is used to approximately invert the Gramm
                  operator of seismic imaging. In the high-frequency
                  limit, this Gramm matrix corresponds to a
                  pseudo-differential operator, which is near diagonal
                  in the curvelet domain.In this talk, directional
                  frames, known as curvelets, are used to recover
                  seismic data and images from noisy and incomplete
                  data. Sparsity and invariance properties of
                  curvelets are exploited to formulate the recovery by
                  a l1-norm promoting program. It is shown that our
                  data recovery approach is closely linked to the
                  recent theory of ``compressive sensing'' and can be
                  seen as a first step towards a nonlinear sampling
                  theory for wavefields. The second problem that will
                  be discussed concerns the recovery of the amplitudes
                  of seismic images in clutter. There, the invariance
                  of curvelets is used to approximately invert the
                  Gramm operator of seismic imaging. In the
                  high-frequency limit, this Gramm matrix corresponds
                  to a pseudo-differential operator, which is near
                  diagonal in the curvelet domain.},
	Author = {F. J. Herrmann},
	Booktitle = {AIP},
	Date-Added = {2008-08-26 15:24:30 -0700},
	Date-Modified = {2008-08-26 15:34:02 -0700},
	Keywords = {SLIM, AIP, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07AIP.pdf},
	Title = {Stable seismic data recovery},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07AIP.pdf}}

@conference{herrmann07srm1,
	Author = {F. J. Herrmann},
	Booktitle = {EAGE},
	Date-Added = {2008-08-26 15:23:23 -0700},
	Date-Modified = {2008-08-26 15:24:15 -0700},
	Keywords = {SLIM, EAGE, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGPRED.pdf},
	Title = {Surface related multiple prediction from incomplete data},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGPRED.pdf}}

@conference{herrmann07rdi1,
	Author = {F. J. Herrmann},
	Booktitle = {EAGE},
	Date-Added = {2008-08-26 15:19:34 -0700},
	Date-Modified = {2008-08-26 15:20:22 -0700},
	Keywords = {SLIM, EAGE, Presentation},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSDEV.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSDEV.pdf},
	Title = {Recent developments in curvelet-based seismic processing},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSDEV.pdf}}

@inproceedings{Herrmann03msa,
	Author = {F. J. Herrmann},
	Booktitle = {SPIE},
	Date-Added = {2008-08-24 17:19:18 -0700},
	Date-Modified = {2008-08-24 17:21:52 -0700},
	Editor = {M. Unser and A. Aldroubi},
	Keywords = {SLIM},
	Organization = {SPIE},
	Pages = {240-258},
	Title = {Multifractional splines: application to seismic imaging},
	Volume = {5207},
	Year = {2003},
	Bdsk-Url-1 = {http://www.eos.ubc.ca/~felix/Preprint/SPIE03DEF.pdf}}

@inproceedings{herrmann07fsd,
	Abstract = {In this essay, a nonlinear and multidisciplinary
                  approach is presented that takes seismic data to the
                  composition of rocks. The presented work has deep
                  roots in the `gedachtengoed' (philosophy) of Delphi
                  spearheaded by Guus Berkhout. Central themes are
                  multiscale, object-orientation and a
                  multidisciplinary approach.},
	Author = {F. J. Herrmann},
	Booktitle = {Berkhout's valedictory address: the conceptual approach of understanding},
	Date-Added = {2008-08-22 13:06:13 -0700},
	Date-Modified = {2008-08-22 13:07:43 -0700},
	Keywords = {SLIM},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/Misc/herrmann07fsd.pdf},
	Title = {From seismic data to the composition of rocks: an interdisciplinary and multiscale approach to exploration seismology},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/Misc/herrmann07fsd.pdf}}

@conference{herrmann07snt,
	Abstract = {In this paper, we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  three problems related to seismic noise, namely the
                  'good', corresponding to noise generated by random
                  sampling; the 'bad', corresponding to coherent noise
                  for which (inaccurate) predictions exist and the
                  'ugly' for which no predictions exist. We will show
                  that the compressive capabilities of curvelets on
                  seismic data and images can be used to tackle these
                  three categories of noise-related problems.},
	Author = {F. J. Herrmann and D. Wilkinson},
	Booktitle = {SEG Summer Research Workshop: Seismic Noise: Origins Preventions, Mitigation, Utilization},
	Date-Added = {2008-08-22 13:03:49 -0700},
	Date-Modified = {2008-08-22 13:05:27 -0700},
	Keywords = {SLIM},
	Note = {Presented at SEG Summer Research Workshop: Seismic Noise: Origins, Prevention, Mitigation, Utilization},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07segws.pdf},
	Title = {Seismic noise: the good, the bad and the ugly},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07segws.pdf}}

@conference{Herrmann08csm,
	Abstract = {Compressive sensing has led to fundamental new
                  insights in the recovery of compressible signals
                  from sub-Nyquist samplings. It is shown how jittered
                  subsampling can be used to create favorable recovery
                  conditions. Applications include mitigation of
                  incomplete acquisitions and wavefield
                  computations. While the former is a direct
                  adaptation of compressive sampling, the latter
                  application represents a new way of compressing
                  wavefield extrapolation operators. Operators are not
                  diagonalized but are compressively sampled reducing
                  the computational costs.},
	Author = {F. J. Herrmann and Y. Erlangga and T. T. Y. Lin},
	Booktitle = {SIAM},
	Date-Added = {2008-08-21 18:22:42 -0700},
	Date-Modified = {2008-08-26 15:46:39 -0700},
	Keywords = {SLIM, SIAM, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08siam08.pdf},
	Title = {Compressive sampling meets seismic imaging},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08siam08.pdf}}

@conference{yarham08bss,
	Abstract = {Accurate and adaptive noise removal is a critical
                  part in seismic processing.  Recent developments in
                  signal separation methods have allowed a more
                  flexible and accurate framework in which to perform
                  ground roll and reflector separation.  The use of a
                  new Bayesian separation scheme developed at the SLIM
                  group that contains control parameters to adjust for
                  the uniqueness of specific problems is used.  The
                  sensitivity and variation of the control parameters
                  is examined and this method is applied to synthetic
                  and real data and the results are compared to
                  previous methods.},
	Author = {C. Yarham},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:42:58 -0700},
	Keywords = {SLIM},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Carson_Gro.pdf},
	Title = {Bayesian signal separation applied to ground-roll removal},
	Year = {2008}}

@conference{wang08rri,
	Abstract = {We present a nonlinear curvelet-based
                  sparsity-promoting formulation for the
                  primary-multiple separation problem. We show that
                  these coherent signal components can be separated
                  robustly by explicitly exploiting the locality of
                  curvelets in phase space (space-spatial frequency
                  plane) and their ability to compress data volumes
                  that contain wavefronts. This work is an extension
                  of earlier results and the presented algorithms are
                  shown to be stable under noise and moderately
                  erroneous multiple predictions.},
	Author = {D. Wang and R. Saab and \"{O} and F. J. Herrmann},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:50:49 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Deli_Rec.pdf},
	Title = {Recent results in curvelet-based primary-multiple separation},
	Year = {2008}}

@conference{lebed08aoc,
	Abstract = {In this talk we explore several applications of
                  the curvelet and surfacelet transforms to seismic
                  data processing. The first application is stable
                  signal recovery in the physical domain - seismic
                  data acquisition is often limited by physical and
                  economic constraints, and the goal is to interpolate
                  the data from a given subset of seismic traces. The
                  second application is signal recovery in a transform
                  domain - we assume that our data comes in a form of
                  a random subset of temporal frequencies and the goal
                  is to recover the missing frequencies from this
                  data. Since seismic signals are generally not
                  bandwidth limited, this in fact becomes an
                  anti-aliasing problem. In both these problems the
                  recovery is resolved via a robust l_1 solver that
                  exploits the sparsity of the signals in
                  curvelet/surfacelet domains. In the last application
                  we explore the problem of primary-multiple
                  separation by simple thresholding.},
	Author = {E. Lebed},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:42:21 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_App.pdf},
	Title = {Applications of Curvelets/Surfacelets to seismic data processing},
	Year = {2008}}

@conference{lebed08aoc,
	Abstract = {Curvelets and Surfacelets are two transforms that
                  aim to achieve a multiscale and a multidirectional
                  decomposition of arbitrary N-dimensional ($N>=2$)
                  signals. While both transforms are Fourier-based,
                  their construction is intrinsically different. In
                  this talk we will give and overview of the
                  construction of the two transforms, and explore
                  their properties such as frequency domain / spatial
                  domain coherence, sparsity, redundancy and
                  computational complexity.},
	Author = {E. Lebed},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:44:29 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_Curv.pdf},
	Title = {Curvelet / Surfacelet comparison},
	Year = {2008}}

@conference{Berg08sat,
	Abstract = {Sparco is a framework for testing and benchmarking
                  algorithms for sparse reconstruction.  It includes a
                  large collection of sparse reconstruction problems
                  drawn from the imaging, compressed sensing, and
                  geophysics literature.  Sparco is also a framework
                  for implementing new test problems and can be used
                  as a tool for reproducible research.  We describe
                  the software environment, and demonstrate its
                  usefulness for testing and comparing solvers for
                  sparse reconstruction.},
	Author = {E. van den Berg},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:54:25 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf},
	Title = {Sparco: A testing framework for sparse reconstruction},
	Year = {2008}}

@conference{herrmann08acd2,
	Abstract = {In many exploration areas, successful separation
                  of primaries and multiples greatly determines the
                  quality of seismic imaging. Despite major advances
                  made by Surface-Related Multiple Elimination (SRME),
                  amplitude errors in the predicted multiples remain a
                  problem. When these errors vary for each type of
                  multiple differently (as a function of offset, time
                  and dip), these amplitude errors pose a serious
                  challenge for conventional least-squares matching
                  and for the recently introduced separation by
                  curvelet-domain thresholding. We propose a
                  data-adaptive method that corrects amplitude errors,
                  which vary smoothly as a function of location, scale
                  (frequency band) and angle. In that case, the
                  amplitudes can be corrected by an element-wise
                  curvelet-domain scaling of the predicted
                  multiples. We show that this scaling leads to a
                  successful estimation of the primaries, despite
                  amplitude, sign, timing and phase errors in the
                  predicted multiples. Our results on synthetic and
                  real data show distinct improvements over
                  conventional least-squares matching, in terms of
                  better suppression of multiple energy and
                  high-frequency clutter and better recovery of the
                  estimated primaries.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:41:23 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Note = {SINBAD 2008},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ada.pdf},
	Title = {Adaptive curvelet-domain primary-multiple separation},
	Year = {2008}}

@conference{herrmann08fwr,
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2008},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Dfo.pdf},
	Title = {(De)-Focused wavefield reconstructions},
	Year = {2008}}

@conference{herrmann08itc,
	Author = {F. J. Herrmann and Y. Erlangga and T. Lin and C. Brown},
	Booktitle = {SINBAD 2008},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Int.pdf},
	Title = {Introduction to compressive (wavefield) computation},
	Year = {2008}}

@conference{herrmann08s2c,
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2008},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ope.pdf},
	Title = {SINBAD 2008 Consortium meeting},
	Year = {2008}}

@conference{herrmann08psm,
	Abstract = {During this talk, I will report on new phase-space
                  regularization functionals defined in terms of
                  splines. This spline representation reduces the
                  dimensionality of estimating our phase-space matched
                  filter.  We will discuss how this filter can be used
                  in migration preconditioning. This is joint work
                  with Christiaan Stolk.},
	Author = {F. J. Herrmann},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:50:20 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Pha.pdf},
	Title = {Phase-space matched filtering and migration preconditioning},
	Year = {2008}}

@conference{hennenfent08sdw2,
	Abstract = {We present a new discrete undersampling scheme
                  designed to favor wavefield reconstruction by
                  sparsity-promoting inversion with transform elements
                  that are localized in the Fourier domain. Our work
                  is motivated by empirical observations in the
                  seismic community, corroborated by recent results
                  from compressive sampling, which indicate favorable
                  (wavefield) reconstructions from random as opposed
                  to regular undersampling. As predicted by theory,
                  random undersampling renders coherent aliases into
                  harmless incoherent random noise, effectively
                  turning the interpolation problem into a much
                  simpler denoising problem.  A practical requirement
                  of wavefield reconstruction with localized
                  sparsifying transforms is the control on the maximum
                  gap size. Unfortunately, random undersampling does
                  not provide such a control and the main purpose of
                  this paper is to introduce a sampling scheme, coined
                  jittered undersampling, that shares the benefits of
                  random sampling, while offering control on the
                  maximum gap size. Our contribution of jittered
                  sub-Nyquist sampling proofs to be key in the
                  formulation of a versatile wavefield
                  sparsity-promoting recovery scheme that follows the
                  principles of compressive sampling.  After studying
                  the behavior of the jittered-undersampling scheme in
                  the Fourier domain, its performance is studied for
                  curvelet recovery by sparsity-promoting inversion
                  (CRSI). Our findings on synthetic and real seismic
                  data indicate an improvement of several decibels
                  over recovery from regularly-undersampled data for
                  the same amount of data collected.},
	Author = {G. Hennenfent},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:53:37 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_jit.pdf},
	Title = {Simply denoise: wavefield reconstruction via jittered undersampling},
	Year = {2008}}

@conference{hennenfent08nii2,
	Abstract = {Several geophysical ill-posed inverse problems are
                  successfully solved by promoting sparsity using
                  one-norm regularization. The practicality of this
                  approach depends on the effectiveness of the
                  one-norm solver used and on its robustness under
                  limited number of iterations. We propose an approach
                  to understand the behavior and evaluate the
                  performance of one-norm solvers. The technique
                  consists of tracking on a graph the data misfit
                  versus the one norm of successive iterates. By
                  comparing the solution paths to the Pareto curve, we
                  are able to assess the performance of the solvers
                  and the quality of the solutions. Such an assessment
                  is particularly relevant given the renewed interest
                  in one-norm regularization.},
	Author = {G. Hennenfent},
	Date-Modified = {2008-08-22 12:49:45 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_New.pdf},
	Title = {New insights into one-norm solvers from the Pareto curve},
	Year = {2008}}

@conference{modzelewski08das,
	Abstract = {The SLIM group is actively developing software for
                  seismic imaging. This talk will give a general
                  overview of the software development during SINBAD
                  project with focus on the final release in February
                  2008. The covered topics will include: 1) adopting
                  Python for object-oriented programming, 2) including
                  parallelism into the algorithms used in seismic
                  imaging/modeling, 3) in-house algorithms for seismic
                  imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session "SINBAD Software
                  releases".},
	Author = {H. Modzelewski},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:46:53 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Henryk_Des.pdf},
	Title = {Design and specifications for SLIM's software framework},
	Year = {2008}}

@conference{johnson08sdi,
	Abstract = {Due to the physics of reciprocity seismic data
                  sets are symmetric in the source and receiver
                  coordinates.  Often seismic data sets are incomplete
                  and the missing data must be
                  interpolated. Typically, missing traces do not occur
                  symmetrically. The purpose of this project is to
                  extend the current formulation for solving the
                  seismic interpolation problems in such a way that
                  they enforce reciprocity.  The method decomposes the
                  seismic data volume into symmetric and antisymmetric
                  parts. This decomposition leads to an augmented
                  system of equations for the L1-solver that promotes
                  sparsity in the curvelet domain. Interpolation is
                  carried out on the entire system during which the
                  asymmetric component of the volume is forced to
                  zero, while the symmetric part of the data volume is
                  matched to the measured data.  },
	Author = {J. Johnson and G. Hennenfent},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:51:51 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_James_Sei.pdf},
	Title = {Seismic Data Interpolation with Symmetry},
	Year = {2008}}

@conference{yan08wru,
	Abstract = {This report represents and compares two methods of
                  wavefield reconstruction from noisy seismic data
                  with missing traces. The two methods are (i) First
                  interpolate incomplete noisy data to get complete
                  noisy data and then denoise, and (ii) Interpolate
                  and denoise the incomplete noisy data
                  simultaneously. A sample test of synthetic data will
                  be presented. The results of tests show that
                  denoising after interpolation is better than
                  simultaneous denoising and interpolation if the
                  parameter of the denoising problem is chosen
                  appropriately.  },
	Author = {J. Yan},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:55:18 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Jiapeng_Wav.pdf},
	Title = {Wavefield Reconstruction Using Simultaneous Denoising Interpolation vs. Denoising after Interpolation},
	Year = {2008}}

@conference{friedlander08afl,
	Abstract = {Many signal processing applications seek to
                  approximate a signal as a linear combination of only
                  a few elementary atoms drawn from a large
                  collection.  This is known as sparse reconstruction,
                  and the theory of compressed sensing allows us to
                  pose it as a structured convex optimization problem.
                  I will discuss the role of duality in revealing some
                  unexpected and useful properties of these problems,
                  and will show how they can lead to practical,
                  large-scale algorithms. I will also describe some
                  applications of these algorithms.},
	Author = {M. P. Friedlander},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:41:41 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Michael_Alg.pdf},
	Title = {Algorithms for Large-Scale Sparse Reconstruction},
	Year = {2008}}

@conference{yilmaz08sse,
	Abstract = {We present theoretical results pertaining to the
                  ability of p-(quasi)norm minimization to recover
                  sparse and compressible signals from incomplete and
                  noisy measurements. In particular, we extend the
                  results of Candes, Romberg and Tao for 1-norm to the
                  p<1 case.  Our results indicate that depending on
                  the restricted isometry constants and the noise
                  level, p-norm minimization with certain values of
                  p<1 provides better theoretical guarantees in terms
                  of stability and robustness compared to 1-norm
                  minimization. This is especially true when the
                  restricted isometry constants are relatively large,
                  or equivalently, when the data is significantly
                  undersampled.},
	Author = {\"{O} Yilmaz},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:54:57 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ozgur_Sta.pdf},
	Title = {Stable sparse expansions via non-convex optimization},
	Year = {2008}}

@conference{moghaddam08rtm,
	Abstract = {We recover the amplitude of a seismic image by
                  approximating the normal (demigration-migration)
                  operator. In this approximation, we make use of the
                  property that curvelets remain invariant under the
                  action of the normal operator. We propose a seismic
                  amplitude recovery method that employs an eigenvalue
                  like decomposition for the normal operator using
                  curvelets as eigenvectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time 'wave-equation' migration code
                  simulating the acoustic wave equation.},
	Author = {P. P. Moghaddam},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:51:21 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Peyman_Mig.pdf},
	Title = {Reverse-time Migration Amplitude Recovery with Curvelets},
	Year = {2008}}

@conference{saab08cps,
	Abstract = {We present a novel primary-multiple separation
                  scheme which makes use of the sparsity of both
                  primaries and multiples in a transform domain, such
                  as the curvelet transform, to provide estimates of
                  each. The proposed algorithm utilizes seismic data
                  as well as the output of a preliminary step that
                  provides (possibly) erroneous predictions of the
                  multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm.},
	Author = {R. Saab},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:45:53 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Rayan_Curv.pdf},
	Title = {Curvelet-Based Primary-Multiple Separation from a Bayesian Perspective},
	Year = {2008}}

@conference{ross08sit,
	Abstract = {Geophysical processing is dominated by many
                  different out of core memory software environments
                  (OOCE). Such environments include Madagascar and SU
                  and are designed to handle data that can not be
                  operated on in memory. Each base operation is
                  created as a main program that reads data from disk
                  and writes the result to disk. The main programs can
                  also be chained together on stdin/out pipes using a
                  shell only writing data to disk at the end.  To be
                  efficient, the algorithm using an OOCE must chain
                  together the longest pipe to avoid disk I/O, as a
                  result it is very difficult to use iterative
                  techniques. The algorithms are written in shell
                  scripts can be difficult to read and understand.
                  SLIMpy is a software library that contains
                  definitions of coordinate free vectors and linear
                  operators. It allows the user to design and run
                  algorithms with any out of core package, in a Matlab
                  style interface while maintaining optimal efficiency
                  and speed. SLIMpy looks at each main program of each
                  OOCE as a Matrix vector operation or vector
                  reduction/transformation operation. It uses operator
                  overloading to generate an abstract syntax tree
                  (AST) which can be optimized in many ways before
                  executing its commands. The AST also provides a
                  pathway for embarrassingly parallel applications by
                  splitting the tree over different nodes and
                  processors.  SLIMpy provides an interface to these
                  OOCE that allows for optimal construction of
                  commands and allows for iterative techniques. It
                  smoothes the transition from other languages such as
                  Matlab and allows the algorithm designer to write
                  readable and reusable code. SLIMpy also adds to OOCE
                  by allowing for easy parallelization.},
	Author = {S. Ross-Ross},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:52:24 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Sean_Sli.pdf},
	Title = {Seismic inversion through operator overloading},
	Year = {2008}}

@conference{erlangga08imf,
	Abstract = {We present an iterative method for solving the
                  2D/3D Helmholtz equation. The method is mainly based
                  on a Krylov method, preconditioned by a special
                  operator which represents a damped Helmholtz
                  operator. The discretization of the preconditioning
                  operator is then solved by one multigrid sweep. It
                  can be shown that while the spectrum is bounded
                  above by one, the smallest eigenvalue of the
                  preconditioned system is of order $k^{-1}$. In this
                  situation, the convergence of a Krylov method will
                  be proportional to the frequency of the problem.
                  Further convergence acceleration can be achieved if
                  eigenvalues of order $k^{-1}$ are projected from the
                  spectrum. This can be done by a projection operator,
                  similar to but more stable than deflation. This
                  projection operator has been the core of a new
                  multilevel method, called multilevel Krylov method,
                  proposed by Erlangga and Nabben only
                  recently. Putting the preconditioned Helmholtz
                  operator in this setting, a convergence which is
                  independent of frequency can be obtained.},
	Author = {Y. Erlangga and K. Vuik and K. Oosterlee and D. Riyanti and R. Nabben},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:48:51 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Erlangga_Ite.pdf},
	Title = {Iterative methods for 2D/3D Helmholtz operator},
	Year = {2008}}

@conference{lin08cwe,
	Author = {T. T. Y. Lin and F. J. Herrmann},
	Booktitle = {SINBAD 2008},
	Keywords = {SLIM, SINBAD, Presentation},
	Title = {Compressed wavefield extrapolation},
	Year = {2008}}

@conference{kumar08cd,
	Abstract = {The separation of signal and noise is an important
                  issue in seismic data processing. By noise we refer
                  to the incoherent noise which is present in the
                  data. In our case, we showed curvelets concentrate
                  seismic signal energy in few significant
                  coefficients unlike noise energy that is spread all
                  over the coefficients. The sparsity of seismic data
                  in the curvelet domain makes curvelets an ideal
                  choice for separating the noise from the seismic
                  data. In our approach the denoising problem is
                  framed as curvelet-regularized inversion
                  problem. After initial processing, we applied the
                  algorithm to the poststack data and compared our
                  results with conventional wavelet denoising.  },
	Author = {V. Kumar},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:45:11 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Den.pdf},
	Title = {Curvelet Denoising},
	Year = {2008}}

@conference{kumar08crd,
	Abstract = {The removal of source signature from seismic data
                  is an important step in seismic data processing. The
                  Curvelet transform provides sparse representations
                  for images that comprise smooth objects separated by
                  piece-wise smooth discontinuities (e.g. seismic
                  reflectivity). In this approach the sparseness of
                  reflectivity in Curvelet domain is used as a prior
                  to stabilize the inversion process. Our
                  Curvelet-regularized deconvolution algorithm uses
                  recently developed SPGL1 solver which does adaptive
                  sampling of the trade-off curve. We applied the
                  algorithm on a synthetic example and compared our
                  results with that of Spiky deconvolution approach.},
	Author = {V. Kumar},
	Booktitle = {SINBAD 2008},
	Date-Modified = {2008-08-22 12:46:31 -0700},
	Keywords = {SLIM, SINBAD, Presentation},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Dec.pdf},
	Title = {Curvelet-Regularized Deconvolution},
	Year = {2008}}


@techreport{hennenfent08onr,
	Abstract = {Geophysical inverse problems typically involve a
                  trade off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain relatively
                  unexplored. First, we show how these curves provide
                  an objective criterion to gauge how robust one-norm
                  solvers are when they are limited by a maximum
                  number of matrix-vector products that they can
                  perform. Second, we use Pareto curves and their
                  properties to define and compute one-norm
                  compressibilities. We argue this notion is key to
                  understand one-norm regularized inversion. Third, we
                  illustrate the correlation between the one-norm
                  compressibility and the performance of Fourier and
                  curvelet reconstructions with sparsity promoting
                  inversion.},
	Author = {G. Hennenfent and F. J. Herrmann},
	Date-Added = {2008-08-14 13:48:52 -0700},
	Date-Modified = {2008-08-15 12:31:09 -0700},
	Institution = {UBC Earth and Ocean Sciences Department},
	Keywords = {SLIM},
	Month = {August},
	Number = {TR-2008-5},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/TechReports/hennenfent08seg.pdf},
	Title = {One-norm regularized inversion: Learning form the Pareto curve},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/hennenfent08seg.pdf}}

@techreport{herrmann08cdm,
	Abstract = {Matching seismic wavefields and images lies at the
                  heart of many pre-/post-processing steps part of
                  seismic imaging--- whether one is matching predicted
                  wavefield components, such as multiples, to the
                  actual to-be-separated wavefield components present
                  in the data or whether one is aiming to restore
                  migration amplitudes by scaling, using an
                  image-to-remigrated-image matching procedure to
                  calculate the scaling coefficients. The success of
                  these wavefield matching procedures depends on our
                  ability to (i) control possible overfitting, which
                  may lead to accidental removal of energy or to
                  inaccurate image-amplitude corrections, (ii) handle
                  data or images with nonunique dips, and (iii) apply
                  subsequent wavefield separations or migraton
                  amplitude corrections stably. In this paper, we show
                  that the curvelet transform allows us to address all
                  these issues by imposing smoothness in phase space,
                  by using their capability to handle conflicting
                  dips, and by leveraging their ability to represent
                  seismic data and images sparsely. This latter
                  property renders curvelet-domain sparsity promotion
                  an effective prior.},
	Author = {F. J. Herrmann and P. P. Moghaddam and D. Wang},
	Date-Added = {2008-08-14 13:46:26 -0700},
	Date-Modified = {2008-08-15 12:31:50 -0700},
	Institution = {UBC Earth and Ocean Sciences Department},
	Keywords = {SLIM},
	Month = {August},
	Number = {TR-2008-6},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann08segmat.pdf},
	Title = {Curvelet-domain matched filtering},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/herrmann08segmat.pdf}}

@techreport{lebed08ahg,
	Abstract = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last decade many
                  transforms such as wavelets, curervelets,
                  contourlets, surfacelets, shearlets, and many other
                  types of 'x-lets' have been developed to try to
                  resolve this issue. In this abstract we compare the
                  properties of four of these commonly used
                  transforms, namely the shift-invariant wavelets,
                  complex wavelets, curvelets and surfacelets. We also
                  briefly explore the performance of these transforms
                  for the problem of recovering seismic wavefields
                  from incomplete measurements.},
	Author = {E. Lebed and F. J. Herrmann},
	Date-Added = {2008-08-14 13:44:26 -0700},
	Date-Modified = {2008-08-15 12:30:46 -0700},
	Institution = {UBC Earth and Ocean Sciences Department},
	Keywords = {SLIM},
	Month = {August},
	Number = {TR-2008-4},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/TechReports/lebed08seg.pdf},
	Title = {A Hitchhiker's guide to the galaxy of transform-domain sparsifiction},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/lebed08seg.pdf}}

@conference{erlangga08aim,
	Abstract = {We describe an iterative multilevel method for
                  solving linear systems representing forward modeling
                  and back propagation of wavefields in
                  frequency-domain seismic inversions. The workhorse
                  of the method is the so-called multilevel Krylov
                  method, applied to a multigrid-preconditioned linear
                  system, and is called multigrid-multilevel Krylov
                  (MKMG) method. Numerical experiments are presented
                  for 2D Marmousi synthetic model for a range of
                  frequencies. The convergence of the method is fast,
                  and depends only mildly on frequency. The method can
                  be considered as the first viable alternative to LU
                  factorization, which is practically prohibitive for
                  3D seismic inversions.},
	Author = {Y. A. Erlangga and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-08-14 13:40:48 -0700},
	Date-Modified = {2008-11-21 14:42:16 -0800},
	Issue = {1},
	Keywords = {SLIM},
	Month = {November},
	Number = {1},
	Pages = {1957-1960},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/erlangga08seg.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/erlangga08imm.pdf},
	Publisher = {SEG},
	Title = {An iterative multilevel method for computing wavefields in frequency-domain seismic inversion},
	Volume = {27},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/erlangga08seg.pdf}}

@conference{kumar08dwc,
	Abstract = {There is an inherent continuity along reflectors
                  of a seismic image. We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.},
	Author = {V. Kumar and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-08-14 13:30:17 -0700},
	Date-Modified = {2008-11-21 14:43:40 -0800},
	Issue = {1},
	Keywords = {SLIM},
	Month = {November},
	Number = {1},
	Pages = {1996-2000},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/kumar08seg.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/kumar08dcs.pdf},
	Publisher = {SEG},
	Title = {Deconvolution with curvelet-domain sparsity},
	Volume = {27},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/kumar08seg.pdf}}

@conference{lin08iso,
	Abstract = {We present an algorithm which allows us to model
                  wavefields with frequency-domain methods using a
                  much smaller number of frequencies than that
                  typically required by the classical sampling theory
                  in order to obtain an alias-free result. The
                  foundation of the algorithm is the recent results on
                  the compressed sensing, which state that data can be
                  successfully recovered from an incomplete
                  measurement if the data is sufficiently
                  sparse. Results from numerical experiment show that
                  only 30\% of the total frequency spectrum is need to
                  capture the full wavefield information when working
                  in the hard 2D synthetic Marmousi model.},
	Author = {T.T.Y. Lin and E. Lebed and Y. A. Erlangga and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-08-14 13:28:24 -0700},
	Date-Modified = {2008-11-21 14:45:48 -0800},
	Issue = {1},
	Keywords = {SLIM},
	Month = {November},
	Number = {1},
	Pages = {2122-2126},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lin08seg.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/lin08ish.pdf},
	Publisher = {SEG},
	Title = {Interpolating solutions of the Helmholtz equation with compressed sensing},
	Volume = {27},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/lin08seg.pdf}}

@conference{eso08ira,
	Abstract = {Iterative soft thresholding of a models wavelet
	coefficients can be used to obtain models that are sparse with
	respect to a known basis function. We generate sparse models
	for non-linear forward operators by applying the soft
	thresholding operator to the model obtained through a
	Gauss-Newton iteration and apply the technique in a synthetic
	2.5D DC resistivity crosswell tomographic example.},
	Author = {R. A. Eso and S. Napier and F. J. Herrmann and D. W. Oldenburg},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-08-14 13:25:50 -0700},
	Date-Modified = {2008-11-21 14:40:13 -0800},
	Issue = {1},
	Keywords = {SLIM},
	Month = {November},
	Number = {1},
	Pages = {579-583},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/eso08seg.pdf},
	Publisher = {SEG},
	Title = {Iterative reconstruction algorithm for non-linear operators},
	Volume = {27},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/eso08seg.pdf}}

@conference{moghaddam08cbm,
	Abstract = {In this paper, we introduce a preconditioner for
                  seismic imaging---i.e., the inversion of the
                  linearized Born scattering operator. This
                  preconditioner approximately corrects for the
                  ``square root'' of the normal---i.e., the
                  demigration-migration operator. This approach
                  consists of three parts, namely (i) a left
                  preconditoner, defined by a fractional time
                  integration designed to make the migration operator
                  zero order, and two right preconditioners that apply
                  (ii) a scaling in the physical domain accounting for
                  a spherical spreading, and (iii) a curvelet-domain
                  scaling that corrects for spatial and reflector-dip
                  dependent amplitude errors. We show that a
                  combination of these preconditioners lead to a
                  significant improvement of the convergence for
                  iterative least-squares solutions to the seismic
                  imaging problem based on reverse-time migration
                  operators.},
	Author = {P. P. Moghaddam and C. R. Brown and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-08-14 13:22:53 -0700},
	Date-Modified = {2008-11-21 14:47:00 -0800},
	Issue = {1},
	Keywords = {SLIM},
	Month = {November},
	Number = {1},
	Pages = {2211-2215},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/moghaddam08seg.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/brown08cmp.pdf},
	Publisher = {SEG},
	Title = {Curvelet-based migration preconditioning},
	Volume = {27},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/moghaddam08seg.pdf}}

@conference{maysami08lcf,
	Abstract = {n this paper, we present a new method for seismic
                  waveform characterization whose aim is threefold,
                  namely (i) extraction of detailed information on the
                  sharpness of transitions in the subsurface from
                  seismic waveforms, (ii) reflector modeling, based on
                  binary-mixture and percolation theory, and (iii)
                  establishment of well-seismic ties, through
                  parameterizations of our waveform and critical
                  reflector model. We test this methodology on the
                  opal-A (Amorphous) to opal-CT
                  (Cristobalite/Tridymite) transition imaged in a
                  migrated section of North Sea field data West of the
                  Shetlands.},
	Author = {M. Maysami and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-08-14 13:20:09 -0700},
	Date-Modified = {2008-11-21 14:44:46 -0800},
	Issue = {1},
	Keywords = {SLIM},
	Month = {November},
	Number = {1},
	Pages = {2011-2015},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/mohammad08seg.pdf},
	Publisher = {SEG},
	Title = {Lithological constraints from seismic waveforms: application to opal-A to opal-CT transition},
	Volume = {27},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/mohammad08seg.pdf}}

@conference{herrmann08swi,
	Abstract = {Inverting seismic wavefields lies at the heart of
                  seismic data processing and imaging--- whether one
                  is applying ``a poor man's inverse'' by correlating
                  wavefields during imaging or whether one inverts
                  wavefields as part of a focal transform
                  interferrometric deconvolution or as part of
                  computing the 'data verse'. The success of these
                  wavefield inversions depends on the stability of the
                  inverse with respect to data imperfections such as
                  finite aperture, bandwidth limitation, and missing
                  data. In this paper, we show how curvelet domain
                  sparsity promotion can be used as a suitable prior
                  to invert seismic wavefields. Examples include,
                  seismic data regularization with the focused
                  curvelet-based recovery by sparsity-promoting
                  inversion (fCRSI), which involves the inversion of
                  the primary-wavefield operator, the prediction of
                  multiples by inverting the adjoint of the primary
                  operator, and finally the inversion of the data
                  itself --- the so-called 'data inverse'. In all
                  cases, curvelet-domain sparsity leads to a stable
                  inversion.},
	Author = {F. J. Herrmann and Deli Wang},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-08-14 13:17:16 -0700},
	Date-Modified = {2008-11-21 14:48:33 -0800},
	Issue = {1},
	Keywords = {SLIM},
	Month = {November},
	Number = {1},
	Pages = {2497-2501},
	Pdf = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/herrmann08segwav.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08swi.pdf},
	Publisher = {SEG},
	Title = {Seismic wavefield inversion with curvelet-domain sparsity promotion},
	Volume = {27},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/herrmann08segwav.pdf}}

@conference{yarham08bgr,
	Abstract = {The removal of coherent noise generated by surface
                  waves in land based seismic is a prerequisite to
                  imaging the subsurface. These surface waves, termed
                  as ground roll, overlay important reflector
                  information in both the t-x and f-k
                  domains. Standard techniques of ground-roll removal
                  commonly alter reflector information. We propose the
                  use of the curvelet domain as a sparsifying
                  transform in which to preform signal-separation
                  techniques that preserves reflector information
                  while increasing ground-roll removal. We look at how
                  this method preforms on synthetic data for which we
                  can build quantitative results and a real field data
                  set.},
	Author = {Carson Yarham and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-08-14 12:34:15 -0700},
	Date-Modified = {2008-11-21 14:49:44 -0800},
	Issue = {1},
	Keywords = {SLIM},
	Month = {November},
	Number = {1},
	Pages = {2576-2580},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/yarham08seg.pdf},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg08/herrmann08bgs.pdf},
	Publisher = {SEG},
	Rating = {1},
	Read = {No},
	Title = {Bayesian ground-roll seperation by curvelet-domain sparsity promotion},
	Volume = {27},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/SEG/2008/yarham08seg.pdf}}

@article{wang08bws,
	Abstract = {Successful removal of coherent noise sources
                  greatly determines the quality of seismic imag-
                  ing. Ma jor advances were made in this direction,
                  e.g., Surface-Related Multiple Elimination (SRME)
                  and interferometric ground-roll removal. Still,
                  moderate phase, timing, amplitude errors and clutter
                  in the predicted signal components can be
                  detrimental. Adopting a Bayesian approach along with
                  the assumption of approximate curvelet-domain
                  independence of the to-be-separated signal
                  components, we construct an iterative algorithm that
                  takes the predictions produced by for example SRME
                  as input and separates these components in a robust
                  fashion. In addition, the proposed algorithm
                  controls the energy mismatch between the separated
                  and predicted components. Such a control, which was
                  lacking in earlier curvelet-domain formulations,
                  produces improved results for primary-multiple
                  separation on both synthetic and real data.  },
	Author = {D. Wang and R. Saab and \"{O}. Yilmaz and F. J. Herrmann},
	Date-Added = {2008-07-03 10:41:14 -0700},
	Date-Modified = {2008-08-14 14:00:00 -0700},
	Doi = {10.1190/1.2952571},
	Issue = {5},
	Journal = {Geophysics},
	Keywords = {SLIM, curvelet transform},
	Month = {September-October},
	Number = {5},
	Pdf = {http://slim.eos.ubc.ca/Publications/Private/Journals/wang08bws.pdf},
	Title = {Bayesian wavefield separation by transform-domain sparsity promotion},
	Url = {http://slim.eos.ubc.ca/Publications/Private/Journals/wang08bayes/paper_html/paper.html},
	Volume = {73},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Journals/wang08bayes/paper.pdf}}

@misc{fomel07mos,
	Abstract = {is an open-source software package for geophysical
                  data analysis and reproducible numerical
                  experiments. Its mission is to provide -a convenient
                  and powerful environment -a convenient technology
                  transfer tool for researchers working with digital
                  image and data processing. The technology developed
                  using the Madagascar project management system is
                  transferred in the form of recorded processing
                  histories, which become "computational recipes" to
                  be verified, exchanged, and modified by users of the
                  system.},
	Author = {S. Fomel and P. Sava},
	Date-Added = {2008-06-26 15:31:10 -0700},
	Date-Modified = {2008-08-14 15:31:44 -0700},
	Keywords = {software},
	Title = {{MADAGASCAR}: open-source software package for geophysical data processing and reproducible numerical experiments},
	Url = {http://rsf.sf.net},
	Year = {2007},
	Bdsk-Url-1 = {http://rsf.sf.net}}

@misc{rossross07sda,
	Abstract = {Inverse problems in (exploration) seismology are
                  known for their large to very large scale. For
                  instance, certain sparsity-promoting inversion
                  techniques involve vectors that easily exceed
                  unknowns while seismic imaging involves the
                  construction and application of matrix-free
                  discretized operators where single matrix-vector
                  evaluations may require hours, days or even weeks on
                  large compute clusters. For these reasons, software
                  development in this field has remained the domain of
                  highly technical codes programmed in low-level
                  languages with little eye for easy development, code
                  reuse and integration with (nonlinear) programs that
                  solve inverse problems.Following ideas from the
                  Symes' Rice Vector Library and Bartlett's C++
                  object-oriented interface, Thyra, and
                  Reduction/Transformation operators (both part of the
                  Trilinos software package), we developed a
                  software-development environment based on
                  overloading. This environment provides a pathway
                  from in-core prototype development to out-of-core
                  and MPI 'production' code with a high level of code
                  reuse. This code reuse is accomplished by
                  integrating the out-of-core and MPI functionality
                  into the dynamic object-oriented programming
                  language Python. This integration is implemented
                  through operator overloading and allows for the
                  development of a coordinate-free solver framework
                  that (i) promotes code reuse; (ii) analyses the
                  statements in an abstract syntax tree and (iii)
                  generates executable statements. In the current
                  implementation, we developed an interface to
                  generate executable statements for the out-of-core
                  unix-pipe based (seismic) processing package
                  RSF-Madagascar (rsf.sf.net). The modular design
                  allows for interfaces to other seismic processing
                  packages and to in-core Python packages such as
                  numpy. So far, the implementation overloads linear
                  operators and element-wise reduction/transformation
                  operators. We are planning extensions towards
                  nonlinear operators and integration with existing
                  (parallel) solver frameworks such as Trilinos.},
	Author = {S. Ross-Ross and H. Modzelewski and C. Brown and F. J. Herrmann},
	Date-Added = {2008-06-26 15:23:23 -0700},
	Date-Modified = {2008-08-14 14:34:25 -0700},
	Keywords = {SLIM, software},
	Title = {{SLIMpy} development and programming interface for seismic processing},
	Url = {http://slim.eos.ubc.ca/SLIMpy},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/SLIMpy}}

@phdthesis{candes98rta,
	Address = {Stanford, CA},
	Author = {E. J. Cand\`es},
	Date-Added = {2008-05-27 18:24:11 -0700},
	Date-Modified = {2008-05-27 18:26:14 -0700},
	Keywords = {ridgelet transform},
	School = {Stanford University},
	Title = {Ridgelets: theory and applications},
	Year = {1998},
	Bdsk-Url-1 = {http://www.acm.caltech.edu/~emmanuel/papers/Thesis.ps.gz}}

@conference{morton98fsr,
	Author = {S. A. Morton and C. C. Ober},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-27 16:44:01 -0700},
	Date-Modified = {2008-05-27 16:45:21 -0700},
	Doi = {10.1190/1.1820088},
	Issue = {1},
	Number = {1},
	Pages = {1131-1134},
	Pdf = {http://link.aip.org/link/?SGA/17/1131/1},
	Publisher = {SEG},
	Title = {Faster shot-record depth migrations using phase encoding},
	Volume = {17},
	Year = {1998},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/17/1131/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1820088}}

@article{romero00peo,
	Abstract = {Frequency-domain shot-record migration can produce
                  higher quality images than Kirchhoff migration but
                  typically at a greater cost. The computing cost of
                  shot-record migration is the product of the number
                  of shots in the survey and the expense of each
                  individual migration. Many attempts to reduce this
                  cost have focused on the speed of the individual
                  migrations, trying to achieve a better trade-off
                  between accuracy and speed. Another approach is to
                  reduce the number of migrations. We investigate the
                  simultaneous migration of shot records using
                  frequency-domain shot-record migration
                  algorithms. The difficulty with this approach is the
                  production of so-called crossterms between unrelated
                  shot and receiver wavefields, which generate
                  unwanted artifacts or noise in the final image. To
                  reduce these artifacts and obtain an image
                  comparable in quality to the
                  single-shot-per-migration result, we have introduced
                  a process called phase encoding, which shifts or
                  disperses these crossterms. The process of phase
                  encoding thus allows one to trade S/N ratio for the
                  speed of migrating the entire survey. Several
                  encoding functions and two application strategies
                  have been tested. The first strategy, combining
                  multiple shots per migration and using each shot
                  only once, reduces computation in direct relation to
                  the number of shots combined. The second strategy,
                  performing multiple migrations of all the shots in
                  the survey, provides a means to reduce the crossterm
                  noise by stacking the resulting images. The
                  additional noise in both strategies may be tolerated
                  if it is no stronger than the inherent seismic noise
                  in the migrated image and if the final image is
                  achieved with less cost. {\copyright}2000 Society of
                  Exploration Geophysicists},
	Author = {L. A. Romero and D. C. Ghiglia and C. C. Ober and S. A. Morton},
	Date-Added = {2008-05-27 16:42:50 -0700},
	Date-Modified = {2008-08-14 15:07:08 -0700},
	Doi = {10.1190/1.1444737},
	Issue = {2},
	Journal = {Geophysics},
	Number = {2},
	Pages = {426-436},
	Pdf = {http://link.aip.org/link/?GPY/65/426/1},
	Publisher = {SEG},
	Title = {Phase encoding of shot records in prestack migration},
	Volume = {65},
	Year = {2000},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/65/426/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1444737}}

@article{candes05cct1,
	Author = {E. J. Cand\`es and D. L. Donoho},
	Date-Added = {2008-05-26 18:23:17 -0700},
	Date-Modified = {2008-05-26 18:24:36 -0700},
	Issue = {2},
	Journal = {Applied and Computational Harmonic Analysis},
	Keywords = {curvelet transform},
	Month = {September},
	Number = {2},
	Pages = {198-222},
	Title = {Continuous curvelet transform: {II.} Discretization and frames},
	Volume = {19},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.acha.2005.02.004}}

@article{candes05cct,
	Author = {E. J. Cand\`es and D. L. Donoho},
	Date-Added = {2008-05-26 18:21:22 -0700},
	Date-Modified = {2008-05-26 18:23:57 -0700},
	Issue = {2},
	Journal = {Applied and Computational Harmonic Analysis},
	Keywords = {curvelet transform},
	Month = {September},
	Number = {2},
	Pages = {162-197},
	Title = {Continuous curvelet transform: {I.} Resolution of the wavefront set},
	Volume = {19},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.acha.2005.02.003}}

@incollection{candes00cas,
	Abstract = {It is widely believed that to efficiently
                  represent an otherwise smooth ob ject with
                  discontinuities along edges, one must use an
                  adaptive representation that in some sense `tracks'
                  the shape of the discontinuity set. This folk-belief
                  --- some would say folk-theorem --- is incorrect. At
                  the very least, the possible quantitative advantage
                  of such adaptation is vastly smaller than commonly
                  believed. We have recently constructed a tight frame
                  of curvelets which provides stable, efficient, and
                  near-optimal representation of otherwise smooth ob
                  jects having discontinuities along smooth curves. By
                  applying naive thresholding to the curvelet
                  transform of such an ob ject, one can form m-term
                  approximations with rate of L2 approximation
                  rivaling the rate obtainable by complex adaptive
                  schemes which attempt to `track' the discontinuity
                  set. In this article we explain the basic issues of
                  efficient m-term approximation, the construction of
                  efficient adaptive representation, the construction
                  of the curvelet frame, and a crude analysis of the
                  performance of curvelet schemes.  },
	Address = {Nashville, TN},
	Author = {E. J. Cand\`es and D. L. Donoho},
	Booktitle = {Curve and surface fitting},
	Date-Added = {2008-05-26 17:48:55 -0700},
	Date-Modified = {2008-08-14 15:26:58 -0700},
	Editor = {A. Cohen, C. Rahut, and L. L. Schumaker},
	Keywords = {curvelet transform},
	Pages = {105-120},
	Publisher = {Vanderbilt University Press},
	Title = {Curvelets: a surprisingly effective nonadaptive representation of objects with edges},
	Year = {2000},
	Bdsk-Url-1 = {http://www.acm.caltech.edu/~emmanuel/papers/Curvelet-SMStyle.pdf}}

@article{starck02tct,
	Abstract = {We describe approximate digital implementations of
                  two new mathematical transforms, namely, the
                  ridgelet transform and the curvelet transform. Our
                  implementations offer exact reconstruction,
                  stability against perturbations, ease of
                  implementation, and low computational complexity. A
                  central tool is Fourier-domain computation of an
                  approximate digital Radon transform. We introduce a
                  very simple interpolation in the Fourier space which
                  takes Cartesian samples and yields samples on a
                  rectopolar grid, which is a pseudo-polar sampling
                  set based on a concentric squares geometry. Despite
                  the crudeness of our interpolation, the visual
                  performance is surprisingly good. Our ridgelet
                  transform applies to the Radon transform a special
                  overcomplete wavelet pyramid whose wavelets have
                  compact support in the frequency domain. Our
                  curvelet transform uses our ridgelet transform as a
                  component step, and implements curvelet subbands
                  using a filter bank of a&grave; trous wavelet
                  filters. Our philosophy throughout is that
                  transforms should be overcomplete, rather than
                  critically sampled. We apply these digital
                  transforms to the denoising of some standard images
                  embedded in white noise. In the tests reported here,
                  simple thresholding of the curvelet coefficients is
                  very competitive with "state of the art" techniques
                  based on wavelets, including thresholding of
                  decimated or undecimated wavelet transforms and also
                  including tree-based Bayesian posterior mean
                  methods. Moreover, the curvelet reconstructions
                  exhibit higher perceptual quality than wavelet-based
                  reconstructions, offering visually sharper images
                  and, in particular, higher quality recovery of edges
                  and of faint linear and curvilinear
                  features. Existing theory for curvelet and ridgelet
                  transforms suggests that these new approaches can
                  outperform wavelet methods in certain image
                  reconstruction problems. The empirical results
                  reported here are in encouraging agreement},
	Author = {J.-L. Starck and E. J. Cand\`es and D. L. Donoho},
	Date-Added = {2008-05-26 17:38:14 -0700},
	Date-Modified = {2008-08-14 15:19:16 -0700},
	Doi = {10.1109/TIP.2002.1014998},
	Issn = {1057-7149},
	Issue = {6},
	Journal = {IEEE Transactions on Image Processing},
	Keywords = {curvelet transform},
	Month = {June},
	Number = {6},
	Pages = {670-684},
	Publisher = {IEEE},
	Title = {The curvelet transform for image denoising},
	Volume = {11},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIP.2002.1014998},
	Bdsk-Url-2 = {http://ieeexplore.ieee.org/iel5/83/21845/01014998.pdf}}

@techreport{donoho99dct,
	Author = {D. L. Donoho and M. R. Duncan},
	Date-Added = {2008-05-26 17:33:51 -0700},
	Date-Modified = {2008-05-26 17:35:32 -0700},
	Institution = {Stanford Statistics Department},
	Keywords = {curvelet transform},
	Month = {November},
	Title = {Digital curvelet transform: strategy, implementation, and experiments},
	Year = {1999},
	Bdsk-Url-1 = {http://citeseer.ist.psu.edu/rd/44392127,300178,1,0.25,Download/http://citeseer.ist.psu.edu/cache/papers/cs/15527/http:zSzzSzwww-stat.stanford.eduzSz~donohozSzReportszSz1999zSzDCvT.pdf/donoho99digital.pdf}}

@book{mallat99awt,
	Author = {S. Mallat},
	Date-Added = {2008-05-22 16:32:31 -0700},
	Date-Modified = {2008-05-22 16:33:57 -0700},
	Howpublished = {Hardcover},
	Isbn = {012466606X},
	Keywords = {wavelet transform},
	Month = {September},
	Publisher = {Academic Press},
	Title = {A Wavelet Tour of Signal Processing, Second Edition},
	Year = {1999}}

@conference{kumar08crs,
	Abstract = {There is an inherent continuity along reflectors
                  of a seismic image. We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.  },
	Author = {V. Kumar and F. J. Herrmann},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-22 11:49:10 -0700},
	Date-Modified = {2008-08-14 14:22:13 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.geoconvention.org/2008abstracts/230.pdf},
	Publisher = {CSEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/kumar08crs.pdf},
	Title = {Curvelet-regularized seismic deconvolution},
	Year = {2008},
	Bdsk-Url-1 = {http://www.geoconvention.org/2008abstracts/230.pdf}}

@article{fomel07rce,
	Abstract = {SCons (from software construction) is a well-known
	open-source program designed primarily for building
	software. In this paper, we describe our method of extending
	SCons for managing data processing flows and reproducible
	computational experiments. We demonstrate our usage of SCons
	with a simple example.},
	Author = {S. Fomel and G. Hennenfent},
	Date-Added = {2008-05-22 11:42:36 -0700},
	Date-Modified = {2008-08-14 13:52:16 -0700},
	Doi = {10.1109/ICASSP.2007.367305},
	Issn = {1520-6149},
	Journal = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	Keywords = {SLIM},
	Month = {April},
	Pages = {IV-1257-IV-1260},
	Pdf = {http://lcav.epfl.ch/reproducible_research/ICASSP07/FomelH07.pdf},
	Title = {Reproducible computational experiments using {SC}ons},
	Volume = {4},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICASSP.2007.367305},
	Bdsk-Url-2 = {http://lcav.epfl.ch/reproducible_research/ICASSP07/FomelH07.pdf},
	Bdsk-Url-3 = {http://www.ee.columbia.edu/~dpwe/LabROSA/proceeds/icassp/2007/pdfs/0401257.pdf}}

@conference{yarham04cpa,
	Abstract = {In this paper we present examples of ground roll
                  attenuation for synthetic and real data gathers by
                  using Contourlet and Curvelet transforms.  These
                  non-separable wavelet transforms are locoalized both
                  (x,t)- and (k,f)-domains and allow for adaptive
                  seperation of signal and ground roll.  Both linear
                  and non-linear filtering are discussed using the
                  unique properties of these basis that allow for
                  simultaneous localization in the both
                  domains. Eventhough, the linear filtering techniques
                  are encouraging the true added value of these
                  basis-function techniques becomes apparent when we
                  use these decompositions to adaptively substract
                  modeled ground roll from data using a non-linear
                  thesholding procedure.  We show real and synthetic
                  examples and the results suggest that these
                  directional-selective basis functions provide a
                  usefull tool for the removal of coherent noise such
                  as ground roll.  },
	Author = {C. Yarham and D. Trad and F. J. Herrmann},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-22 11:28:07 -0700},
	Date-Modified = {2008-08-14 14:29:15 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/060S0201-Yarham_C_Curvelet_Ground_Roll.pdf},
	Publisher = {CSEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/yarham04cpa.pdf},
	Title = {Curvelet processing and imaging: adaptive ground roll removal},
	Year = {2004},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/060S0201-Yarham_C_Curvelet_Ground_Roll.pdf}}

@conference{herrmann04cdl1,
	Abstract = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio.  Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding. This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.  },
	Author = {F. J. Herrmann and P. P. Moghaddam},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:28:59 -0700},
	Date-Modified = {2008-08-14 14:09:43 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://slim.eos.ubc.ca/~felix/public/EAGEIM2004.pdf},
	Publisher = {CSEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage04/herrmann04cdl.pdf},
	Title = {Curvelet-domain least-squares migration with sparseness constraints},
	Year = {2004},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/061S0201-Herrmann_F_Curvelet_Multiple.pdf}}

@conference{herrmann04cia2,
	Abstract = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio.  Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding. This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is, like-wise to the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
	Author = {F. J. Herrmann and P. P. Moghaddam},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:27:57 -0700},
	Date-Modified = {2008-08-14 14:12:32 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/069S0202-Herrmann_F_Curvelet_imaging.pdf},
	Publisher = {CSEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage04/herrmann04cdl.pdf},
	Title = {Curvelet imaging and processing: sparseness-constrained least-squares migration},
	Year = {2004},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/069S0202-Herrmann_F_Curvelet_imaging.pdf}}

@conference{herrmann04cia1,
	Abstract = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, in which
                  multiples are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data.
                  The last step appears crucial in practice: an
                  incorrect adaptive subtraction method will cause
                  multiples to be sub-optimally subtracted or
                  primaries being distorted, or both.  Therefore, we
                  propose a new domain for separation of primaries and
                  multiples via the Curvelet transform.  This
                  transform maps the data into almost orthogonal
                  localized events with a directional and
                  spatial-temporal component.  The multiples are
                  suppressed by thresholding the input data at those
                  Curvelet components where the predicted multiples
                  have large amplitudes.  In this way the more
                  traditional filtering of predicted multiples to fit
                  the input data is avoided.  An initial field data
                  example shows a considerable improvement in multiple
                  suppression.},
	Author = {F. J. Herrmann and D. J. Verschuur},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:26:51 -0700},
	Date-Modified = {2008-08-14 14:11:28 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/068S0202-Herrmann_F_Curvelet_Multiple.pdf},
	Publisher = {CSEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/herrmann04cip1.pdf},
	Title = {Curvelet imaging and processing: adaptive multiple elimination},
	Year = {2004},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/068S0202-Herrmann_F_Curvelet_Multiple.pdf}}

@conference{herrmann04cia,
	Abstract = {In this paper an overview is given on the
                  application of directional basis functions, known
                  under the name Curvelets/Contourlets, to various
                  aspects of seismic processing and imaging.  Key
                  conceps in the approach are the use of (i)
                  directional basis functions that localize in both
                  domains (e.g. space and angle); (ii) non-linear
                  estimation, which corresponds to localized muting on
                  the coefficients, possibly supplemented by
                  constrained optimization (iii) invariance of the
                  basis functions under the imaging operators. We will
                  discuss applications that include multiple and
                  ground roll removal; sparseness-constrained
                  least-squares migration and the computation of 4-D
                  difference cubes.},
	Author = {F. J. Herrmann},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:25:48 -0700},
	Date-Modified = {2008-08-14 14:10:49 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/070S0202-Herrmann_F_Curvelet_overview.pdf},
	Publisher = {CSEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/herrmann04cip2.pdf},
	Title = {Curvelet imaging and processing: an overview},
	Year = {2004},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/070S0202-Herrmann_F_Curvelet_overview.pdf}}

@conference{cristall04cpa,
	Abstract = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging holds great promise to better monitor and
                  optimise reservoir production. The basic idea behind
                  4D seismic is that when multiple 3D surveys are
                  acquired at separate calendar times over a producing
                  field, the reservoir geology will not change from
                  survey to survey but the state of the reservoir
                  fluids will change. Thus, taking the difference
                  between two 3D surveys should remove the static
                  geologic contribution to the data and isolate the
                  time-varying fluid flow component. However, a major
                  challenge in 4D seismic is that acquisition and
                  processing differences between 3D surveys often
                  overshadow the changes caused by fluid flow. This
                  problem is compounded when 4D effects are sought to
                  be derived from legacy 3D data sets that were not
                  originally acquired with 4D in mind. The goal of
                  this study is to remove the acquisition and imaging
                  artefacts from a 4D seismic difference cube using
                  Curvelet processing techniques.  },
	Author = {J. Cristall and M. Beyreuther and F. J. Herrmann},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:24:40 -0700},
	Date-Modified = {2008-08-14 14:02:47 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/059S0201-Cristall_J_Curvelet_4D.pdf},
	Publisher = {CSEG},
	Title = {Curvelet processing and imaging: 4-D adaptive subtraction},
	Year = {2004},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/059S0201-Cristall_J_Curvelet_4D.pdf}}

@conference{herrmann05nlr,
	Abstract = {Two complementary solution strategies to the
                  least-squares imaging problem with sparseness \&
                  continuity continuity constraints are proposed. The
                  applied formalism explores the sparseness of
                  curvelets coefficients of the reflectivity and their
                  invariance under the demigration-migration
                  operator. We achieve the solution by jointly
                  minimizing a weighted l1-norm on the curvelet
                  coefficients and an anisotropic difussion or total
                  variation norm on the imaged reflectivity model. The
                  l1-norm exploits the sparsenss of the reflectivity
                  in the curvelet domain whereas the anisotropic norm
                  enhances the continuity along the reflections while
                  removing artifacts residing in between
                  reflectors. While the two optimization methods
                  (convex versus non-convex) share the same type of
                  regularization, they differ in flexibility how to
                  handle additional constraints on the coefficients of
                  the imaged reflectivity and in computational
                  expense.  A brief sketch of the theory is provided
                  along with a number of synthetic examples.  },
	Author = {F. J. Herrmann and P. P. Moghaddam},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:22:40 -0700},
	Date-Modified = {2008-08-14 14:15:09 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2005/2005abstracts/037S0131-Herrmann_F_Migration_nonlinear_regularization.pdf},
	Publisher = {CSEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/herrmann05nlr.pdf},
	Title = {Non-linear regularization in seismic imaging},
	Year = {2005},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2005/2005abstracts/037S0131-Herrmann_F_Migration_nonlinear_regularization.pdf}}

@conference{herrmann05nld,
	Abstract = {We propose an efficient iterative data
                  interpolation method using continuity along
                  reflectors in seismic images via curvelet and
                  discrete cosine transforms. The curvelet transform
                  is a new multiscale transform that provides sparse
                  representations for images that comprise smooth
                  objects separated by piece-wise smooth
                  discontinuities (e.g. seismic images). The advantage
                  of using curvelets is that these frames are sparse
                  for high-frequency caustic-free solutions of the
                  wave-equation. Since we are dealing with less than
                  ideal data (e.g. bandwidth-limited), we compliment
                  the curvelet frames with the discrete cosine
                  transform. The latter is motivated by the successful
                  data continuation with the discrete Fourier
                  transform. By choosing generic basis functions we
                  circumvent the necessity to make parametric
                  assumptions (e.g. through linear/parabolic Radon or
                  demigration) regarding the shape of events in
                  seismic data.  Synthetic and real data examples
                  demonstrate that our algorithm provides interpolated
                  traces that accurately reproduce the wavelet shape
                  as well as the AVO behavior along events in shot
                  gathers.},
	Author = {F. J. Herrmann and G. Hennenfent},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:21:13 -0700},
	Date-Modified = {2008-08-14 14:14:02 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2005/2005abstracts/101S0201-Herrmann_F_Non_Linear_Data_Continuation.pdf},
	Publisher = {CSEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/herrmann05nld.pdf},
	Title = {Non-linear data continuation with redundant frames},
	Year = {2005},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2005/2005abstracts/101S0201-Herrmann_F_Non_Linear_Data_Continuation.pdf}}

@conference{hennenfent05scs,
	Abstract = {Continuity along reflectors in seismic images is
                  used via Curvelet representation to stabilize the
                  convolution operator inversion. The Curvelet
                  transform is a new multiscale transform that
                  provides sparse representations for images that
                  comprise smooth objects separated by piece-wise
                  smooth discontinuities (e.g. seismic images). Our
                  iterative Curvelet-regularized deconvolution
                  algorithm combines conjugate gradient-based
                  inversion with noise regularization performed using
                  non-linear Curvelet coefficient thresholding.  The
                  thresholding operation enhances the sparsity of
                  Curvelet representations. We show on a synthetic
                  example that our algorithm provides improved
                  resolution and continuity along reflectors as well
                  as reduced ringing effect compared to the iterative
                  Wiener-based deconvolution approach.  },
	Author = {G. Hennenfent and F. J. Herrmann and R. Neelamani},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:19:56 -0700},
	Date-Modified = {2008-08-14 14:04:36 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2005/2005abstracts/030S0131-Hennenfent_G_Sparseness_Constrained_Deconvoluti.pdf},
	Publisher = {CSEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/hennenfent05cseg_pres.pdf},
	Title = {Sparseness-constrained seismic deconvolution with curvelets},
	Year = {2005},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2005/2005abstracts/030S0131-Hennenfent_G_Sparseness_Constrained_Deconvoluti.pdf}}

@conference{moghaddam07sac,
	Abstract = {A non-linear singularity-preserving solution to
                  the least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes.  Our method is based on the estimation
                  of the normal operator in the form of an
                  'eigenvalue' decompsoition with curvelets as the
                  eigenvectors'. Subsequently, we propose an inversion
                  method that derives from estimation of the normal
                  operator and is formulated as a convex optimization
                  problem. Sparsity in the curvelet domain as well as
                  continuity along the reflectors in the image domain
                  are promoted as part of this optimization. Our
                  method is tested with a reverse-time 'wave-equation'
                  migration code simulating the acoustic wave
                  equation.  },
	Author = {P. P. Moghaddam and F. J. Herrmann and C. C. Stolk},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:17:43 -0700},
	Date-Modified = {2008-08-14 14:25:15 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf},
	Publisher = {CSEG},
	Title = {Sparsity and continuity enhancing seismic imaging},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf}}

@conference{moghaddam07mar,
	Abstract = {In this paper, we recover the amplitude of a
                  seismic image by approximating the normal operator
                  and subsequently inverting it. Normal operator
                  (migration followed by modeling) is an example of
                  pseudo-differential. curvelets are proven to be
                  invariant under the action of pseudo-differential
                  operators under certain conditions. Subsequently,
                  curvelets are forming as eigen-vectors for such an
                  operator. We propose a seismic amplitude recovery
                  method that employs an eigen-value decomposition for
                  normal operator using curvelets as eigen-vectors and
                  to be estimated eigenvalues. A post-stack
                  reverse-time, wave-equation migration is used for
                  evaluation of the proposed method.  },
	Author = {P. P. Moghaddam and F. J. Herrmann and C. C. Stolk},
	Booktitle = {CSEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:14:31 -0700},
	Date-Modified = {2008-08-14 14:24:08 -0700},
	Keywords = {SLIM},
	Month = {May},
	Pdf = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf},
	Publisher = {CSEG},
	Title = {Migration amplitude recovery using curvelets},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf}}

@conference{herrmann01sas,
	Abstract = {AVO analysis of seismic data is based on the
                  assumption that transitions in the earth consist of
                  jump discontinuities only. Generalization of these
                  transitions to more realistic transitions shows a
                  drastic change in observed AVO behavior, especially
                  for the large angles currently attained by
                  increasing cable lengths. We propose a simple
                  approach that accounts for this anomalous behavior
                  by renormalizing the observed AVO. This approach
                  allows for a separation of the observed AVO effects
                  in terms of a conventional Zoeppritz contribution
                  and a scaling contribution, when the transitions can
                  no longer be considered as isolated jump
                  discontinuities. After renormalization, the inverted
                  fluctuations regain their relative magnitudes which,
                  due to the scaling, may have been significantly
                  distorted.},
	Author = {F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 14:00:14 -0700},
	Date-Modified = {2008-08-14 14:08:10 -0700},
	Keywords = {SLIM},
	Month = {June},
	Publisher = {EAGE},
	Title = {Scaling and seismic reflectivity: implications of scaling on {AVO}},
	Year = {2001}}

@conference{beyreuther04cdo,
	Abstract = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging is an emerging technology that holds great
                  promise to better monitor and optimise reservoir
                  production. The basic idea behind 4D seismic is that
                  when multiple 3D surveys are acquired at separate
                  calendar times over a producing field, the reservoir
                  geology will not change from survey to survey but
                  the state of the reservoir fluids will change. Thus,
                  taking the difference between two 3D surveys should
                  remove the static geologic contribution to the data
                  and isolate the time- varying fluid flow
                  component. However, a major challenge in 4D seismic
                  is that acquisition and processing differences
                  between 3D surveys often overshadow the changes
                  caused by fluid flow. This problem is compounded
                  when 4D effects are sought to be derived from
                  vintage 3D data sets that were not originally
                  acquired with 4D in mind. The goal of this study is
                  to remove the acquisition and imaging artefacts from
                  a 4D seismic difference cube using Curvelet
                  processing techniques.},
	Author = {M. Beyreuther and F. J. Herrmann and J. Cristall},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:59:23 -0700},
	Date-Modified = {2008-08-14 14:01:05 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/~felix/public/EAGE4D2004.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage04/beyreuther04cdo.pdf},
	Title = {Curvelet denoising of 4-D seismic},
	Year = {2004},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/~felix/public/EAGE4D2004.pdf}}

@conference{herrmann04sop,
	Abstract = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, in which
                  multiples are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.  },
	Author = {F. J. Herrmann and D. J. Verschuur},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:58:28 -0700},
	Date-Modified = {2008-08-14 14:13:27 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/~felix/public/EAGEM2004.pdf},
	Publisher = {EAGE},
	Title = {Separation of primaries and multiples by non-linear estimation in the curvelet domain},
	Year = {2004},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/~felix/public/EAGEM2004.pdf}}

@conference{herrmann04cdl,
	Abstract = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal- to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding. This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
	Author = {F. J. Herrmann and P. P. Moghaddam},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:57:26 -0700},
	Date-Modified = {2008-08-14 14:09:05 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/~felix/public/EAGEIM2004.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage04/herrmann04cdl.pdf},
	Title = {Curvelet-domain least-squares migration with sparseness constraints},
	Year = {2004},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/~felix/public/EAGEIM2004.pdf}}

@conference{herrmann05rcd1,
	Abstract = {A non-linear primary-multiple separation method
                  using curvelets frames is presented. The advantage
                  of this method is that curvelets arguably provide an
                  optimal sparse representation for both primaries and
                  multiples. As such curvelets frames are ideal
                  candidates to separate primaries from multiples
                  given inaccurate predictions for these two data
                  components. The method derives its robustness
                  regarding the presence of noise; errors in the
                  prediction and missing data from the curvelet
                  frame's ability (i) to represent both signal
                  components with a limited number of multi-scale and
                  directional basis functions; (ii) to separate the
                  components on the basis of differences in location,
                  orientation and scales and (iii) to minimize
                  correlations between the coefficients of the two
                  components.  A brief sketch of the theory is
                  provided as well as a number of examples on
                  synthetic and real data.  },
	Author = {F. J. Herrmann and D. J. Verschuur},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:55:44 -0700},
	Date-Modified = {2008-08-14 14:16:50 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/EAGEM2005.pdf},
	Publisher = {EAGE},
	Title = {Robust curvelet-domain primary-multiple separation with sparseness constraints},
	Year = {2005},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/EAGEM2005.pdf}}

@conference{hennenfent05sdr,
	Abstract = {We propose an efficient iterative
                  curvelet-regularized deconvolution algorithm that
                  exploits continuity along reflectors in seismic
                  images. Curvelets are a new multiscale transform
                  that provides sparse representations for images
                  (such as seismic images) that comprise smooth
                  objects separated by piece-wise smooth
                  discontinuities. Our technique combines conjugate
                  gradient-based convolution operator inversion with
                  noise regularization that is performed using
                  non-linear curvelet coefficient shrinkage
                  (thresholding). The shrinkage operation leverages
                  the sparsity of curvelets
                  representations. Simulations demonstrate that our
                  algorithm provides improved resolution compared to
                  the traditional Wiener-based deconvolution
                  approach.},
	Author = {G. Hennenfent and R. Neelamani and F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:54:53 -0700},
	Date-Modified = {2008-08-14 14:05:06 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/DeconEAGE.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/hennenfent05eage_poster.pdf},
	Title = {Seismic deconvolution revisited with curvelet frames},
	Year = {2005},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/DeconEAGE.pdf}}

@conference{herrmann05osf,
	Abstract = {Two complementary solution strategies to the
                  least-squares migration problem with sparseness- and
                  continuity constraints are proposed. The applied
                  formalism explores the sparseness of curvelets on
                  the reflectivity and their invariance under the
                  demigration-migration operator. Sparseness is
                  enhanced by (approximately) minimizing a (weighted)
                  l1-norm on the curvelet coefficients.  Continuity
                  along imaged reflectors is brought out by minimizing
                  the anisotropic diffusion or total variation norm
                  which penalizes variations along and in between
                  reflectors. A brief sketch of the theory is provided
                  as well as a number of synthetic examples.
                  Technical details on the implementation of the
                  optimization strategies are deferred to an
                  accompanying paper: implementation},
	Author = {F. J. Herrmann and P. P. Moghaddam and R. Kirlin},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:53:35 -0700},
	Date-Modified = {2008-08-14 14:15:50 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/EAGEIM12005.pdf},
	Publisher = {EAGE},
	Title = {Optimization strategies for sparseness- and continuity-enhanced imaging: theoretical considerations},
	Year = {2005},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/EAGEIM12005.pdf}}

@conference{herrmann05rcd,
	Abstract = {A robust data interpolation method using curvelets
                  frames is presented. The advantage of this method is
                  that curvelets arguably provide an optimal sparse
                  representation for solutions of wave equations with
                  smooth coefficients. As such curvelets frames
                  circum- vent -- besides the assumption of
                  caustic-free data -- the necessity to make
                  parametric assumptions (e.g. through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. A brief sketch of
                  the theory is provided as well as a number of
                  examples on synthetic and real data. },
	Author = {F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:52:50 -0700},
	Date-Modified = {2008-08-14 14:16:25 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/EAGEIN2005.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage05/herrmann05rcd.pdf},
	Title = {Robust curvelet-domain data continuation with sparseness constraints},
	Year = {2005},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/EAGEIN2005.pdf}}

@conference{hennenfent07isf,
	Abstract = {Seismic data is often irregularly and/or sparsely
                  sampled along spatial coordinates.  We show that
                  these acquisition geometries are not necessarily a
                  source of adversity in order to accurately
                  reconstruct adequately-sampled data. We use two
                  examples to illustrate that it may actually be
                  better than equivalent regularly subsampled
                  data. This comment was already made in earlier works
                  by other authors. We explain this behavior by two
                  key observations. Firstly, a noise-free
                  underdetermined problem can be seen as a noisy
                  well-determined problem. Secondly, regularly
                  subsampling creates strong coherent acquisition
                  noise (aliasing) difficult to remove unlike the
                  noise created by irregularly subsampling that is
                  typically weaker and Gaussian-like.},
	Author = {G. Hennenfent and F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:52:04 -0700},
	Date-Modified = {2008-08-14 14:06:39 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07eage.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07eage_pres.pdf},
	Title = {Irregular sampling: from aliasing to noise},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07eage.pdf}}

@conference{herrmann07srm,
	Abstract = {Incomplete data, unknown source-receiver
                  signatures and free-surface reflectivity represent
                  challenges for a successful prediction and
                  subsequent removal of multiples. In this paper, a
                  new method will be represented that tackles these
                  challenges by combining what we know about wavefield
                  (de-)focussing, by weighted
                  convolutions/correlations, and recently developed
                  curvelet-based recovery by sparsity-promoting
                  inversion (CRSI).  With this combination, we are
                  able to leverage recent insights from wave physics
                  to- wards a nonlinear formulation for the
                  multiple-prediction problem that works for
                  incomplete data and without detailed knowledge on
                  the surface effects.},
	Author = {F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:51:17 -0700},
	Date-Modified = {2008-08-14 14:21:35 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07a.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage07/herrmann07srm.pdf},
	Title = {Surface related multiple prediction from incomplete data},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07a.pdf}}

@conference{maysami07src,
	Abstract = {Seismic transitions of the subsurface are
                  typically considered as zero-order singularities
                  (step functions). According to this model, the
                  conventional deconvolution problem aims at
                  recovering the seismic reflectivity as a sparse
                  spike train. However, recent multiscale analysis on
                  sedimentary records revealed the existence of
                  accumulations of varying order singularities in the
                  subsurface, which give rise to fractional-order
                  discontinuities. This observation not only calls for
                  a richer class of seismic reflection waveforms, but
                  it also requires a different methodology to detect
                  and characterize these reflection events. For
                  instance, the assumptions underlying conventional
                  deconvolution no longer hold. Because of the
                  bandwidth limitation of seismic data, multiscale
                  analysis methods based on the decay rate of wavelet
                  coefficients may yield ambiguous results. We avoid
                  this problem by formulating the estimation of the
                  singularity orders by a parametric nonlinear
                  inversion method.},
	Author = {M. Maysami and F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:50:08 -0700},
	Date-Modified = {2008-08-14 14:23:09 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07eage.pdf},
	Publisher = {EAGE},
	Title = {Seismic reflector characterization by a multiscale detection-estimation method},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07eage.pdf}}

@conference{herrmann07sia,
	Abstract = {In this paper, we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  three problems in seismic processing and imaging
                  namely, seismic data regularization from data with
                  large percentages of traces missing; seismic
                  amplitude recovery for sub-salt images obtained by
                  reverse-time migration and primary-multiple
                  separation, given an inaccurate multiple
                  prediction. We argue why these nonlinear
                  formulations are beneficial.  },
	Author = {F. J. Herrmann and G. Hennenfent and P. P. Moghaddam},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:49:06 -0700},
	Date-Modified = {2008-08-14 14:21:10 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2007/herrmann07b.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEIMPROC.pdf},
	Title = {Seismic imaging and processing with curvelets},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2007/herrmann07b.pdf}}

@conference{moghaddam07sar,
	Abstract = {A non-linear singularity-preserving solution to
                  the least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes. Our method is based on the estimation of
                  the normal operator in the form of an 'eigenvalue'
                  decompsoition with curvelets as the
                  'eigenvectors'. Subsequently, we propose an
                  inversion method that derives from estimation of the
                  normal operator and is formulated as a convex
                  optimization problem. Sparsity in the curvelet
                  domain as well as continuity along the reflectors in
                  the image domain are promoted as part of this
                  optimization. Our method is tested with a
                  reverse-time 'wave-equation' migration code
                  simulating the acoustic wave equation.},
	Author = {P. P. Moghaddam and F. J. Herrmann and C. C. Stolk},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:47:17 -0700},
	Date-Modified = {2008-08-14 14:26:21 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/moghaddam07.pdf},
	Publisher = {EAGE},
	Title = {Seismic amplitude recovery with curvelets},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/moghaddam07.pdf}}

@conference{challa07srf,
	Abstract = {Constrained by practical and economical
                  considerations, one often uses seismic data with
                  missing traces. The use of such data results in
                  image artifacts and poor spatial
                  resolution. Sometimes due to practical limitations,
                  measurements may be available on a perturbed grid,
                  instead of on the designated grid. Due to
                  algorithmic requirements, when such measurements are
                  viewed as those on the designated grid, the recovery
                  procedures may result in additional artifacts. This
                  paper interpolates incomplete data onto regular grid
                  via the Fourier domain, using a recently developed
                  greedy algorithm. The basic objective is to study
                  experimentally as to what could be the size of the
                  perturbation in measurement coordinates that allows
                  for the measurements on the perturbed grid to be
                  considered as on the designated grid for faithful
                  recovery. Our experimental work shows that for
                  compressible signals, a uniformly distributed
                  perturbation can be offset with slightly more number
                  of measurements.  },
	Author = {S. S. Challa and G. Hennenfent and F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:39:57 -0700},
	Date-Modified = {2008-08-14 14:02:11 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/sastry07.pdf},
	Publisher = {EAGE},
	Title = {Signal reconstruction from incomplete and misplaced measurements},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/sastry07.pdf}}

@conference{herrmann07rdi,
	Abstract = {Combinations of parsimonious signal
                  representations with nonlinear sparsity promoting
                  programs hold the key to the next-generation of
                  seismic data processing algorithms ...  Since they
                  allow for a formulation that is stable w.r.t. noise
                  \& incomplete data do not require prior information
                  on the velocity or locations and dips of the events
                  },
	Author = {F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:38:07 -0700},
	Date-Modified = {2008-08-14 14:19:53 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07rdi.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSDEV.pdf},
	Title = {Recent developments in curvelet-based seismic processing},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSDEV.pdf}}

@conference{hennenfent07crw,
	Abstract = {In this overview of the recent Curvelet
                  Reconstruction with Sparsity-promoting Inver- sion
                  (CRSI) method, we present our latest 2-D and 3-D
                  interpolation results on both synthetic and real
                  datasets. We compare these results to interpolated
                  data using other ex- isting methods. Finally, we
                  discuss the challenges related to sparsity-promoting
                  solvers for the large-scale problems the industry
                  faces.},
	Author = {G. Hennenfent and F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Datasets = {Oseberg},
	Date-Added = {2008-05-21 13:34:04 -0700},
	Date-Modified = {2008-08-14 14:06:14 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07eage_workshop.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07eage_Wcurvelet_pres.pdf},
	Title = {Curvelet reconstruction with sparsity-promoting inversion: successes an challenges},
	Year = {2007},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07eage_workshop.pdf}}

@conference{herrmann07jda,
	Abstract = {In this presentation we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  the recovery of seismic amplitudes. We show that the
                  curvelet's wavefront detection capability and
                  invariance under wave propagation lead to a
                  formulation of this recovery problem that is stable
                  under noise and missing data.},
	Author = {F. J. Herrmann and P. P. Moghaddam},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:32:40 -0700},
	Date-Modified = {2008-08-14 14:18:13 -0700},
	Keywords = {SLIM},
	Month = {June},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07jda.pdf},
	Publisher = {EAGE},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSIM.pdf},
	Title = {Just diagonalize: a curvelet-based approach to seismic amplitude recovery},
	Year = {2007}}

@conference{yarham07cai,
	Abstract = {Ground roll removal of seismic signals can be a
                  challenging prospect. Dealing with undersampleing
                  causing aliased waves amplitudes orders of magnitude
                  higher than reflector signals and low frequency loss
                  of information due to band ...  },
	Author = {C. Yarham and G. Hennenfent and F. J. Herrmann},
	Booktitle = {EAGE Technical Program Expanded Abstracts},
	Date-Added = {2008-05-21 13:28:41 -0700},
	Date-Modified = {2008-08-14 14:31:36 -0700},
	Keywords = {SLIM},
	Month = {June},
	Publisher = {EAGE},
	Title = {Curvelet applications in surface wave removal},
	Year = {2007}}

@conference{lin07cwe1,
	Abstract = {An explicit algorithm for the extrapolation of
                  one-way wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from ``compressed sensing'', we are able
                  to formulate the (inverse) wavefield extrapolation
                  problem on small subsets of the data volume, thereby
                  reducing the size of the operators. According to
                  compressed sensing theory, signals can successfully
                  be recovered from an imcomplete set of measurements
                  when the measurement basis is incoherent with the
                  representation in which the wavefield is sparse. In
                  this new approach, the eigenfunctions of the
                  Helmholtz operator are recognized as a basis that is
                  incoherent with curvelets that are known to compress
                  seismic wavefields. By casting the wavefield
                  extrapolation problem in this framework, wavefields
                  can successfully be extrapolated in the modal domain
                  via a computationally cheaper operation. A proof of
                  principle for the ``compressed sensing'' method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. {\copyright}2007
                  Society of Exploration Geophysicists},
	Author = {T. T. Y. Lin and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 19:09:37 -0700},
	Date-Modified = {2008-08-14 14:22:40 -0700},
	Doi = {10.1190/1.2792882},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {1997-2001},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin2007seg.pdf},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/lin07cwe.pdf},
	Title = {Compressed wavefield extrapolation with curvelets},
	Volume = {26},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/26/1997/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2792882},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin2007seg.pdf}}

@conference{herrmann07sdp,
	Abstract = {In this abstract, we present a nonlinear
                  curvelet-based sparsity-promoting formulation of a
                  seismic processing flow, consisting of the following
                  steps: seismic data regularization and the
                  restoration of migration amplitudes. We show that
                  the curvelet's wavefront detection capability and
                  invariance under the migration-demigration operator
                  lead to a formulation that is stable under noise and
                  missing data. {\copyright}2007 Society of
                  Exploration Geophysicists},
	Author = {F. J. Herrmann and D. Wang and G. Hennenfent and P. P. Moghaddam},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 19:08:35 -0700},
	Date-Modified = {2008-08-14 14:20:26 -0700},
	Doi = {10.1190/1.2792927},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2220-2224},
	Pdf = {http://link.aip.org/link/?SGA/26/2220/1},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07SEGPROC.pdf},
	Title = {Seismic data processing with curvelets: a multiscale and nonlinear approach},
	Volume = {26},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/26/2220/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2792927},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07sega.pdf}}

@conference{moghaddam07rsi,
	Abstract = {In this paper, we recover the amplitude of a
                  seismic image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate non-linear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time `wave-equation' migration code
                  simulating the acoustic wave equation on the SEG-AA
                  salt model. {\copyright}2007 Society of Exploration
                  Geophysicists},
	Author = {P. P. Moghaddam and F. J. Herrmann and C. C. Stolk},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 19:07:03 -0700},
	Date-Modified = {2008-08-14 14:24:40 -0700},
	Doi = {10.1190/1.2792928},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2225-2229},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07seg.pdf},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/moghaddam07rsi.pdf},
	Title = {Robust seismic-images amplitude recovery using curvelets},
	Volume = {26},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/26/2225/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2792928},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07seg.pdf}}

@conference{wang07rri,
	Abstract = {In this abstract, we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  the primary-multiple separation problem. We show
                  that these coherent signal components can be
                  separated robustly by explicitly exploting the
                  locality of curvelets in phase space (space-spatial
                  frequency plane) and their ability to compress data
                  volumes that contain wavefronts. This work is an
                  extension of earlier results and the presented
                  algorithms are shown to be stable under noise and
                  moderately erroneous multiple
                  predictions. {\copyright}2007 Society of Exploration
                  Geophysicists},
	Author = {D. Wang and R. Saab and \"{O}. Yilmaz and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 19:06:02 -0700},
	Date-Modified = {2008-08-14 14:28:23 -0700},
	Doi = {10.1190/1.2792986},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2500-2504},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07seg.pdf},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/wang07rri.pdf},
	Title = {Recent results in curvelet-based primary-multiple separation: application to real data},
	Volume = {26},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/26/2500/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2792986},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07seg.pdf}}

@conference{herrmann07mpf,
	Abstract = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach. During the first step, the
                  recenly developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation. {\copyright}2007
                  Society of Exploration Geophysicists},
	Author = {F. J. Herrmann and D. Wang and G. Hennenfent},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 19:05:09 -0700},
	Date-Modified = {2008-08-14 14:18:42 -0700},
	Doi = {10.1190/1.2792987},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2505-2509},
	Pdf = {http://link.aip.org/link/?SGA/26/2505/1},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/herrmann07mpf.pdf},
	Title = {Multiple prediction from incomplete data with the focused curvelet transform},
	Volume = {26},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/26/2505/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2792987},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07segb.pdf}}

@conference{saab07cbp,
	Abstract = {In this abstract, we present a novel
                  primary-multiple separation scheme which makes use
                  of the sparsity of both primaries and multiples in a
                  transform domain, such as the curvelet transform, to
                  provide estimates of each. The proposed algorithm
                  utilizes seismic data as well as the output of a
                  preliminary step that provides (possibly) erroneous
                  predictions of the multiples. The algorithm
                  separates the signal components, i.e., the primaries
                  and multiples, by solving an optimization problem
                  that assumes noisy input data and can be derived
                  from a Bayesian perspective. More precisely, the
                  optimization problem can be arrived at via an
                  assumption of a weighted Laplacian distribution for
                  the primary and multiple coefficients in the
                  transform domain and of white Gaussian noise
                  contaminating both the seismic data and the
                  preliminary prediction of the multiples, which both
                  serve as input to the algorithm. {\copyright}2007
                  Society of Exploration Geophysicists},
	Author = {R. Saab and D. Wang and \"{O}. Yilmaz and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 19:01:25 -0700},
	Date-Modified = {2008-08-14 14:27:03 -0700},
	Doi = {10.1190/1.2792988},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2510-2514},
	Pdf = {http://link.aip.org/link/?SGA/26/2510/1},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/saab07cbp.pdf},
	Title = {Curvelet-based primary-multiple separation from a {B}ayesian perspective},
	Volume = {26},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/26/2510/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2792988},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/saab07seg.pdf}}

@conference{verschuur07mmp,
	Abstract = {The surface-related multiple elimination (SRME)
                  method has proven to be successful on a large number
                  of data cases. Most of the applications are still
                  2D, as the full 3D implementation is still expensive
                  and under development. However, the earth is a 3D
                  medium, such that 3D effects are difficult to
                  avoid. Most of the 3D effects come from diffractive
                  structures, whereas the specular reflections
                  normally have less of a 3D behavior. By separating
                  the seismic data in a specular reflecting and a
                  diffractive part, multiple prediction can be carried
                  out with these different subsets of the input data,
                  resulting in several categories of predicted
                  multiples. Because each category of predicted
                  multiples can be subtracted from the input data with
                  different adaptation filters, a more flexible SRME
                  procedure is obtained. Based on some initial results
                  from a Gulf of Mexico dataset, the potential of this
                  approach is investigated. {\copyright}2007 Society
                  of Exploration Geophysicists},
	Author = {D. J. Verschuur and D. Wang and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 19:00:28 -0700},
	Date-Modified = {2008-08-14 14:27:59 -0700},
	Doi = {10.1190/1.2792993},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2535-2539},
	Pdf = {http://link.aip.org/link/?SGA/26/2535/1},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg07/verschuur07mtm.pdf},
	Title = {Multiterm multiple prediction using separated reflections and diffractions combined with curvelet-based subtraction},
	Volume = {26},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/26/2535/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2792993},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/verschuur07seg.pdf}}

@conference{hennenfent07rsn,
	Abstract = {In this paper, we turn the interpolation problem
                  of coarsely-sampled data into a denoising
                  problem. From this point of view, we illustrate the
                  benefit of random sampling at sub-Nyquist rate over
                  regular sampling at the same rate. We show that,
                  using nonlinear sparsity-promoting optimization,
                  coarse random sampling may actually lead to
                  significantly better wavefield reconstruction than
                  equivalent regularly sampled data. {\copyright}2007
                  Society of Exploration Geophysicists},
	Author = {G. Hennenfent and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 18:59:27 -0700},
	Date-Modified = {2008-08-14 14:07:08 -0700},
	Doi = {10.1190/1.2793002},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2575-2579},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07seg.pdf},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07seg_pres.pdf},
	Title = {Random sampling: New insights into the reconstruction of coarsely sampled wavefields},
	Volume = {26},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/26/2575/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2793002},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07seg.pdf}}

@conference{thomson06apw,
	Abstract = {We propose using overlapping, tapered windows to
                  process seismic data in parallel. This method
                  consists of numerically tight linear operators and
                  adjoints that are suitable for use in iterative
                  algorithms. This method is also highly scalable and
                  makes parallel processing of large seismic data sets
                  feasible. We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we apply to a seismic data interpolation
                  algorithm. The successful performance of our
                  parallel processing scheme and algorithm on a
                  two-dimensional synthetic data is
                  shown. {\copyright}2006 Society of Exploration
                  Geophysicists},
	Author = {D. Thomson and G. Hennenfent and H. Modzelewski and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 18:58:14 -0700},
	Date-Modified = {2008-08-14 14:27:34 -0700},
	Doi = {10.1190/1.2370099},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2767-2771},
	Pdf = {http://link.aip.org/link/?SGA/25/2767/1},
	Publisher = {SEG},
	Title = {A parallel windowed fast discrete curvelet transform applied to seismic processing},
	Volume = {25},
	Year = {2006},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/25/2767/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2370099},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/thomson06seg.pdf}}

@conference{yarham06cbg,
	Abstract = {We have effectively identified and removed ground
                  roll through a two-step process. The first step is
                  to identify the major components of the ground roll
                  through various methods including multiscale
                  separation, directional or frequency filtering or by
                  any other method that identifies the ground
                  roll. Given this estimate for ground roll, the
                  recorded signal is separated during the second step
                  through a block-coordinate relaxation method that
                  seeks the sparsest set for weighted curvelet
                  coefficients of the ground roll and the sought-after
                  reflectivity. The combination of these two methods
                  allows us to separate out the ground roll signal
                  while preserving the reflector information. Since
                  our method is iterative, we have control of the
                  separation process. We successfully tested our
                  algorithm on a real data set with a complex ground
                  roll and reflector structure. {\copyright}2006
                  Society of Exploration Geophysicists},
	Author = {C. Yarham and U. Boeniger and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 18:57:02 -0700},
	Date-Modified = {2008-08-14 14:29:43 -0700},
	Doi = {10.1190/1.2370101},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2777-2782},
	Pdf = {http://link.aip.org/link/?SGA/25/2777/1},
	Publisher = {SEG},
	Title = {Curvelet-based ground roll removal},
	Volume = {25},
	Year = {2006},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/25/2777/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2370101},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/yarham06seg.pdf}}

@conference{hennenfent06aos,
	Abstract = {We propose a method for seismic data interpolation
                  based on 1) the reformulation of the problem as a
                  stable signal recovery problem and 2) the fact that
                  seismic data is sparsely represented by
                  curvelets. This method does not require information
                  on the seismic velocities. Most importantly, this
                  formulation potentially leads to an explicit
                  recovery condition. We also propose a large-scale
                  problem solver for the 1-regularization minimization
                  involved in the recovery and successfully illustrate
                  the performance of our algorithm on 2D synthetic and
                  real examples. {\copyright}2006 Society of
                  Exploration Geophysicists},
	Author = {G. Hennenfent and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 18:55:59 -0700},
	Date-Modified = {2008-08-14 14:05:38 -0700},
	Doi = {10.1190/1.2370105},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2797-2801},
	Pdf = {http://link.aip.org/link/?SGA/25/2797/1},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/hennenfent06seg_pres.pdf},
	Title = {Application of stable signal recovery to seismic data interpolation},
	Volume = {25},
	Year = {2006},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/25/2797/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2370105},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06seg.pdf}}

@conference{hennenfent05scd,
	Abstract = {We present a robust iterative
                  sparseness-constrained interpolation algorithm using
                  2-/3-D curvelet frames and Fourier-like transforms
                  that exploits continuity along reflectors in seismic
                  data. By choosing generic transforms, we circumvent
                  the necessity to make parametric assumptions
                  (e.g. through linear/parabolic Radon or demigration)
                  regarding the shape of events in seismic
                  data. Simulation and real data examples for data
                  with moderately sized gaps demonstrate that our
                  algorithm provides interpolated traces that
                  accurately reproduce the wavelet shape as well as
                  the AVO behavior. Our method also shows good results
                  for de-aliasing judged by the behavior of the
                  ($f-k$)-spectrum before and after
                  regularization. {\copyright}2005 Society of
                  Exploration Geophysicists},
	Author = {G. Hennenfent and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 18:54:29 -0700},
	Date-Modified = {2008-08-14 14:04:00 -0700},
	Doi = {10.1190/1.2148142},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2162-2165},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/SEG2005_Data_Cont.pdf},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/hennenfent05seg_pres.pdf},
	Title = {Sparseness-constrained data continuation with frames: applications to missing traces and aliased signals in {2/3-D}},
	Volume = {24},
	Year = {2005},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/24/2162/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2148142},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/SEG2005_Data_Cont.pdf}}

@conference{beyreuther05cot,
	Abstract = {We present an alternative method of extracting
                  production related differences from time-lapse
                  seismic data sets. Our method is not based on the
                  actual subtraction of the two data sets, risking the
                  enhancement of noise and introduction of artifacts
                  due to local phase rotation and slightly misaligned
                  events. Rather, it mutes events of the monitor
                  survey with respect to the baseline survey based on
                  the magnitudes of coefficients in a sparse and local
                  atomic decomposition. Our technique is demonstrated
                  to be an effective tool for enhancing the time-lapse
                  signal from surveys which have been
                  cross-equalized. {\copyright}2005 Society of
                  Exploration Geophysicists},
	Author = {M. Beyreuther and J. Cristall and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Datasets = {Ula},
	Date-Added = {2008-05-20 18:53:06 -0700},
	Date-Modified = {2008-08-14 14:01:48 -0700},
	Doi = {10.1190/1.2148227},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2488-2491},
	Pdf = {http://link.aip.org/link/?SGA/24/2488/1},
	Publisher = {SEG},
	Title = {Computation of time-lapse differences with {3-D} directional frames},
	Volume = {24},
	Year = {2005},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/24/2488/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2148227},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/seg4D2005.pdf}}

@conference{herrmann04cdm,
	Abstract = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, in which
                  multiples are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple
                  suppression. {\copyright}2004 Society of Exploration
                  Geophysicists},
	Author = {F. J. Herrmann and D. J. Verschuur},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 18:40:33 -0700},
	Date-Modified = {2008-08-14 14:10:16 -0700},
	Doi = {10.1190/1.1851110},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {1333-1336},
	Pdf = {http://link.aip.org/link/?SGA/23/1333/1},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg04/herrmann04cdm.pdf},
	Title = {Curvelet-domain multiple elimination with sparseness constraints},
	Volume = {23},
	Year = {2004},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/23/1333/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1851110},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/~felix/public/SEGM2004.pdf}}

@conference{herrmann04cbn,
	Abstract = {In this paper an overview is given on the
                  application of directional basis functions, known
                  under the name Curvelets/Contourlets, to various
                  aspects of seismic processing and imaging, which
                  involve adaptive subtraction. Key concepts in the
                  approach are the use of directional basis functions
                  that localize in both domains (e.g. space and
                  angle); non-linear estimation, which corresponds to
                  localized muting on the coefficients, possibly
                  supplemented by constrained optimization. We will
                  discuss applications that include multiple,
                  ground-roll removal and migration
                  denoising. {\copyright}2004 Society of Exploration
                  Geophysicists},
	Author = {F. J. Herrmann and P. P. Moghaddam},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 18:36:09 -0700},
	Date-Modified = {2008-08-14 14:08:36 -0700},
	Doi = {10.1190/1.1851181},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {1977-1980},
	Pdf = {http://link.aip.org/link/?SGA/23/1977/1},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg04/herrmann04cbn.pdf},
	Title = {Curvelet-based non-linear adaptive subtraction with sparseness constraints},
	Volume = {23},
	Year = {2004},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/23/1977/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1851181},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/~felix/public/SEGAD2004.pdf}}

@conference{hennenfent04tta,
	Abstract = {We present a new method to stabilize the
                  three-term AVO inversion using Curvelet and Wavelet
                  transforms. Curvelets are basis functions that
                  effectively represent otherwise smooth objects
                  having discontinuities along smooth curves. The
                  applied formalism explores them to make the most of
                  the continuity along reflectors in seismic
                  images. Combined with Wavelets, Curvelets are used
                  to denoise the data by penalizing high frequencies
                  and small contributions in the AVO-cube. This
                  approach is based on the idea that rapid amplitude
                  changes along the ray-parameter axis are most likely
                  due to noise. The AVO-inverse problem is linearized,
                  formulated and solved for all (x, z) at once. Using
                  densities and velocities of the Marmousi model to
                  define the fluctuations in the elastic properties,
                  the performance of the proposed method is studied
                  and compared with the smoothing along the
                  ray-parameter direction only. We show that our
                  method better approximates the true data after the
                  denoising step, especially when noise level
                  increases. {\copyright}2004 Society of Exploration
                  Geophysicists},
	Author = {G. Hennenfent and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 18:31:38 -0700},
	Date-Modified = {2008-08-14 14:03:31 -0700},
	Doi = {10.1190/1.1851201},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {211-214},
	Pdf = {http://link.aip.org/link/?SGA/23/211/1},
	Publisher = {SEG},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/seg/seg04/hennenfent04tta.pdf},
	Title = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by curvelet and wavelet transforms},
	Volume = {23},
	Year = {2004},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/23/211/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1851201},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/hennenfent04seg.pdf}}

@conference{moghaddam04mpw,
	Abstract = {In this paper, the property of Curvelet transforms
                  for preconditioning the migration and normal
                  operators is investigated. These operators belong to
                  the class of Fourier integral operators and
                  pseudo-differential operator, respectively. The
                  effect of this pre-conditioner is shown in term of
                  improvement of sparsity, convergence rate, number of
                  iteration for the Krylov-subspace solver and
                  clustering of singular(eigen) values. The migration
                  operator, which we employed in this work is the
                  common-offset Kirchoff-Born
                  migration. {\copyright}2004 Society of Exploration
                  Geophysicists},
	Author = {P. P. Moghaddam and F. J. Herrmann},
	Booktitle = {SEG Technical Program Expanded Abstracts},
	Date-Added = {2008-05-20 18:30:12 -0700},
	Date-Modified = {2008-08-14 14:23:35 -0700},
	Doi = {10.1190/1.1845213},
	Issue = {1},
	Keywords = {SLIM},
	Number = {1},
	Pages = {2204-2207},
	Pdf = {http://link.aip.org/link/?SGA/23/2204/1},
	Publisher = {SEG},
	Title = {Migration preconditioning with curvelets},
	Volume = {23},
	Year = {2004},
	Bdsk-Url-1 = {http://link.aip.org/link/?SGA/23/2204/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1845213},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/~felix/public/SEGLAN2004.pdf}}

@book{snieder93giu,
	Author = {R. Snieder},
	Date-Added = {2008-05-20 17:16:42 -0700},
	Date-Modified = {2008-05-20 17:19:44 -0700},
	Keywords = {sampling},
	Publisher = {Chapman and Hall},
	Title = {Global inversions using normal mode and long-period surface waves},
	Year = {1993}}

@incollection{feichtinger94tap,
	Address = {Boca Raton, FL},
	Author = {H. G. Feichtinger and K. Grochenig},
	Booktitle = {Wavelets: mathematics and applications},
	Chapter = {8},
	Date-Added = {2008-05-20 17:10:18 -0700},
	Date-Modified = {2008-05-20 17:24:38 -0700},
	Editor = {J. J. Benedetto and M. Frazier},
	Keywords = {sampling},
	Pages = {305-363},
	Pdf = {http://www.univie.ac.at/nuhag-php/bibtex/open_files/fegr94_fgthpra.pdf},
	Publisher = {CRC Press},
	Series = {Studies in Advanced Mathematics},
	Title = {Theory and practice of irregular sampling},
	Year = {1994},
	Bdsk-Url-1 = {http://www.univie.ac.at/nuhag-php/bibtex/open_files/fegr94_fgthpra.pdf}}

@article{hindriks00ro3,
	Abstract = {Seismic signals are often irregularly sampled
                  along spatial coordinates, leading to suboptimal
                  processing and imaging results. Least-squares
                  estimation of Fourier components is used for the
                  reconstruction of band-limited seismic signals that
                  are irregularly sampled along two spatial
                  coordinates. A simple and efficient diagonal
                  weighting scheme, based on the areas surrounding the
                  spatial samples, takes the properties of the noise
                  (signal outside the bandwidth) into account in an
                  approximate sense. Diagonal stabilization based on
                  the energies of the signal and the noise ensures
                  robust estimation. Reconstruction by temporal
                  frequency component allows the specification of
                  varying bandwidth in two dimensions, depending on
                  the minimum apparent velocity. This parameterization
                  improves the reconstruction capability for lower
                  temporal frequencies. The shape of the spatial
                  aperture affects the method of sampling the Fourier
                  domain. Taking into account this property, a larger
                  bandwidth can be recovered. The properties of the
                  least-squares estimator allow a very efficient
                  implementation which, when using a conjugate
                  gradient algorithm, requires a modest number of 2-D
                  fast Fourier transforms per temporal frequency. The
                  method shows signicant improvement over the
                  conventionally used binning and stacking method on
                  both synthetic and real data. The method can be
                  applied to any subset of seismic data with two
                  varying spatial coordinates. {\copyright}2000
                  Society of Exploration Geophysicists},
	Author = {K. Hindriks and A. J. W. Duijndam},
	Date-Added = {2008-05-20 16:12:37 -0700},
	Date-Modified = {2008-08-14 15:05:01 -0700},
	Doi = {10.1190/1.1444716},
	Issue = {1},
	Journal = {Geophysics},
	Keywords = {reconstruction},
	Number = {1},
	Pages = {253-263},
	Pdf = {http://link.aip.org/link/?GPY/65/253/1},
	Publisher = {SEG},
	Title = {Reconstruction of {3-D} seismic signals irregularly sampled along two spatial coordinates},
	Volume = {65},
	Year = {2000},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/65/253/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1444716}}

@book{yilmaz01sda,
	Address = {Tulsa, OK},
	Author = {\"{O}. Yilmaz},
	Date-Added = {2008-05-20 14:02:49 -0700},
	Date-Modified = {2008-05-22 11:39:57 -0700},
	Publisher = {SEG},
	Title = {Seismic data analysis},
	Year = {2001}}

@article{paige82lsq,
	Address = {New York, NY, USA},
	Author = {C. C. Paige and M. A. Saunders},
	Date-Added = {2008-05-20 14:00:44 -0700},
	Date-Modified = {2008-05-20 19:47:37 -0700},
	Doi = {http://doi.acm.org/10.1145/355984.355989},
	Issn = {0098-3500},
	Issue = {1},
	Journal = {Transactions on Mathematical Software},
	Keywords = {LSQR},
	Number = {1},
	Pages = {43-71},
	Publisher = {ACM},
	Title = {{LSQR}: an algorithm for sparse linear equations and sparse least squares},
	Volume = {8},
	Year = {1982},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/355984.355989}}

@article{daubechies04ait,
	Abstract = {We consider linear inverse problems where the
                  solution is assumed to have a sparse expansion on an
                  arbitrary preassigned orthonormal basis. We prove
                  that replacing the usual quadratic regularizing
                  penalties by weighted p-penalties on the
                  coefficients of such expansions, with 1 p 2, still
                  regularizes the problem. Use of such p-penalized
                  problems with p < 2 is often advocated when one
                  expects the underlying ideal noiseless solution to
                  have a sparse expansion with respect to the basis
                  under consideration. To compute the corresponding
                  regularized solutions, we analyze an iterative
                  algorithm that amounts to a Landweber iteration with
                  thresholding (or nonlinear shrinkage) applied at
                  each iteration step. We prove that this algorithm
                  converges in norm. {\copyright} 2004 Wiley
                  Periodicals, Inc.},
	Author = {I. Daubechies and M. Defrise and C. {De Mol}},
	Date-Added = {2008-05-20 13:58:17 -0700},
	Date-Modified = {2008-08-14 15:01:17 -0700},
	Issue = {11},
	Journal = {Communications on Pure and Applied Mathematics},
	Number = {11},
	Pages = {1413-1457},
	Pdf = {http://dx.doi.org/10.1002/cpa.20042},
	Refer1 = {10.1002/cpa.20042},
	Title = {An iterative thresholding algorithm for linear inverse problems with a sparsity constraint},
	Volume = {57},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/cpa.20042}}

@mastersthesis{maysami08msc,
	Abstract = {In this work, we present a new method for seismic
                  waveform characterization, which is aimed at
                  extracting detailed litho-stratigraphical
                  information from seismic data. We attempt to
                  estimate the lithological attributes from seismic
                  data according to our parametric representation of
                  stratigraphical horizons, where the parameter values
                  provide us with a direct link to nature of
                  lithological transitions. We test our method on a
                  seismic dataset with a strong diagenetic transition
                  (opal-A to opal-CT transition). Given some
                  information from cutting samples of well, we use a
                  percolation-based model to construct the elastic
                  profile of lithological transitions. Our goal is to
                  match parametric representation for the diagenetic
                  transition in both real data and synthetic data
                  given by these elastic profiles. This match may be
                  interpreted as a well-seismic tie, which reveals
                  lithological information about stratigraphical
                  horizons.},
	Address = {Vancouver, BC Canada},
	Author = {M. Maysami},
	Date-Added = {2008-05-20 13:50:46 -0700},
	Date-Modified = {2008-08-14 14:32:47 -0700},
	Keywords = {SLIM},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/maysami08msc.pdf},
	School = {The University of British Columbia},
	Title = {Lithology constraints from seismic waveforms: application to opal-A to opal-CT transition},
	Year = {2008},
	Bdsk-Url-1 = {http://hdl.handle.net/2429/540},
	Bdsk-Url-2 = {https://dspace.library.ubc.ca/dspace/bitstream/2429/540/1/ubc_2008_spring_maysami_mohammad.pdf},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/maysami08msc.pdf}}

@mastersthesis{dupuis05msc,
	Abstract = {We consider seismic signals as a superposition of
                  waveforms parameterized by their fractional-
                  orders. Each waveform models the reflection of a
                  seismic wave at a particular transition between two
                  lithological layers in the subsurface. The location
                  of the waveforms in the seismic signal corresponds
                  to the depth of the transitions in the subsurface,
                  whereas their fractional-order constitutes a measure
                  of the sharpness of the transitions. By considering
                  fractional-order tran- sitions, we generalize the
                  zero-order transition model of the conventional
                  deconvolution problem, and aim at capturing the
                  different types of transitions. The goal is to
                  delineate and characterize transitions from seismic
                  signals by recovering the locations and
                  fractional-orders of its corre- sponding
                  waveforms. This problem has received increasing
                  interest, and several methods have been proposed,
                  including multi- and monoscale analysis based on
                  Mallat's wavelet transform modulus maxima, and
                  seismic atomic decomposition.  We propose a new
                  method based on a two-step approach, which divides
                  the initial problem of delineating and
                  characterizing transitions over the whole seismic
                  signal, into two easier sub- problems. The algorithm
                  first partitions the seismic signal into its ma jor
                  components, and then estimates the fractional-orders
                  and locations of each component. Both steps are
                  based on the sparse decomposition of seismic signals
                  in overcomplete dictionaries of waveforms parameter-
                  ized by their fractional-orders, and involve  1
                  minimizations solved by an iterative thresholding
                  algorithm. We present the method and show numerical
                  results on both synthetic and real data.},
	Address = {Vancouver, BC Canada},
	Author = {C. Dupuis},
	Date-Added = {2008-05-20 13:47:19 -0700},
	Date-Modified = {2008-08-14 14:32:22 -0700},
	Keywords = {SLIM},
	Pdf = {http://www.iam.ubc.ca/theses/Dupuis/CDupuis_MSc_thesis.pdf},
	School = {The University of British Columbia},
	Title = {Seismic singularity characterization with redundant dictionaries},
	Year = {2005},
	Bdsk-Url-1 = {http://www.iam.ubc.ca/theses/Dupuis/CDupuis_MSc_thesis.pdf},
	Bdsk-Url-2 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2005/dupuis05msc.pdf}}

@phdthesis{hennenfent08phd,
	Abstract = {Wavefield reconstruction is a crucial step in the
                  seismic processing flow.  For instance, unsuccessful
                  interpolation leads to erroneous multiple
                  predictions that adversely affect the performance of
                  multiple elimination, and to imaging artifacts. We
                  present a new non-parametric transform-based
                  reconstruction method that exploits the compression
                  of seismic data by the recently developed curvelet
                  transform. The elements of this transform, called
                  curvelets, are multi-dimensional, multi-scale, and
                  multi-directional. They locally resemble wavefronts
                  present in the data, which leads to a compressible
                  representation for seismic data. This compression
                  enables us to formulate a new curvelet-based seismic
                  data recovery algorithm through sparsity-promoting
                  inversion (CRSI). The concept of sparsity-promoting
                  inversion is in itself not new to
                  geophysics. However, the recent insights from the
                  field of ``compressed sensing'' are new since they
                  clearly identify the three main ingredients that go
                  into a successful formulation of a reconstruction
                  problem, namely a sparsifying transform, a
                  sub-Nyquist sampling strategy that subdues coherent
                  aliases in the sparsifying domain, and a
                  data-consistent sparsity-promoting program.  After a
                  brief overview of the curvelet transform and our
                  seismic-oriented extension to the fast discrete
                  curvelet transform, we detail the CRSI formulation
                  and illustrate its performance on synthetic and real
                  datasets. Then, we introduce a sub-Nyquist sampling
                  scheme, termed jittered undersampling, and show
                  that, for the same amount of data acquired, jittered
                  data are best interpolated using CRSI compared to
                  regular or random undersampled data. We also discuss
                  the large-scale one-norm solver involved in CRSI.
                  Finally, we extend CRSI formulation to other
                  geophysical applications and present results on
                  multiple removal and migration-amplitude recovery.},
	Address = {Vancouver, BC Canada},
	Author = {G. Hennenfent},
	Date-Added = {2008-05-20 13:46:07 -0700},
	Date-Modified = {2008-08-14 14:35:08 -0700},
	Keywords = {reconstruction, curvelet transform, SLIM},
	Month = {May},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/hennenfent08phd.pdf},
	School = {The University of British Columbia},
	Talkurl = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/hennenfent08phd_pres.pdf},
	Title = {Sampling and reconstruction of seismic wavefields in the curvelet domain},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/hennenfent08phd.pdf}}

@mastersthesis{yarham08msc,
	Abstract = {The removal of coherent noise generated by surface
                  waves in land based seismic is a prerequisite to
                  imaging the subsurface. These surface waves, termed
                  as ground roll, overlay important reflector
                  information in both the t-x and f-k
                  domains. Standard techniques of ground roll removal
                  commonly alter reflector information as a
                  consequence of the ground roll removal. We propose
                  the combined use of the curvelet domain as a
                  sparsifying basis in which to perform signal
                  separation techniques that can preserve reflector
                  informa- tion while increasing ground roll
                  removal. We examine two signal separation
                  techniques, a block-coordinate relaxation method and
                  a Bayesian separation method. The derivations and
                  background for both methods are presented and the
                  parameter sensitivity is examined. Both methods are
                  shown to be effective in certain situations
                  regarding synthetic data and erroneous surface wave
                  predictions. The block-coordinate relaxation method
                  is shown to have ma jor weaknesses when dealing with
                  seismic signal separation in the pres- ence of noise
                  and with the production of artifacts and reflector
                  degradation.  The Bayesian separation method is
                  shown to improve overall separation for both seismic
                  and real data. The Bayesian separation scheme is
                  used on a real data set with a surface wave
                  prediction containing reflector information. It is
                  shown to improve the signal separation by recovering
                  reflector information while improving the surface
                  wave removal. The abstract contains a separate real
                  data example where both the block-coordinate
                  relaxation method and the Bayesian separation method
                  are compared.  },
	Address = {Vancouver, BC Canada},
	Author = {C. Yarham},
	Date-Added = {2008-05-20 13:43:05 -0700},
	Date-Modified = {2008-08-14 14:33:19 -0700},
	Keywords = {signal separation, curvelet transform, SLIM},
	Month = {May},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/cyarham08msc.pdf},
	School = {The University of British Columbia},
	Title = {Seismic ground-roll separation using sparsity promoting l1 minimization},
	Year = {2008},
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Theses/2008/cyarham08msc.pdf}}

@techreport{wang08bss,
	Abstract = {Successful removal of coherent noise sources
                  greatly determines the quality of seismic
                  imaging. Major advances were made in this direction,
                  e.g., Surface-Related Multiple Elimination (SRME)
                  and interferometric ground-roll removal. Still,
                  moderate phase, timing, amplitude errors and clutter
                  in the predicted signal components can be
                  detrimental. Adopting a Bayesian approach along with
                  the assumption of approximate curvelet-domain
                  independence of the to-be-separated signal
                  components, we construct an iterative algorithm that
                  takes the predictions produced by for example SRME
                  as input and separates these components in a robust
                  fashion. In addition, the proposed algorithm
                  controls the energy mismatch between the separated
                  and predicted components. Such a control, which was
                  lacking in earlier curvelet-domain formulations,
                  produces improved results for primary-multiple
                  separation on both synthetic and real data.},
	Author = {D. Wang and R. Saab and \"{O}. Yilmaz and F. J. Herrmann},
	Date-Added = {2008-05-20 12:41:08 -0700},
	Date-Modified = {2008-08-14 14:35:59 -0700},
	Institution = {UBC Earth and Ocean Sciences Department},
	Keywords = {signal separation, SLIM},
	Month = {January},
	Number = {TR-2008-1},
	Title = {Bayesian-signal separation by sparsity promotion: application to primary-multiple separation},
	Url = {http://slim.eos.ubc.ca/Publications/Private/Journals/wang08bayes/paper_html/paper.html},
	Year = 2008,
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Private/Journals/wang08bayes/paper_html/paper.html}}

@techreport{vandenberg07sat,
	Abstract = {Sparco is a framework for testing and benchmarking
                  algorithms for sparse reconstruction. It includes a
                  large collection of sparse reconstruction problems
                  drawn from the imaging, compressed sensing, and
                  geophysics literature. Sparco is also a framework
                  for implementing new test problems and can be used
                  as a tool for reproducible research.  Sparco is
                  implemented entirely in Matlab, and is released as
                  open-source software under the GNU Public License.},
	Author = {E. van den Berg and M. P. Friedlander and G. Hennenfent and F. J. Herrmann and R. Saab and \"{O}. Yilmaz},
	Date-Added = {2008-05-20 12:34:37 -0700},
	Date-Modified = {2008-08-14 14:35:38 -0700},
	Institution = {UBC Computer Science Department},
	Keywords = {Sparco, SLIM},
	Number = {TR-2007-20},
	Pdf = {http://www.cs.ubc.ca/labs/scl/sparco/uploads/Main/sparco.pdf},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf},
	Title = {Sparco: a testing framework for sparse reconstruction},
	Year = {2007},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/labs/scl/sparco/uploads/Main/sparco.pdf}}

@article{herrmann08acd,
	Abstract = {In many exploration areas, successful separation
                  of primaries and multiples greatly determines the
                  quality of seismic imaging. Despite major advances
                  made by surface-related multiple elimination (SRME),
                  amplitude errors in the predicted multiples remain a
                  problem. When these errors vary for each type of
                  multiple in different ways (as a function of offset,
                  time, and dip), they pose a serious challenge for
                  conventional least-squares matching and for the
                  recently introduced separation by curvelet-domain
                  thresholding. We propose a data-adaptive method that
                  corrects amplitude errors, which vary smoothly as a
                  function of location, scale (frequency band), and
                  angle. With this method, the amplitudes can be
                  corrected by an elementwise curvelet-domain scaling
                  of the predicted multiples. We show that this
                  scaling leads to successful estimation of primaries,
                  despite amplitude, sign, timing, and phase errors in
                  the predicted multiples. Our results on synthetic
                  and real data show distinct improvements over
                  conventional least-squares matching in terms of
                  better suppression of multiple energy and
                  high-frequency clutter and better recovery of
                  estimated primaries. {\copyright}2008 Society of
                  Exploration Geophysicists},
	Author = {F. J. Herrmann and D. Wang and D. J. Verschuur},
	Date-Added = {2008-05-20 12:32:18 -0700},
	Date-Modified = {2008-08-14 13:57:08 -0700},
	Doi = {10.1190/1.2904986},
	Issue = {3},
	Journal = {Geophysics},
	Keywords = {SLIM},
	Number = {3},
	Pages = {A17-A21},
	Pdf = {http://link.aip.org/link/?GPY/73/A17/1},
	Publisher = {SEG},
	Title = {Adaptive curvelet-domain primary-multiple separation},
	Url = {http://slim.eos.ubc.ca/Publications/Private/Journals/herrmann08match/paper_html/paper.html},
	Volume = {73},
	Year = {2008},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/73/A17/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2904986}}

@article{herrmann08cbs,
	Abstract = {Mitigating missing data, multiples, and erroneous
                  migration amplitudes are key factors that determine
                  image quality. Curvelets, little ``plane waves,''
                  complete with oscillations in one direction and
                  smoothness in the other directions, sparsify a
                  property we leverage explicitly with sparsity
                  promotion. With this principle, we recover seismic
                  data with high fidelity from a small subset (20\%)
                  of randomly selected traces. Similarly, sparsity
                  leads to a natural decorrelation and hence to a
                  robust curvelet-domain primary-multiple separation
                  for North Sea data. Finally, sparsity helps to
                  recover migration amplitudes from noisy data. With
                  these examples, we show that exploiting the
                  curvelet's ability to sparsify wavefrontlike
                  features is powerful, and our results are a clear
                  indication of the broad applicability of this
                  transform to exploration
                  seismology. {\copyright}2008 Society of Exploration
                  Geophysicists},
	Author = {F. J. Herrmann and D. Wang and G. Hennenfent and P. P. Moghaddam},
	Date-Added = {2008-05-20 12:29:54 -0700},
	Date-Modified = {2008-08-14 13:57:40 -0700},
	Doi = {10.1190/1.2799517},
	Issue = {1},
	Journal = {Geophysics},
	Keywords = {curvelet transform, SLIM},
	Number = {1},
	Pages = {A1-A5},
	Pdf = {http://link.aip.org/link/?GPY/73/A1/1},
	Publisher = {SEG},
	Title = {Curvelet-based seismic data processing: a multiscale and nonlinear approach},
	Volume = {73},
	Year = {2008},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/73/A1/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2799517}}

@article{lin07cwe,
	Abstract = {An explicit algorithm for the extrapolation of
                  one-way wavefields is proposed that combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation. Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3D. By
                  using ideas from compressed sensing, we are able to
                  formulate the (inverse) wavefield extrapolation
                  problem on small subsets of the data volume, thereby
                  reducing the size of the operators. Compressed
                  sensing entails a new paradigm for signal recovery
                  that provides conditions under which signals can be
                  recovered from incomplete samplings by nonlinear
                  recovery methods that promote sparsity of the
                  to-be-recovered signal. According to this theory,
                  signals can be successfully recovered when the
                  measurement basis is incoherent with the
                  representa-tion in which the wavefield is sparse. In
                  this new approach, the eigenfunctions of the
                  Helmholtz operator are recognized as a basis that is
                  incoherent with curvelets that are known to compress
                  seismic wavefields. By casting the wavefield
                  extrapolation problem in this framework, wavefields
                  can be successfully extrapolated in the modal
                  domain, despite evanescent wave modes. The degree to
                  which the wavefield can be recovered depends on the
                  number of missing (evanescent) wavemodes and on the
                  complexity of the wavefield. A proof of principle
                  for the compressed sensing method is given for
                  inverse wavefield extrapolation in 2D, together with
                  a pathway to 3D during which the multiscale and
                  multiangular properties of curvelets, in relation to
                  the Helmholz operator, are exploited. The results
                  show that our method is stable, has reduced dip
                  limitations, and handles evanescent waves in inverse
                  extrapolation. {\copyright}2007 Society of
                  Exploration Geophysicists},
	Author = {T. T. Y. Lin and F. J. Herrmann},
	Date-Added = {2008-05-20 12:27:22 -0700},
	Date-Modified = {2008-08-14 13:59:14 -0700},
	Doi = {10.1190/1.2750716},
	Issue = {5},
	Journal = {Geophysics},
	Keywords = {wave propagation, SLIM},
	Number = {5},
	Pages = {SM77-SM93},
	Pdf = {http://link.aip.org/link/?GPY/72/SM77/1},
	Publisher = {SEG},
	Title = {Compressed wavefield extrapolation},
	Volume = {72},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/72/SM77/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2750716}}

@article{herrmann07nlp,
	Abstract = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, during which
                  multiples are predicted from seismic data, and a
                  primary-multiple separation step, during which the
                  predicted multiples are 'matched' with the true
                  multiples in the data and subsequently removed. This
                  second separation step, which we will call the
                  estimation step, is crucial in practice: an
                  incorrect separation will cause residual multiple
                  energy in the result or may lead to a distortion of
                  the primaries, or both. To reduce these adverse
                  effects, a new transformed-domain method is proposed
                  where primaries and multiples are separated rather
                  than matched. This separation is carried out on the
                  basis of differences in the multiscale and
                  multidirectional characteristics of these two signal
                  components. Our method uses the curvelet transform,
                  which maps multidimensional data volumes into almost
                  orthogonal localized multidimensional prototype
                  waveforms that vary in directional and
                  spatio-temporal content. Primaries-only and
                  multiples-only signal components are recovered from
                  the total data volume by a non-linear optimization
                  scheme that is stable under noisy input data. During
                  the optimization, the two signal components are
                  separated by enhancing sparseness (through weighted
                  l1-norms) in the transformed domain subject to
                  fitting the observed data as the sum of the
                  separated components to within a user-defined
                  tolerance level. Whenever, during the optimization,
                  the estimates for the primaries in the transformed
                  domain correlate with the predictions for the
                  multiples, the recovery of the coefficients for the
                  estimated primaries will be suppressed while for
                  regions where the correlation is small the method
                  seeks the sparsest set of coefficients that
                  represent the estimation for the primaries. Our
                  algorithm does not seek a matched filter and as such
                  it differs fundamentally from traditional adaptive
                  subtraction methods. The method derives its
                  stability from the sparseness obtained by a
                  non-parametric (i.e. not depending on a parametrized
                  physical model) multiscale and multidirectional
                  overcomplete signal representation. This sparsity
                  serves as prior information and allows for a
                  Bayesian interpretation of our method during which
                  the log-likelihood function is minimized while the
                  two signal components are assumed to be given by a
                  superposition of prototype waveforms, drawn
                  independently from a probability function that is
                  weighted by the predicted primaries and
                  multiples. In this paper, the predictions are based
                  on the data-driven surface-related multiple
                  elimination method. Synthetic and field data
                  examples show a clean separation leading to a
                  considerable improvement in multiple suppression
                  compared to the conventional method of adaptive
                  matched filtering. This improved separation
                  translates into an improved stack.},
	Author = {F. J. Herrmann and U. Boeniger and D. J. Verschuur},
	Date-Added = {2008-05-20 12:22:10 -0700},
	Date-Modified = {2008-08-14 13:56:35 -0700},
	Doi = {10.1111/j.1365-246X.2007.03360.x},
	Eprint = {http://www.blackwell-synergy.com/doi/pdf/10.1111/j.1365-246X.2007.03360.x},
	Issue = {2},
	Journal = {Geophysical Journal International},
	Keywords = {signal separation, SLIM},
	Number = {2},
	Pages = {781-799},
	Pdf = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1365-246X.2007.03360.x},
	Title = {Non-linear primary-multiple separation with directional curvelet frames},
	Volume = {170},
	Year = {2007},
	Bdsk-Url-1 = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1365-246X.2007.03360.x},
	Bdsk-Url-2 = {http://dx.doi.org/10.1111/j.1365-246X.2007.03360.x}}

@article{herrmann05sdb,
	Abstract = {In this paper an alternative approach to the blind
                  seismic deconvolution problem is presented that aims
                  for two goals namely recovering the location and
                  relative strength of seismic reflectors, possibly
                  with super-localization, as well as obtaining
                  detailed parametric characterizations for the
                  reflectors. We hope to accomplish these goals by
                  decomposing seismic data into a redundant dictionary
                  of parameterized waveforms designed to closely match
                  the properties of reflection events associated with
                  sedimentary records. In particular, our method
                  allows for highly intermittent non-Gaussian records
                  yielding a reflectivity that can no longer be
                  described by a stationary random process or by a
                  spike train. Instead, we propose a reflector
                  parameterization that not only recovers the
                  reflector's location and relative strength but which
                  also captures reflector attributes such as its local
                  scaling, sharpness and instantaneous
                  phase-delay. The first set of parameters delineates
                  the stratigraphy whereas the second provides
                  information on the lithology. As a consequence of
                  the redundant parameterization, finding the matching
                  waveforms from the dictionary involves the solution
                  of an ill-posed problem. Two complementary
                  sparseness-imposing methods Matching and Basis
                  Pursuit are compared for our dictionary and applied
                  to seismic data.  },
	Address = {Amsterdam, The Netherlands, The Netherlands},
	Author = {F. J. Herrmann},
	Date-Added = {2008-05-20 12:17:03 -0700},
	Date-Modified = {2008-08-14 13:56:08 -0700},
	Issn = {1069-2509},
	Issue = {1},
	Journal = {Integrated Computer-Aided Engineering},
	Keywords = {deconvolution, SLIM},
	Month = {January},
	Number = {1},
	Pages = {69-90},
	Pdf = {http://slim.eos.ubc.ca/~felix/public/RobinsonSub.pdf},
	Publisher = {IOS Press},
	Title = {Seismic deconvolution by atomic decomposition: a parametric approach with sparseness constraints},
	Volume = {12},
	Year = {2005}}

@article{bernabe04pas,
	Abstract = {As a first step toward determining the mixing laws
                  for the transport properties of rocks, we prepared
                  binary mixtures of high- and low-permeability
                  materials by isostatically hot-pressing mixtures of
                  fine powders of calcite and quartz. The resulting
                  rocks were marbles containing varying concentrations
                  of dispersed quartz grains. Pores were present
                  throughout the rock, but the largest ones were
                  preferentially associated with the quartz particles,
                  leading us to characterize the material as being
                  composed of two phases, one with high permeability
                  and the second with low permeability. We measured
                  the permeability and storativity of these materials
                  using the oscillating flow technique, while
                  systematically varying the effective pressure and
                  the period and amplitude of the input fluid
                  oscillation. Control measurements performed using
                  the steady state flow and pulse decay techniques
                  agreed well with the oscillating flow tests. The
                  hydraulic properties of the marbles were highly
                  sensitive to the volume fraction of the
                  high-permeability phase (directly related to the
                  quartz content). Below a critical quartz content,
                  slightly less than 20 wt %, the high-permeability
                  volume elements were disconnected, and the overall
                  permeability was low. Above the critical quartz
                  content the high-permeability volume elements formed
                  throughgoing paths, and permeability increased
                  sharply. We numerically simulated fluid flow through
                  binary materials and found that permeability
                  approximately obeys a percolation-based mixing law,
                  consistent with the measured permeability of the
                  calcite-quartz aggregates.},
	Author = {Y. Bernab{\'e} and U. Mok and B. Evans and F. J. Herrmann},
	Date-Added = {2008-05-20 12:11:58 -0700},
	Date-Modified = {2008-08-14 13:51:23 -0700},
	Doi = {10.1029/2004JB00311},
	Journal = {Journal of Geophysical Research},
	Keywords = {permeability, porosity, SLIM},
	Pages = {B12207},
	Pdf = {http://www.agu.org/journals/jb/jb0412/2004JB003111/},
	Title = {Permeability and storativity of binary mixtures of high-and low-porosity materials},
	Volume = {109},
	Year = {2004},
	Bdsk-Url-1 = {http://www.agu.org/journals/jb/jb0412/2004JB003111/}}

@article{herrmann04ssa,
	Abstract = {Mineralogical phase transitions are usually
                  invoked to account for the sharpness of globally
                  observed upper-mantle seismic discontinuities. We
                  propose a percolation-based model for the elastic
                  properties of the phase mixture in the coexistence
                  regions associated with these transitions. The major
                  consequence of the model is that the elastic moduli
                  (but not the density) display a singularity at the
                  percolation threshold of the high-pressure
                  phase. This model not only explains the sharp but
                  continuous change in seismic velocities across the
                  phase transition, but also predicts its abruptness
                  and scale invariance, which are characterized by a
                  non-integral scale exponent. Using the
                  receiver-function approach and new, powerful
                  signal-processing techniques, we quantitatively
                  determine the singularity exponent from recordings
                  of converted seismic waves at two Australian
                  stations (CAN and WRAB). Using the estimated values,
                  we construct velocity--depth profiles across the
                  singularities and verify that the calculated
                  converted waveforms match the observations under
                  CAN. Finally, we point out a series of additional
                  predictions that may provide new insights into the
                  physics and fine structure of the upper-mantle
                  transition zone.},
	Author = {F. J. Herrmann and Y. Bernab{\'e}},
	Date-Added = {2008-05-20 12:07:35 -0700},
	Date-Modified = {2008-08-14 13:55:05 -0700},
	Doi = {10.1111/j.1365-246X.2004.02464.x},
	Eprint = {http://www.blackwell-synergy.com/doi/pdf/10.1111/j.1365-246X.2004.02464.x},
	Issue = {3},
	Journal = {Geophysical Journal International},
	Keywords = {percolation, SLIM},
	Number = {3},
	Pages = {949-960},
	Pdf = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1365-246X.2004.02464.x},
	Title = {Seismic singularities at upper-mantle phase transitions: a site percolation model},
	Volume = {159},
	Year = {2004},
	Bdsk-Url-1 = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1365-246X.2004.02464.x},
	Bdsk-Url-2 = {http://dx.doi.org/10.1111/j.1365-246X.2004.02464.x}}

@article{xu05aft,
	Abstract = {Seismic data regularization, which spatially
                  transforms irregularly sampled acquired data to
                  regularly sampled data, is a long-standing problem
                  in seismic data processing. Data regularization can
                  be implemented using Fourier theory by using a
                  method that estimates the spatial frequency content
                  on an irregularly sampled grid. The data can then be
                  reconstructed on any desired grid. Difficulties
                  arise from the nonorthogonality of the global
                  Fourier basis functions on an irregular grid, which
                  results in the problem of "spectral leakage": energy
                  from one Fourier coefficient leaks onto others. We
                  investigate the nonorthogonality of the Fourier
                  basis on an irregularly sampled grid and propose a
                  technique called "antileakage Fourier transform" to
                  overcome the spectral leakage. In the antileakage
                  Fourier transform, we first solve for the most
                  energetic Fourier coefficient, assuming that it
                  causes the most severe leakage. To attenuate all
                  aliases and the leakage of this component onto other
                  Fourier coefficients, the data component
                  corresponding to this most energetic Fourier
                  coefficient is subtracted from the original input on
                  the irregular grid. We then use this new input to
                  solve for the next Fourier coefficient, repeating
                  the procedure until all Fourier coefficients are
                  estimated. This procedure is equivalent to
                  "reorthogonalizing" the global Fourier basis on an
                  irregularly sampled grid. We demonstrate the
                  robustness and effectiveness of this technique with
                  successful applications to both synthetic and real
                  data examples. {\copyright}2005 Society of
                  Exploration Geophysicists},
	Author = {S. Xu and Y. Zhang and D. Pham and G. Lambar\'{e}},
	Date-Added = {2008-05-09 17:43:47 -0700},
	Date-Modified = {2008-08-14 15:21:45 -0700},
	Doi = {10.1190/1.1993713},
	Issue = {4},
	Journal = {Geophysics},
	Keywords = {Fourier transform, reconstruction},
	Number = {4},
	Pages = {V87-V95},
	Pdf = {http://link.aip.org/link/?GPY/70/V87/1},
	Publisher = {SEG},
	Title = {Antileakage {F}ourier transform for seismic data regularization},
	Volume = {70},
	Year = {2005},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/70/V87/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1993713}}

@article{herrmann08nps,
	Abstract = {Seismic data recovery from data with missing
                  traces on otherwise regular acquisition grids forms
                  a crucial step in the seismic processing flow. For
                  instance, unsuccessful recovery leads to imaging
                  artifacts and to erroneous predictions for the
                  multiples, adversely affecting the performance of
                  multiple elimination. A non-parametric
                  transform-based recovery method is presented that
                  exploits the compression of seismic data volumes by
                  recently developed curvelet frames. The elements of
                  this transform are multidimensional and directional
                  and locally resem- ble wavefronts present in the
                  data, which leads to a compressible representation
                  for seismic data. This compression enables us to
                  formulate a new curvelet-based seismic data recovery
                  algorithm through sparsity-promoting inversion. The
                  concept of sparsity-promoting inversion is in itself
                  not new to geophysics. However, the recent insights
                  from the field of `compressed sensing' are new since
                  they clearly identify the three main ingredients
                  that go into a successful formulation of a re-
                  covery problem, namely a sparsifying transform, a
                  sampling strategy that subdues coherent aliases and
                  a sparsity-promoting program that recovers the
                  largest entries of the curvelet-domain vector while
                  explaining the measurements. These concepts are
                  illustrated with a stylized experiment that stresses
                  the importance of the degree of compression by the
                  sparsifying transform. With these findings, a
                  curvelet-based recovery algorithms is developed,
                  which recovers seismic wavefields from seismic data
                  volumes with large percentages of traces
                  missing. During this construction, we benefit from
                  the main three ingredients of compressive sampling,
                  namely the curvelet compression of seismic data, the
                  existence of a favorable sam- pling scheme and the
                  formulation of a large-scale sparsity-promoting
                  solver based on a cooling method. The recovery
                  performs well on synthetic as well as real data and
                  performs better by virtue of the sparsifying
                  property of curvelets. Our results are applicable to
                  other areas such as global seismology.},
	Author = {F. J. Herrmann and G. Hennenfent},
	Date-Added = {2008-05-09 14:42:33 -0700},
	Date-Modified = {2008-08-14 15:04:35 -0700},
	Doi = {10.1111/j.1365-246X.2007.03698.x},
	Journal = {Geophysical Journal International},
	Keywords = {curvelet transform, reconstruction,SLIM},
	Month = {April},
	Pages = {233-248},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Journals/CRSI.pdf},
	Title = {Non-parametric seismic data recovery with curvelet frames},
	Volume = 173,
	Year = 2008,
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Journals/CRSI.pdf}}

@book{biondo063ds,
	Author = {B. L. Biondi},
	Date-Added = {2008-05-08 15:25:18 -0700},
	Date-Modified = {2008-05-20 19:45:00 -0700},
	Issue = {14},
	Keywords = {imaging},
	Number = {14},
	Publisher = {SEG},
	Series = {Investigations in Geophysics},
	Title = {3-{D} seismic imaging},
	Year = {2006}}

@article{claerbout71tau,
	Abstract = {Schemes for seismic mapping of reflectors in the
                  presence of an arbitrary velocity model, dipping and
                  curved reflectors, diffractions, ghosts, surface
                  elevation variations, and multiple reflections are
                  reviewed and reduced to a single formula involving
                  up and downgoing waves. The mapping formula may be
                  implemented without undue complexity by means of
                  difference approximations to the relativistic
                  Schroedinger equation. {\copyright}1971 Society of
                  Exploration Geophysicists},
	Author = {J. F. Claerbout},
	Date-Added = {2008-05-08 14:59:36 -0700},
	Date-Modified = {2008-08-14 14:59:35 -0700},
	Doi = {10.1190/1.1440185},
	Issue = {3},
	Journal = {Geophysics},
	Keywords = {WEM, imaging},
	Number = {3},
	Pages = {467-481},
	Pdf = {http://link.aip.org/link/?GPY/36/467/1},
	Publisher = {SEG},
	Title = {Toward a unified theory of reflector mapping},
	Volume = {36},
	Year = {1971},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/36/467/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1440185}}

@article{hennenfent08nii,
	Abstract = {Geophysical inverse problems typically involve a
                  trade off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain relatively unexplored. We
                  show how these curves lead to new insights in
                  one-norm regularization. First, we confirm the
                  theoretical properties of smoothness and convexity
                  of these curves from a stylized and a geophysical
                  example. Second, we exploit these crucial properties
                  to approximate the Pareto curve for a large-scale
                  problem. Third, we show how Pareto curves provide an
                  objective criterion to gauge how different one-norm
                  solvers advance towards the solution.},
	Author = {G. Hennenfent and E. van den Berg and M. P. Friedlander and F. J. Herrmann},
	Date-Added = {2008-05-08 14:51:29 -0700},
	Date-Modified = {2008-08-14 13:53:37 -0700},
	Issue = 4,
	Journal = {Geophysics},
	Keywords = {Pareto, SLIM},
	Month = {July-August},
	Number = {4},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07pareto.pdf},
	Talkurl = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_New.pdf},
	Title = {New insights into one-norm solvers from the {P}areto curve},
	Url = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07pareto/paper_html},
	Volume = 73,
	Year = 2008,
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07pareto/paper_html},
	Bdsk-Url-2 = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07pareto.pdf}}

@techreport{vandenberg08ptp,
	Abstract = {The basis pursuit problem seeks a minimum one-norm
                  solution of an underdetermined least-squares
                  problem. Basis pursuit denoise (BPDN) fits the
                  least-squares problem only approximately, and a
                  single parameter determines a curve that traces the
                  optimal trade-off between the least-squares fit and
                  the one-norm of the solution. We prove that this
                  curve is convex and continuously differentiable over
                  all points of interest, and show that it gives an
                  explicit relationship to two other optimization
                  problems closely related to BPDN. We describe a
                  root-finding algorithm for finding arbitrary points
                  on this curve; the algorithm is suitable for
                  problems that are large scale and for those that are
                  in the complex domain. At each iteration, a spectral
                  gradient-projection method approximately minimizes a
                  least-squares problem with an explicit one-norm
                  constraint. Only matrix-vector operations are
                  required. The primal-dual solution of this problem
                  gives function and derivative information needed for
                  the root-finding method. Numerical experiments on a
                  comprehensive set of test problems demonstrate that
                  the method scales well to large problems.},
	Author = {E. van den Berg and M. P. Friedlander},
	Date-Added = {2008-05-08 14:45:34 -0700},
	Date-Modified = {2008-08-14 15:35:59 -0700},
	Institution = {UBC Computer Science Department},
	Keywords = {Pareto, one-norm solver,SLIM},
	Month = {January},
	Number = {TR-2008-01},
	Pdf = {http://www.optimization-online.org/DB_FILE/2008/07/2056.pdf},
	Title = {Probing the {P}areto frontier for basis pursuit solutions},
	Year = 2008,
	Bdsk-Url-1 = {http://www.optimization-online.org/DB_HTML/2008/01/1889.html}}

@article{symes07rtm,
	Abstract = {Reverse time migration (RTM) requires that fields
                  computed in forward time be accessed in reverse
                  order. Such out-of-order access, to recursively
                  computed fields, requires that some part of the
                  recursion history be stored (checkpointed), with the
                  remainder computed by repeating parts of the forward
                  computation. Optimal checkpointing algorithms choose
                  checkpoints in such a way that the total storage is
                  minimized for a prescribed level of excess
                  computation, or vice versa. Optimal checkpointing
                  dramatically reduces the storage required by RTM,
                  compared to that needed for nonoptimal
                  implementations, at the price of a small increase in
                  computation. This paper describes optimal
                  checkpointing in a form which applies both to RTM
                  and other applications of the adjoint state method,
                  such as construction of velocity updates from
                  prestack wave equation migration. {\copyright}2007
                  Society of Exploration Geophysicists},
	Author = {W. W. Symes},
	Date-Added = {2008-05-08 14:42:11 -0700},
	Date-Modified = {2008-08-14 15:19:43 -0700},
	Doi = {10.1190/1.2742686},
	Issue = {5},
	Journal = {Geophysics},
	Keywords = {RTM, imaging},
	Number = {5},
	Pages = {SM213-SM221},
	Pdf = {http://link.aip.org/link/?GPY/72/SM213/1},
	Publisher = {SEG},
	Title = {Reverse time migration with optimal checkpointing},
	Volume = {72},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/72/SM213/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2742686}}

@article{trad03lvo,
	Abstract = {The Radon transform (RT) suffers from the typical
                  problems of loss of resolution and aliasing that
                  arise as a consequence of incomplete information,
                  including limited aperture and
                  discretization. Sparseness in the Radon domain is a
                  valid and useful criterion for supplying this
                  missing information, equivalent somehow to assuming
                  smooth amplitude variation in the transition between
                  known and unknown (missing) data. Applying this
                  constraint while honoring the data can become a
                  serious challenge for routine seismic processing
                  because of the very limited processing time
                  available, in general, per common midpoint. To
                  develop methods that are robust, easy to use and
                  flexible to adapt to different problems we have to
                  pay attention to a variety of algorithms, operator
                  design, and estimation of the hyperparameters that
                  are responsible for the regularization of the
                  solution.In this paper, we discuss fast
                  implementations for several varieties of RT in the
                  time and frequency domains. An iterative conjugate
                  gradient algorithm with fast Fourier transform
                  multiplication is used in all cases. To preserve the
                  important property of iterative subspace methods of
                  regularizing the solution by the number of
                  iterations, the model weights are incorporated into
                  the operators. This turns out to be of particular
                  importance, and it can be understood in terms of the
                  singular vectors of the weighted transform. The
                  iterative algorithm is stopped according to a
                  general cross validation criterion for subspaces. We
                  apply this idea to several known implementations and
                  compare results in order to better understand
                  differences between, and merits of, these
                  algorithms. {\copyright}2003 Society of Exploration
                  Geophysicists},
	Author = {D. Trad and T. J. Ulrych and M. D. Sacchi},
	Date-Added = {2008-05-07 19:03:39 -0700},
	Date-Modified = {2008-08-14 15:20:56 -0700},
	Doi = {10.1190/1.1543224},
	Issue = {1},
	Journal = {Geophysics},
	Keywords = {Radon transform},
	Number = {1},
	Pages = {386-399},
	Pdf = {http://link.aip.org/link/?GPY/68/386/1},
	Publisher = {SEG},
	Title = {Latest views of the sparse {R}adon transform},
	Volume = {68},
	Year = {2003},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/68/386/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1543224}}

@phdthesis{kunis06nff,
	Author = {S. Kunis},
	Date-Added = {2008-05-07 18:51:16 -0700},
	Date-Modified = {2008-05-20 11:49:04 -0700},
	Keywords = {NFFT},
	Pdf = {http://www-user.tu-chemnitz.de/~skunis/paper/KunisDiss.pdf},
	School = {L\"ubeck university},
	Title = {Nonequispaced {FFT}: generalisation and inversion},
	Year = {2006},
	Bdsk-Url-1 = {http://www-user.tu-chemnitz.de/~skunis/paper/KunisDiss.pdf}}

@incollection{potts01mst,
	Abstract = {In this section, we consider approximate methods
                  for the fast computiation of multivariate discrete
                  Fourier transforms for nonequispaced data (NDFT) in
                  the time domain and in the frequency domain. In
                  particular, we are interested in the approximation
                  error as function of arithmetic complexity of the
                  algorithm. We discuss the robustness of
                  NDFT-algorithms with respect to roundoff errors and
                  apply NDFT-algorithms for the fast computation of
                  Bessel transforms.},
	Author = {D. Potts and G. Steidl and M. Tasche},
	Booktitle = {Modern sampling theory: mathematics and applications},
	Chapter = {12},
	Date-Added = {2008-05-07 18:44:29 -0700},
	Date-Modified = {2008-08-14 15:28:37 -0700},
	Editor = {J. J. Benedetto and P. Ferreira},
	Keywords = {NFFT},
	Pages = {249-274},
	Pdf = {http://www.tu-chemnitz.de/~potts/paper/ndft.pdf},
	Publisher = {Birkhauser},
	Title = {Fast {F}ourier transforms for nonequispaced data: a tutorial},
	Year = {2001},
	Bdsk-Url-1 = {http://www.tu-chemnitz.de/~potts/paper/ndft.pdf}}

@article{verschuur97eom,
	Abstract = {A surface-related multiple-elimination method can
                  be formulated as an iterative procedure: the output
                  of one iteration step is used as input for the next
                  iteration step (part I of this paper). In this paper
                  (part II) it is shown that the procedure can be made
                  very efficient if a good initial estimate of the
                  multiple-free data set can be provided in the first
                  iteration, and in many situations, the Radon-based
                  multiple-elimination method may provide such an
                  estimate. It is also shown that for each iteration,
                  the inverse source wavelet can be accurately
                  estimated by a linear (least-squares) inversion
                  process. Optionally, source and detector variations
                  and directivity effects can be included, although
                  the examples are given without these options. The
                  iterative multiple elimination process, together
                  with the source wavelet estimation, are illustrated
                  with numerical experiments as well as with field
                  data examples. The results show that the
                  surface-related multiple-elimination process is very
                  effective in time gates where the moveout properties
                  of primaries and multiples are very similar
                  (generally deep data), as well as for situations
                  with a complex multiple-generating
                  system. {\copyright}1997 Society of Exploration
                  Geophysicists},
	Author = {D. J. Verschuur and A. J. Berkhout},
	Date-Added = {2008-05-07 18:40:45 -0700},
	Date-Modified = {2008-08-14 15:21:18 -0700},
	Doi = {10.1190/1.1444262},
	Issue = {5},
	Journal = {Geophysics},
	Keywords = {SRME},
	Number = {5},
	Pages = {1596-1611},
	Pdf = {http://link.aip.org/link/?GPY/62/1596/1},
	Publisher = {SEG},
	Title = {Estimation of multiple scattering by iterative inversion, Part {II}: Practical aspects and examples},
	Volume = {62},
	Year = {1997},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/62/1596/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1444262}}

@article{berkhout97eom,
	Abstract = {A review has been given of the surface-related
                  multiple problem by making use of the so-called
                  feedback model. From the resulting equations it has
                  been concluded that the proposed solution does not
                  require any properties of the subsurface. However,
                  source-detector and reflectivity properties of the
                  surface need be specified. Those properties have
                  been quantified in a surface operator and this
                  operator is estimated as part of the multiple
                  removal problem. The surface-related multiple
                  removal algorithm has been formulated in terms of a
                  Neumann series and in terms of an iterative
                  equation. The Neumann formulation requires a
                  nonlinear optimization process for the surface
                  operator; while the iterative formulation needs a
                  number of linear optimizations. The iterative
                  formulation also has the advantage that it can be
                  integrated easily with another multiple removal
                  method. An algorithm for the removal of internal
                  multiples has been proposed as well. This algorithm
                  is an extension of the surface-related
                  method. Removal of internal multiples requires
                  knowledge of the macro velocity model between the
                  surface and the upper boundary of the multiple
                  generating layer. In part II (also published in this
                  issue) the success of the proposed algorithms has
                  been demonstrated on numerical experiments and field
                  data examples. {\copyright}1997 Society of
                  Exploration Geophysicists},
	Author = {A. J. Berkhout and D. J. Verschuur},
	Date-Added = {2008-05-07 18:38:50 -0700},
	Date-Modified = {2008-08-14 14:46:15 -0700},
	Doi = {10.1190/1.1444261},
	Issue = {5},
	Journal = {Geophysics},
	Keywords = {SRME},
	Number = {5},
	Pages = {1586-1595},
	Pdf = {http://link.aip.org/link/?GPY/62/1586/1},
	Publisher = {SEG},
	Title = {Estimation of multiple scattering by iterative inversion, Part {I}: Theoretical considerations},
	Volume = {62},
	Year = {1997},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/62/1586/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.1444261}}

@article{herrmann08sac,
	Abstract = {A nonlinear singularity-preserving solution to
                  seismic image recovery with sparseness and
                  continuity constraints is proposed. We observe that
                  curvelets, as a directional frame expan- sion, lead
                  to sparsity of seismic images and exhibit invariance
                  under the normal operator of the linearized imaging
                  problem. Based on this observation we derive a
                  method for stable recovery of the migration
                  amplitudes from noisy data. The method corrects the
                  amplitudes during a post-processing step after
                  migration, such that the main additional cost is one
                  ap- plication of the normal operator, i.e. a
                  modeling followed by a migration. Asymptotically
                  this normal operator corresponds to a
                  pseudodifferential operator, for which a convenient
                  diagonal approximation in the curvelet domain is
                  derived, including a bound for its error and a
                  method for the estimation of the diagonal from a
                  compound operator consisting of discrete
                  implementations for the scattering operator and its
                  adjoint the migration opera- tor. The solution is
                  formulated as a nonlinear optimization problem where
                  sparsity in the curvelet domain as well as
                  continuity along the imaged reflectors are jointly
                  promoted. To enhance sparsity, the l1 -norm on the
                  curvelet coefficients is minimized, while continuity
                  is promoted by minimizing an anisotropic diffusion
                  norm on the image. The performance of the recovery
                  scheme is evaluated with a time-reversed
                  'wave-equation' migration code on synthetic
                  datasets, including the complex SEG/EAGE AA salt
                  model. },
	Author = {F. J. Herrmann and P. P. Moghaddam and C. C. Stolk},
	Date-Added = {2008-05-07 14:54:39 -0700},
	Date-Modified = {2008-08-14 13:58:32 -0700},
	Doi = {10.1016/j.acha.2007.06.007},
	Issue = {2},
	Journal = {Applied and Computational Harmonic Analysis},
	Keywords = {curvelet transform, imaging, SLIM},
	Month = {March},
	Number = {2},
	Pages = {150-173},
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Journals/Herrmann07ACHA.pdf},
	Title = {Sparsity- and continuity-promoting seismic image recovery with curvelet frames},
	Volume = {24},
	Year = 2008,
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Journals/Herrmann07ACHA.pdf}}

@article{hennenfent06sdw,
	Abstract = {The authors present an extension of the fast
                  discrete curvelet transform (FDCT) to nonuniformly
                  sampled data. This extension not only restores
                  curvelet compression rates for nonuniformly sampled
                  data but also removes noise and maps the data to a
                  regular grid.  },
	Author = {G. Hennenfent and F. J. Herrmann},
	Date-Added = {2008-05-07 14:50:56 -0700},
	Date-Modified = {2008-08-14 13:52:55 -0700},
	Doi = {10.1109/MCSE.2006.49},
	Issue = {3},
	Journal = {Computing in Science and Engineering},
	Keywords = {curvelet transform, SLIM},
	Month = {May-June},
	Number = 3,
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Journals/CiSE/hennenfent06nfdct.pdf},
	Publisher = {IEEE},
	Title = {Seismic denoising with non-uniformly sampled curvelets},
	Volume = 8,
	Year = 2006,
	Bdsk-Url-1 = {http://slim.eos.ubc.ca/Publications/Public/Journals/CiSE/hennenfent06nfdct.pdf}}

@article{chauris08sdm,
	Abstract = {Curvelets can represent local plane waves. They
                  efficiently decompose seismic images and possibly
                  imaging operators. We study how curvelets are
                  distorted after demigration followed by migration in
                  a different velocity model. We show that for small
                  local velocity perturbations, the
                  demigration/migration is reduced to a simple
                  morphing of the initial curvelet. The derivation of
                  the expected curvature of the curvelets shows that
                  it is easier to sparsify the demigration/migration
                  operator than the migration operator. An application
                  on a 2D synthetic data set, generated in a smooth
                  heterogeneous velocity model and with a complex
                  reflectivity, demonstrates the usefulness of
                  curvelets to predict what a migrated image would
                  become in a locally different velocity model without
                  the need for remigrating the full input data
                  set. Curvelets are thus well suited to study the
                  sensitivity of a prestack depth-migrated image with
                  respect to the heterogeneous velocity model used for
                  migration. {\copyright}2008 Society of Exploration
                  Geophysicists},
	Author = {H. Chauris and T. Nguyen},
	Date-Added = {2008-05-07 14:48:33 -0700},
	Date-Modified = {2008-08-14 14:59:04 -0700},
	Doi = {10.1190/1.2831933},
	Issue = {2},
	Journal = {Geophysics},
	Keywords = {curvelet transform, imaging},
	Number = {2},
	Pages = {S35-S46},
	Pdf = {http://link.aip.org/link/?GPY/73/S35/1},
	Publisher = {SEG},
	Title = {Seismic demigration/migration in the curvelet domain},
	Volume = {73},
	Year = {2008},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/73/S35/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2831933}}

@article{douma07los,
	Abstract = {Curvelets are plausible candidates for
                  simultaneous compression of seismic data, their
                  images, and the imaging operator itself. We show
                  that with curvelets, the leading-order approximation
                  (in angular frequency, horizontal wavenumber, and
                  migrated location) to common-offset (CO) Kirchhoff
                  depth migration becomes a simple transformation of
                  coordinates of curvelets in the data, combined with
                  amplitude scaling. This transformation is calculated
                  using map migration, which employs the local slopes
                  from the curvelet decomposition of the data. Because
                  the data can be compressed using curvelets, the
                  transformation needs to be calculated for relatively
                  few curvelets only. Numerical examples for
                  homogeneous media show that using the leading-order
                  approximation only provides a good approximation to
                  CO migration for moderate propagation times. As the
                  traveltime increases and rays diverge beyond the
                  spatial support of a curvelet; however, the
                  leading-order approximation is no longer accurate
                  enough. This shows the need for correction beyond
                  leading order, even for homogeneous
                  media. {\copyright}2007 Society of Exploration
                  Geophysicists},
	Author = {H. Douma and M. V. de Hoop},
	Date-Added = {2008-05-07 14:35:47 -0700},
	Date-Modified = {2008-08-14 15:02:25 -0700},
	Doi = {10.1190/1.2785047},
	Issue = {6},
	Journal = {Geophysics},
	Keywords = {curvelet transform, imaging},
	Number = {6},
	Pages = {S231-S248},
	Pdf = {http://link.aip.org/link/?GPY/72/S231/1},
	Publisher = {SEG},
	Title = {Leading-order seismic imaging using curvelets},
	Volume = {72},
	Year = {2007},
	Bdsk-Url-1 = {http://link.aip.org/link/?GPY/72/S231/1},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2785047}}

@article{ying053dd,
	Abstract = {In this paper, we present the first 3D discrete
                  curvelet transform. This transform is an extension
                  to the 2D transform described in Candes et al..1 The
                  resulting curvelet frame preserves the important
                  properties, such as parabolic scaling, tightness and
                  sparse representation for singularities of
                  codimension one. We describe three different
                  implementations: in-core, out-of-core and MPI-based
                  parallel implementations. Numerical results verify
                  the desired properties of the 3D curvelets and
                  demonstrate the efficiency of our implementations.
                  },
	Author = {L. Ying and L. Demanet and E. J. Cand\`es},
	Date-Added = {2008-05-07 14:14:59 -0700},
	Date-Modified = {2008-08-14 15:21:59 -0700},
	Doi = {10.1117/12.616205},
	Journal = {Proceedings SPIE wavelets XI, San Diego},
	Keywords = {curvelet transform},
	Month = {January},
	Pages = {344-354},
	Title = {{3-D} discrete curvelet transform},
	Volume = {5914},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1117/12.616205}}

@article{smith98ahs,
	Author = {H. Smith},
	Date-Added = {2008-05-07 12:25:03 -0700},
	Date-Modified = {2008-08-14 15:09:47 -0700},
	Issue = {4},
	Journal = {Journal of Geometric Analysis},
	Keywords = {FIO},
	Number = {4},
	Pages = {629-653},
	Title = {A Hardy space for {F}ourier integral operators},
	Volume = {8},
	Year = {1998}}

@article{lu07mdf,
	Abstract = {In 1992, Bamberger and Smith proposed the
                  directional filter bank (DFB) for an efficient
                  directional decomposition of 2-D signals. Due to the
                  nonseparable nature of the system, extending the DFB
                  to higher dimensions while still retaining its
                  attractive features is a challenging and previously
                  unsolved problem. We propose a new family of filter
                  banks, named NDFB, that can achieve the directional
                  decomposition of arbitrary N-dimensional (Nges2)
                  signals with a simple and efficient tree-structured
                  construction. In 3-D, the ideal passbands of the
                  proposed NDFB are rectangular-based pyramids
                  radiating out from the origin at different
                  orientations and tiling the entire frequency
                  space. The proposed NDFB achieves perfect
                  reconstruction via an iterated filter bank with a
                  redundancy factor of N in N-D. The angular
                  resolution of the proposed NDFB can be iteratively
                  refined by invoking more levels of decomposition
                  through a simple expansion rule. By combining the
                  NDFB with a new multiscale pyramid, we propose the
                  surfacelet transform, which can be used to
                  efficiently capture and represent surface-like
                  singularities in multidimensional data},
	Author = {Y. M. Lu and M. N. Do},
	Date-Added = {2008-05-07 12:19:48 -0700},
	Date-Modified = {2008-08-14 15:05:31 -0700},
	Doi = {10.1109/TIP.2007.891785},
	Issn = {1057-7149},
	Issue = {4},
	Journal = {IEEE Transactions on Image Processing},
	Keywords = {surfacelet transform},
	Month = {April},
	Number = {4},
	Pages = {918-931},
	Publisher = {IEEE},
	Title = {Multidimensional directional filter banks and surfacelets},
	Volume = {16},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIP.2007.891785}}

@article{guo07osm,
	Author = {K. Guo and D. Labate},
	Date-Added = {2008-05-07 12:03:03 -0700},
	Date-Modified = {2008-05-08 10:28:30 -0700},
	Doi = {10.1137/060649781},
	Issue = {1},
	Journal = {Journal of Mathematical Analysis},
	Keywords = {shearlet transform},
	Number = {1},
	Pages = {298-318},
	Pdf = {http://www4.ncsu.edu/~dlabate/shear_GL.pdf},
	Publisher = {SIAM},
	Title = {Optimally sparse multidimensional representation using shearlets},
	Volume = {39},
	Year = {2007},
	Bdsk-Url-1 = {http://www4.ncsu.edu/~dlabate/shear_GL.pdf},
	Bdsk-Url-2 = {http://dx.doi.org/10.1137/060649781}}

@article{do2002can,
	Abstract = {We propose a new scheme, named contourlet, that
                  provides a flexible multiresolution, local and
                  directional image expansion. The contourlet
                  transform is realized efficiently via a double
                  iterated filter bank structure. Furthermore, it can
                  be designed to satisfy the anisotropy scaling
                  relation for curves, and thus offers a fast and
                  structured curvelet-like decomposition. As a result,
                  the contourlet transform provides a sparse
                  representation for two-dimensional piecewise smooth
                  signals resembling images. Finally, we show some
                  numerical experiments demonstrating the potential of
                  contourlets in several image processing tasks.},
	Author = {M. N. Do and M. Vetterli},
	Date-Added = {2008-05-07 11:58:00 -0700},
	Date-Modified = {2008-08-14 15:01:55 -0700},
	Doi = {10.1109/ICIP.2002.1038034},
	Journal = {Proceedings. 2002 International Conference on Image Processing.},
	Keywords = {contourlet transform},
	Title = {Contourlets: a new directional multiresolution image representation},
	Volume = {1},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICIP.2002.1038034}}

@article{cordoba78wpa,
	Author = {A. C\'ordoba and C. Fefferman},
	Date-Added = {2008-05-07 11:53:23 -0700},
	Date-Modified = {2008-05-20 11:48:08 -0700},
	Doi = {10.1080/03605307808820083},
	Issue = {11},
	Journal = {Communications in Partial Differential Equations},
	Keywords = {wave packets, FIO},
	Number = {11},
	Pages = {979-1005},
	Publisher = {Taylor \& Francis},
	Title = {Wave packets and {F}ourier integral operators},
	Volume = {3},
	Year = {1978},
	Bdsk-Url-1 = {http://dx.doi.org/10.1080/03605307808820083}}

@article{candes04ntf,
	Author = {E. J. Cand\`es and D. L. Donoho},
	Date-Added = {2008-05-07 11:47:59 -0700},
	Date-Modified = {2008-08-14 14:46:59 -0700},
	Doi = {10.1002/cpa.10116},
	Issue = {2},
	Journal = {Communications on Pure and Applied Mathematics},
	Keywords = {curvelet transform},
	Number = {2},
	Pages = {219-266},
	Pdf = {http://www.acm.caltech.edu/~emmanuel/papers/CurveEdges.pdf},
	Title = {New tight frames of curvelets and optimal representations of objects with piecewise-{C}$^2$ singularities},
	Volume = {57},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1002/cpa.10116},
	Bdsk-Url-2 = {http://www.acm.caltech.edu/~emmanuel/papers/CurveEdges.pdf}}

@article{candes05tcr,
	Abstract = {This paper argues that curvelets provide a
                  powerful tool for representing very general linear
                  symmetric systems of hyperbolic differential
                  equations. Curvelets are a recently developed
                  multiscale system [10, 7] in which the elements are
                  highly anisotropic at fine scales, with effective
                  support shaped according to the parabolic scaling
                  principle width ≈ length^2 at fine scales. We prove
                  that for a wide class of linear hyperbolic
                  differential equations, the curvelet representation
                  of the solution operator is both optimally sparse
                  and well organized.  * It is sparse in the sense
                  that the matrix entries decay nearly exponentially
                  fast (i.e. faster than any negative polynomial), *
                  and well-organized in the sense that the very few
                  nonnegligible entries occur near a few shifted
                  diagonals.  Indeed, we show that the wave group maps
                  each curvelet onto a sum of curvelet-like waveforms
                  whose locations and orientations are obtained by
                  following the different Hamiltonian flows---hence
                  the diagonal shifts in the curvelet
                  representation. A physical interpretation of this
                  result is that curvelets may be viewed as coherent
                  waveforms with enough frequency localization so that
                  they behave like waves but at the same time, with
                  enough spatial localization so that they
                  simultaneously behave like particles.  },
	Author = {E. J. Cand\`es and L. Demanet},
	Date-Added = {2008-05-07 11:10:43 -0700},
	Date-Modified = {2008-08-14 14:57:23 -0700},
	Doi = {10.1002/cpa.20078},
	Issue = {11},
	Journal = {Communications on Pure and Applied Mathematics},
	Keywords = {curvelet transform, FIO},
	Number = {11},
	Pages = {1472-1528},
	Pdf = {http://www.acm.caltech.edu/~emmanuel/papers/CurveletsWaves.pdf},
	Title = {The curvelet representation of wave propagators is optimally sparse},
	Volume = {58},
	Year = {2005},
	Bdsk-Url-1 = {http://www.acm.caltech.edu/~emmanuel/papers/CurveletsWaves.pdf}}

@article{candes06fdc,
	Abstract = {This paper describes two digital implementations
                  of a new mathematical transform, namely, the second
                  generation curvelet transform [12, 10] in two and
                  three dimensions. The first digital transformation
                  is based on unequally-spaced fast Fourier transforms
                  (USFFT) while the second is based on the wrapping of
                  specially selected Fourier samples. The two
                  implementations essentially differ by the choice of
                  spatial grid used to translate curvelets at each
                  scale and angle. Both digital transformations return
                  a table of digital curvelet coefficients indexed by
                  a scale parameter, an orientation parameter, and a
                  spatial location parameter. And both implementations
                  are fast in the sense that they run in O(n^2 log n)
                  flops for n by n Cartesian arrays; in addition, they
                  are also invertible, with rapid inversion algorithms
                  of about the same complexity.  Our digital
                  transformations improve upon earlier
                  implementations---based upon the first generation of
                  curvelets---in the sense that they are conceptually
                  simpler, faster and far less redundant. The software
                  CurveLab, which implements both transforms presented
                  in this paper, is available at
                  http://www.curvelet.org.  },
	Author = {E. J. Cand\`es and L. Demanet and D. L. Donoho and L. Ying},
	Date-Added = {2008-05-06 19:34:41 -0700},
	Date-Modified = {2008-08-14 14:58:30 -0700},
	Doi = {10.1137/05064182X},
	Issue = {3},
	Journal = {Multiscale Modeling and Simulation},
	Keywords = {curvelet transform},
	Number = {3},
	Pages = {861-899},
	Pdf = {http://www.acm.caltech.edu/~emmanuel/papers/FDCT.pdf},
	Publisher = {SIAM},
	Title = {Fast discrete curvelet transforms},
	Volume = {5},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1137/05064182X},
	Bdsk-Url-2 = {http://www.acm.caltech.edu/~emmanuel/papers/FDCT.pdf}}

@article{spitz91sti,
	Abstract = {Interpolation of seismic traces is an effective
                  means of improving migration when the data set
                  exhibits spatial aliasing. A major difficulty of
                  standard interpolation methods is that they depend
                  on the degree of reliability with which the various
                  geological events can be separated. In this respect,
                  a multichannel interpolation method is described
                  which requires neither a priori knowledge of the
                  directions of lateral coherence of the events, nor
                  estimation of these directions. The method is based
                  on the fact that linear events present in a section
                  made of equally spaced traces may be interpolated
                  exactly, regardless of the original spatial
                  interval, without any attempt to determine their
                  true dips. The predictability of linear events in
                  the f-x domain allows the missing traces to be
                  expressed as the output of a linear system, the
                  input of which consists of the recorded traces. The
                  interpolation operator is obtained by solving a set
                  of linear equations whose coefficients depend only
                  on the spectrum of the spatial prediction filter
                  defined by the recorded traces. Synthetic examples
                  show that this method is insensitive to random noise
                  and that it correctly handles curvatures and lateral
                  amplitude variations. Assessment of the method with
                  a real data set shows that the interpolation yields
                  an improved migrated section. {\copyright}1991
                  Society of Exploration Geophysicists},
	Author = {S. Spitz},
	Date-Added = {2008-05-06 19:29:12 -0700},
	Date-Modified = {2008-08-14 15:18:16 -0700},
	Doi = {10.1190/1.1443096},
	Issue = {6},
	Journal = {Geophysics},
	Keywords = {PEF},
	Number = {6},
	Pages = {785-794},
	Publisher = {SEG},
	Title = {Seismic trace interpolation in the {FX} domain},
	Volume = {56},
	Year = {1991},
	Bdsk-Url-1 = {http://dx.doi.org/10.1190/1.1443096}}

@book{claerbout92esa,
	Address = {Boston},
	Author = {J. F. Claerbout},
	Date-Added = {2008-05-06 19:27:28 -0700},
	Date-Modified = {2008-05-07 11:44:19 -0700},
	Keywords = {PEF},
	Pdf = {http://sepwww.stanford.edu/sep/prof/pvi.pdf},
	Publisher = {Blackwell Scientific Publications},
	Title = {Earth soundings analysis: processing versus inversion},
	Year = {1992},
	Bdsk-Url-1 = {http://sepwww.stanford.edu/sep/prof/pvi.pdf}}

@article{hennenfent08sdw,
	Abstract = {In this paper, we present a new discrete
                  undersampling scheme designed to favor wavefield
                  reconstruction by sparsity-promoting inversion with
                  transform elements that are localized in the Fourier
                  domain. Our work is motivated by empirical
                  observations in the seismic community, corroborated
                  by recent results from compressive sampling, which
                  indicate favorable (wavefield) reconstructions from
                  random as opposed to regular undersampling. As
                  predicted by theory, random undersampling renders
                  coherent aliases into harmless incoherent random
                  noise, effectively turning the interpolation problem
                  into a much simpler denoising problem.  A practical
                  requirement of wavefield reconstruction with
                  localized sparsifying transforms is the control on
                  the maximum gap size. Unfortunately, random
                  undersampling does not provide such a control and
                  the main purpose of this paper is to introduce a
                  sampling scheme, coined jittered undersampling, that
                  shares the benefits of random sampling, while
                  offering control on the maximum gap size. Our
                  contribution of jittered sub-Nyquist sampling proves
                  to be key in the formulation of a versatile
                  wavefield sparsity-promoting recovery scheme that
                  follows the principles of compressive sampling.
                  After studying the behavior of the jittered
                  undersampling scheme in the Fourier domain, its
                  performance is studied for curvelet recovery by
                  sparsity-promoting inversion (CRSI). Our findings on
                  synthetic and real seismic data indicate an
                  improvement of several decibels over recovery from
                  regularly-undersampled data for the same amount of
                  data collected.},
	Author = {G. Hennenfent and F. J. Herrmann},
	Datasets = {Lacona},
	Date-Added = {2008-05-06 19:23:30 -0700},
	Date-Modified = {2008-08-14 13:54:23 -0700},
	Doi = {10.1190/1.2841038},
	Issue = {3},
	Journal = {Geophysics},
	Keywords = {sampling, SLIM},
	Month = {May-June},
	Number = 3,
	Pdf = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07jitter.pdf},
	Publisher = {SEG},
	Title = {Simply denoise: wavefield reconstruction via jittered undersampling},
	Url = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07jitter/paper_html},
	Volume = 73,
	Year = 2008,
	Bdsk-Url-1 = {http://dx.doi.org/10.1190/1.2841038},
	Bdsk-Url-2 = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07jitter/paper_html},
	Bdsk-Url-3 = {http://slim.eos.ubc.ca/Publications/Public/Journals/hennenfent07jitter.pdf}}

@article{sacchi98iae,
	Abstract = {We present an iterative nonparametric approach to
                  spectral estimation that is particularly suitable
                  for estimation of line spectra. This approach
                  minimizes a cost function derived from Bayes'
                  theorem. The method is suitable for line spectra
                  since a ``long tailed'' distribution is used to
                  model the prior distribution of spectral
                  amplitudes. An important aspect of this method is
                  that since the data themselves are used as
                  constraints, phase information can also be recovered
                  and used to extend the data outside the original
                  window.  The objective function is formulated in
                  terms of hyperpa- rameters that control the degree
                  of fit and spectral resolution.  Noise rejection can
                  also be achieved by truncating the number of
                  iterations. Spectral resolution and extrapolation
                  length are controlled by a single parameter. When
                  this parameter is large compared with the spectral
                  powers, the algorithm leads to zero extrapolation of
                  the data, and the estimated Fourier transform yields
                  the periodogram.  When the data are sampled at a
                  constant rate, the algorithm uses one Levinson
                  recursion per iteration. For irregular sampling
                  (unevenly sampled and/or gapped data), the algorithm
                  uses one Cholesky decomposition per iteration.  The
                  performance of the algorithm is illustrated with
                  three different problems that frequently arise in
                  geophysical data processing: 1) harmonic retrieval
                  from a time series contaminated with noise; 2)
                  linear event detection from a finite aperture array
                  of receivers [which, in fact, is an extension of
                  1)], 3) interpolation/extrapolation of gapped data.
                  The performance of the algorithm as a spectral
                  estimator is tested with the Kay and Marple data
                  set. It is shown that the achieved resolution is
                  comparable with parametric methods but with more
                  accurate representation of the relative power in the
                  spectral lines. },
	Author = {M. D. Sacchi and T. J. Ulrych and C. J. Walker},
	Date-Added = {2008-05-06 19:18:50 -0700},
	Date-Modified = {2008-08-14 15:08:37 -0700},
	Doi = {10.1109/78.651165},
	Issue = {1},
	Journal = {IEEE Transactions on Signal Processing},
	Keywords = {Fourier transform, reconstruction},
	Number = {1},
	Pages = {31-38},
	Pdf = {http://www-geo.phys.ualberta.ca/saig/papers/Sacchi_Ulrych_Walker_IEEE_98.pdf},
	Publisher = {IEEE},
	Title = {Interpolation and extrapolation using a high-resolution discrete {F}ourier transform},
	Volume = {46},
	Year = {1998},
	Bdsk-Url-1 = {http://www-geo.phys.ualberta.ca/saig/papers/Sacchi_Ulrych_Walker_IEEE_98.pdf}}

@article{hampson86ivs,
	Author = {D. Hampson},
	Date-Added = {2008-05-06 19:09:45 -0700},
	Date-Modified = {2008-05-07 11:44:52 -0700},
	Issue = {1},
	Journal = {Journal of the Canadian Society of Exploration Geophysicists},
	Keywords = {Radon transform},
	Number = {1},
	Pages = {44-45},
	Pdf = {http://www.cseg.ca/publications/journal/1986_12/1986_Hampson_D_inverse_velocity_stacking.pdf},
	Publisher = {CSEG},
	Title = {Inverse Velocity Stacking for Multiple Elimination},
	Volume = {22},
	Year = {1986},
	Bdsk-Url-1 = {http://www.cseg.ca/publications/journal/1986_12/1986_Hampson_D_inverse_velocity_stacking.pdf}}

@article{thorson85vsa,
	Abstract = {Normal moveout (NMO) and stacking, an important
                  step in analysis of reflection seismic data,
                  involves summation of seismic data over paths
                  represented by a family of hyperbolic curves. This
                  summation process is a linear transformation and
                  maps the data into what might be called a velocity
                  space: a two-dimensional set of points indexed by
                  time and velocity. Examination of data in velocity
                  space is used for analysis of subsurface velocities
                  and filtering of undesired coherent events (e.g.,
                  multiples), but the filtering step is useful only if
                  an approximate inverse to the NMO and stack
                  operation is available. One way to effect velocity
                  filtering is to use the operator LT (defined as NMO
                  and stacking) and its adjoint L as a transform pair,
                  but this leads to unacceptable filtered
                  output. Designing a better estimated inverse to L
                  than LT is a generalization of the inversion problem
                  of computerized tomography: deconvolving out the
                  point-spread function after back projection. The
                  inversion process is complicated by missing data,
                  because surface seismic data are recorded only
                  within a finite spatial aperture on the Earth's
                  surface. Our approach to solving the problem of an
                  ill-conditioned or nonunique inverse L--1, brought
                  on by missing data, is to design a stochastic
                  inverse to L. Starting from a maximum a posteriori
                  (MAP) estimator, a system of equations can be set up
                  in which a priori information is incorporated into a
                  sparseness measure: the output of the stochastic
                  inverse is forced to be locally focused, in order to
                  obtain the best possible resolution in velocity
                  space. The size of the resulting nonlinear system of
                  equations is immense, but using a few iterations
                  with a gradient descent algorithm is adequate to
                  obtain a reasonable solution. This theory may also
                  be applied to other large, sparse linear
                  operators. The stochastic inverse of the slant-stack
                  operator (a particular form of the Radon transform),
                  can be developed in a parallel manner, and will
                  yield an accurate slant-stack inverse
                  pair. {\copyright}1985 Society of Exploration
                  Geophysicists},
	Author = {J. R. Thorson and J. F. Claerbout},
	Date-Added = {2008-05-06 19:06:15 -0700},
	Date-Modified = {2008-08-14 15:20:19 -0700},
	Doi = {10.1190/1.1441893},
	Issue = {12},
	Journal = {Geophysics},
	Keywords = {Radon transform},
	Number = {12},
	Pages = {2727-2741},
	Publisher = {SEG},
	Title = {Velocity-stack and slant-stack stochastic inversion},
	Volume = {50},
	Year = {1985},
	Bdsk-Url-1 = {http://dx.doi.org/10.1190/1.1441893}}

@phdthesis{schonewille00phd,
	Address = {Delft, The Netherlands},
	Author = {M. A. Schonewille},
	Date-Added = {2008-05-06 19:03:35 -0700},
	Date-Modified = {2008-05-09 14:43:57 -0700},
	Keywords = {Fourier transform, reconstruction},
	Month = {November},
	Rating = {0},
	Read = {Yes},
	School = {Delft University of Technology},
	Title = {Fourier reconstruction of irregularly sampled seismic data},
	Year = {2000}}

@phdthesis{zwartjes05phd,
	Address = {Delft, The Netherlands},
	Author = {P. M. Zwartjes},
	Date-Added = {2008-05-06 18:58:35 -0700},
	Date-Modified = {2008-05-09 14:44:04 -0700},
	Keywords = {Fourier transform, reconstruction},
	Month = {December},
	Rating = {0},
	Read = {Yes},
	School = {Delft University of Technology},
	Title = {Fourier reconstruction with sparse inversion},
	Year = {2005}}

@comment{BibDesk Smart Groups{
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Keywords</string>
				<key>value</key>
				<string>SLIM</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>SLIM</string>
	</dict>
</array>
</plist>
}}
