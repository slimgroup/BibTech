% This file was created with JabRef 2.9.
% Encoding: ISO8859_1

@CONFERENCE{dasilva2013SAMPTAhtuck,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical Tucker Tensor Optimization - Applications to Tensor Completion},
  organization = {SAMPTA},
  year = {2013},
  abstract = {In this work, we develop an optimization framework for
                  problems whose solutions are well-approximated by
                  Hierarchical Tucker tensors, an efficient structured
                  tensor format based on recursive subspace
                  factorizations. Using the differential geometric
                  tools presented here, we construct standard
                  optimization algorithms such as Steepest Descent and
                  Conjugate Gradient, for interpolating tensors in HT
                  format. We also empirically examine the importance
                  of one's choice of data organization in the success
                  of tensor recovery by drawing upon insights from the
                  Matrix Completion literature. Using these
                  algorithms, we recover various seismic data sets
                  with randomly missing source pairs.},
  keywords = {SAMPTA, hierarchical tucker, structured tensor, tensor
                  interpolation, differential geometry, riemannian
                  optimization},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2013/dasilva2013SAMPTAhtuck/dasilva2013SAMPTAhtuck.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2013/dasilva2013SAMPTAhtuck/dasilva2013SAMPTAhtuck_pres.pdf}
}


@CONFERENCE{dasilva2013EAGEhtucktensor,
  author = {Curt Da Silva and Felix J. Herrmann},
  title = {Hierarchical Tucker Tensor Optimization - Applications to 4D Seismic Data Interpolation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {In this work, we develop optimization algorithms on the
                  manifold of Hierarchical Tucker (HT) tensors, an
                  extremely efficient format for representing
                  high-dimensional tensors exhibiting particular
                  low-rank structure. With some minor alterations to
                  existing theoretical developments, we develop an
                  optimization framework based on the geometric
                  understanding of HT tensors as a smooth manifold, a
                  generalization of smooth curves/surfaces. Building
                  on the existing research of solving optimization
                  problems on smooth manifolds, we develop Steepest
                  Descent and Conjugate Gradient methods for HT
                  tensors. The resulting algorithms converge quickly,
                  are immediately parallelizable, and do not require
                  the computation of SVDs. We also extend ideas about
                  favourable sampling conditions for missing-data
                  recovery from the field of Matrix Completion to
                  Tensor Completion and demonstrate how the
                  organization of data can affect the success of
                  recovery. As a result, if one has data with randomly
                  missing source pairs, using these ideas, coupled
                  with an efficient solver, one can interpolate
                  large-scale seismic data volumes with missing
                  sources and/or receivers by exploiting the
                  multidimensional dependencies in the data. We are
                  able to recover data volumes amidst extremely high
                  subsampling ratios (in some cases, > 75%) using this
                  approach.},
  keywords = {EAGE, structured tensor, 3D data interpolation, riemannian optimization},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/dasilva2013EAGEhtucktensor/dasilva2013EAGEhtucktensor.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/dasilva2013EAGEhtucktensor/dasilva2013EAGEhtucktensor_pres.pdf}
}


@CONFERENCE{kumar2013EAGEsind,
  author = {Rajiv Kumar and Aleksandr Y. Aravkin and Hassan Mansour and Ben Recht and Felix J. Herrmann},
  title = {Seismic data interpolation and denoising using SVD-free low-rank matrix factorization},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Recent developments in rank optimization have allowed
                  new approaches for seismic data interpolation and
                  denoising. In this paper, we propose an approach for
                  simultaneous seismic data interpolation and
                  denoising using robust rank-regularized
                  formulations. The proposed approach is suitable for
                  large scale problems, since it avoids SVD
                  computations by using factorized formulations. We
                  illustrate the advantages of the new approach using
                  a seismic line from Gulf of Suez and 5D synthetic
                  seismic data to obtain high quality results for
                  interpolation and denoising, a key application in
                  exploration geophysics.},
  keywords = {EAGE, interpolation, denoising},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/kumar2013EAGEsind/kumar2013EAGEsind.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/kumar2013EAGEsind/kumar2013EAGEsind_pres.pdf}
}


@CONFERENCE{lin2013EAGEcsd,
  author = {Tim T. Y. Lin and Felix J. Herrmann},
  title = {Cosparse seismic data interpolation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Many modern seismic data interpolation and redatuming
                  algorithms rely on the promotion of transform-domain
                  sparsity for high-quality results. Amongst the large
                  diversity of methods and different ways of realizing
                  sparse reconstruction lies a central question that
                  often goes unaddressed: is it better for the
                  transform-domain sparsity to be achieved through
                  explicit construction of sparse representations
                  (e.g., by thresholding of small transform-domain
                  coefficients), or by demanding that the algorithm
                  return physical signals which produces sparse
                  coefficients when hit with the forward transform?
                  Recent results show that the two approaches give
                  rise to different solutions when the transform is
                  redundant, and that the latter approach imposes a
                  whole new class of constraints related to where the
                  forward transform produces zero coefficients. From
                  this framework, a new reconstruction algorithm is
                  proposed which may allow better reconstruction from
                  subsampled signaled than what the sparsity
                  assumption alone would predict. In this work we
                  apply the new framework and algorithm to the case of
                  seismic data interpolation under the curvelet
                  domain, and show that it admits better
                  reconstruction than some existing L1 sparsity-based
                  methods derived from compressive sensing for a range
                  of subsampling factors.},
  keywords = {EAGE, cosparsity, interpolation, curvelet, algorithm, optimization},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/lin2013EAGEcsd/lin2013EAGEcsd.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/lin2013EAGEcsd/lin2013EAGEcsd_pres.pdf}
}


@CONFERENCE{tu2013EAGElsm,
  author = {Ning Tu and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Fast least-squares migration with multiples and source estimation},
  booktitle = {EAGE},
  year = {2013},
  abstract = {The advent of modern computing has made it possible to
                  do seismic imaging using least-squares reverse-time
                  migration. We obtain superior images by solving an
                  optimization problem that recovers the
                  true-amplitude images. However, its success hinges
                  on overcoming several issues, including overwhelming
                  problem size, unknown source wavelet, and
                  interfering coherent events like multiples. In this
                  abstract, we reduce the problem size by using ideas
                  from compressive sensing, and estimate source
                  wavelet by generalized variable projection. We also
                  demonstrate how to invert for subsurface information
                  encoded in surface-related multiples by
                  incorporating the free-surface operator as an areal
                  source in reverse-time migration. Our synthetic
                  examples show that multiples help to improve the
                  resolution of the image, as well as remove the
                  amplitude ambiguity in wavelet estimation.},
  keywords = {EAGE, imaging, sparse, source estimation, multiples},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/tu2013EAGElsm/tu2013EAGElsm.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/tu2013EAGElsm/tu2013EAGElsm_pres.pdf}
}


@CONFERENCE{vanleeuwen2013EAGErobustFWI,
  author = {Tristan van Leeuwen and Aleksandr Y. Aravkin and Henri Calandra and Felix J. Herrmann},
  title = {In which domain should we measure the misfit for robust full waveform inversion?},
  booktitle = {EAGE},
  year = {2013},
  abstract = {Full-waveform inversion relies on minimizing the
                  difference between observed and modeled data, as
                  measured by some penalty function. A popular choice,
                  of course, is the least-squares penalty. However,
                  when outliers are present in the data, the use of
                  robust penalties such as the Huber or Student's t
                  may significantly improve the results since they put
                  relatively less weight on large residuals. In order
                  for robust penalties to be effective, the outliers
                  must be somehow localized and distinguishable from
                  the good data. We propose to first transform the
                  residual into a domain where the outliers are
                  localized before measuring the misfit with a robust
                  penalty. This is exactly how one would normally
                  devise filters to remove the noise before applying
                  conventional FWI. We propose to merge the two steps
                  and let the inversion process implicitly filter out
                  the noise. Results on a synthetic dataset show the
                  effectiveness of the approach.},
  keywords = {EAGE, full waveform inversion},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/vanleeuwen2013EAGErobustFWI/vanleeuwen2013EAGErobustFWI.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/vanleeuwen2013EAGErobustFWI/vanleeuwen2013EAGErobustFWI_pres.pdf}
}


@CONFERENCE{wason2013EAGEobs,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Ocean bottom seismic acquisition via jittered sampling},
  booktitle = {EAGE},
  year = {2013},
  abstract = {We present a pragmatic marine acquisition scheme where
                  multiple source vessels sail across an ocean-bottom
                  array firing at airgunsjittered source locations and
                  instances in time. Following the principles of
                  compressive sensing, we can significantly impact the
                  reconstruction quality of conventional seismic data
                  (from jittered data) and demonstrate successful
                  recovery by sparsity promotion. In contrast to
                  random (under)sampling, acquisition via jittered
                  (under)sampling helps in controlling the maximum gap
                  size, which is a practical requirement of wavefield
                  reconstruction with localized sparsifying
                  transforms. Results are illustrated with simulations
                  of time-jittered marine acquisition, which
                  translates to jittered source locations for a given
                  speed of the source vessel, for two source vessels.},
  keywords = {EAGE, acquisition, blended, marine, deblending, interpolation},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/wason2013EAGEobs/wason2013EAGEobs.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2013/wason2013EAGEobs/wason2013EAGEobs_pres.pdf}
}


@CONFERENCE{aravkin2013ICASSPssi,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Ning Tu},
  title = {Sparse seismic imaging using variable projection},
  booktitle = {ICASSP},
  year = {2013},
  abstract = {We consider an important class of signal processing
                  problems where the signal of interest is known to be
                  sparse, and can be recovered from data given
                  auxiliary information about how this data was
                  generated. For example, a sparse green's function
                  may be recovered from seismic experimental data
                  using sparsity optimization when the source
                  signature is known. Unfortunately, in practice this
                  information is often missing, and must be recovered
                  from data along with the signal using deconvolution
                  techniques. In this paper, we present a novel
                  methodology to simulta- neously solve for the sparse
                  signal and auxiliary parameters using a recently
                  proposed variable projection technique. Our main
                  contribution is to combine variable projection with
                  spar- sity promoting optimization, obtaining an
                  efficient algorithm for large-scale sparse
                  deconvolution problems. We demon- strate the
                  algorithm on a seismic imaging example.},
  keywords = {imaging, sparsity, optimization, variable projection},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2013/aravkin2013ICASSPssi/aravkin2013ICASSPssi.pdf}
}


@CONFERENCE{petrenko2013HPCSsaoc,
  author = {Art Petrenko and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Software acceleration of CARP, an iterative linear solver and preconditioner},
  organization = {HPCS},
  year = {2013},
  abstract = {We present the results of software optimization of a
                  row-wise preconditioner (Component Averaged Row
                  Projections) for the method of conjugate gradients,
                  which is used to solve the diagonally banded
                  Helmholtz system representing frequency domain,
                  isotropic acoustic seismic wave simulation. We
                  demonstrate that in our application, a
                  preconditioner bound to one processor core and
                  accessing memory contiguously reduces execution time
                  by 7% for matrices having on the order of 108
                  non-zeros. For reference we note that our C
                  implementation is over 80 times faster than the
                  corresponding code written for a high-level
                  numerical analysis language.},
  keywords = {HPCS,Helmholtz equation, Kaczmarz, software, wave
                  propagation, frequency-domain},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/HPCS/2013/petrenko2013HPCSsaoc/petrenko2013HPCSsaoc.pdf},
  url2 = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/HPCS/2013/petrenko2013HPCSsaoc/petrenko2013HPCSsaoc_poster.pdf}
}


@CONFERENCE{herrmann2013KAUSTrse,
  author = {Felix J. Herrmann},
  title = {Randomized sampling in exploration seismology},
  booktitle = {KAUST},
  organization = {KAUST},
  year = {2013},
  keywords = {KAUST, randomized sampling, exploration seismology},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/KAUST/2013/herrmann2013KAUSTrse/herrmann2013KAUSTrse_pres.pdf}
}


@CONFERENCE{herrmann2013SEGOMANrdw,
  author = {Felix J. Herrmann},
  title = {Recent developments in wave-equation based inversion technology},
  booktitle = {SEG FWI Workshop, Oman},
  year = {2013},
  keywords = {SEG, Oman, randomized sampling, exploration seismology, 3D, FWI},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2013/herrmann2013SEGOMANrdw/herrmann2013SEGOMANrdw_pres.pdf}
}


@CONFERENCE{herrmann2013SIAMdrfwi,
  author = {Felix J. Herrmann},
  title = {Dimensionality reduction in FWI},
  booktitle = {SIAM},
  year = {2013},
  keywords = {SIAM, FWI, dimensionality reduction},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SIAM/2013/herrmann2013SIAMdrfwi/herrmann2013SIAMdrfwi_pres.pdf}
}


@CONFERENCE{miao2013CSEGaospsa,
  author = {Lina Miao and Felix J. Herrmann},
  title = {Acceleration on Sparse Promoting Seismic Applications},
  booktitle = {CSEG},
  year = {2013},
  abstract = {Sparse promoting oriented problems are never new in
                  seismic applications. Back in 1970s, geophysicists
                  had well exploited the robustness of sparse
                  solutions. Moreover, with the emerging usage of
                  compressed sensing in recent years, sparse recovery
                  have been favored in dealing with 'curse of
                  dimensionality' in various seismic field
                  acquisition, data processing, and imaging
                  applications. Although sparsity has provided a
                  promising approach, solving for it presents a big
                  challenge. How to work efficiently with the
                  extremely large-scale seismic problem, and how to
                  improve the convergence rate reducing computation
                  time are most frequently asked questions in this
                  content. In this abstract, the author proposed a new
                  algorithm -- PQN$\ell_1$, trying to address those
                  questions. One example on seismic data processing is
                  included.},
  keywords = {CSEG,sparsity-promotion,SPG$\ell_1$,Projected Quasi Newton},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2013/miao2013CSEGaospsa/miao2013CSEGaospsa.pdf}
}


@CONFERENCE{oghenekohwo2013CSEGnratld,
  author = {Felix Oghenekohwo and Felix J. Herrmann},
  title = {Assessing the need for repeatability in acquisition of time-lapse data},
  booktitle = {CSEG},
  year = {2013},
  abstract = {There are several factors that affect the repeatability
                  of 4D(time-lapse) seismic data. One of the most
                  significant factors is the repeatability of the
                  acquisition, particularly the locations of the
                  sources and receivers. It is important to repeat the
                  source-receiver locations, used during the baseline
                  survey, in the monitor or repeat survey. Also, it is
                  essential that the stacked data volumes used for
                  time-lapse analysis are created using the same
                  offset ranges for each survey. This condition is
                  crucial in order to be able to produce an image of
                  the same location over a period of time and enhances
                  proper reservoir characterization. The cost of
                  repeating the seismic acquisition is very expensive,
                  as often times, the receiver array has to be left at
                  the same location over the period for which the data
                  will be acquired. In other words, it is important to
                  repeat the acquisition geometry as much as
                  possible. In this talk, we investigate the results
                  of changing the acquisition geometry, by a random
                  placement of the receivers for both the baseline
                  surveys and newer (monitor) surveys. Results show
                  that we are still able to observe any time-lapse
                  effects from the proposed acquisition geometry. Our
                  experiments have been performed on a synthetic
                  model.},
  keywords = {acquisition,CSEG, time-lapse, migration},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2013/oghenekohwo2013CSEGnratld/oghenekohwo2013CSEGnratld.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2013/oghenekohwo2013CSEGnratld/oghenekohwo2013CSEGnratld_pres.pdf}
}


@CONFERENCE{Mansour11TRwmmw,
  author = {Hassan Mansour and Ozgur Yilmaz},
  title = {Weighted -$\ell_1$ minimization with multiple weighting sets},
  booktitle = {Proc. SPIE},
  year = {2011},
  volume = {8138},
  number = {813809},
  abstract = {In this paper, we study the support recovery conditions
                  of weighted -$\ell_1$ minimization for signal
                  reconstruction from compressed sensing measurements
                  when multiple support estimate sets with different
                  accuracy are available. We identify a class of
                  signals for which the recovered vector from
                  -$\ell_1$ minimization provides an accurate support
                  estimate. We then derive stability and robustness
                  guarantees for the weighted -$\ell_1$ minimization
                  problem with more than one support estimate. We show
                  that applying a smaller weight to support estimate
                  that enjoy higher accuracy improves the recovery
                  conditions compared with the case of a single
                  support estimate and the case with standard, i.e.,
                  non-weighted,-$\ell_1$ minimization. Our theoretical
                  results are supported by numerical simulations on
                  synthetic signals and real audio signals.},
  keywords = {Compressive Sensing,Optimization},
  notes = {TR-2011-07},
  optmonth = {09/2011},
  optdoi = {doi:10.1117/12.894165},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SPIE/2011/Mansour11TRwmmw/Mansour11TRwmmw.pdf}
}


@CONFERENCE{aravkin2011EAGEnspf,
  author = {Aleksandr Y. Aravkin and James V. Burke and Felix J. Herrmann and
	Tristan van Leeuwen},
  title = {A Nonlinear Sparsity Promoting Formulation and Algorithm for Full
	Waveform Inversion},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2011},
  organization = {EAGE},
  abstract = {Full Waveform Inversion (FWI) is a computational procedure to extract
	medium parameters from seismic data. FWI is typically formulated
	as a nonlinear least squares optimization problem, and various regularization
	techniques are used to guide the optimization because the problem
	is illposed. In this paper, we propose a novel sparse regularization
	which exploits the ability of curvelets to efficiently represent
	geophysical images. We then formulate a corresponding sparsity promoting
	constrained optimization problem, which we call Nonlinear Basis Pursuit
	Denoise (NBPDN) and present an algorithm to solve this problem to
	recover medium parameters. The utility of the NBPDN formulation and
	efficacy of the algorithm are demonstrated on a stylized cross-well
	exper- iment, where a sparse velocity perturbation is recovered with
	higher quality than the standard FWI formulation (solved with LBFGS).
	The NBPDN formulation and algorithm can recover the sparse perturbation
	even when the data volume is compressed to 5 percent of the original
	size using random superposition.},
  keywords = {EAGE,Presentation,Full-waveform inversion,Optimization},
  optmonth = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/aravkin11EAGEnspf/aravkin11EAGEnspf_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/aravkin11EAGEnspf/aravkin11EAGEnspf.pdf}
}

@CONFERENCE{aravkin2012ICASSProbustb,
  author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Tristan van Leeuwen},
  title = {Robust inversion via semistochastic dimensionality reduction},
  booktitle = {ICASSP},
  year = {2012},
  pages = {5245-5248},
  organization = {ICASSP},
  abstract = {We consider a class of inverse problems where it is possible to aggregate
	the results of multiple experiments. This class includes problems
	where the forward model is the solution operator to linear ODEs or
	PDEs. The tremendous size of such problems motivates the use dimensionality
	reduction (DR) techniques based on randomly mixing experiments. These
	techniques break down, however, when robust data-fitting formulations
	are used, which are essential in cases of missing data, unusually
	large errors, and systematic features in the data unexplained by
	the forward model. We survey robust methods within a statistical
	framework, and propose a sampling optimization approach that allows
	DR. The efficacy of the methods are demonstrated for a large-scale
	seismic inverse problem using the robust Student's t-distribution,
	where a useful synthetic velocity model is recovered in the extreme
	scenario of 60% corrupted data. The sampling approach achieves this
	recovery using 20% of the effort required by a direct robust approach.},
  keywords = {ICASSP},
  optdoi = {10.1109/ICASSP.2012.6289103},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/AravkinFriedlanderLeeuwen/AravkinFriedlanderLeeuwen.pdf }
}

@CONFERENCE{aravkin2011SIAMfwi,
  author = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen
	and James V. Burke and Xiang Li},
  title = {Full Waveform Inversion with Compressive Updates},
  booktitle = {SIAM},
  year = {2011},
  organization = {SIAM CS\&E 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment data volumes.
	While improvements in acquisition and inversion have been extremely
	successful, the current push for higher quality models reveals fundamental
	shortcomings handling increasing problem sizes numerically. To address
	this fundamental issue, we propose a randomized dimensionality-reduction
	strategy motivated by recent developments in stochastic optimization
	and compressive sensing. In this formulation conventional Gauss-Newton
	iterations are replaced by dimensionality-reduced sparse recovery
	problems with source encodings.},
  keywords = {SLIM,Presentation,Full-waveform inversion},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SIAM/2011/aravkin2011SIAMfwi/aravkin2011SIAMfwi.pdf}
}

@CONFERENCE{herrmann2011SEGffw,
  author = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen
	and Xiang Li},
  title = {Fast full-waveform inversion with compressive sensing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  keywords = {SEG,SLIM,Presentation,Full-waveform inversion},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/HerrmannSEG2011fws/HerrmannSEG2011fws.pdf}
}

@CONFERENCE{aravkin2011EAGEspfwi,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and
	Felix J. Herrmann},
  title = {Sparsity Promoting Formulations and Algorithms for FWI},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2011},
  keywords = {EAGE,Presentation, Full-waveform inversion},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/Aravkin2011EAGEspfwi/Aravkin2011EAGEspfwi.pdf
	}
}

@CONFERENCE{aravkin2011ICIAMspf,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and
	Felix J. Herrmann},
  title = {Sparsity promoting formulations and algorithms for FWI. Presented
	at AMP Medical and Seismic Imaging, 2011, Vancouver BC.},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Full Waveform Inversion (FWI) is a computational procedure to extract
	medium parameters from seismic data. FWI is typically formulated
	as a nonlinear least squares optimization problem, and various regularization
	techniques are used to guide the optimization because the problem
	is ill-posed. We propose a novel sparse regularization which exploits
	the ability of curvelets to efficiently represent geophysical images.
	We then formulate a corresponding sparsity promoting constrained
	optimization problem, which we solve using an open source algorithm.
	The techniques are applicable to any inverse problem where sparsity
	modeling is appropriate. We demonstrate the efficacy of the formulation
	on a toy example (stylized cross-well experiment) and on a realistic
	Seismic example (partial Marmoussi model). We also discuss the tradeoff
	between model fit and sparsity promotion, with a view to extend existing
	techniques for linear inverse problems to the case where the forward
	model is nonlinear. },
  date-added = {2011-07-15},
  keywords = {SLIM,Presentation,Full-waveform inversion,Optimization},
  optmonth = {07/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICIAM/2011/aravkin2011ICIAMspf/aravkin2011ICIAMspf_pres.pdf}
}

@CONFERENCE{aravkin2011ICIAMrfwiu,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust FWI using Student's t-distribution},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated simulation of 3D time-dependent
	acoustic, elastic, or electromagnetic wave fields, extending hundreds
	of wavelengths and hundreds of periods. Also, seismic data is rich
	in information at every representable scale. Thus simulation-driven
	optimization approaches to inversion impose great demands on simulator
	efficiency and accuracy. While computer hardware advances have been
	of critical importance in bringing inversion closer to practical
	application, algorithmic advances in simulator methodology have been
	equally important. Speakers in this two-part session will address
	a variety of numerical issues arising in the wave simulation, and
	in its application to inversion. },
  date-added = {2011-07-20},
  keywords = {SLIM,Presentation,ICIAM,Full-waveform inversion,Optimization},
  optmonth = {07/2011},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICIAM/2011/aravkin2011ICIAMrfwiu/aravkin2011ICIAMrfwiu.pdf}
}

@CONFERENCE{aravkin2011SEGrobust,
  author = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title = {Robust full-waveform inversion using the Student's t-distribution},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  volume = {30},
  pages = {2669-2673},
  organization = {SEG},
  abstract = {Full-waveform inversion (FWI) is a computational procedure to extract
	medium parameters from seismic data. Robust meth- ods for FWI are
	needed to overcome sensitivity to noise and in cases where modeling
	is particularly poor or far from the real data generating process.
	We survey previous robust methods from a statistical perspective,
	and use this perspective to derive a new robust method by assuming
	the random errors in our model arise from the Student's t-distribution.
	We show that in contrast to previous robust methods, the new method
	progres- sively down-weighs large outliers, effectively ignoring
	them once they are large enough. This suggests that the new method
	is more robust and suitable for situations with very poor data quality
	or modeling. Experiments show that the new method recovers as well
	or better than previous robust methods, and can recover models with
	quality comparable to standard meth- ods on noise-free data when
	some of the data is completely corrupted, and even when a marine
	acquisition mask is entirely ignored in the modeling. The ability
	to ignore a marine acqui- sition mask via robust FWI methods offers
	an opportunity for stochastic optimization methods in marine acquisition.},
  keywords = {SEG,Full-waveform inversion,Optimization},
  optdoi = {10.1190/1.3627747},
  optmonth = {04/2011},
  timestamp = {2011-04-06 15:00:00 -0700},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/aravkin11SEGrobust/aravkin11SEGrobust.pdf }
}

@CONFERENCE{aravkin2012ICASSPfastseis,
  author = {Aleksandr Y. Aravkin and Xiang Li and Felix J. Herrmann },
  title = {Fast seismic imaging for marine data},
  booktitle = {ICASSP},
  year = {2012},
  organization = {ICASSP},
  abstract = {Seismic imaging can be formulated as a linear inverse problem where
	a medium perturbation is obtained via minimization of a least-squares
	misfit functional. The demand for higher resolution images in more
	geophysically complex areas drives the need to develop techniques
	that handle problems of tremendous size with limited computational
	resources. While seismic imaging is amenable to dimensionality reduction
	techniques that collapse the data volume into a smaller set of “super-shots”,
	these techniques break down for complex acquisition geometries such
	as marine acquisition, where sources and receivers move during acquisition.
	To meet these challenges, we propose a novel method that combines
	sparsity-promoting (SP) solvers with random sub- set selection of
	sequential shots, yielding a SP algorithm that only ever sees a small
	portion of the full data, enabling its application to very large-scale
	problems. Application of this technique yields excellent results
	for a complicated synthetic, which underscores the robustness of
	sparsity promotion and its suitability for seismic imaging.},
  keywords = {ICASSP},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/AravkinLiHerrmann/AravkinLiHerrmann.pdf}
}

@CONFERENCE{aravkin2012SEGST,
  author = {Aleksandr Y. Aravkin and Tristan {van Leeuwen} and Kenneth Bube and
	Felix J. Herrmann},
  title = {On Non-Uniqueness of the Student's t-formulation for Linear Inverse
	Problems},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  organization = {SEG},
  abstract = {We review the statistical interpretation of inverse problem formulations,
	and the motivations for selecting non-convex penalties for robust
	behaviour with respect to measurement outliers or artifacts in the
	data. An important downside of using non-convex formulations such
	as the Student's t is the potential for non-uniqueness, and we present
	a simple example where the Student's t penalty can be made to have
	many local minima by appropriately selecting the degrees of freedom
	parameter. On the other hand, the non-convexity of the Student's
	t is precisely what gives it the ability to ignore artifacts in the
	data. We explain this idea, and present a stylized imaging experiment,
	where the Student's t is able to recover a velocity perturbation
	from data contaminated by a very peculiar artifact --- data from
	a different velocity perturbation. The performance of Student's t
	inversion is investigated empirically for different values of the
	degrees of freedom parameter, and different initial conditions.},
  keywords = {student's t, robust, non-convex, uniqueness, SEG},
  optmonth = {04/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/aravkin2012SEGST/aravkin2012SEGST.pdf}
}

@CONFERENCE{aravkin2012EAGErobust,
  author = {Aleksandr Y. Aravkin and Tristan {van Leeuwen} and Henri Calandra
	and Felix J. Herrmann},
  title = {Source estimation for frequency-domain FWI with robust penalties},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {Source estimation is an essential component of full waveform inversion.
	In the standard frequency domain formulation, there is closed form
	solution for the the optimal source weights, which can thus be cheaply
	estimated on the fly. A growing body of work underscores the importance
	of robust modeling for data with large outliers or artifacts that
	are not captured by the forward model. Effectively, the least-squares
	penalty on the residual is replaced by a robust penalty, such as
	Huber, Hybrid `1-`2 or Student’s t. As we will demonstrate, it
	is essential to use the same robust penalty for source estimation.
	In this abstract, we present a general approach to robust waveform
	inversion with robust source estimation. In this general formulation,
	there is no closed form solution for the optimal source weights so
	we need to solve a scalar optimization problem to obtain these weights.
	We can efficiently solve this optimization problem with a Newton-like
	method in a few iterations. The computational cost involved is of
	the same order as the usual least-squares source estimation procedure.
	We show numerical examples illustrating robust source estimation
	and robust waveform inversion on synthetic data with outliers.},
  keywords = {EAGE},
  optmonth = {01/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust.pdf}
}

@CONFERENCE{vandenberg07VONipo,
  author = {Ewout van den Berg and Michael P. Friedlander},
  title = {In Pursuit of a Root},
  year = {2007},
  month = {July},
  organization = {Von Neumann Symposium},
  quality = {1},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/vonNeuman/2007/vandenberg07VONipo/vandenberg07VONipo.pdf}
}

@CONFERENCE{beyreuther2005SEGcot,
  author = {Moritz Beyreuther and Jamin Cristall and Felix J. Herrmann},
  title = {Computation of time-lapse differences with {3-D} directional frames},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2005},
  volume = {24},
  pages = {2488-2491},
  organization = {SEG},
  abstract = {We present an alternative method of extracting production related
	differences from time-lapse seismic data sets. Our method is not
	based on the actual subtraction of the two data sets, risking the
	enhancement of noise and introduction of artifacts due to local phase
	rotation and slightly misaligned events. Rather, it mutes events
	of the monitor survey with respect to the baseline survey based on
	the magnitudes of coefficients in a sparse and local atomic decomposition.
	Our technique is demonstrated to be an effective tool for enhancing
	the time-lapse signal from surveys which have been cross-equalized.
	{\copyright}2005 Society of Exploration Geophysicists},
  keywords = {SLIM},
  optdoi = {10.1190/1.2148227},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Beyreuther05SEGcot/Beyreuther05SEGcot.pdf },
  url2 = {http://dx.doi.org/10.1190/1.2148227}
}

@CONFERENCE{beyreuther2004EAGEcdo,
  author = {Moritz Beyreuther and Felix J. Herrmann and Jamin Cristall},
  title = {Curvelet denoising of {4-D} seismic},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2004},
  organization = {EAGE},
  abstract = {With burgeoning world demand and a limited rate of discovery of new
	reserves, there is increasing impetus upon the industry to optimize
	recovery from already existing fields. 4D, or time-lapse, seismic
	imaging is an emerging technology that holds great promise to better
	monitor and optimise reservoir production. The basic idea behind
	4D seismic is that when multiple 3D surveys are acquired at separate
	calendar times over a producing field, the reservoir geology will
	not change from survey to survey but the state of the reservoir fluids
	will change. Thus, taking the difference between two 3D surveys should
	remove the static geologic contribution to the data and isolate the
	time- varying fluid flow component. However, a major challenge in
	4D seismic is that acquisition and processing differences between
	3D surveys often overshadow the changes caused by fluid flow. This
	problem is compounded when 4D effects are sought to be derived from
	vintage 3D data sets that were not originally acquired with 4D in
	mind. The goal of this study is to remove the acquisition and imaging
	artefacts from a 4D seismic difference cube using Curveket processing
	techniques.},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {06/2004},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Beyreuther04EAGEcdo/Beyreuther04EAGEcdo_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Beyreuther04EAGEcdo/beyreuther2004EAGEcdo_paper.pdf},
  url2 = {https://circle.ubc.ca/bitstream/handle/2429/453/EAGE4D2004.pdf?sequence=1}
}

@CONFERENCE{herrmann2012SEGfwi,
  author = {Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv
	Kumar and Tristan van Leeuwen and Xiang Li and Brendan Smithyman
	and Eric Takam Takougang and Haneet Wason and Felix J. Herrmann},
  title = {FWI from the West Coasts: lessons learned from "Gulf of Mexico Imaging
	Challenges: What Can Full Waveform Inversion Achieve?"},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  organization = {SEG},
  keywords = {workshop, fwi, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/herrmann2012SEGfwi/herrmann2012SEGfwi_pres.pdf}
}

@CONFERENCE{cristall2004CSEGcpa,
  author = {Jamin Cristall and Moritz Beyreuther and Felix J. Herrmann},
  title = {Curvelet processing and imaging: {4-D} adaptive subtraction},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2004},
  organization = {CSEG},
  abstract = {With burgeoning world demand and a limited rate of discovery of new
	reserves, there is increasing impetus upon the industry to optimize
	recovery from already existing fields. 4D, or time-lapse, seismic
	imaging holds great promise to better monitor and optimise reservoir
	production. The basic idea behind 4D seismic is that when multiple
	3D surveys are acquired at separate calendar times over a producing
	field, the reservoir geology will not change from survey to survey
	but the state of the reservoir fluids will change. Thus, taking the
	difference between two 3D surveys should remove the static geologic
	contribution to the data and isolate the time-varying fluid flow
	component. However, a major challenge in 4D seismic is that acquisition
	and processing differences between 3D surveys often overshadow the
	changes caused by fluid flow. This problem is compounded when 4D
	effects are sought to be derived from legacy 3D data sets that were
	not originally acquired with 4D in mind. The goal of this study is
	to remove the acquisition and imaging artefacts from a 4D seismic
	difference cube using Curvelet processing techniques.},
  keywords = {SLIM},
  optmonth = {May},
  url = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/059S0201-Cristall_J_Curvelet_4D.pdf}
}

@CONFERENCE{dasilva2012EAGEprobingprecond,
  author = {Curt {Da Silva} and Felix J. Herrmann},
  title = {Matrix Probing and Simultaneous Sources: A New Approach for Preconditioning
	the Hessian},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {Recent advances based on the mathematical understanding of the Hessian
	as, under certain conditions, a pseudo-differential operator have
	resulted in a new preconditioner by L. Demanet et al. Basing their
	approach on a suitable basis expansion for the Hessian, by suitably
	'probing' the Hessian, i.e. applying the Hessian to a small number
	of randomized model perturbations, one can obtain an approximation
	to the inverse Hessian in an efficient manner. Building upon this
	approach, we consider this preconditioner in the context of least-squares
	migration and Full Waveform Inversion and specifically dimensionality
	reduction techniques in these domains. By utilizing previous work
	in simultaneous sources, we are able to develop an efficient least-squares
	migration scheme which recovers higher quality images and hence higher
	quality search directions in the context of a Gauss-Newton method
	for Full Waveform Inversion while simultaneously avoiding inordinate
	amounts of additional work.},
  keywords = {EAGE, matrix probing, pseudo-differential operator},
  optmonth = {01/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond.pdf}
}

@CONFERENCE{erlangga2009EAGEmwi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Migration with implicit solvers for the time-harmonic Helmholtz equation},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2009},
  organization = {EAGE},
  abstract = {From the measured seismic data, the location and the amplitude of
	reflectors can be determined via a migration algorithm. Classically,
	following Claerbout{\textquoteright}s imaging principle [2], a reflector
	is located at the position where the source{\textquoteright}s forward-propagated
	wavefield correlates with the backward-propagated wavefield of the
	receiver data. Lailly and Tarantola later showed that this imaging
	principle is an instance of inverse problems, with the associated
	migration operator formulated via a least-squares functional; see
	[6, 12, 13]. Furthermore, they showed that the migrated image is
	associated with the gradient of this functional with respect to the
	image. If the solution of the least-squares functional is done iteratively,
	the correlation-based image coincides up to a constant with the first
	iteration of a gradient method. In practice, this migration is done
	either in the time domain or in the frequency domain. In the frequency-domain
	migration, the main bottleneck thus far, which renders its full implementation
	to large scale problems, is the lack of efficient solvers for computing
	wavefields. Robust direct methods easily run into excessive memory
	requirements as the size of the problem increases. On the other hand,
	iterative methods, which are less demanding in terms of memory, suffered
	from lack of convergence. During the past years, however, progress
	has been made in the development of an efficient iterative method
	[4, 3] for the frequency-domain wavefield computations. In this paper,
	we will show the significance of this method (called MKMG) in the
	context of the frequency-domain migration, where multi-shot-frequency
	wavefields (of order of 10,000 related wavefields) need to be computed.},
  keywords = {Presentation,EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Erlangga09EAGEmwi/Erlangga09EAGEmwi_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Erlangga09EAGEmwi/Erlangga09EAGEmwi.pdf}
}

@CONFERENCE{erlangga2009SEGfwi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Full-waveform Inversion with Gauss-Newton-Krylov Method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  abstract = {This abstract discusses an implicit implementation of the Gauss-Newton
	method, used for the frequency-domain full-waveform inversion, where
	the inverse of the Hessian for the update is never formed explicitly.
	Instead, the inverse of the Hessian is computed approximately by
	a conjugate gradient (CG) method, which only requires the action
	of the Hessian on the CG search direction. This procedure avoids
	an excessive computer storage, usually needed for storing the Hessian,
	at the expense of extra computational work in CG. An effective preconditioner
	for the Hessian is important to improve the convergence of CG, and
	hence to reduce the overall computational work.},
  keywords = {Presentation, SEG, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga2009SEGfwi/erlangga2009SEGfwi.pdf}
}

@CONFERENCE{erlangga2009SEGswi,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {Seismic waveform inversion with Gauss-Newton-Krylov method},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {2357-2361},
  organization = {SEG},
  abstract = {This abstract discusses an implicit implementation of the Gauss-Newton
	method, used for the frequency-domain full-waveform inversion, where
	the inverse of the Hessian for the update is never formed explicitly.
	Instead, the inverse of the Hessian is computed approximately by
	a conjugate gradient (CG) method, which only requires the action
	of the Hessian on the CG search direction. This procedure avoids
	an excessive computer storage, usually needed for storing the Hessian,
	at the expense of extra computational work in CG. An effective preconditioner
	for the Hessian is important to improve the convergence of CG, and
	hence to reduce the overall computational work.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255332},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi.pdf}
}

@CONFERENCE{erlangga2008SEGaim,
  author = {Yogi A. Erlangga and Felix J. Herrmann},
  title = {An iterative multilevel method for computing wavefields in frequency-domain
	seismic inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {1957-1960},
  organization = {SEG},
  abstract = {We describe an iterative multilevel method for solving linear systems
	representing forward modeling and back propagation of wavefields
	in frequency-domain seismic inversions. The workhorse of the method
	is the so-called multilevel Krylov method, applied to a multigrid-preconditioned
	linear system, and is called multigrid-multilevel Krylov (MKMG) method.
	Numerical experiments are presented for 2D Marmousi synthetic model
	for a range of frequencies. The convergence of the method is fast,
	and depends only mildly on frequency. The method can be considered
	as the first viable alternative to LU factorization, which is practically
	prohibitive for 3D seismic inversions.},
  keywords = {Presentation,SLIM,SEG},
  optdoi = {10.1190/1.3059279},
  optmonth = {11/2008},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim_pres.pdf
	},
  url = { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim.pdf }
}

@CONFERENCE{eso2008SEGira,
  author = {R. A. Eso and S. Napier and Felix J. Herrmann and D. W. Oldenburg},
  title = {Iterative reconstruction algorithm for non-linear operators},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {579-583},
  organization = {SEG},
  abstract = {Iterative soft thresholding of a models wavelet coefficients can be
	used to obtain models that are sparse with respect to a known basis
	function. We generate sparse models for non-linear forward operators
	by applying the soft thresholding operator to the model obtained
	through a Gauss-Newton iteration and apply the technique in a synthetic
	2.5D DC resistivity crosswell tomographic example.},
  keywords = {SLIM, SEG},
  optdoi = {10.1190/1.3063719},
  optmonth = {11/2008},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/eso08SEGira/eso08SEGira.pdf }
}

@CONFERENCE{fomel2007ICASSPrepro,
  author = {Sergey Fomel and Gilles Hennenfent},
  title = {Reproducible computational experiments using scons},
  booktitle = {ICASSP},
  year = {2007},
  organization = {ICASSP},
  abstract = {SCons (from Software Construction) is a well-known open- source program
	designed primarily for building software. In this paper, we describe
	our method of extending SCons for managing data processing flows
	and reproducible computational experiments. We demonstrate our usage
	of SCons with a simple example.},
  keywords = {ICASSP},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2007/fomel07ICASSPrepro/fomel07ICASSPrepro.pdf }
}

@CONFERENCE{friedlander2009NUalssr,
  author = {Michael P. Friedlander},
  title = {Algorithms for large-scale sparse reconstruction},
  booktitle = {IEMS},
  year = {2009},
  address = {Northwestern University},
  organization = {IEMS Colloquim Speaker},
  keywords = {minimization, SLIM}
}

@CONFERENCE{friedlander2009VIETcsgpa,
  author = {Michael P. Friedlander},
  title = {Computing sparse and group-sparse approximations},
  booktitle = {VIET},
  year = {2009},
  address = {Hanoi, Vietnam},
  organization = {2009 High Performance Scientific Computing Conference},
  keywords = {minimization, SLIM}
}

@CONFERENCE{friedlander2008SIAMasa,
  author = {Michael P. Friedlander},
  title = {Active-set Approaches to Basis Pursuit Denoising},
  booktitle = {SIAM Optimization},
  year = {2008},
  organization = {SIAM Optimization},
  file = {http//www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  keywords = {Presentation, SLIM},
  optmonth = {05/2008},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}

@CONFERENCE{friedlander2008SINBADafl,
  author = {Michael P. Friedlander},
  title = {Algorithms for Large-Scale Sparse Reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Many signal processing applications seek to approximate a signal as
	a linear combination of only a few elementary atoms drawn from a
	large collection. This is known as sparse reconstruction, and the
	theory of compressed sensing allows us to pose it as a structured
	convex optimization problem. I will discuss the role of duality in
	revealing some unexpected and useful properties of these problems,
	and will show how they can lead to practical, large-scale algorithms.
	I will also describe some applications of these algorithms.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/friedlander2008SINBADafl/friedlander2008SINBADafl.pdf}
}

@CONFERENCE{friedlander2008WCOMasm,
  author = {Michael P. Friedlander and M. A. Saunders},
  title = {Active-set methods for basis pursuit},
  booktitle = {WCOM},
  year = {2008},
  organization = {West Coast Opitmization Meeting (WCOM)},
  abstract = {Many imaging and compressed sensing applications seek sparse solutions
	to large under-determined least-squares problems. The basis pursuit
	(BP) approach minimizes the 1-norm of the solution, and the BP denoising
	(BPDN) approach balances it against the least-squares fit. The duals
	of these problems are conventional linear and quadratic programs.
	We introduce a modified parameterization of the BPDN problem and
	explore the effectiveness of active-set methods for solving its dual.
	Our basic algorithm for the BP dual unifies several existing algorithms
	and is applicable to large-scale examples.},
  file = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  optmonth = {07/2008},
  url = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}

@CONFERENCE{frijlink2010EAGEcos,
  author = {M.O. Frijlink and Reza Shahidi and Felix J. Herrmann and R.G. van
	Borselen},
  title = {Comparison of Standard Adaptive Subtraction and Primary-multiple
	Separation in the Curvelet Domain},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2010},
  organization = {EAGE},
  abstract = {In recent years, data-driven multiple prediction methods and wavefield
	extrapolation methods have proven to be powerful methods to attenuate
	multiples from data acquired in complex 3-D geologic environments.
	These methods make use of a two-stage approach, where first the multiples
	(surface-related and / or internal) multiples are predicted before
	they are subtracted from the original input data in an adaptively.
	The quality of these predicted multiples often raises high expectations
	for the adaptive subtraction techniques, but for various reasons
	these expectations are not always met in practice. Standard adaptive
	subtraction methods use the well-known minimum energy criterion,
	stating that the total energy after optimal multiple attenuation
	should be minimal. When primaries and multiples interfere , the minimum
	energy criterion is no longer appropriate. Also, when multiples of
	different orders interfere, adaptive energy minimization will lead
	to a compromise between different amplitudes corrections for the
	different orders of multiples. This paper investigates the performance
	of two multiple subtraction schemes for a real data set that exhibits
	both interference problems. Results from an adaptive subtraction
	in the real curvelet domain, separating primaries and multiples,
	are compared to those obtained using a more conventional adaptive
	subtraction method in the spatial domain.},
  keywords = {EAGE},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/frijlink10EAGEcos/frijlink10EAGEcos.pdf}
}

@CONFERENCE{hennenfent2008SINBADnii2,
  author = {Gilles Hennenfent},
  title = {New insights into one-norm solvers from the Pareto curve},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {Several geophysical ill-posed inverse problems are successfully solved
	by promoting sparsity using one-norm regularization. The practicality
	of this approach depends on the effectiveness of the one-norm solver
	used and on its robustness under limited number of iterations. We
	propose an approach to understand the behavior and evaluate the performance
	of one-norm solvers. The technique consists of tracking on a graph
	the data misfit versus the one norm of successive iterates. By comparing
	the solution paths to the Pareto curve, we are able to assess the
	performance of the solvers and the quality of the solutions. Such
	an assessment is particularly relevant given the renewed interest
	in one-norm regularization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/hennenfent2008SINBADnii2/hennenfent2008SINBADnii2.pdf}
}

@CONFERENCE{hennenfent2008SINBADsdw2,
  author = {Gilles Hennenfent},
  title = {Simply denoise: wavefield reconstruction via jittered undersampling},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a new discrete undersampling scheme designed to favor wavefield
	reconstruction by sparsity-promoting inversion with transform elements
	that are localized in the Fourier domain. Our work is motivated by
	empirical observations in the seismic community, corroborated by
	recent results from compressive sampling, which indicate favorable
	(wavefield) reconstructions from random as opposed to regular undersampling.
	As predicted by theory, random undersampling renders coherent aliases
	into harmless incoherent random noise, effectively turning the interpolation
	problem into a much simpler denoising problem. A practical requirement
	of wavefield reconstruction with localized sparsifying transforms
	is the control on the maximum gap size. Unfortunately, random undersampling
	does not provide such a control and the main purpose of this paper
	is to introduce a sampling scheme, coined jittered undersampling,
	that shares the benefits of random sampling, while offering control
	on the maximum gap size. Our contribution of jittered sub-Nyquist
	sampling proofs to be key in the formulation of a versatile wavefield
	sparsity-promoting recovery scheme that follows the principles of
	compressive sampling. After studying the behavior of the jittered-undersampling
	scheme in the Fourier domain, its performance is studied for curvelet
	recovery by sparsity-promoting inversion (CRSI). Our findings on
	synthetic and real seismic data indicate an improvement of several
	decibels over recovery from regularly-undersampled data for the same
	amount of data collected.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/hennenfent2008SINBADsdw2/hennenfent2008SINBADsdw2.pdf}
}

@CONFERENCE{hennenfent07gradsem,
  author = {Gilles Hennenfent},
  title = {Reproducible research in computational (geo)sciences},
  booktitle = {Graduate seminar series},
  year = {2007},
  month = {January},
  organization = {Graduate Seminar Series},
  owner = {shruti},
  quality = {1},
  timestamp = {2013.01.16},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Misc/hennenfent07gradsem.pdf}
}

@CONFERENCE{hennenfent2007SINBADjdn,
  author = {Gilles Hennenfent},
  title = {Just denoise. Nonlinear recovery from randomly sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we turn the interpolation problem of coarsely-sampled
	data into a denoising problem. From this point of view, we illustrate
	the benefit of random sampling at sub-Nyquist rate over regular sampling
	at the same rate. We show that, using nonlinear sparsity-promoting
	optimization, coarse random sampling may actually lead to significantly
	better wavefield reconstruction than equivalent regularly sampled
	data.},
  keywords = {Presentation, SINBAD, SLIM}
}

@CONFERENCE{hennenfent06SINBADscons,
  author = {Gilles Hennenfent},
  title = {Basic Processing flows with SCons},
  booktitle = {SINBAD},
  year = {2006},
  organization = {SINBAD},
  quality = {1},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/hennenfent06SINBADscons/hennenfent06SINBADscons.pdf}
}

@CONFERENCE{hennenfent06SINBADssr,
  author = {Gilles Hennenfent},
  title = {A primer on stable signal recovery},
  booktitle = {SINBAD},
  year = {2006},
  organization = {SINBAD},
  quality = {1},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/hennenfent06SINBADssr/hennenfent06SINBADssr.pdf}
}

@CONFERENCE{hennenfent2006SINBADapo,
  author = {Gilles Hennenfent},
  title = {A primer on stable signal recovery},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given on the method
	of stable recovery from noisy and incomplete data. Strong recovery
	conditions that guarantee the recovery for arbitrary acquisition
	geometries will be reviewed and numerical recovery examples will
	be presented.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/hennenfent2006SINBADapo/hennenfent2006SINBADapo.pdf}
}

@CONFERENCE{hennenfent2006SINBADros,
  author = {Gilles Hennenfent},
  title = {Recovery of seismic data: practical considerations},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose a method for seismic data interpolation based on 1) the
	reformulation of the problem as a stable signal recovery problem
	and 2) the fact that seismic data is sparsely represented by curvelets.
	This method does not require information on the seismic velocities.
	Most importantly, this formulation potentially leads to an explicit
	recovery condition. We also propose a large-scale problem solver
	for the l1-regularization minimization involved in the recovery and
	successfully illustrate the performance of our algorithm on 2D synthetic
	and real examples.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/hennenfent2006SINBADros/hennenfent2006SINBADros.pdf}
}

@CONFERENCE{hennenfent2006SINBADtnf,
  author = {Gilles Hennenfent},
  title = {The Nonuniform Fast Discrete Curvelet Transform (NFDCT)},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The authors present an extension of the fast discrete curvelet transform
	(FDCT) to nonuniformly sampled data. This extension not only restores
	curvelet compression rates for nonuniformly sampled data but also
	removes noise and maps the data to a regular grid.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/hennenfent2006SINBADtnf/hennenfent2006SINBADtnf.pdf}
}

@CONFERENCE{hennenfent2008SEGonri,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {One-norm regularized inversion: learning from the Pareto curve},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  organization = {SEG},
  abstract = {Geophysical inverse problems typically involve a trade off between
	data misfit and some prior. Pareto curves trace the optimal trade
	off between these two competing aims. These curves are commonly used
	in problems with two-norm priors where they are plotted on a log-log
	scale and are known as L-curves. For other priors, such as the sparsity-promoting
	one norm, Pareto curves remain rela- tively unexplored. First, we
	show how these curves provide an objective criterion to gauge how
	robust one-norm solvers are when they are limited by a maximum number
	of matrix-vector products that they can perform. Second, we use Pareto
	curves and their properties to define and compute one-norm compressibilities.
	We argue this notion is key to understand one-norm regularized inversion.
	Third, we illustrate the correlation between the one-norm compressibility
	and the perfor- mance of Fourier and curvelet reconstructions with
	sparsity promoting inversion.},
  keywords = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/hennenfent08SEGonri/hennenfent08SEGonri.pdf}
}

@CONFERENCE{hennenfent2007EAGEcrw,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet reconstruction with sparsity-promoting inversion: successes
	and challenges},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {In this overview of the recent Curvelet Reconstruction with Sparsity-promoting
	Inver- sion (CRSI) method, we present our latest 2-D and 3-D interpolation
	results on both synthetic and real datasets. We compare these results
	to interpolated data using other ex- isting methods. Finally, we
	discuss the challenges related to sparsity-promoting solvers for
	the large-scale problems the industry faces.},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {June},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw.pdf }
}

@CONFERENCE{hennenfent2007EAGEisf,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Irregular sampling: from aliasing to noise},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {Seismic data is often irregularly and/or sparsely sampled along spatial
	coordinates. We show that these acquisition geometries are not necessarily
	a source of adversity in order to accurately reconstruct adequately-sampled
	data. We use two examples to illustrate that it may actually be better
	than equivalent regularly subsampled data. This comment was already
	made in earlier works by other authors. We explain this behavior
	by two key observations. Firstly, a noise-free underdetermined problem
	can be seen as a noisy well-determined problem. Secondly, regularly
	subsampling creates strong coherent acquisition noise (aliasing)
	difficult to remove unlike the noise created by irregularly subsampling
	that is typically weaker and Gaussian-like.},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {June},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf.pdf },
  url2 = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07eage_pres.pdf
	}
}

@CONFERENCE{hennenfent2007SEGrsn,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Random sampling: New insights into the reconstruction of coarsely
	sampled wavefields},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2575-2579},
  organization = {SEG},
  abstract = {In this paper, we turn the interpolation problem of coarsely-sampled
	data into a denoising problem. From this point of view, we illustrate
	the benefit of random sampling at sub-Nyquist rate over regular sampling
	at the same rate. We show that, using nonlinear sparsity-promoting
	optimization, coarse random sampling may actually lead to significantly
	better wavefield reconstruction than equivalent regularly sampled
	data. {\copyright}2007 Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM},
  optdoi = {10.1190/1.2793002},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn.pdf }
}

@CONFERENCE{hennenfent2007SINBADrii,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Recent insights in $L_1$ solvers},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {During this talk, an overview is given on our work on norm-one solvers
	as part of the DNOISE project. Gilles will explain the ins and outs
	of our iterative thresholding solver based on log cooling while Felix
	will present the work of Michael Friedlander "A Newton root-finding
	algorithms for large-scale basis pursuit denoise". Both approaches
	involve the solution of the basis pursuit problem that seeks a minimum
	one-norm solution of an underdetermined least-squares problem. Basis
	pursuit denoise (BPDN) fits the least-squares problem only approximately,
	and a single parameter determines a curve that traces the trade-off
	between the least-squares fit and the one-norm of the solution. In
	the work of Friedlander, it is shown show that the function that
	describes this curve is convex and continuously differentiable over
	all points of interest. They describe an efficient procedure for
	evaluating this function and its derivatives. As a result, they can
	compute arbitrary points on this curve. Their method is suitable
	for large-scale problems. Only matrix-vector operations are required.
	This is joint work with Ewout van der Berg and Michael P. Friedlander},
  keywords = {Presentation, SINBAD, SLIM}
}

@CONFERENCE{hennenfent2006SEGaos,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Application of stable signal recovery to seismic data interpolation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  volume = {25},
  pages = {2797-2801},
  organization = {SEG},
  abstract = {We propose a method for seismic data interpolation based on 1) the
	reformulation of the problem as a stable signal recovery problem
	and 2) the fact that seismic data is sparsely represented by curvelets.
	This method does not require information on the seismic velocities.
	Most importantly, this formulation potentially leads to an explicit
	recovery condition. We also propose a large-scale problem solver
	for the 1-regularization minimization involved in the recovery and
	successfully illustrate the performance of our algorithm on 2D synthetic
	and real examples. {\copyright}2006 Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, curvelets, interpolation, seismic data, regularization
	minimization, iterative thresholding, amplitude, SEG, continuity,
	fast transform},
  optdoi = {10.1190/1.2370105},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos.pdf },
  url2 = {http://dx.doi.org/10.1190/1.2370105 }
}

@CONFERENCE{hennenfent2005SEGscd,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Sparseness-constrained data continuation with frames: applications
	to missing traces and aliased signals in {2/3-D}},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2005},
  volume = {24},
  pages = {2162-2165},
  organization = {SEG},
  abstract = {We present a robust iterative sparseness-constrained interpolation
	algorithm using 2-/3-D curvelet frames and Fourier-like transforms
	that exploits continuity along reflectors in seismic data. By choosing
	generic transforms, we circumvent the necessity to make parametric
	assumptions (e.g. through linear/parabolic Radon or demigration)
	regarding the shape of events in seismic data. Simulation and real
	data examples for data with moderately sized gaps demonstrate that
	our algorithm provides interpolated traces that accurately reproduce
	the wavelet shape as well as the AVO behavior. Our method also shows
	good results for de-aliasing judged by the behavior of the ($f-k$)-spectrum
	before and after regularization. {\copyright}2005 Society of Exploration
	Geophysicists},
  keywords = {Presentation, SEG, SLIM},
  optdoi = {10.1190/1.2148142},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd.pdf }
}

@CONFERENCE{hennenfent2004SEGtta,
  author = {Gilles Hennenfent and Felix J. Herrmann},
  title = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by
	curvelet and wavelet transforms},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {211-214},
  organization = {SEG},
  abstract = {We present a new method to stabilize the three-term AVO inversion
	using Curvelet and Wavelet transforms. Curvelets are basis functions
	that effectively represent otherwise smooth objects having discontinuities
	along smooth curves. The applied formalism explores them to make
	the most of the continuity along reflectors in seismic images. Combined
	with Wavelets, Curvelets are used to denoise the data by penalizing
	high frequencies and small contributions in the AVO-cube. This approach
	is based on the idea that rapid amplitude changes along the ray-parameter
	axis are most likely due to noise. The AVO-inverse problem is linearized,
	formulated and solved for all (x, z) at once. Using densities and
	velocities of the Marmousi model to define the fluctuations in the
	elastic properties, the performance of the proposed method is studied
	and compared with the smoothing along the ray-parameter direction
	only. We show that our method better approximates the true data after
	the denoising step, especially when noise level increases. {\copyright}2004
	Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM},
  optdoi = {10.1190/1.1851201},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta.pdf }
}

@CONFERENCE{hennenfent2005CSEGscs,
  author = {Gilles Hennenfent and Felix J. Herrmann and R. Neelamani},
  title = {Sparseness-constrained seismic deconvolution with curvelets},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2005},
  organization = {CSEG},
  abstract = {Continuity along reflectors in seismic images is used via Curvelet
	representation to stabilize the convolution operator inversion. The
	Curvelet transform is a new multiscale transform that provides sparse
	representations for images that comprise smooth objects separated
	by piece-wise smooth discontinuities (e.g. seismic images). Our iterative
	Curvelet-regularized deconvolution algorithm combines conjugate gradient-based
	inversion with noise regularization performed using non-linear Curvelet
	coefficient thresholding. The thresholding operation enhances the
	sparsity of Curvelet representations. We show on a synthetic example
	that our algorithm provides improved resolution and continuity along
	reflectors as well as reduced ringing effect compared to the iterative
	Wiener-based deconvolution approach.},
  keywords = {Presentation, SLIM},
  optmonth = {May},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs.pdf}
}

@CONFERENCE{hennenfent2005EAGEsdr,
  author = {Gilles Hennenfent and R. Neelamani and Felix J. Herrmann},
  title = {Seismic deconvolution revisited with curvelet frames},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2005},
  organization = {EAGE},
  abstract = {We propose an efficient iterative curvelet-regularized deconvolution
	algorithm that exploits continuity along reflectors in seismic images.
	Curvelets are a new multiscale transform that provides sparse representations
	for images (such as seismic images) that comprise smooth objects
	separated by piece-wise smooth discontinuities. Our technique combines
	conjugate gradient-based convolution operator inversion with noise
	regularization that is performed using non-linear curvelet coefficient
	shrinkage (thresholding). The shrinkage operation leverages the sparsity
	of curvelets representations. Simulations demonstrate that our algorithm
	provides improved resolution compared to the traditional Wiener-based
	deconvolution approach.},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {June},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Hennenfent05EAGEsdr/Hennenfent05EAGEsdr_poster.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Hennenfent05EAGEsdr/Hennenfent05EAGEsdr.pdf }
}

@CONFERENCE{herrmann2003SPIEmsa,
  author = {Felix J. Herrmann},
  title = {Multifractional splines: application to seismic imaging},
  booktitle = {Proceedings of SPIE Technical Conference on Wavelets: Applications
	in Signal and Image Processing X},
  year = {2003},
  editor = {Michael A. Unser and Akram Aldroubi and Andrew F. Laine},
  volume = {5207},
  pages = {240-258},
  organization = {SPIE},
  abstract = {Seismic imaging commits itself to locating singularities in the elastic
	properties of the Earth{\textquoteright}s subsurface. Using the high-frequency
	ray-Born approximation for scattering from non-intersecting smooth
	interfaces, seismic data can be represented by a generalized Radon
	transform mapping the singularities in the medium to seismic data.
	Even though seismic data are bandwidth limited, signatures of the
	singularities in the medium carry through this transform and its
	inverse and this mapping property presents us with the possibility
	to develop new imaging techniques that preserve and characterize
	the singularities from incomplete, bandwidth-limited and noisy data.
	In this paper we propose a non-adaptive Curvelet/Contourlet technique
	to image and preserve the singularities and a data-adaptive Matching
	Pursuit method to characterize these imaged singularities by Multi-fractional
	Splines. This first technique borrows from the ideas within the Wavelet-Vaguelette/Quasi-SVD
	approach. We use the almost diagonalization of the scattering operator
	to approximately compensate for (i) the coloring of the noise and
	hence facilitate estimation; (ii) the normal operator itself. Results
	of applying these techniques to seismic imaging are encouraging although
	many open fundamental questions remain.},
  keywords = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SPIE/2003/herrmann2003SPIEmsa/herrmann2003SPIEmsa.pdf}
}

@CONFERENCE{Herrmann13NIPSrse,
  author = {Felix J. Herrmann},
  title = {Randomized sampling in exploration seismology},
  booktitle = {NIPS},
  year = {2013},
  timestamp = {2013.01.09},
  url = {http://techtalks.tv/talks/randomized-sampling-in-exploration-seismology/57871/},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/NIPS/2013/Herrmann13NIPSrse/Herrmann13NIPSrse.pdf}
}

@CONFERENCE{herrmann2012EAGEpmr,
  author = {Felix J. Herrmann},
  title = {Pass on the message: recent insights in large-scale sparse recovery},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {Data collection, data processing, and imaging in exploration seismology
	increasingly hinge on large-scale sparsity promoting solvers to remove
	artifacts caused by efforts to reduce costs. We show how the inclusion
	of a “message term“ in the calculation of the residuals improves
	the convergence of these iterative solvers by breaking correlations
	that develop between the model iterate and the linear system that
	needs to be inverted. We compare this message-passing scheme to state-of-the-art
	solvers for problems in missing-trace interpolation and in dimensionality-reduced
	imaging with phase encoding.},
  keywords = {EAGE, message passing, sparse inversion},
  optmonth = {01/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr.pdf}
}

@CONFERENCE{herrmann2012SEGals,
  author = {Felix J. Herrmann},
  title = {Accelerated large-scale inversion with message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  organization = {SEG},
  abstract = {To meet current-day challenges, exploration seismology increasingly
	relies on more and more sophisticated algorithms that require multiple
	paths through all data. This requirement leads to problems because
	the size of seismic data volumes is increasing exponentially, exposing
	bottlenecks in IO and computational capability. To overcome these
	bottlenecks, we follow recent trends in machine learning and compressive
	sensing by proposing a sparsity-promoting inversion technique that
	works on small randomized subsets of data only. We boost the performance
	of this algorithm significantly by modifying a state-of-the-art l1-norm
	solver to benefit from message passing, which breaks the build up
	of correlations between model iterates and the randomized linear
	forward model. We demonstrate the performance of this algorithm on
	a toy sparse-recovery problem and on a realistic reverse-time-migration
	example with random source encoding. The improvements in speed, memory
	use, and output quality are truly remarkable.},
  keywords = {Imaging, Optimization, Compressive Sensing, SEG},
  optmonth = {04/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals.pdf}
}

@CONFERENCE{herrmann2012SSPamp,
  author = {Felix J. Herrmann},
  title = {Approximate message passing meets exploration seismology},
  booktitle = {2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
  year = {2012},
  address = {Ann Arbor, Michigan, USA},
  organization = {IEEE},
  abstract = {Data collection, data processing, and imaging in exploration seismology
	increasingly hinge on large-scale sparsity promoting solvers to remove
	artifacts caused by efforts to reduce costs. We show how the inclusion
	of a 'message term' in the calculation of the residuals improves
	the convergence of these iterative solvers by breaking correlations
	that develop between the model iterate and the linear system that
	needs to be inverted. We compare this message-passing scheme to state-of-the-art
	solvers for problems in missing-trace interpo- lation and in dimensionality-reduced
	imaging with phase en- coding.},
  keywords = {Exploration seismology, compressive sensing, transform-domain sparsity
	promotion, seismic imaging},
  optmonth = {03/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp.pdf}
}

@CONFERENCE{herrmann2012UW,
  author = {Felix J. Herrmann},
  title = {Compressive sensing and sparse recovery in exploration seismology},
  booktitle = {Talk at University of Wisconsin},
  year = {2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Wisconsin/2012/herrmann2012UW/herrmann2012UW.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Wisconsin/2012/herrmann2012UW/herrmann2012UW.pdf}
}

@CONFERENCE{herrmann11SLIMsummer2,
  author = {Felix J. Herrmann},
  title = {Lecture 2. Gene Golub SIAM Summer School July 4 - 15, 2011},
  booktitle = {SLIM},
  year = {2011},
  keywords = {Presentation},
  optmonth = {08/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SIAM/2011/herrmann11SLIMsummer2/herrmann11SLIMsummer2.pdf},
  timestamp = {2011.08.05}
}

@CONFERENCE{herrmann2011SLIMsummer1,
  author = {Felix J. Herrmann},
  title = {Gene Golub SIAM Summer School July 4 - 15, 2011},
  booktitle = {SLIM},
  year = {2011},
  keywords = {Presentation},
  optmonth = {08/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SIAM/2011/herrmann11SLIMsummer1/herrmann11SLIMsummer1.pdf},
  timestamp = {2011.08.05}
}

@CONFERENCE{herrmann2010EAGErss,
  author = {Felix J. Herrmann},
  title = {Randomized sampling strategies},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2010},
  organization = {EAGE},
  abstract = {Seismic exploration relies on the collection of massive data volumes
	that are subsequently mined for information during seismic processing.
	While this approach has been extremely successful in the past, the
	current trend towards higher quality images in increasingly complicated
	regions continues to reveal fundamental shortcomings in our workflows
	for high-dimensional data volumes. Two causes can be identified..
	First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution of our survey areas continues to increase.
	Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this curse of dimensionality. In this paper, we offer a way out
	of this situation by a deliberate randomized subsampling combined
	with structure-exploiting transform-domain sparsity promotion. Our
	approach is successful because it reduces the size of seismic data
	volumes without loss of information. As such we end up with a new
	technology where the costs of acquisition and processing are no longer
	dictated by the size of the acquisition but by the transform-domain
	sparsity of the end-product.},
  keywords = {Presentation,EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss.pdf}
}

@CONFERENCE{herrmann2010IRISsns,
  author = {Felix J. Herrmann},
  title = {Sub-Nyquist sampling and sparsity: getting more information from
	fewer samples. Presented at the IRIS Workshop},
  booktitle = {IRIS},
  year = {2010},
  abstract = {Many seismic exploration techniques rely on the collection of massive
	data volumes. While this approach has been extremely successful in
	the past, current efforts toward higher resolution images in increasingly
	complicated regions of the Earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly amongst these is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which disproportionately
	strains current acquisition and processing systems as the size and
	desired resolution of our survey areas continues to increase. In
	this presentation, we offer an alternative sampling method leveraging
	recent insights from compressive sensing towards seismic acquisition
	and processing of severely under-sampled data. The main outcome of
	this approach is a new technology where acquisition and processing
	related costs are no longer determined by overly stringent sampling
	criteria, such as Nyquist. At the heart of our approach lies randomized
	incoherent sampling that breaks subsampling related interferences
	by turning them into harmless noise, which we subsequently remove
	by promoting transform-domain sparsity. Now, costs no longer grow
	significantly with resolution and dimensionality of the survey area,
	but instead depend on transform-domain sparsity only. Our contribution
	is twofold. First, we demonstrate by means of carefully designed
	numerical experiments that compressive sensing can successfully be
	adapted to seismic exploration. Second, we show that accurate recovery
	can be accomplished for compressively sampled data volumes sizes
	that exceed the size of conventional transform-domain data volumes
	by only a small factor. Because compressive sensing combines transformation
	and encoding by a single linear encoding step, this technology is
	directly applicable to acquisition and to dimensionality reduction
	during processing. In either case, sampling, storage, and processing
	costs scale with transform-domain sparsity. Many seismic exploration
	techniques rely on the collection of massive data volumes. While
	this approach has been extremely successful in the past, current
	efforts toward higher resolution images in increasingly complicated
	regions of the Earth continue to reveal fundamental shortcomings
	in our workflows. Chiefly amongst these is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which disproportionately
	strains current acquisition and processing systems as the size and
	desired resolution of our survey areas continues to increase. In
	this presentation, we offer an alternative sampling method leveraging
	recent insights from compressive sensing towards seismic acquisition
	and processing of severely under-sampled data. The main outcome of
	this approach is a new technology where acquisition and processing
	related costs are no longer determined by overly stringent sampling
	criteria, such as Nyquist. At the heart of our approach lies randomized
	incoherent sampling that breaks subsampling related interferences
	by turning them into harmless noise, which we subsequently remove
	by promoting transform-domain sparsity. Now, costs no longer grow
	significantly with resolution and dimensionality of the survey area,
	but instead depend on transform-domain sparsity only. Our contribution
	is twofold. First, we demonstrate by means of carefully designed
	numerical experiments that compressive sensing can successfully be
	adapted to seismic exploration. Second, we show that accurate recovery
	can be accomplished for compressively sampled data volumes sizes
	that exceed the size of conventional transform-domain data volumes
	by only a small factor. Because compressive sensing combines transformation
	and encoding by a single linear encoding step, this technology is
	directly applicable to acquisition and to dimensionality reduction
	during processing. In either case, sampling, storage, and processing
	costs scale with transform-domain sparsity.},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/IRIS/2010/herrmann2010IRISsns/herrmann2010IRISsns.pdf}
}

@CONFERENCE{herrmann2010MATHIAScssr,
  author = {Felix J. Herrmann},
  title = {Compressive Sensing and Sparse Recovery in Exploration Seismology.
	Presented at MATHIAS 2010 organized by Total SA. Paris.},
  booktitle = {MATHIAS},
  year = {2010},
  abstract = {During this presentation, I will talk about how recent results from
	compressive sensing and sparse recovery can be used to solve problems
	in exploration seismology where incomplete sampling is ubiquitous.
	I will also talk about how these ideas apply to dimensionality reduction
	of full-waveform inversion by randomly phase encoded sources.},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/MATHIAS/2010/herrmann2010MATHIAScssr/herrmann2010MATHIAScssr.pdf}
}

@CONFERENCE{herrmann2010SEGerc,
  author = {Felix J. Herrmann},
  title = {Empirical recovery conditions for seismic sampling},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  abstract = {In this paper, we offer an alternative sampling method leveraging
	recent insights from compressive sensing towards seismic acquisition
	and processing for data that are traditionally considered to be undersampled.
	The main outcome of this approach is a new technology where acquisition
	and processing related costs are no longer determined by overly stringent
	sampling criteria, such as Nyquist. At the heart of our approach
	lies randomized incoherent sampling that breaks subsampling related
	interferences by turning them into harmless noise, which we subsequently
	remove by promoting transform-domain sparsity. Now, costs no longer
	grow with resolution and dimensionality of the survey area, but instead
	depend on transform-domain sparsity only. Our contribution is twofold.
	First, we demonstrate by means of carefully designed numerical experiments
	that compressive sensing can successfully be adapted to seismic acquisition.
	Second, we show that accurate recovery can be accomplished for compressively
	sampled data volumes sizes that exceed the size of conventional transform-domain
	data volumes by only a small factor. Because compressive sensing
	combines transformation and encoding by a single linear encoding
	step, this technology is directly applicable to acquisition and to
	dimensionality reduction during processing. In either case, sampling,
	storage, and processing costs scale with transform-domain sparsity.
	We illustrate this principle by means of number of case studies.},
  keywords = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/herrmann10SEGerc/herrmann10SEGerc.pdf}
}

@CONFERENCE{herrmann2009PIMScssr1,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology.
	Lecture I presented at the PIMS Summer School on Seismic Imaging.
	Seattle.},
  booktitle = {PIMS},
  year = {2009},
  abstract = {In this course, I will present how recent results from compressed
	sensing and sparse recovery apply to exploration seismology. During
	the first lecture, I will present the basic principles of compressive
	sensing; the importance of random jitter sampling and sparsifying
	transforms; and large-scale one-norm solvers. I will discuss the
	application of these techniques to missing trace interpolation. The
	second lecture will be devoted to coherent signal separation based
	on curveletdomain matched filtering and Bayesian separation with
	sparsity promotion. Applications of these techniques to the primary-multiple
	wavefield-separation problem on real data will be discussed as well.
	The third lecture will be devoted towards sparse recovery in seismic
	modeling and imaging and includes the problem of preconditioning
	the imaging operators, and the recovery from simultaneous source-acquired
	data},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/PIMS/2009/herrmann2009PIMScssr1/herrmann2009PIMScssr1.pdf}
}

@CONFERENCE{herrmann2009PIMScssr2,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology.
	Lecture II presented at the PIMS Summer School on Seismic Imaging.
	Seattle.},
  booktitle = {PIMS},
  year = {2009},
  abstract = {In this course, I will present how recent results from compressed
	sensing and sparse recovery apply to exploration seismology. During
	the first lecture, I will present the basic principles of compressive
	sensing; the importance of random jitter sampling and sparsifying
	transforms; and large-scale one-norm solvers. I will discuss the
	application of these techniques to missing trace interpolation. The
	second lecture will be devoted to coherent signal separation based
	on curveletdomain matched filtering and Bayesian separation with
	sparsity promotion. Applications of these techniques to the primary-multiple
	wavefield-separation problem on real data will be discussed as well.
	The third lecture will be devoted towards sparse recovery in seismic
	modeling and imaging and includes the problem of preconditioning
	the imaging operators, and the recovery from simultaneous source-acquired
	data.},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/PIMS/2009/herrmann2009PIMScssr2/herrmann2009PIMScssr2.pdf}
}

@CONFERENCE{herrmann2009PIMScssr3,
  author = {Felix J. Herrmann},
  title = {Compressed sensing and sparse recovery in exploration seismology.
	Lecture III presented at the PIMS Summer School on Seismic Imaging.
	Seattle.},
  booktitle = {PIMS},
  year = {2009},
  abstract = {In this course, I will present how recent results from compressed
	sensing and sparse recovery apply to exploration seismology. During
	the first lecture, I will present the basic principles of compressive
	sensing; the importance of random jitter sampling and sparsifying
	transforms; and large-scale one-norm solvers. I will discuss the
	application of these techniques to missing trace interpolation. The
	second lecture will be devoted to coherent signal separation based
	on curveletdomain matched filtering and Bayesian separation with
	sparsity promotion. Applications of these techniques to the primary-multiple
	wavefield-separation problem on real data will be discussed as well.
	The third lecture will be devoted towards sparse recovery in seismic
	modeling and imaging and includes the problem of preconditioning
	the imaging operators, and the recovery from simultaneous source-acquired
	data.},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/PIMS/2009/herrmann2009PIMScssr3/herrmann2009PIMScssr3.pdf}
}

@CONFERENCE{herrmann2009SEGcib,
  author = {Felix J. Herrmann},
  title = {Compressive imaging by wavefield inversion with group sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {2337-2341},
  organization = {SEG},
  abstract = {Migration relies on multi-dimensional correlations between source-
	and residual wavefields. These multi-dimensional correlations are
	computationally expensive because they involve operations with explicit
	and full matrices that contain both wavefields. By leveraging recent
	insights from compressive sampling, we present an alternative method
	where linear correlation-based imaging is replaced by imaging via
	multidimensional deconvolutions of compressibly sampled wavefields.
	Even though this approach goes at the expense of having to solve
	a sparsity-promotion recovery program for the image, our wavefield
	inversion approach has the advantage of reducing the system size
	in accordance to transform-domain sparsity of the image. Because
	seismic images also exhibit a focusing of the energy towards zero
	offset, the compressive-wavefield inversion itself is carried out
	using a recent extension of one-norm solver technology towards matrix-valued
	problems. These so-called hybrid $(1,\,2)$-norm solvers allow us
	to penalize pre-stack energy away from zero offset while exploiting
	joint sparsity amongst near-offset images. Contrary to earlier work
	to reduce modeling and imaging costs through random phase-encoded
	sources, our method compressively samples wavefields in model space.
	This approach has several advantages amongst which improved system-size
	reduction, and more flexibility during subsequent inversions for
	subsurface properties.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255328},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib.pdf}
}

@CONFERENCE{herrmann2009SEGrpl,
  author = {Felix J. Herrmann},
  title = {Reflector-preserved lithological upscaling},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {3466-3470},
  organization = {SEG},
  abstract = {By combining Percolation models with lithological smoothing, we arrive
	at method for upscaling rock elastic constants that preserves reflections.
	In this approach, the Percolation model predicts sharp onsets in
	the elastic moduli of sand-shale mixtures when the shales reach a
	critical volume fraction. At that point, the shale inclusions form
	a connected cluster, and the macroscopic rock properties change with
	the power-law growth of the cluster. This switch-like nonlinearity
	preserves singularities, and hence reflections, even if no sharp
	transition exists in the lithology or if they are smoothed out using
	standard upscaling procedures.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255582},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl.pdf}
}

@CONFERENCE{herrmann2009SEGsns,
  author = {Felix J. Herrmann},
  title = {Sub-Nyquist sampling and sparsity: How to get more information from
	fewer samples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {3410-3415},
  organization = {SEG},
  abstract = {Seismic exploration relies on the collection of massive data volumes
	that are subsequently mined for information during seismic processing.
	While this approach has been extremely successful in the past, the
	current trend of incessantly pushing for higher quality images in
	increasingly complicated regions of the Earth continues to reveal
	fundamental shortcomings in our workflows to handle massive high-dimensional
	data volumes. Two causes can be identified as the main culprits responsible
	for this barrier. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution of our survey areas continues to increase.
	Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this curse of dimensionality. In this paper, we offer a way out
	of this situation by a deliberate \emph{randomized} subsampling combined
	with structure-exploiting transform-domain sparsity promotion. Our
	approach is successful because it reduces the size of seismic data
	volumes without loss of information. Because of this size reduction
	both impediments are removed and we end up with a new technology
	where the costs of acquisition and processing are no longer dictated
	by the \emph{size of the acquisition} but by the transform-domain
	\emph{sparsity} of the end-product after processing.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255570},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns.pdf}
}

@CONFERENCE{herrmann2008IONcsa,
  author = {Felix J. Herrmann},
  title = {Compressive sampling: a new paradigm for seismic data acquistion
	and processing?},
  booktitle = {ION},
  year = {2008},
  abstract = {Seismic data processing and imaging are firmly rooted in the well-established
	paradigm of regular Nyquist sampling. Faced with a typical uncooperative
	environment, practitioners of seismic data acquisition make all efforts
	to comply to this theory by creating regularly-sampled seismic-data
	volumes that are suitable for Fourier-based processing flows. The
	current advent of new alternative transform domains{\textendash}-
	such as the sparsifying curvelet domain, where seismic data is decomposed
	into localized, multiscale and multidirectional plane waves{\textendash}-
	opens the possibility to change this paradigm by no longer combating
	sampling irregularity but by embracing it. During this talk, we show
	that as long as seismic data volumes permit a compressible representation{\textendash}-i.e.,
	data can be represented as a superposition of relatively few number
	of elementary waveforms{\textendash}- Nyquist sampling is unnecessary
	pessimistic. So far, nothing new, we all know from the work on Fourier-
	or other transform-based seismic-data regularization methodologies
	that wavefields can be recovered accurately from sub-Nyquist samplings
	through some sort of optimization procedure. What is new, however,
	are recent insights from the field of "compressive sampling", which
	dictate the conditions that guarantee or, at least, in practice provide
	conditions that favor sparsity-promoting recovery from sub-Nyquist
	sampling. Random sub-sampling, or to be more precise, jitter sub-sampling
	creates favorable conditions for curvelet-based recovery. We explain
	this phenomenon by arguing that this type of sampling leads to noisy
	data, hence our slogan "Simply denoise: wavefield reconstruction
	via jittered undersampling", where we bank on separating incoherent
	sub-sampling noise with curvelet-domain sparsity promotion. During
	our presentation, we introduce you to what curvelets are, why random
	jitter sampling is important and why this opens a pathway towards
	a new paradigm of curvelet-domain seismic data processing. Our claims
	will be supported by examples on synthetic and field data. This is
	joint work with Gilles Hennenfent, PhD. student at SLIM.},
  keywords = {ION, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ION/herrmann2008IONcsa/herrmann08ion_pres.pdf}
}

@CONFERENCE{herrmann2008SEGcdm3,
  author = {Felix J. Herrmann},
  title = {Curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {3643-3649},
  organization = {SEG},
  abstract = {Matching seismic wavefields lies at the heart of seismic processing
	whether one is adaptively subtracting multiples predictions or groundroll.
	In both cases, the predictions are matched to the actual to-be-separated
	wavefield components in the observed data. The success of these wavefield
	matching procedures depends on our ability to (i) control possible
	overfitting, which may lead to accidental removal of primary energy,
	(ii) handle data with nonunique dips, and (iii) apply wavefield separation
	after matching stably. In this paper, we show that the curvelet transform
	allows us to address these issues by imposing smoothness in phase
	space, by using their capability to handle conflicting dips, and
	by leveraging their ability to represent seismic data sparsely.},
  keywords = {SEG, SLIM},
  optdoi = {10.1190/1.3064089},
  optmonth = {11/2008},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/Herrmann08SEGcdm3/Herrmann08SEGcdm3.pdf }
}

@CONFERENCE{herrmann2008SEGgbu,
  author = {Felix J. Herrmann},
  title = {Seismic noise: the good, the bad, \& the ugly},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  keywords = {Presentation, SEG, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGgbu/herrmann08SEGgbu.pdf
	}
}

@CONFERENCE{herrmann2008SINBADacd2,
  author = {Felix J. Herrmann},
  title = {Adaptive curvelet-domain primary-multiple separation},
  booktitle = {SINBAD},
  year = {2008},
  organization = {SINBAD},
  note = {SINBAD 2008},
  abstract = {In many exploration areas, successful separation of primaries and
	multiples greatly determines the quality of seismic imaging. Despite
	major advances made by Surface-Related Multiple Elimination (SRME),
	amplitude errors in the predicted multiples remain a problem. When
	these errors vary for each type of multiple differently (as a function
	of offset, time and dip), these amplitude errors pose a serious challenge
	for conventional least-squares matching and for the recently introduced
	separation by curvelet-domain thresholding. We propose a data-adaptive
	method that corrects amplitude errors, which vary smoothly as a function
	of location, scale (frequency band) and angle. In that case, the
	amplitudes can be corrected by an element-wise curvelet-domain scaling
	of the predicted multiples. We show that this scaling leads to a
	successful estimation of the primaries, despite amplitude, sign,
	timing and phase errors in the predicted multiples. Our results on
	synthetic and real data show distinct improvements over conventional
	least-squares matching, in terms of better suppression of multiple
	energy and high-frequency clutter and better recovery of the estimated
	primaries.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/herrmann2008SINBADacd2/herrmann2008SINBADacd2.pdf}
}

@CONFERENCE{herrmann2008SINBADfwr,
  author = {Felix J. Herrmann},
  title = {(De)-Focused wavefield reconstructions},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/herrmann2008SINBADfwr/herrmann2008SINBADfwr.pdf}
}

@CONFERENCE{herrmann2008SINBADpsm,
  author = {Felix J. Herrmann},
  title = {Phase-space matched filtering and migration preconditioning},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {During this talk, I will report on new phase-space regularization
	functionals defined in terms of splines. This spline representation
	reduces the dimensionality of estimating our phase-space matched
	filter. We will discuss how this filter can be used in migration
	preconditioning. This is joint work with Christiaan Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/herrmann2008SINBADpsm/herrmann2008SINBADpsm.pdf}
}

@CONFERENCE{herrmann2008SINBADs2c,
  author = {Felix J. Herrmann},
  title = {SINBAD 2008 Consortium meeting},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/herrmann2008SINBADs2c/herrmann2008SINBADs2c.pdf}
}

@CONFERENCE{herrmann2007AIPsit,
  author = {Felix J. Herrmann},
  title = {Seismic inversion through operator overloading},
  booktitle = {AIP},
  year = {2007},
  abstract = {Inverse problems in (exploration) seismology are known for their large
	to very large scale. For instance, certain sparsity-promoting inversion
	techniques involve vectors that easily exceed 230 unknowns while
	seismic imaging involves the construction and application of matrix-free
	discretized operators where single matrix-vector evaluations may
	require hours, days or even weeks on large compute clusters. For
	these reasons, software development in this field has remained the
	domain of highly technical codes programmed in low-level languages
	with little eye for easy development, code reuse and integration
	with (nonlinear) programs that solve inverse problems. Following
	ideas from the Symes{\textquoteright} Rice Vector Library and Bartlett{\textquoteright}s
	C++ object-oriented interface, Thyra, and Reduction/Transformation
	operators (both part of the Trilinos software package), we developed
	a software-development environment based on overloading. This environment
	provides a pathway from in-core prototype development to out-of-core
	and MPI {\textquoteright}production{\textquoteright} code with a
	high level of code reuse. This code reuse is accomplished by integrating
	the out-of-core and MPI functionality into the dynamic object-oriented
	programming language Python. This integration is implemented through
	operator overloading and allows for the development of a coordinate-free
	solver framework that (i) promotes code reuse; (ii) analyses the
	statements in an abstract syntax tree and (iii) generates executable
	statements. In the current implementation, we developed an interface
	to generate executable statements for the out-of-core unix-pipe based
	(seismic) processing package RSF-Madagascar (rsf.sf.net). The modular
	design allows for interfaces to other seismic processing packages
	and to in-core Python packages such as numpy. So far, the implementation
	overloads linear operators and elementwise reduction/transformation
	operators. We are planning extensions towards nonlinear operators
	and integration with existing (parallel) solver frameworks such as
	Trilinos.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/AIP/2007/herrmann07AIPsit/herrmann07AIPsit.pdf
	}
}

@CONFERENCE{herrmann2007AIPssd,
  author = {Felix J. Herrmann},
  title = {Stable seismic data recovery},
  booktitle = {AIP},
  year = {2007},
  abstract = {In this talk, directional frames, known as curvelets, are used to
	recover seismic data and images from noisy and incomplete data. Sparsity
	and invariance properties of curvelets are exploited to formulate
	the recovery by a {\textquoteleft}1-norm promoting program. It is
	shown that our data recovery approach is closely linked to the recent
	theory of {\textquoteleft}{\textquoteleft}compressive sensing{\textquoteright}{\textquoteright}
	and can be seen as a first step towards a nonlinear sampling theory
	for wavefields. The second problem that will be discussed concerns
	the recovery of the amplitudes of seismic images in clutter. There,
	the invariance of curvelets is used to approximately invert the Gramm
	operator of seismic imaging. In the high-frequency limit, this Gramm
	matrix corresponds to a pseudo-differential operator, which is near
	diagonal in the curvelet domain.In this talk, directional frames,
	known as curvelets, are used to recover seismic data and images from
	noisy and incomplete data. Sparsity and invariance properties of
	curvelets are exploited to formulate the recovery by a l1-norm promoting
	program. It is shown that our data recovery approach is closely linked
	to the recent theory of {\textquoteleft}{\textquoteleft}compressive
	sensing{\textquoteright}{\textquoteright} and can be seen as a first
	step towards a nonlinear sampling theory for wavefields. The second
	problem that will be discussed concerns the recovery of the amplitudes
	of seismic images in clutter. There, the invariance of curvelets
	is used to approximately invert the Gramm operator of seismic imaging.
	In the high-frequency limit, this Gramm matrix corresponds to a pseudo-differential
	operator, which is near diagonal in the curvelet domain.},
  keywords = {AIP, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/AIP/2007/herrmann07AIPssd/herrmann07AIPssd.pdf
	}
}

@CONFERENCE{herrmann2007AMScsi,
  author = {Felix J. Herrmann},
  title = {Compressive seismic imaging},
  booktitle = {AMS Von Neumann},
  year = {2007},
  abstract = {Seismic imaging involves the solution of an inverse-scattering problem
	during which the energy of (extremely) large data volumes is collapsed
	onto the Earth's reflectors. We show how the ideas from "compressive
	sampling" can alleviate this task by exploiting the curvelet transform's
	"wavefront-set detection" capability and "invariance" property under
	wave propagation. First, a wavelet-vaguellete technique is reviewed,
	where seismic amplitudes are recovered from complete data by diagonalizing
	the Gramm matrix of the linearized scattering problem. Next, we show
	how the recovery of seismic wavefields from incomplete data can be
	cast into a compressive sampling problem, followed by a proposal
	to compress wavefield extrapolation operators via compressive sampling
	in the modal domain. During the latter approach, we explicitly exploit
	the mutual incoherence between the eigenfunctions of the Helmholtz
	operator and the curvelet frame elements that compress the extrapolated
	wavefield. This is joint work with Gilles Hennenfent, Peyman Moghaddam,
	Tim Lin, Chris Stolk and Deli Wang.},
  keywords = {AMS Von Neumann, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/vonNeuman/2007/herrmann07AMScsi/herrmann07AMScsi_pres.pdf
	}
}

@CONFERENCE{herrmann2007COIPpti,
  author = {Felix J. Herrmann},
  title = {Phase transitions in explorations seismology: statistical mechanics
	meets information theory},
  booktitle = {COIP},
  year = {2007},
  abstract = {In this paper, two different applications of phase transitions to
	exploration seismology will be discussed. The first application concerns
	a phase diagram ruling the recovery conditions for seismic data volumes
	from incomplete and noisy data while the second phase transition
	describes the behavior of bi-compositional mixtures as a function
	of the volume fraction. In both cases, the phase transitions are
	the result of randomness in large system of equations in combination
	with nonlinearity. The seismic recovery problem from incomplete data
	involves the inversion of a rectangular matrix. Recent results from
	the field of "compressive sensing" provide the conditions for a successful
	recovery of functions that are sparse in some basis (wavelet) or
	frame (curvelet) representation, by means of a sparsity ($\ell_1$-norm)
	promoting nonlinear program. The conditions for a successful recovery
	depend on a certain randomness of the matrix and on two parameters
	that express the matrix{\textquoteright} aspect ratio and the ratio
	of the number of nonzero entries in the coefficient vector for the
	sparse signal representation over the number of measurements. It
	appears that the ensemble average for the success rate for the recovery
	of the sparse transformed data vector by a nonlinear sparsity promoting
	program, can be described by a phase transition, demarcating the
	regions for the two ratios for which recovery of the sparse entries
	is likely to be successful or likely to fail. Consistent with other
	phase transition phenomena, the larger the system the sharper the
	transition. The randomness in this example is related to the construction
	of the matrix, which for the recovery of spike trains corresponds
	to the randomly restricted Fourier matrix. It is shown, that these
	ideas can be extended to the curvelet recovery by sparsity-promoting
	inversion (CRSI) . The second application of phase transitions in
	exploration seismology concerns the upscaling problem. To counter
	the intrinsic smoothing of singularities by conventional equivalent
	medium upscaling theory, a percolation-based nonlinear switch model
	is proposed. In this model, the transport properties of bi-compositional
	mixture models for rocks undergo a sudden change in the macroscopic
	transport properties as soon as the volume fraction of the stronger
	material reaches a critical point. At this critical point, the stronger
	material forms a connected cluster, which leads to the creation of
	a cusp-like singularity in the elastic moduli, which in turn give
	rise to specular reflections. In this model, the reflectivity is
	no longer explicitly due to singularities in the rocks composition.
	Instead, singularities are created whenever the volume fraction exceeds
	the critical point. We will show that this concept can be used for
	a singularity-preserved lithological upscaling.},
  keywords = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/COIP/2007/herrmann07COIPpti/herrmann07COIPpti_pres.pdf
	}
}

@CONFERENCE{herrmann2007CYBERsmc,
  author = {Felix J. Herrmann},
  title = {Seismology meets compressive sampling presented at the joint NSF-IPAM
	meeting. Los Angeles. October, 2007.},
  booktitle = {Cyber},
  year = {2007},
  abstract = {Presented at Cyber-Enabled Discovery and Innovation: Knowledge Extraction
	as a success story lecture. See for more detail https://www.ipam.ucla.edu/programs/cdi2007/},
  keywords = {Cyber, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Cyber/2007/herrmann07CYBERsmc/herrmann07CYBERsmc.pdf
	}
}

@CONFERENCE{herrmann2007EAGErdi,
  author = {Felix J. Herrmann},
  title = {Recent developments in curvelet-based seismic processing},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {Combinations of parsimonious signal representations with nonlinear
	sparsity promoting programs hold the key to the next-generation of
	seismic data processing algorithms ... Since they allow for a formulation
	that is stable w.r.t. noise \& incomplete data do not require prior
	information on the velocity or locations and dips of the events},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {June},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi.pdf }
}

@CONFERENCE{herrmann2007EAGEsrm,
  author = {Felix J. Herrmann},
  title = {Surface related multiple prediction from incomplete data},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {Incomplete data, unknown source-receiver signatures and free-surface
	reflectivity represent challenges for a successful prediction and
	subsequent removal of multiples. In this paper, a new method will
	be represented that tackles these challenges by combining what we
	know about wavefield (de-)focussing, by weighted convolutions/correlations,
	and recently developed curvelet-based recovery by sparsity-promoting
	inversion (CRSI). With this combination, we are able to leverage
	recent insights from wave physics to- wards a nonlinear formulation
	for the multiple-prediction problem that works for incomplete data
	and without detailed knowledge on the surface effects.},
  keywords = {Presentation, SLIM, EAGE},
  optmont = {06/2007},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm.pdf}
}

@CONFERENCE{herrmann2007PIMScsm,
  author = {Felix J. Herrmann},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {PIMS},
  year = {2007},
  keywords = {PIMS, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/PIMS/2007/herrmann07PIMScsm/herrmann07PIMScsm_pres.pdf
	}
}

@CONFERENCE{herrmann2007SEGmpf,
  author = {Felix J. Herrmann},
  title = {Multiple prediction from incomplete data with the focused curvelet
	transform},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2505-2600},
  abstract = {Incomplete data represents a major challenge for a successful prediction
	and subsequent removal of multiples. In this paper, a new method
	will be represented that tackles this challenge in a two-step approach.
	During the first step, the recenly developed curvelet-based recovery
	by sparsity-promoting inversion (CRSI) is applied to the data, followed
	by a prediction of the primaries. During the second high-resolution
	step, the estimated primaries are used to improve the frequency content
	of the recovered data by combining the focal transform, defined in
	terms of the estimated primaries, with the curvelet transform. This
	focused curvelet transform leads to an improved recovery, which can
	subsequently be used as input for a second stage of multiple prediction
	and primary-multiple separation.},
  keywords = {SEG, Presentation, SLIM},
  optdoi = {10.1190/1.2792987},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf.pdf}
}

@CONFERENCE{herrmann2007SINBADcwe,
  author = {Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {An explicit algorithm for the extrapolation of one-way wavefields
	is proposed which combines recent developments in information theory
	and theoretical signal processing with the physics of wave propagation.
	Because of excessive memory requirements, explicit formulations for
	wave propagation have proven to be a challenge in 3-D. By using ideas
	from {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright},
	we are able to formulate the (inverse) wavefield extrapolation problem
	on small subsets of the data volume, thereby reducing the size of
	the operators. According to compressed sensing theory, signals can
	successfully be recovered from an incomplete set of measurements
	when the measurement basis is incoherent with the representation
	in which the wavefield is sparse. In this new approach, the eigenfunctions
	of the Helmholtz operator are recognized as a basis that is incoherent
	with curvelets that are known to compress seismic wavefields. By
	casting the wavefield extrapolation problem in this framework, wavefields
	can successfully be extrapolated in the modal domain via a computationally
	cheaper operation. A proof of principle for the {\textquoteleft}{\textquoteleft}compressed
	sensing{\textquoteright}{\textquoteright} method is given for wavefield
	extrapolation in 2-D. The results show that our method is stable
	and produces identical results compared to the direct application
	of the full extrapolation operator. This is joint work with Tim Lin.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2007/herrmann2007SINBADcwe/herrmann2007SINBADcwe.pdf}
}

@CONFERENCE{herrmann2007SINBADfrw,
  author = {Felix J. Herrmann},
  title = {Focused recovery with the curvelet transform},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Incomplete data represents a major challenge for a successful prediction
	and subsequent removal of multiples. In this paper, a new method
	will be represented that tackles this challenge in a two-step approach.
	During the first step, the recently developed curvelet-based recovery
	by sparsity-promoting inversion (CRSI) is applied to the data, followed
	by a prediction of the primaries. During the second high-resolution
	step, the estimated primaries are used to improve the frequency content
	of the recovered data by combining the focal transform, defined in
	terms of the estimated primaries, with the curvelet transform. This
	focused curvelet transform leads to an improved recovery, which can
	subsequently be used as input for a second stage of multiple prediction
	and primary-multiple separation. This is joint work with Deli Wang
	and Gilles Hennenfent.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/sites/data/Papers/Herrmann2007SINBADfoc.pdf
	}
}

@CONFERENCE{herrmann2007SINBADrdi2,
  author = {Felix J. Herrmann},
  title = {Recent developments in primary-multiple separation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we present a novel primary-multiple separation scheme
	which makes use of the sparsity of both primaries and multiples in
	a transform domain, such as the curvelet transform, to provide estimates
	of each. The proposed algorithm utilizes seismic data as well as
	the output of a preliminary step that provides (possibly) erroneous
	predictions of the multiples. The algorithm separates the signal
	components, i.e., the primaries and multiples, by solving an optimization
	problem that assumes noisy input data and can be derived from a Bayesian
	perspective. More precisely, the optimization problem can be arrived
	at via an assumption of a weighted Laplacian distribution for the
	primary and multiple coefficients in the transform domain and of
	white Gaussian noise contaminating both the seismic data and the
	preliminary prediction of the multiples, which both serve as input
	to the algorithm. Time permitted, we will also briefly discuss a
	propasal for adaptive curvelet-domain matched filtering. This is
	joint work with Deli Wang, Rayan Saaba, {\o}zgur Yilmaz and Eric
	Verschuur.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2007/herrmann07SINBADrdi2/herrmann07SINBADrdi2_pres.pdf
	}
}

@CONFERENCE{herrmann2007SINBADsia2,
  author = {Felix J. Herrmann},
  title = {Seismic image amplitude recovery},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {In this talk, we recover the amplitude of a seismic image by approximating
	the normal (demigration-migration) operator. In this approximation,
	we make use of the property that curvelets remain invariant under
	the action of the normal operator. We propose a seismic amplitude
	recovery method that employs an eigenvalue like decomposition for
	the normal operator using curvelets as eigen-vectors. Subsequently,
	we propose an approximate nonlinear singularity-preserving solution
	to the least-squares seismic imaging problem with sparseness in the
	curvelet domain and spatial continuity constraints. Our method is
	tested with a reverse-time {\textquoteright}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation on the SEG-AA
	salt model. This is joint work with Peyman Moghaddam and Chris Stolk
	(University of Twente)},
  keywords = {Presentation, SINBAD, SLIM}
}

@CONFERENCE{herrmann2007SLIMfsd,
  author = {Felix J. Herrmann},
  title = {From seismic data to the composition of rocks: an interdisciplinary
	and multiscale approach to exploration seismology},
  booktitle = {Berkhout{\textquoteright}s valedictory address: the conceptual approach
	of understanding},
  year = {2007},
  abstract = {In this essay, a nonlinear and multidisciplinary approach is presented
	that takes seismic data to the composition of rocks. The presented
	work has deep roots in the {\textquoteleft}gedachtengoed{\textquoteright}
	(philosophy) of Delphi spearheaded by Guus Berkhout. Central themes
	are multiscale, object-orientation and a multidisciplinary approach.},
  keywords = {SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Misc/herrmann07SLIMfsd/herrmann07SLIMfsd.pdf }
}

@CONFERENCE{herrmann2006SINBADapo1,
  author = {Felix J. Herrmann},
  title = {A primer on sparsity transforms: curvelets and wave atoms},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on the different
	sparsity transforms that are used at SLIM. Emphasis will be on two
	directional and multiscale wavelet transforms, namely the curvelet
	and the recently introduced wave-atom transforms. The main properties
	of these transforms will be listed and their performance on seismic
	data will be discussed.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/hennenfent2006SINBADapo/hennenfent2006SINBADapo.pdf}
}

@CONFERENCE{herrmann2006SINBADapow,
  author = {Felix J. Herrmann},
  title = {A primer on weak conditions for stable recovery},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an introduction will be given on the method
	of stable recovery from noisy and incomplete data. Weak recovery
	conditions that guarantee the recovery for typical acquisition geometries
	will be reviewed and numerical recovery examples will be presented.
	The advantage of these weak conditions is that they are less pessimistic
	and {\textquoteleft}verifiable{\textquoteright} or very large-scale
	acquisition geometries.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/herrmann2006SINBADapow/herrmann2006SINBADapow.pdf}
}

@CONFERENCE{herrmann2006SINBADmpf,
  author = {Felix J. Herrmann},
  title = {Multiple prediction from incomplete data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/herrmann2006SINBADmpf/herrmann2006SINBADmpf.pdf}
}

@CONFERENCE{herrmann2006SINBADom,
  author = {Felix J. Herrmann},
  title = {Opening meeting},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/herrmann2006SINBADom/herrmann2006SINBADom.pdf}
}

@CONFERENCE{herrmann2006SINBADsac,
  author = {Felix J. Herrmann},
  title = {Sparsity- and continuity-promoting seismic image recovery with curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {A nonlinear singularity-preserving solution to seismic image recovery
	with sparseness and continuity constraints is proposed. The method
	explicitly explores the curvelet transform as a directional frame
	expansion that, by virtue of its sparsity on seismic images and its
	invariance under the Hessian of the linearized imaging problem, allows
	for a stable recovery of the migration amplitudes from noisy data.
	The method corresponds to a preconditioning that corrects the amplitudes
	during a post-processing step. The solution is formulated as a nonlinear
	optimization problem where sparsity in the curvelet domain as well
	as continuity along the imaged reflectors are jointly promoted. To
	enhance sparsity, the l1-norm on the curvelet coefficients is minimized
	while continuity is promoted by minimizing an anisotropic diffusion
	norm on the image. The performance of the recovery scheme is evaluated
	with {\textquoteright}wave-equation{\textquoteright} migration code
	on a synthetic dataset. This is joint work with Peyman Moghaddam.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/herrmann2006SINBADsac/herrmann2006SINBADsac.pdf}
}


@CONFERENCE{herrmann2006SINBADsra,
  author = {Felix J. Herrmann},
  title = {Stable recovery and separation of seismic data},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation an overview will be given on how seismic
	data regularization and separation problems can be cast into the
	framework of stable signal recovery. It is shown that the successful
	solution of these two problems depends on the existence of signal
	expansions that are compressible. Preliminary examples will be shown.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/herrmann2006SINBADsra/herrmann2006SINBADsra.pdf}
}

@CONFERENCE{herrmann2004CSEGcia,
  author = {Felix J. Herrmann},
  title = {Curvelet imaging and processing: an overview},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2004},
  organization = {CSEG},
  abstract = {In this paper an overview is given on the application of directional
	basis functions, known under the name Curvelets/Contourlets, to various
	aspects of seismic processing and imaging. Key conceps in the approach
	are the use of (i) directional basis functions that localize in both
	domains (e.g. space and angle); (ii) non-linear estimation, which
	corresponds to localized muting on the coefficients, possibly supplemented
	by constrained optimization (iii) invariance of the basis functions
	under the imaging operators. We will discuss applications that include
	multiple and ground roll removal; sparseness-constrained least-squares
	migration and the computation of 4-D difference cubes.},
  keywords = {Presentation, SLIM},
  optmonth = {05/2004},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia/Herrmann04CSEGcia.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia/Herrmann04CSEGcia_paper.pdf}
}

@CONFERENCE{herrmann2003SEGoiw,
  author = {Felix J. Herrmann},
  title = {"Optimal" imaging with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2003},
  volume = {22},
  pages = {997-1000},
  abstract = {In this paper we present a non-linear edge-preserving solution to
	linear inverse scattering problems based on optimal basis- function
	decompositions. Optimality of the basis functions allow us to (i)
	reduce the dimensionality of the inverse problem; (ii) devise non-linear
	thresholding operators that approximate minimax (minimize the maximal
	mean square error given the worst possible prior) and that significantly
	improve the signal-to-noise ratio on the image. We present a reformulation
	of the standard generalized least-squares formulation of the seismic
	inversion problem into a formulation based on thresh- olding, where
	the singular values, vectors and linear estimators are replaced by
	quasi-singular values, basis-functions and thresholding. To limit
	the computational burden we use a Monte-Carlo sampling method to
	compute the quasi-singular values. With the proposed method, we aim
	to significantly improve the signal-to-noise ratio (SNR) on the model
	space and hence the resolution of the seismic image. While classical
	Tikhonov-regularized methods only gain the square-root of the SNR
	on the data for the SNR on the model our method scales almost linearly.
	This significant improvement of the SNR allows us to discern events
	at high frequencies which would normally be in the noise.},
  keywords = {Presentation, SEG, SLIM},
  optdoi = {10.1190/1.1818117},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2003/Herrmann03SEGoiw/Herrmann03SEGoiw_pres.pdf}
}

@CONFERENCE{herrmann2001EAGEsas,
  author = {Felix J. Herrmann},
  title = {Scaling and seismic reflectivity: implications of scaling on {AVO}},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2001},
  organization = {EAGE},
  abstract = {AVO analysis of seismic data is based on the assumption that transitions
	in the earth consist of jump discontinuities only. Generalization
	of these transitions to more realistic transitions shows a drastic
	change in observed AVO behavior, especially for the large angles
	currently attained by increasing cable lengths. We propose a simple
	ities. After renormalization, the inverted fluctuations regain their
	relative magnitudes which, due to the scaling, may have been significantly
	distorted.},
  keywords = {SLIM},
  optmonth = {June}
}

@CONFERENCE{herrmann2011ICIAMconvexcompfwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Tristan van Leeuwen
	and Xiang Li},
  title = {FWI with sparse recovery: a convex-composite approach},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {Iterative inversion algorithms require repeated simulation of 3D time-dependent
	acoustic, elastic, or electromagnetic wave fields, extending hundreds
	of wavelengths and hundreds of periods. Also, seismic data is rich
	in information at every representable scale. Thus simulation-driven
	optimization approaches to inversion impose great demands on simulator
	efficiency and accuracy. While computer hardware advances have been
	of critical importance in bringing inversion closer to practical
	application, algorithmic advances in simulator methodology have been
	equally important. Speakers in this two-part session will address
	a variety of numerical issues arising in the wave simulation, and
	in its application to inversion. },
  date-added = {2011-07-20},
  keywords = {ICIAM,Presentation,Full-waveform inversion,Optimization},
  optmonth = {07/2011},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICIAM/2011/herrmann2011ICIAMconvexcompfwi/herrmann2011ICIAMconvexcompfwi.pdf}
}

@CONFERENCE{herrmann2011SLRAfwi,
  author = {Felix J. Herrmann and Aleksandr Y. Aravkin and Xiang Li and Tristan
	van Leeuwen},
  title = {Full Waveform Inversion with Compressive Updates},
  booktitle = {SLRA},
  year = {2011},
  organization = {Sparse and Low Rank Approximation 2011},
  abstract = {Full-waveform inversion relies on large multi-experiment data volumes.
	While improvements in acquisition and inversion have been extremely
	successful, the current push for higher quality models reveals fundamental
	shortcomings handling increasing problem sizes numerically. To address
	this fundamental issue, we propose a randomized dimensionality-reduction
	strategy motivated by recent developments in stochastic optimization
	and compressive sensing. In this formulation conventional Gauss-Newton
	iterations are replaced by dimensionality-reduced sparse recovery
	problems with source encodings.},
  keywords = {Presentation,Full-waveform inversion},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SLRA/2011/herrmann2011SLRAfwi/herrmann2011SLRAfwi.pdf}
}

@CONFERENCE{herrmann2006SINBADpms,
  author = {Felix J. Herrmann and Urs Boeniger and D. J. Verschuur},
  title = {Primary-multiple separation by curvelet frames},
  booktitle = {SINBAD 2006},
  year = {2006},
  volume = {170},
  pages = {781-799},
  organization = {Geophysical Journal International},
  abstract = {Predictive multiple suppression methods consist of two main steps:
	a prediction step, during which multiples are predicted from seismic
	data, and a primary-multiple separation step, during which the predicted
	multiples are {\textquoteright}matched{\textquoteright} with the
	true multiples in the data and subsequently removed. The last step
	is crucial in practice: an incorrect separation will cause residual
	multiple energy in the result or may lead to a distortion of the
	primaries, or both. To reduce these adverse effects, a new transformed-domain
	method is proposed where primaries and multiples are separated rather
	than matched. This separation is carried out on the basis of differences
	in the multiscale and multidirectional characteristics of these two
	signal components. Our method uses the curvelet transform, which
	maps multidimensional data volumes into almost orthogonal localized
	multidimensional prototype waveforms that vary in directional and
	spatio-temporal content. Primaries-only and multiples-only signal
	components are recovered from the total data volume by a nonlinear
	optimization scheme that is stable under noisy input data. During
	the optimization, the two signal components are separated by enhancing
	sparseness (through weighted l1-norms) in the transformed domain
	subject to fitting the observed data as the sum of the separated
	components to within a user-defined tolerance level. Whenever the
	prediction for the two signal components in the transformed domain
	correlate, the recovery is suppressed while for regions where the
	correlation is small the method seeks the sparsest set of coefficients
	that represent each signal component. Our algorithm does not seek
	a matched filter and as such it differs fundamentally from traditional
	adaptive subtraction methods. The method derives its stability from
	the sparseness obtained by a non-parametric multiscale and multidirectional
	overcomplete signal representation. This sparsity serves as prior
	information and allows for a Bayesian interpretation of our method
	during which the log-likelihood function is minimized while the two
	signal components are assumed to be given by a superposition of prototype
	waveforms, drawn independently from a probability function that is
	weighted by the predicted primaries and multiples. In this paper,
	the predictions are based on the data-driven surface-related multiple
	elimination (SRME) method. Synthetic and field data examples show
	a clean separation leading to a considerable improvement in multiple
	suppression compared to the conventional method of adaptive matched
	filtering. This improved separation translates into an improved stack.},
  keywords = {Presentation, SINBAD, SLIM},
  optdoi = {10.1111/j.1365-246X.2007.03360.x},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/herrmann2006SINBADpms/herrmann2006SINBADpms.pdf}
}

@CONFERENCE{herrmann2009EAGEcsa,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sensing applied to full-waveform inversion},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2009},
  organization = {EAGE},
  abstract = {With the recent resurgence of full-waveform inversion, the computational
	cost of solving forward modeling problems has become{\textendash}-aside
	from issues with non-uniqueness{\textendash}-one of the major impediments
	withstanding successful application of this technology to industry-size
	data volumes. To overcome this impediment, we argue that further
	improvements in this area will depend on a problem formulation with
	a computational complexity that is no longer strictly determined
	by the size of the discretization but by transform-domain sparsity
	of its solution. In this new paradigm, we bring computational costs
	in par with our ability to compress seismic data and images. This
	premise is related to two recent developments. First, there is the
	new field of compressive sensing (CS in short throughout the paper,
	Cand{\textquoteleft}es et al., 2006; Donoho, 2006){\textendash}-where
	the argument is made, and rigorously proven, that compressible signals
	can be recovered from severely sub-Nyquist sampling by solving a
	sparsity promoting program. Second, there is in the seismic community
	the recent resurgence of simultaneous-source acquisition (Beasley,
	2008; Krohn and Neelamani, 2008; Herrmann et al., 2009; Berkhout,
	2008; Neelamani et al., 2008), and continuing efforts to reduce the
	cost of seismic modeling, imaging, and inversion through phase encoding
	of simultaneous sources (Morton and Ober, 1998; Romero et al., 2000;
	Krohn and Neelamani, 2008; Herrmann et al., 2009), removal of subsets
	of angular frequencies (Sirgue and Pratt, 2004; Mulder and Plessix,
	2004; Lin et al., 2008) or plane waves (Vigh and Starr, 2008). By
	using CS principles, we remove sub-sampling interferences asocciated
	with these approaches through a combination of exploiting transform-domain
	sparsity, properties of certain sub-sampling schemes, and the existence
	of sparsity promoting solvers.},
  keywords = {Presentation,EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Herrmann09EAGEcsa/Herrmann09EAGEcsa_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Herrmann09EAGEcsa/Herrmann09EAGEcsa.pdf}
}

@CONFERENCE{herrmann2009IAPcsisa,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive seismic imaging with simultaneous acquisition presented
	at the IAP meeting, Vienna,},
  booktitle = {IAP},
  year = {2009},
  abstract = {The shear size of seismic data volumes forms one of the major impediments
	for the inversion of seismic data. Turning forward modeling and inversion
	into a compressive sensing (CS) problem - where simulated data are
	recovered from a relatively small number of independent sources -
	can effectively mitigate this high-cost impediment. Our key contribution
	lies in the design of a sub-sampling operator that commutes with
	the time-harmonic Helmholtz system. As in compressive sensing, this
	leads to a reduction of simulation cost. This reduction is commensurate
	with the transform-domain sparsity of the solution., implying that
	computational costs are no longer determined by the size of the discretization
	but by transform-domain sparsity of the solution of the CS problem
	that recovers the data. The combination of this sub-sampling strategy
	with our recent work on preconditioned implicit solvers for the time-harmonic
	Helmholtz equation provides a viable alternative to full-waveform
	inversion schemes based on explicit time-domain finite-difference
	methods.},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/AIP/2009/herrmann2009IAPcsisa/Herrmann09AIP1.pdf}
}

@CONFERENCE{herrmann2009SAMPTAcws,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive-wavefield simulations},
  booktitle = {SAMPTA},
  year = {2009},
  organization = {SAMPTA},
  abstract = {Full-waveform inversion{\textquoteright}s high demand on computational
	resources forms, along with the non-uniqueness problem, the major
	impediment withstanding its widespread use on industrial-size datasets.
	Turning modeling and inversion into a compressive sensing problem{\textendash}-where
	simulated data are recovered from a relatively small number of independent
	simultaneous sources{\textendash}-can effectively mitigate this high-cost
	impediment. The key is in showing that we can design a sub-sampling
	operator that commutes with the time-harmonic Helmholtz system. As
	in compressive sensing, this leads to a reduction in simulation cost.
	Moreover, this reduction is commensurate with the transform-domain
	sparsity of the solution, implying that computational costs are no
	longer determined by the size of the discretization but by transform-domain
	sparsity of the solution of the CS problem which forms our data.
	The combination of this sub-sampling strategy with our recent work
	on implicit solvers for the Helmholtz equation provides a viable
	alternative to full-waveform inversion schemes based on explicit
	finite-difference methods.},
  keywords = {SAMPTA},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2009/Herrmann09SAMPTAcws/Herrmann09SAMPTAcws.pdf}
}

@CONFERENCE{herrmann2008SIAMcsm,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title = {Compressive sampling meets seismic imaging},
  booktitle = {SIAM},
  year = {2008},
  abstract = {Compressive sensing has led to fundamental new insights in the recovery
	of compressible signals from sub-Nyquist samplings. It is shown how
	jittered subsampling can be used to create favorable recovery conditions.
	Applications include mitigation of incomplete acquisitions and wavefield
	computations. While the former is a direct adaptation of compressive
	sampling, the latter application represents a new way of compressing
	wavefield extrapolation operators. Operators are not diagonalized
	but are compressively sampled reducing the computational costs.},
  keywords = {Presentation, SIAM, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SIAM/2008/herrmann2008SIAMcsm/herrmann2008SIAMcsm.pdf}
}

@CONFERENCE{herrmann2008SINBADitc,
  author = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin and Cody
	R. Brown},
  title = {Introduction to compressive (wavefield) computation},
  booktitle = {SINBAD 2008},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/herrmann2008SINBADitc/herrmann2008SINBADitc.pdf}
}

@CONFERENCE{herrmann2005CSEGnld,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Non-linear data continuation with redundant frames},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2005},
  organization = {CSEG},
  abstract = {We propose an efficient iterative data interpolation method using
	continuity along reflectors in seismic images via curvelet and discrete
	cosine transforms. The curvelet transform is a new multiscale transform
	that provides sparse representations for images that comprise smooth
	objects separated by piece-wise smooth discontinuities (e.g. seismic
	images). The advantage of using curvelets is that these frames are
	sparse for high-frequency caustic-free solutions of the wave-equation.
	Since we are dealing with less than ideal data (e.g. bandwidth-limited),
	we compliment the curvelet frames with the discrete cosine transform.
	The latter is motivated by the successful data continuation with
	the discrete Fourier transform. By choosing generic basis functions
	we circumvent the necessity to make parametric assumptions (e.g.
	through linear/parabolic Radon or demigration) regarding the shape
	of events in seismic data. Synthetic and real data examples demonstrate
	that our algorithm provides interpolated traces that accurately reproduce
	the wavelet shape as well as the AVO behavior along events in shot
	gathers.},
  keywords = {Presentation, SLIM},
  optmonth = {05/2005},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnld/Herrmann05CSEGnld_pres.pdf},
  url = {http://www.cseg.ca/conventions/abstracts/2005/2005abstracts/101S0201-Herrmann_F_Non_Linear_Data_Continuation.pdf}
}

@CONFERENCE{herrmann2005EAGErcd,
  author = {Felix J. Herrmann and Gilles Hennenfent},
  title = {Robust curvelet-domain data continuation with sparseness constraints},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2005},
  organization = {EAGE},
  abstract = {A robust data interpolation method using curvelets frames is presented.
	The advantage of this method is that curvelets arguably provide an
	optimal sparse representation for solutions of wave equations with
	smooth coefficients. As such curvelets frames circum- vent {\textendash}
	besides the assumption of caustic-free data {\textendash} the necessity
	to make parametric assumptions (e.g. through linear/parabolic Radon
	or demigration) regarding the shape of events in seismic data. A
	brief sketch of the theory is provided as well as a number of examples
	on synthetic and real data.},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {06/2005},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf }
}

@CONFERENCE{herrmann2007EAGEsia,
  author = {Felix J. Herrmann and Gilles Hennenfent and Peyman P. Moghaddam},
  title = {Seismic imaging and processing with curvelets},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {In this paper, we present a nonlinear curvelet-based sparsity-promoting
	formulation for three problems in seismic processing and imaging
	namely, seismic data regularization from data with large percentages
	of traces missing; seismic amplitude recovery for sub-salt images
	obtained by reverse-time migration and primary-multiple separation,
	given an inaccurate multiple prediction. We argue why these nonlinear
	formulations are beneficial.},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {06/2007},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia.pdf }
}

@CONFERENCE{Herrmann2011BG,
  author = {Felix J. Herrmann and Tristan van Leeuwen},
  title = {SINBAD's research program},
  year = {2011},
  month = {November},
  owner = {shruti},
  quality = {1},
  timestamp = {2013.01.16},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Misc/Herrmann2011BG.pdf}
}

@CONFERENCE{herrmann2011EAGEefmsp,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Efficient least-squares migration with sparsity promotion},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2011},
  organization = {EAGE},
  abstract = {Seismic imaging relies on the collection of multi-experimental data
	volumes in combination with a sophisticated back-end to create high-fidelity
	inversion results. While significant improve- ments have been made
	in linearized inversion, the current trend of incessantly pushing
	for higher quality models in increasingly complicated regions reveals
	fundamental shortcomings in handling increasing problem sizes numerically.
	The so-called Òcurse of dimensionalityÓ is the main culprit because
	it leads to an exponential growth in the number of sources and the
	corresponding number of wavefield simulations required by Ôwave-equationÕ
	migration. We address this issue by reducing the number of sources
	by a randomized dimensionality reduction technique that combines
	recent developments in stochastic optimization and compressive sensing.
	As a result, we replace the cur- rent formulations of imaging that
	rely on all data by a sequence of smaller imaging problems that use
	the output of the previous inversion as input for the next. Empirically,
	we find speedups of at least one order-of-magnitude when each reduced
	experiment is considered theoretically as a separate compressive-sensing
	experiment.},
  keywords = {Presentation,EAGE,Imaging},
  optmonth = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/herrmann11EAGEefmsp/herrmann11EAGEefmsp_pres.pdf
	},
  timestamp = {2011-01-14},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/herrmann11EAGEefmsp/herrmann11EAGEefmsp.pdf }
}

@CONFERENCE{herrmann2010EAGErds,
  author = {Felix J. Herrmann and Xiang Li},
  title = {Randomized dimensionality reduction for full-waveform inversion},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2010},
  organization = {EAGE},
  abstract = {Full-waveform inversion relies on the collection of large multi-experiment
	data volumes in combination with a sophisticated back-end to create
	high-fidelity inversion results. While improvements in acquisition
	and inversion have been extremely successful, the current trend of
	incessantly pushing for higher quality models in increasingly complicated
	regions of the Earth continues to reveal fundamental shortcomings
	in our ability to handle the ever increasing problem size numerically.
	Two causes can be identified as the main culprits responsible for
	this barrier. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution of our survey areas continues to increase.
	Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this. In this paper, we address this situation by randomized dimensionality
	reduction, which we adapt from the field of compressive sensing.
	In this approach, we combine deliberate randomized subsampling with
	structure-exploiting transform-domain sparsity promotion. Our approach
	is successful because it reduces the size of seismic data volumes
	without loss of information. With this reduction, we compute Newton-like
	updates at the cost of roughly one gradient update for the fully-sampled
	wavefield.},
  keywords = {Presentation,EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds.pdf}
}

@CONFERENCE{herrmann2011SPIEmsp,
  author = {Felix J. Herrmann and Xiang Li and Aleksandr Y. Aravkin and Tristan
	van Leeuwen},
  title = {A modified, sparsity promoting, {G}auss-{N}ewton algorithm for seismic
	waveform inversion},
  booktitle = {Proc. SPIE},
  year = {2011},
  number = {81380V},
  abstract = {Images obtained from seismic data are used by the oil and gas industry
	for geophysical exploration. Cutting-edge methods for transforming
	the data into interpretable images are moving away from linear approximations
	and high-frequency asymptotics towards Full Waveform Inversion (FWI),
	a nonlinear data-fitting procedure based on full data modeling using
	the wave-equation. The size of the problem, the nonlinearity of the
	for- ward model, and ill-posedness of the formulation all contribute
	to a pressing need for fast algorithms and novel regularization techniques
	to speed up and improve inversion results. In this paper, we design
	a modified Gauss-Newton algorithm to solve the PDE- constrained optimization
	problem using ideas from stochastic optimization and com- pressive
	sensing. More specifically, we replace the Gauss-Newton subproblems
	by ran- domly subsampled, -$\ell_1$ regularized subproblems. This
	allows us us significantly reduce the computational cost of calculating
	the updates and exploit the compressibility of wavefields in Curvelets.
	We explain the relationships and connections between the new method
	and stochastic optimization and compressive sensing (CS), and demonstrate
	the efficacy of the new method on a large-scale synthetic seismic
	example.},
  issn = {1},
  keywords = {SLIM,Compressive Sensing,Optimization,Full-waveform inversion},
  notes = {TR-2011-05},
  optdoi = {doi:10.1117/12.893861},
  optmonth = {08/2011},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SPIE/2011/herrmann2011SPIEmsp/herrmann2011SPIEmsp.pdf}
}

@CONFERENCE{herrmann2007EAGEjda,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Just diagonalize: a curvelet-based approach to seismic amplitude
	recovery},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {In his presentation we present a nonlinear curvelet-based sparsity-promoting
	formulation for the recovery of seismic amplitudes. We show that
	the curvelet's wavefront detection capability and invariance under
	wave propagation lead to a formulation of this recovery problem that
	is stable under noise and missing data. {\copyright}2007 Society
	of Exploration Geophysicists},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {June},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2007/herrmann2007EAGEjda/herrmann2007EAGEjda_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2007/herrmann2007EAGEjda/herrmann2007EAGEjda_paper.pdf}
}

@CONFERENCE{herrmann2005CSEGnlr,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Non-linear regularization in seismic imaging},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2005},
  organization = {CSEG},
  abstract = {Two complementary solution strategies to the least-squares imaging
	problem with sparseness \& continuity continuity constraints are
	proposed. The applied formalism explores the sparseness of curvelets
	coefficients of the reflectivity and their invariance under the demigration-migration
	operator. We achieve the solution by jointly minimizing a weighted
	l1-norm on the curvelet coefficients and an anisotropic difussion
	or total variation norm on the imaged reflectivity model. The l1-norm
	exploits the sparsenss of the reflectivity in the curvelet domain
	whereas the anisotropic norm enhances the continuity along the reflections
	while removing artifacts residing in between reflectors. While the
	two optimization methods (convex versus non-convex) share the same
	type of regularization, they differ in flexibility how to handle
	additional constraints on the coefficients of the imaged reflectivity
	and in computational expense. A brief sketch of the theory is provided
	along with a number of synthetic examples.},
  keywords = {Presentation, SLIM},
  optmonth = {05/2005},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr.pdf}
}

@CONFERENCE{herrmann2004CSEGcia2,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet imaging and processing: sparseness-constrained least-squares
	migration},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2004},
  organization = {CSEG},
  abstract = {A non-linear edge-preserving solution to the least-squares migration
	problem with sparseness constraints is introduced. The applied formalism
	explores Curvelets as basis functions that, by virtue of their sparseness
	and locality, not only allow for a reduction of the dimensionality
	of the imaging problem but which also naturally lead to a non-linear
	solution with significantly improved signal-to-noise ratio. Additional
	conditions on the image are imposed by solving a constrained optimization
	problem on the estimated Curvelet coefficients initialized by thresholding.
	This optimization is designed to also restore the amplitudes by (approximately)
	inverting the normal operator, which is, like-wise to the (de)-migration
	operators, almost diagonalized by the Curvelet transform.},
  keywords = {Presentation, SLIM},
  optmonth = {May},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia2/Herrmann04CSEGcia2.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia2/Herrmann04CSEGcia2_paper.pdf}
}

@CONFERENCE{herrmann2004EAGEcdl,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet-domain least-squares migration with sparseness constraints},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2004},
  organization = {EAGE},
  abstract = {A non-linear edge-preserving solution to the least-squares migration
	problem with sparseness constraints is introduced. The applied formalism
	explores Curvelets as basis functions that, by virtue of their sparseness
	and locality, not only allow for a reduction of the dimensionality
	of the imaging problem but which also naturally lead to a non-linear
	solution with significantly improved signal-to-noise ratio. Additional
	conditions on the image are imposed by solving a constrained optimization
	problem on the estimated Curvelet coefficients initialized by thresholding.
	This optimization is designed to also restore the amplitudes by (approximately)
	inverting the normal operator, which is like-wise the (de)-migration
	operators, almost diagonalized by the Curvelet transform.},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {May},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl.pdf }
}

@CONFERENCE{herrmann2004EAGEcdp,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet-domain preconditioned 'wave-equation' depth-migration with
	sparseness and illumination constraints},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2004},
  organization = {EAGE},
  abstract = {A non-linear edge-preserving solution to the least-squares migration
	problem with sparseness constraints is introduced. The applied formalism
	explores Curvelets as basis functions that, by virtue of their sparseness
	and locality, not only allow for a reduction of the dimensionality
	of the imaging problem but which also naturally lead to a non-linear
	solution with significantly improved signal- to-noise ratio. Additional
	conditions on the image are imposed by solving a constrained optimization
	problem on the estimated Curvelet coefficients initialized by thresholding.
	This optimization is designed to also restore the amplitudes by (approximately)
	inverting the normal operator, which is like-wise the (de)-migration
	operators, almost diagonalized by the Curvelet transform.},
  keywords = {SLIM},
  optmonth = {June},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/TechReport/2004/herrmann2004EAGEcdp/herrmann2004EAGEcdp.pdf}
}

@CONFERENCE{herrmann2004SEGcbn,
  author = {Felix J. Herrmann and Peyman P. Moghaddam},
  title = {Curvelet-based non-linear adaptive subtraction with sparseness constraints},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {1977-1980},
  organization = {SEG},
  abstract = {In this paper an overview is given on the application of directional
	basis functions, known under the name Curvelets/Contourlets, to various
	aspects of seismic processing and imaging, which involve adaptive
	subtraction. Key concepts in the approach are the use of directional
	basis functions that localize in both domains (e.g. space and angle);
	non-linear estimation, which corresponds to localized muting on the
	coefficients, possibly supplemented by constrained optimization.
	We will discuss applications that include multiple, ground-roll removal
	and migration denoising. {\copyright}2004 Society of Exploration
	Geophysicists},
  keywords = {Presentation, SLIM},
  optdoi = {10.1190/1.1851181},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn.pdf }
}

@CONFERENCE{herrmann2005EAGEosf,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and R. Kirlin},
  title = {Optimization strategies for sparseness- and continuity-enhanced imaging:
	theory},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2005},
  organization = {EAGE},
  abstract = {Two complementary solution strategies to the least-squares migration
	problem with sparseness- and continuity constraints are proposed.
	The applied formalism explores the sparseness of curvelets on the
	reflectivity and their invariance under the demigration-migration
	operator. Sparseness is enhanced by (approximately) minimizing a
	(weighted) l1-norm on the curvelet coefficients. Continuity along
	imaged reflectors is brought out by minimizing the anisotropic diffusion
	or total variation norm which penalizes variations along and in between
	reflectors. A brief sketch of the theory is provided as well as a
	number of synthetic examples. Technical details on the implementation
	of the optimization strategies are deferred to an accompanying paper:
	implementation.},
  keywords = {SLIM, EAGE},
  optmonth = {June},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGEosf/Herrmann05EAGEosf.pdf}
}

@CONFERENCE{herrmann2008SEGcdm,
  author = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
  title = {Curvelet-domain matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  number = {TR-2008-6},
  organization = {SEG},
  abstract = {Matching seismic wavefields and images lies at the heart of many pre-/post-processing
	steps part of seismic imaging{\textendash}- whether one is matching
	predicted wavefield components, such as multiples, to the actual
	to-be-separated wavefield components present in the data or whether
	one is aiming to restore migration amplitudes by scaling, using an
	image-to-remigrated-image matching procedure to calculate the scaling
	coefficients. The success of these wavefield matching procedures
	depends on our ability to (i) control possible overfitting, which
	may lead to accidental removal of energy or to inaccurate image-amplitude
	corrections, (ii) handle data or images with nonunique dips, and
	(iii) apply subsequent wavefield separations or migraton amplitude
	corrections stably. In this paper, we show that the curvelet transform
	allows us to address all these issues by imposing smoothness in phase
	space, by using their capability to handle conflicting dips, and
	by leveraging their ability to represent seismic data and images
	sparsely. This latter property renders curvelet-domain sparsity promotion
	an effective prior.},
  keywords = {SLIM,Presentation, SEG},
  optmonth = {08/2008},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm.pdf }
}

@CONFERENCE{herrmann09EAGEbnrs,
  author = {Felix J. Herrmann and Gang Tang and Reza Shahidi and Gilles Hennenfent
	and Tim T.Y. Lin},
  title = {Beating Nyquist by randomized sampling. Presented at the EAGE (workshop),
	Amsterdam},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2009},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/herrmann09EAGEbnrs/herrmann09EAGEbnrs.pdf}
}

@CONFERENCE{herrmann2005EAGErcd1,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Robust curvelet-domain primary-multiple separation with sparseness
	constraints},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2005},
  organization = {EAGE},
  abstract = {A non-linear primary-multiple separation method using curvelets frames
	is presented. The advantage of this method is that curvelets arguably
	provide an optimal sparse representation for both primaries and multiples.
	As such curvelets frames are ideal candidates to separate primaries
	from multiples given inaccurate predictions for these two data components.
	The method derives its robustness regarding the presence of noise;
	errors in the prediction and missing data from the curvelet frame{\textquoteright}s
	ability (i) to represent both signal components with a limited number
	of multi-scale and directional basis functions; (ii) to separate
	the components on the basis of differences in location, orientation
	and scales and (iii) to minimize correlations between the coefficients
	of the two components. A brief sketch of the theory is provided as
	well as a number of examples on synthetic and real data.},
  keywords = {SLIM, EAGE},
  optmonth = {June},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd1/Herrmann05EAGErcd1.pdf}
}

@CONFERENCE{herrmann2004CSEGcia1,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Curvelet imaging and processing: adaptive multiple elimination},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2004},
  organization = {CSEG},
  abstract = {Predictive multiple suppression methods consist of two main steps:
	a prediction step, in which multiples are predicted from the seismic
	data, and a subtraction step, in which the predicted multiples are
	matched with the true multiples in the data. The last step appears
	crucial in practice: an incorrect adaptive subtraction method will
	cause multiples to be sub-optimally subtracted or primaries being
	distorted, or both. Therefore, we propose a new domain for separation
	of primaries and multiples via the Curvelet transform. This transform
	maps the data into almost orthogonal localized events with a directional
	and spatial-temporal component. The multiples are suppressed by thresholding
	the input data at those Curvelet components where the predicted multiples
	have large amplitudes. In this way the more traditional filtering
	of predicted multiples to fit the input data is avoided. An initial
	field data example shows a considerable improvement in multiple suppression.},
  keywords = {Presentation, SLIM},
  optmonth = {05/2004},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia1/Herrmann04CSEGcia1.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia1/Herrmann04CSEGcia1_paper.pdf}
}

@CONFERENCE{herrmann2004EAGEsop,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Separation of primaries and multiples by non-linear estimation in
	the curvelet domain},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2004},
  organization = {EAGE},
  abstract = {Predictive multiple suppression methods consist of two main steps:
	a prediction step, in which multiples are predicted from the seismic
	data, and a subtraction step, in which the predicted multiples are
	matched with the true multiples in the data. The last step appears
	crucial in practice: an incorrect adaptive subtraction method will
	cause multiples to be sub-optimally subtracted or primaries being
	distorted, or both. Therefore, we propose a new domain for separation
	of primaries and multiples via the Curvelet transform. This transform
	maps the data into almost orthogonal localized events with a directional
	and spatial-temporal component. The multiples are suppressed by thresholding
	the input data at those Curvelet components where the predicted multiples
	have large amplitudes. In this way the more traditional filtering
	of predicted multiples to fit the input data is avoided. An initial
	field data example shows a considerable improvement in multiple suppression.},
  keywords = {SLIM},
  optmonth = {June},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/herrmann2004EAGEsop/herrmann2004EAGEsop.pdf}
}

@CONFERENCE{herrmann2004SEGcdm,
  author = {Felix J. Herrmann and D. J. Verschuur},
  title = {Curvelet-domain multiple elimination with sparseness constraints},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {1333-1336},
  organization = {SEG},
  abstract = {Predictive multiple suppression methods consist of two main steps:
	a prediction step, in which multiples are predicted from the seismic
	data, and a subtraction step, in which the predicted multiples are
	matched with the true multiples in the data. The last step appears
	crucial in practice: an incorrect adaptive subtraction method will
	cause multiples to be sub-optimally subtracted or primaries being
	distorted, or both. Therefore, we propose a new domain for separation
	of primaries and multiples via the Curvelet transform. This transform
	maps the data into almost orthogonal localized events with a directional
	and spatial-temporal component. The multiples are suppressed by thresholding
	the input data at those Curvelet components where the predicted multiples
	have large amplitudes. In this way the more traditional filtering
	of predicted multiples to fit the input data is avoided. An initial
	field data example shows a considerable improvement in multiple suppression.
	{\copyright}2004 Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM},
  optdoi = {10.1190/1.1851110},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm.pdf}
}

@CONFERENCE{herrmann2008SEGswi,
  author = {Felix J. Herrmann and Deli Wang},
  title = {Seismic wavefield inversion with curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2497-2501},
  organization = {SEG},
  abstract = {Inverting seismic wavefields lies at the heart of seismic data processing
	and imaging{\textendash}- whether one is applying {\textquoteleft}{\textquoteleft}a
	poor man{\textquoteright}s inverse{\textquoteright}{\textquoteright}
	by correlating wavefields during imaging or whether one inverts wavefields
	as part of a focal transform interferrometric deconvolution or as
	part of computing the {\textquoteright}data verse{\textquoteright}.
	The success of these wavefield inversions depends on the stability
	of the inverse with respect to data imperfections such as finite
	aperture, bandwidth limitation, and missing data. In this paper,
	we show how curvelet domain sparsity promotion can be used as a suitable
	prior to invert seismic wavefields. Examples include, seismic data
	regularization with the focused curvelet-based recovery by sparsity-promoting
	inversion (fCRSI), which involves the inversion of the primary-wavefield
	operator, the prediction of multiples by inverting the adjoint of
	the primary operator, and finally the inversion of the data itself
	{\textendash}- the so-called {\textquoteright}data inverse{\textquoteright}.
	In all cases, curvelet-domain sparsity leads to a stable inversion.},
  keywords = {Presentation,SLIM},
  optdoi = {10.1190/1.3063862},
  optmonth = {11/2008},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi.pdf }
}

@CONFERENCE{herrmann2007SEGsdp,
  author = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman
	P. Moghaddam},
  title = {Seismic data processing with curvelets: a multiscale and nonlinear
	approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2220-2224},
  organization = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based sparsity-promoting
	formulation of a seismic processing flow, consisting of the following
	steps: seismic data regularization and the restoration of migration
	amplitudes. We show that the curvelet{\textquoteright}s wavefront
	detection capability and invariance under the migration-demigration
	operator lead to a formulation that is stable under noise and missing
	data. {\copyright}2007 Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  optdoi = {10.1190/1.2792927},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp.pdf }
}

@CONFERENCE{herrmann2012EAGEcsm,
  author = {Felix J. Herrmann and Haneet Wason},
  title = {Compressive sensing in marine acquisition and beyond},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {Simultaneous-source marine acquisition is an example of compressive
	sensing where acquisition with a single vessel is replaced by simultaneous
	acquisition by multiple vessels with sources that fire at randomly
	dithered times. By identifying simultaneous acquisition as compressive
	sensing, we are able to design acquisitions that favour recovery
	by sparsity promotion. Compared to conventional processing that yields
	estimates for sequential data, sparse recovery leads to significantly
	improved results for simultaneous data volumes that are collected
	in shorter times. These improvements are the result of proper design
	of the acquisition, selection of the appropriate transform domain,
	and solution of the recovery problem by sparsity promotion. During
	this talk, we will show how these design principles can be applied
	to marine acquisition and to other problems in exploration seismology
	that can benefit from compressive sensing.},
  keywords = {workshop, acquisition, marine},
  optmonth = {02/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm.pdf}
}

@CONFERENCE{herrmann2007SEGsnt,
  author = {Felix J. Herrmann and D. Wilkinson},
  title = {Seismic noise: the good, the bad and the ugly},
  booktitle = {SEG Summer Research Workshop: Seismic Noise: Origins Preventions,
	Mitigation, Utilization},
  year = {2007},
  note = {Presented at SEG Summer Research Workshop: Seismic Noise: Origins,
	Prevention, Mitigation, Utilization},
  abstract = {In this paper, we present a nonlinear curvelet-based sparsity-promoting
	formulation for three problems related to seismic noise, namely the
	{\textquoteright}good{\textquoteright}, corresponding to noise generated
	by random sampling; the {\textquoteright}bad{\textquoteright}, corresponding
	to coherent noise for which (inaccurate) predictions exist and the
	{\textquoteright}ugly{\textquoteright} for which no predictions exist.
	We will show that the compressive capabilities of curvelets on seismic
	data and images can be used to tackle these three categories of noise-related
	problems.},
  keywords = {SLIM, SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsnt/herrmann07SEGsnt.pdf }
}

@CONFERENCE{johnson2008SINBADsdi,
  author = {James Johnson and Gilles Hennenfent},
  title = {Seismic Data Interpolation with Symmetry},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Due to the physics of reciprocity seismic data sets are symmetric
	in the source and receiver coordinates. Often seismic data sets are
	incomplete and the missing data must be interpolated. Typically,
	missing traces do not occur symmetrically. The purpose of this project
	is to extend the current formulation for solving the seismic interpolation
	problems in such a way that they enforce reciprocity. The method
	decomposes the seismic data volume into symmetric and antisymmetric
	parts. This decomposition leads to an augmented system of equations
	for the L1-solver that promotes sparsity in the curvelet domain.
	Interpolation is carried out on the entire system during which the
	asymmetric component of the volume is forced to zero, while the symmetric
	part of the data volume is matched to the measured data.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/johnson2008SINBADsdi/johnson2008SINBADsdi.pdf}
}

@CONFERENCE{johnson2010EAGEeop,
  author = {James Johnson and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimation of primaries via sparse inversion with reciprocity},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2010},
  organization = {EAGE},
  abstract = {Accurate removal of surface related multiples is a key step in seismic
	data processing. The industry standard for removing multiples is
	SRME, which involves convolving the data with itself to predict the
	multiples, followed by an adaptive subtraction procedure to recover
	the primaries (Verschuur and Berkhout, 1997). Other methods involve
	multidimensional division of the up-going and down-going wavefields
	(Amundsen, 2001). However, this approach may suffer from stability
	problems. With the introduction of the {\textquoteleft}{\textquoteleft}estimation
	of primaries by sparse inversion{\textquoteright}{\textquoteright}(EPSI),
	van Groenestijn and Verschuur (2009) recentely reformulated SRME
	to jointly estimate the surface-free impulse response and the source
	signature directly from the data. The advantage of EPSI is that it
	recovers the primary response directly, and does not require a second
	processing step for the subtraction of estimated multiples from the
	original data. However, because it estimates both the primary impulse
	response and source signature from the data EPSI must be regularized.
	Motivated by recent successful application of the curvelet transform
	in seismic data processing (Herrmann et al., 2007), we formulate
	EPSI as a bi-convex optimization problem that seeks sparsity on the
	surface-free Green{\textquoteright}s function and Fourier-domain
	smoothness on the source wavelet. Our main contribution compared
	to previous work (Lin and Herrmann, 2009), and the contribution of
	that author to the proceedings of this meeting(Lin and Herrmann,
	2010), is that we employ the physical principle of as source-receiver
	reciprocity to improve the inversion.},
  keywords = {EAGE},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/johnson10EAGEeop/johnson10EAGEeop.pdf}
}

@CONFERENCE{jumah2011SEGdrepsi,
  author = {Bander Jumah and Felix J. Herrmann},
  title = {Dimensionality-reduced estimation of primaries by sparse inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  volume = {30},
  pages = {3520-3525},
  organization = {SEG},
  abstract = {Data-driven methods---such as the estimation of primaries by sparse
	inversion---suffer from the "curse of dimensionality", which leads
	to disproportional growth in computational and storage demands when
	moving to realistic 3-D field data. To re- move this fundamental
	impediment, we propose a dimensional- ity reduction technique where
	the "data matrix" is approximated adaptively by a randomized low-rank
	approximation. Com- pared to conventional methods, our approach has
	the advantage that the cost of the low-rank approximation is reduced
	significantly, which may lead to considerable reductions in storage
	and computational costs of the sparse inversion. Application of the
	proposed formalism to synthetic data shows that significant improvements
	are achievable at low computational overhead required to compute
	the low-rank approximations.},
  date-added = {2011-04-06 15:00:00 -0700},
  keywords = {Presentation,SEG,Processing},
  optdoi = {10.1190/1.3627931},
  optmonth = {04/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi.pdf}
}

@CONFERENCE{kumar2008SINBADcd,
  author = {Vishal Kumar},
  title = {Curvelet Denoising},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The separation of signal and noise is an important issue in seismic
	data processing. By noise we refer to the incoherent noise which
	is present in the data. In our case, we showed curvelets concentrate
	seismic signal energy in few significant coefficients unlike noise
	energy that is spread all over the coefficients. The sparsity of
	seismic data in the curvelet domain makes curvelets an ideal choice
	for separating the noise from the seismic data. In our approach the
	denoising problem is framed as curvelet-regularized inversion problem.
	After initial processing, we applied the algorithm to the poststack
	data and compared our results with conventional wavelet denoising.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/kumar2008SINBADcd/kumar2008SINBADcd.pdf
}
}

@CONFERENCE{kumar2008SINBADcrd,
  author = {Vishal Kumar},
  title = {Curvelet-Regularized Deconvolution},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The removal of source signature from seismic data is an important
	step in seismic data processing. The Curvelet transform provides
	sparse representations for images that comprise smooth objects separated
	by piece-wise smooth discontinuities (e.g. seismic reflectivity).
	In this approach the sparseness of reflectivity in Curvelet domain
	is used as a prior to stabilize the inversion process. Our Curvelet-regularized
	deconvolution algorithm uses recently developed SPGL1 solver which
	does adaptive sampling of the trade-off curve. We applied the algorithm
	on a synthetic example and compared our results with that of Spiky
	deconvolution approach.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/kumar08SINBADcrd/kumar08SINBADcrd_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/kumar2008SINBADcrd/kumar2008SINBADcrd.pdf}
}

@CONFERENCE{kumar2009SEGins,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Incoherent noise suppression with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {3356-3360},
  organization = {SEG},
  abstract = {The separation of signal and noise is a key issue in seismic data
	processing. By noise we refer to the incoherent noise that is present
	in the data. We use the recently introduced multiscale and multidirectional
	curvelet transform for suppression of random noise. The curvelet
	transform decomposes data into directional plane waves that are local
	in nature. The coherent features of the data occupy the large coefficients
	in the curvelet domain, whereas the incoherent noise lives in the
	small coefficients. In other words, signal and noise have minimal
	overlap in the curvelet domain. This gives us a chance to use curvelets
	to suppress noise present in data.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255557},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins.pdf}
}

@CONFERENCE{kumar2008CSEGcrs,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Curvelet-regularized seismic deconvolution},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2008},
  organization = {CSEG},
  abstract = {There is an inherent continuity along reflectors of a seismic image.
	We use the recently introduced multiscale and multidirectional curvelet
	transform to exploit this continuity along reflectors for cases in
	which the assumption of spiky reflectivity may not hold. We show
	that such type of seismic reflectivity can be represented in the
	curvelet-domain by a vector whose entries decay rapidly. This curvelet-domain
	compression of reflectivity opens new perspectives towards solving
	classical problems in seismic processing including the deconvolution
	problem. In this paper, we present a formulation that seeks curvelet-domain
	sparsity for non-spiky reflectivity and we compare our results with
	those of spiky deconvolution.},
  keywords = {Presentation,SLIM},
  optmonth = {05/2008},
  presentation = {http://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs_pres.pdf},
  url = {http://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs.pdf}
}

@CONFERENCE{kumar2008SEGdwc,
  author = {Vishal Kumar and Felix J. Herrmann},
  title = {Deconvolution with curvelet-domain sparsity},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {1996-2000},
  organization = {SEG},
  abstract = {There is an inherent continuity along reflectors of a seismic image.
	We use the recently introduced multiscale and multidirectional curvelet
	transform to exploit this continuity along reflectors for cases in
	which the assumption of spiky reflectivity may not hold. We show
	that such type of seismic reflectivity can be represented in the
	curvelet-domain by a vector whose entries decay rapidly. This curvelet-domain
	compression of reflectivity opens new perspectives towards solving
	classical problems in seismic processing including the deconvolution
	problem. In this paper, we present a formulation that seeks curvelet-domain
	sparsity for non-spiky reflectivity and we compare our results with
	those of spiky deconvolution.},
  keywords = {SLIM,Presentation, SEG},
  optdoi = {10.1190/1.3059287},
  optmonth = {11/2008},
  presentation = { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc.pdf }
}

@CONFERENCE{lebed2008SINBADaoc,
  author = {Evgeniy Lebed},
  title = {Curvelet / Surfacelet comparison},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {Curvelets and Surfacelets are two transforms that aim to achieve a
	multiscale and a multidirectional decomposition of arbitrary N-dimensional
	($N>=2$) signals. While both transforms are Fourier-based, their
	construction is intrinsically different. In this talk we will give
	and overview of the construction of the two transforms, and explore
	their properties such as frequency domain / spatial domain coherence,
	sparsity, redundancy and computational complexity.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/lebed2008SINBADaoc/lebed2008SINBADaoc.pdf}
}

@CONFERENCE{lebed2008SINBADaoc1,
  author = {Evgeniy Lebed},
  title = {Applications of Curvelets/Surfacelets to seismic data processing},
  booktitle = {SINBAD},
  year = {2008},
  abstract = {In this talk we explore several applications of the curvelet and surfacelet
	transforms to seismic data processing. The first application is stable
	signal recovery in the physical domain - seismic data acquisition
	is often limited by physical and economic constraints, and the goal
	is to interpolate the data from a given subset of seismic traces.
	The second application is signal recovery in a transform domain -
	we assume that our data comes in a form of a random subset of temporal
	frequencies and the goal is to recover the missing frequencies from
	this data. Since seismic signals are generally not bandwidth limited,
	this in fact becomes an anti-aliasing problem. In both these problems
	the recovery is resolved via a robust l_1 solver that exploits the
	sparsity of the signals in curvelet/surfacelet domains. In the last
	application we explore the problem of primary-multiple separation
	by simple thresholding.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/lebed2008SINBADaoc1/lebed2008SINBADaoc1.pdf}
}

@CONFERENCE{lebed2008SEGhggt,
  author = {Evgeniy Lebed and Felix J. Herrmann},
  title = {A hitchhiker{\textquoteright}s guide to the galaxy of transform-domain
	sparsification},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  organization = {SEG},
  abstract = {The ability to efficiently and sparsely represent seismic data is
	becoming an increasingly important problem in geophysics. Over the
	last decade many transforms such as wavelets, curvelets, contourlets,
	surfacelets, shearlets, and many other types of {\textquoteright}x-lets{\textquoteright}
	have been developed to try to resolve this issue. In this abstract
	we compare the properties of four of these commonly used transforms,
	namely the shift-invariant wavelets, complex wavelets, curvelets
	and surfacelets. We also briefly explore the performance of these
	transforms for the problem of recovering seismic wavefields from
	incomplete measurements.},
  keywords = {SEG},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lebed08SEGhggt/lebed08SEGhggt.pdf}
}

@CONFERENCE{vanleeuwen2011ICIAMcbmcwe,
  author = {Tristan van Leeuwen},
  title = {A correlation-based misfit criterion for wave-equation traveltime
	tomography. Presented at ICIAM 2011, Vancouver BC.},
  booktitle = {ICIAM},
  year = {2011},
  organization = {ICIAM 2011},
  abstract = {The inference of subsurface medium parameters from seismic data can
	be posed as a PDE-constrained data-fitting procedure. This approach
	is successful in reconstructing medium perturbations that are in
	the order of the wavelength. In practice, the data lack low frequency
	content and this means that one needs a good initial guess of the
	slowly varying component of the medium. For a wrong starting model
	an iterative reconstruction procedure is likely to end up in a local
	minimum. We propose to use a different measure of the misfit that
	makes the optimization problem well-posed in terms of the slowly
	varying velocity structures. This procedure can be seen as a generalization
	of ray-based traveltime tomography. We discuss the theoretical underpinnings
	of the method and give some numerical examples.},
  date-added = {2011-07-19},
  keywords = {Presentation,ICIAM,Imaging},
  optmonth = {07/2011},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011ICIAMcbmcwe/vanleeuwen2011ICIAMcbmcwe.pdf}
}

@CONFERENCE{vanleeuwen2011SEGext,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  volume = {30},
  pages = {4045-4050},
  organization = {SEG},
  abstract = {The prestack image volume can be defined as a cross- correlation of
	the source and receivers wavefields for non-zero space and time lags.
	If the background velocity is kinemati- cally acceptable, this image
	volume will have its main contri- butions at zero lag, even for complex
	models. Thus, it is an ideal tool for wave-equation migration velocity
	analysis in the presence of strong lateral heterogeneity. In particular,
	it allows us to pose migration velocity analysis as a PDE-constrained
	optimization problem, where the goal is to minimize the en- ergy
	in the image volume at non-zero lag subject to fitting the data approximately.
	However, it is computationally infeasi- ble to explicitly form the
	whole image volume. In this paper, we discuss several ways to reduce
	the computational costs in- volved in computing the image volume
	and evaluating the fo- cusing criterion. We reduce the costs for
	calculating the data by randomized source synthesis. We also present
	an efficient way to subsample the image volume. Finally, we propose
	an alternative optimization criterion and suggest a multiscale in-
	version strategy for wave-equation MVA.},
  keywords = {SEG,Imaging},
  optdoi = {10.1190/1.3628051},
  optmonth = {04/2011},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/vanleeuwen11SEGext/vanleeuwen11SEGext.pdf}
}

@CONFERENCE{vanleeuwen2011WAVESpeiv,
  author = {Tristan van Leeuwen and Felix J. Herrmann},
  title = {Probing the extended image volume for seismic velocity inversion},
  booktitle = {WAVES},
  year = {2011},
  organization = {Waves 2011},
  abstract = {In seismic velocity inversion one aims to reconstruct a kinematically
	correct subsurface velocity model that can be used as input for further
	processing and inversion of the data. An important tool in velocity
	inversion is the prestack image volume. This image volume can be
	defined as a cross- correlation of the source and receivers wavefields
	for non-zero space and time lags. If the background velocity is kinematically
	acceptable, this image volume will have its main contributions at
	zero lag, even for complex models. Thus, it is an ideal tool for
	wave-equation migration velocity analysis in the presence of strong
	lateral heterogeneity. In particular, it allows us to pose migration
	velocity analysis as a PDE- constrained optimization problem, where
	the goal is to minimize the energy in the image volume at non-zero
	lag subject to fitting the data approximately. However, it is computationally
	infeasible to explicitly form the whole image volume. In this paper,
	we discuss several ways to reduce the computational costs involved
	in computing the image volume and evaluating the focusing criterion.
	We reduce the costs for calculating the data by randomized source
	synthesis. We also present an efficient way to subsample the image
	volume. Finally, we propose an alternative optimization criterion
	and suggest a multiscale inversion strategy for wave-equation MVA.
	},
  date-added = {2011-07-29},
  keywords = {Presentation},
  optmonth = {07/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011WAVESpeiv/vanleeuwen2011WAVESpeiv.pdf}
}

@CONFERENCE{vanleeuwen2011EAGEhsdomwi,
  author = {Tristan van Leeuwen and Felix J. Herrmann and Mark Schmidt and Michael
	P. Friedlander},
  title = {A hybrid stochastic-deterministic optimization method for waveform
	inversion},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2011},
  organization = {EAGE},
  abstract = {Present-day high quality 3D acquisition can give us lower frequencies
	and longer offsets with which to invert. However, the computational
	costs involved in handling this data explosion are tremendous. Therefore,
	recent developments in full-waveform inversion have been geared towards
	reducing the computational costs involved. A key aspect of several
	approaches that have been proposed is a dramatic reduction in the
	number of sources used in each iteration. A reduction in the number
	of sources directly translates to less PDE-solves and hence a lower
	computational cost. Re- cent attention has been drawn towards reducing
	the sources by randomly combining the sources in to a few supershots,
	but other strategies are also possible. In all cases, the full data
	misfit, which involves all the sequential sources, is replaced by
	a reduced misfit that is much cheaper to evaluate because it involves
	only a small number of sources (batchsize). The batchsize controls
	the accuracy with which the reduced misfit approximates the full
	misfit. The optimization of such an inaccurate, or noisy, misfit
	is the topic of stochastic optimization. In this paper, we propose
	an optimization strategy that borrows ideas from the field of stochastic
	optimization. The main idea is that in the early stage of the optimization,
	far from the true model, we do not need a very accurate misfit. The
	strategy consists of gradually increasing the batchsize as the iterations
	proceed. We test the proposed strategy on a synthetic dataset. We
	achieve a very reasonable inversion result at the cost of roughly
	13 evaluations of the full misfit. We observe a speed-up of roughly
	a factor 20.},
  keywords = {Presentation,EAGE,Full-waveform inversion,Optimization},
  optmonth = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi.pdf}
}

@CONFERENCE{vanleeuwen2011SIAMGEOmawt,
  author = {Tristan van Leeuwen and Wim Mulder},
  title = {Multiscale aspects of waveform tomography},
  booktitle = {SIAMGEO},
  year = {2011},
  organization = {SIAM GeoSciences 2011},
  abstract = {We consider the inference of medium velocity from transmitted acoustic
	waves. Typically, the measurements are done in a narrow frequency
	band. As a result the sensitivity of the data with respect to velocity
	perturbations varies dramatically with the scale of the perturbation.
	{\textquoteleft}Smooth{\textquoteright} perturbations will cause
	a phase shift, whereas perturbations that vary on the wavelength-scale
	cause amplitude variations. We investigate how to incorporate this
	scale dependent behavior in the formulation of the inverse problem.},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011SIAMGEOmawt/vanleeuwen2011SIAMGEOmawt.pdf}
}

@CONFERENCE{vanleeuwen2011AMPhsdmwi,
  author = {Tristan van Leeuwen and Mark Schmidt and Michael P. Friedlander and
	Felix J. Herrmann},
  title = {A hybrid stocahstic-deterministic method for waveform inversion.
	Presented at AMP Medical and Seismic Imaging, 2011, Vancouver BC.},
  booktitle = {AMP},
  year = {2011},
  organization = {WAVES 2011},
  abstract = {A lot of seismic and medical imaging problems can be written as a
	least-squares data- fitting problem. In particular, we consider the
	case of multi-experiment data, where the data consists of a large
	number of ÔindependentÕ measurements. Solving the inverse prob-
	lem then involves repeatedly forward modeling the data for each of
	these experiments. In case the number of experiments is large and
	the modeling kernel expensive to apply, such an approach may be prohibitively
	expensive. We review techniques from stochastic opti- mization which
	aim at dramatically reducing the number of experiments that need
	to be modeled at each iteration. This reduction is typically achieved
	by randomly subsampling the data. Special care needs to be taken
	in the optimization to deal with the stochasticity that is introduced
	in this way. },
  date-added = {2011-07-15},
  keywords = {Presentation},
  optmonth = {07/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICIAM/2011/vanleeuwen2011AMPhsdmwi/vanleeuwen2011AMPhsdmwi.pdf}
}

@CONFERENCE{li2011EAGEfwirr,
  author = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix
	J. Herrmann},
  title = {Full-waveform inversion with randomized L1 recovery for the model
	updates},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2011},
  organization = {EAGE},
  abstract = {Full-waveform inversion (FWI) is a data fitting procedure that relies
	on the collection of seis- mic data volumes and sophisticated computing
	to create high-resolution results. With the advent of FWI, the improvements
	in acquisition and inversion have been substantial, but these improvements
	come at a high cost because FWI involves extremely large multi-experiment
	data volumes. The main obstacle is the Ôcurse of dimensionalityÕ
	exemplified by NyquistÕs sampling criterion, which puts a disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution increases. In this paper, we address the Ôcurse
	of dimensionalityÕ by randomized dimen- sionality reduction of the
	FWI problem adapted from the field of CS. We invert for model updates
	by replacing the Gauss-Newton linearized subproblem for subsampled
	FWI with a sparsity promoting formulation, and solve this formulation
	using the SPGl1 algorithm. We speed up the algorithm and avoid overfitting
	the data by solving for the linearized updates only approximately.
	Our approach is successful because it reduces the size of seismic
	data volumes without loss of information. With this reduction, we
	can compute a Newton-like update with the reduced data volume at
	the cost of roughly one gradient update for the fully sampled wavefield.},
  keywords = {Presentation,EAGE,Full-waveform inversion},
  optmonth = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr.pdf}
}

@CONFERENCE{li2011CSEGefimag,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Efficient full-waveform inversion with marine acquisition geometry},
  booktitle = {CSEG technical program},
  year = {2012},
  abstract = {Full-waveform inversion (FWI) is a nonlinear data fitting procedure
	based on seismic data to derive a accurate velocity model. With the
	increasing demand for high resolution images in complex geological
	settings, the importance of improvements in acquisition and inversion
	become more and more critical. However, these improvements will be
	obtained at high computational cost, as a typical marine survey contains
	thousands of shot and receiver positions, and FWI needs several passes
	through massive seismic data. Computational cost of FWI will grow
	exponentially as the size of seismic data and desired resolution
	increase. In this paper we present a modified Gauss-Newton (GN) method
	that borrows ideas from compressive sensing, where we compute the
	GN updates from a few randomly selected sequential shots. Each subproblem
	is solved by using a sparsity promoting algorithm. With this approach,
	we dramatically reduce the size and hence the computational costs
	of the problem, whilst we control information loss by redrawing a
	different set of sequential shots for each subproblem.},
  keywords = {CSEG},
  optmonth = {02/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2012/li2011CSEGefimag/li2011CSEGefimag.pdf}
}

@CONFERENCE{li2012SEGspmamp,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Sparsity-promoting migration accelerated by message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  organization = {SEG},
  abstract = {Seismic imaging via linearized inversion requires multiple iterations
	to minimize the least-squares misfit as a function of the medium
	perturbation. Unfortunately, the cost for these iterations are prohibitive
	because each iteration requires many wave-equation simulations, which
	without direct solvers require an expensive separate solve for each
	source. To overcome this problem, we use dimensionality-reduction
	to decrease the size of seismic imaging problem by turning the large
	number of sequential shots into a much small number of simultaneous
	shots. In our approach, we take advantage of sparsifying transforms
	to remove source crosstalk resulting from randomly weighting and
	stacking sequential shots into a few super shots. We also take advantage
	of the fact that the convergence of large-scale sparsity-promoting
	solvers can be improved significantly by borrowing ideas from message
	passing, which are designed to break correlation built up between
	the linear system and the model iterate. In this way, we arrive at
	a formulation where we run the sparsity-promoting solver for a relatively
	large number of very iterations. Aside from leading to a significant
	speed up, our approach had the advantage of greatly reducing the
	memory imprint and IO requirements. We demonstrate this feature by
	solving a sparsity-promoting imaging problem with operators of reverse-time
	migration, which is computationally infeasible without the dimensionality
	reduction.},
  keywords = {SEG, Imaging, Inversion},
  optmonth = {04/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp.pdf}
}

@CONFERENCE{li2010SEGfwi,
  author = {Xiang Li and Felix J. Herrmann},
  title = {Full-waveform inversion from compressively recovered model updates},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  volume = {29},
  pages = {1029-1033},
  organization = {SEG},
  abstract = {Full-waveform inversion relies on the collection of large multi-experiment
	data volumes in combination with a sophisticated back-end to create
	high-fidelity inversion results. While improvements in acquisition
	and inversion have been extremely successful, the current trend of
	incessantly pushing for higher quality models in increasingly complicated
	regions of the Earth reveals fundamental shortcomings in our ability
	to handle increasing problem size numerically. Two main culprits
	can be identified. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution increases. Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to lower our expectations to compute ourselves out
	of this. In this paper, we address this situation by randomized dimensionality
	reduction, which we adapt from the field of compressive sensing.
	In this approach, we combine deliberate randomized subsampling with
	structure-exploiting transform-domain sparsity promotion. Our approach
	is successful because it reduces the size of seismic data volumes
	without loss of information. With this reduction, we compute Newton-like
	updates at the cost of roughly one gradient update for the fully-sampled
	wavefield.},
  keywords = {Presentation,SEG,Full-waveform inversion},
  optdoi = {10.1190/1.3513022},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi.pdf}
}

@CONFERENCE{li2011SBGFmgnsu,
  author = {Xiang Li and Felix J. Herrmann and Tristan van Leeuwen and Aleksandr
	Y. Aravkin},
  title = {Modified Gauss-Newton with Sparse Updates},
  booktitle = {SBGF},
  year = {2011},
  organization = {SBGF},
  abstract = {Full-waveform inversion (FWI) is a data fitting procedure that relies
	on the collection of seismic data volumes and sophisticated computing
	to create high-resolution models.With the advent of FWI, the improvements
	in acquisition and inversion have been substantial, but these improvements
	come at a high cost because FWI involves extremely large multi-experiment
	data volumes. The main obstacle is the {\textquoteleft}curse of dimensionality{\textquoteright}
	exemplified by Nyquist{\textquoteright}s sampling criterion, which
	puts a disproportionate strain on current acquisition and processing
	systems as the size and desired resolution increases. In this paper,
	we address the {\textquoteleft}curse of dimensionality{\textquoteright}
	by using randomized dimensionality reduction of the FWI problem,
	coupled with a modified Gauss-Newton (GN) method designed to promote
	curvelet-domain sparsity of model updates. We solve for these updates
	using the spectral projected gradient method, implemented in the
	SPG￿1 software package. Our approach is successful because it reduces
	the size of seismic data volumes without loss of information. With
	this reduction, we can compute Gauss-Newton updates with the reduced
	data volume at the cost of roughly one gradient update for the fully
	sampled wavefield},
  keywords = {SBGF,Full-waveform inversion},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SBGF/2011/li11SBGFmgnsu/li11SBGFmgnsu.pdf}
}

@CONFERENCE{lin2006SINBADci,
  author = {Tim T.Y. Lin},
  title = {Compressed imaging},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In 1998 Grimbergen et. al. introduced a new method for computing wavefield
	propagation which improved on the previously employed local explicit
	operator method in that it exhibited no dip limitation, accurately
	handled laterally varying background ground velocity models, and
	is unconditionally stable. These desirable properties are mainly
	attributed to bringing the propagation problem into an eigenvector
	basis that diagonalizes the propagation operators. This modal-transform
	method, however, requires at each depth-level the solution of a large-scale
	sparse eigenvalue problem to compute the square-root of the Helmholtz
	operator. By using recent results from compressed sensing, we hope
	to reduce these computational costs that typically involve the synthesizes
	of the imaging operators and the cost of matrix-vector products.
	To reduce these costs, we compress the extrapolation operators by
	using only a fraction of the positive eigenvalues and temporal frequencies.
	This reduction not only leads to smaller matrices but also to reduced
	synthesis costs. These reductions go at the expense of solving a
	recovery problem from incomplete data. During the presentation, we
	show that wavefields can accurately be extrapolated with a compressed
	operators and competitive costs.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/lin2006SINBADci/lin2006SINBADci.pdf}
}

@CONFERENCE{lin2009SEGcsf,
  author = {Tim T.Y. Lin and Yogi A. Erlangga and Felix J. Herrmann},
  title = {Compressive simultaneous full-waveform simulation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {2577-2581},
  organization = {SEG},
  abstract = {The fact that the computational complexity of wavefield simulation
	is proportional to the size of the discretized model and acquisition
	geometry, and not to the complexity of the simulated wavefield, is
	a major impediment within seismic imaging. By turning simulation
	into a compressive sensing problem{\textendash}where simulated data
	is recovered from a relatively small number of independent simultaneous
	sources{\textendash}we remove this impediment by showing that compressively
	sampling a simulation is equivalent to compressively sampling the
	sources, followed by solving a reduced system. As in compressive
	sensing, this allows for a reduction in sampling rate and hence in
	simulation costs. We demonstrate this principle for the time-harmonic
	Helmholtz solver. The solution is computed by inverting the reduced
	system, followed by a recovery of the full wavefield with a sparsity
	promoting program. Depending on the wavefield{\textquoteright}s sparsity,
	this approach can lead to a significant cost reduction, in particular
	when combined with the implicit preconditioned Helmholtz solver,
	which is known to converge even for decreasing mesh sizes and increasing
	angular frequencies. These properties make our scheme a viable alternative
	to explicit time-domain finite-difference.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255381},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf.pdf}
}

@CONFERENCE{lin2011EAGEepsic,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Estimating primaries by sparse inversion in a curvelet-like representation
	domain},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2011},
  organization = {EAGE},
  abstract = {We present an uplift in the fidelity and wavefront continuity of results
	obtained from the Estimation of Primaries by Sparse Inversion (EPSI)
	program by reconstructing the primary events in a hybrid wavelet-curvelet
	representation domain. EPSI is a multiple removal technique that
	belongs to the class of wavefield inversion methods, as an alternative
	to the traditional adaptive-subtraction process. The main assumption
	is that the correct primary events should be as sparsely-populated
	in time as possible. A convex reformulation of the original EPSI
	algorithm allows its convergence property to be preserved even when
	the solution wavefield is not formed in the physical domain. Since
	wavefronts and edge-type singularities are sparsely represented in
	the curvelet domain, sparse solutions formed in this domain will
	exhibit vastly improved continuity when compared to those formed
	in the physical domain, especially for the low-energy events at later
	arrival times. Further- more, a wavelet-type representation domain
	will preserve sparsity in the reflected events even if they originate
	from non-zero-order discontinuities in the subsurface, providing
	an additional level of robustness. This method does not require any
	changes in the underlying computational algorithm and does not explicitly
	impose continuity constraints on each update.},
  keywords = {Presentation,EAGE,Processing},
  optmonth = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic.pdf}
}

@CONFERENCE{lin2011SEGrssde,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Robust source signature deconvolution and the estimation of primaries
	by sparse inversion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  volume = {30},
  pages = {4354-4359},
  organization = {Dept. of Earth and Ocean Sciences, University of British Columbia},
  abstract = {The past few years had seen some concentrated interest on a particular
	wavefield-inversion approach to the popular SRME multiple removal
	technique called Estimation of Primaries by Sparse Inversion (EPSI).
	EPSI promises greatly improved tol- erance to noise, missing data,
	edge effect, and other physi- cal phenomenon generally not described
	by the SRME relation (van Groenestijn and Verschuur, 2009a,b). It
	is based on the premise that it is possible to stably invert for
	both the primary impulse response and the source signature despite
	beforehand having no (or very limited) explicit knowledge of latter.
	The key to successful applications of EPSI, as shown in very recent
	works (Savels et al., 2010), is a robust way to reconstruct very
	sparse primary impulse response events as part of the inver- sion
	process. Based on the various successful demonstrations in literature,
	there is a very strong sense that EPSI will also play an important
	role in future developments of source sig- nature deconvolution and
	the general recovering of wavefield spectrum. },
  keywords = {Presentation,deconvolution, SEG, sparse inversion,Processing},
  optdoi = {10.1190/1.3628116},
  optmonth = {04/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde.pdf}
}

@CONFERENCE{lin2010EAGEseo,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Stabilization of estimation of primaries via sparse inversion},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2010},
  organization = {EAGE},
  abstract = {Recent works on surface-related multiple removal include a direct
	estimation method proposed by van Groenestijn and Verschuur (2009),
	where under a sparsity assumption the primary impulse response is
	determined directly from a data-driven wavefield inversion process
	called Estimation of Primaries by Sparse Inversion (EPSI). The authors
	have shown that this approach is superior to traditional estimation
	subtraction processes such as SRME on shallow bottom marine data,
	where by expanding the model to simultaneously invert for the near-offset
	traces, which are not directly available in most situation but are
	observable in the data multiples, a large improvement over Radon
	interpolation is demonstrated. One of the major roadblocks to the
	widespread adoption of EPSI is that one must have precise knowledge
	of a time-window that contains multiple-free primaries during each
	update. There is some anecdotal evidence that the inversion result
	is unstable under errors in the time-window length, a behavior that
	runs contrary to the strengths of EPSI and diminishes its effectiveness
	for shallow-bottom marine data where multiples are closely spaced.
	Moreover, due to the nuances involved in regularizing the model impulse
	response in the inverse problem, the EPSI approach has an additional
	number of inversion parameters to choose and often also does not
	often lead to a stable solution under perturbations to these parameters.
	We show that the specific sparsity constraint on the EPSI updates
	lead to an inherently intractable problem, and that the time-window
	and other inversion variables arise as additional regularizations
	on the unknown towards a meaningful solution. We furthermore suggest
	a way to remove almost all of these parameters via a L0 to L1 convexification,
	which stabilizes the inversion while preserving the crucial sparsity
	assumption in the primary impulse response model.},
  keywords = {Presentation,EAGE,Processing},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo.pdf}
}

@CONFERENCE{lin2009EAGEdsa,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Designing simultaneous acquisitions with compressive sensing},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2009},
  organization = {EAGE},
  abstract = {The goal of this paper is in designing a functional simultaneous acquisition
	scheme by applying the principles of compressive sensing. By framing
	the acquisition in a compressive sensing setting we immediately gain
	insight into not only how to choose the source signature and shot
	patterns, but also in how well we can hope to demultiplex the data
	when given a set amount of reduction in the number of sweeps. The
	principles of compressive sensing dictates that the quality of the
	demultiplexed data is closely related to the transform-domain sparsity
	of the solution. This means that, given an estimate in the complexity
	of the expectant data wavefield, it is possible to controllably reduce
	the number of shots that needs to be recorded in the field. We show
	a proof of concept by introducing an acquisition compatible with
	compressive sensing based on randomly phase-encoded vibroseis sweeps.},
  keywords = {Presentation,EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/lin09EAGEdsa/lin09EAGEdsa_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/lin09EAGEdsa/lin09EAGEdsa.pdf}
}

@CONFERENCE{lin2009SEGucs,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Unified compressive sensing framework for simultaneous acquisition
	with primary estimation},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {3113-3117},
  organization = {SEG},
  abstract = {The central promise of simultaneous acquisition is a vastly improved
	crew efficiency during acquisition at the cost of additional post-processing
	to obtain conventional source-separated data volumes. Using recent
	theories from the field of compressive sensing, we present a way
	to systematically model the effects of simultaneous acquisition.
	Our formulation form a new framework in the study of acquisition
	design and naturally leads to an inversion-based approach for the
	separation of shot records. Furthermore, we show how other inversion-based
	methods, such as a recently proposed method from van Groenestijn
	and Verschuur (2009) for primary estimation, can be processed together
	with the demultiplexing problem to achieve a better result compared
	to a separate treatment of these problems.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255502},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs.pdf}
}

@CONFERENCE{lin2008SINBADcwe,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation},
  booktitle = {SINBAD},
  year = {2008},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/sites/data/Papers/lin08cwe.pdf}
}

@CONFERENCE{lin2007SEGcwe1,
  author = {Tim T.Y. Lin and Felix J. Herrmann},
  title = {Compressed wavefield extrapolation with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {1997-2001},
  organization = {SEG},
  abstract = {An explicit algorithm for the extrapolation of one-way wavefields
	is proposed which combines recent developments in information theory
	and theoretical signal processing with the physics of wave propagation.
	Because of excessive memory requirements, explicit formulations for
	wave propagation have proven to be a challenge in 3-D. By using ideas
	from {\textquoteleft}{\textquoteleft}compressed sensing{\textquoteright}{\textquoteright},
	we are able to formulate the (inverse) wavefield extrapolation problem
	on small subsets of the data volume, thereby reducing the size of
	the operators. According to compressed sensing theory, signals can
	successfully be recovered from an imcomplete set of measurements
	when the measurement basis is incoherent with the representation
	in which the wavefield is sparse. In this new approach, the eigenfunctions
	of the Helmholtz operator are recognized as a basis that is incoherent
	with curvelets that are known to compress seismic wavefields. By
	casting the wavefield extrapolation problem in this framework, wavefields
	can successfully be extrapolated in the modal domain via a computationally
	cheaper operation. A proof of principle for the {\textquoteleft}{\textquoteleft}compressed
	sensing{\textquoteright}{\textquoteright} method is given for wavefield
	extrapolation in 2-D. The results show that our method is stable
	and produces identical results compared to the direct application
	of the full extrapolation operator. {\copyright}2007 Society of Exploration
	Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  optdoi = {10.1190/1.2792882},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1.pdf }
}

@CONFERENCE{lin2009DELPHIrwi,
  author = {Tim T.Y. Lin and Felix J. Herrmann and Yogi A. Erlangga},
  title = {Randomized wavefield inversion presented at the DELPHI meeting. The
	Hague.},
  booktitle = {DELPHI},
  year = {2009},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Delphi/2009/lin2009DELPHIrwi/lin2009DELPHIrwi.pdf}
}

@CONFERENCE{lin2008SEGiso,
  author = {Tim T.Y. Lin and Evgeniy Lebed and Yogi A. Erlangga and Felix J.
	Herrmann},
  title = {Interpolating solutions of the Helmholtz equation with compressed
	sensing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2122-2126},
  organization = {SEG},
  abstract = {We present an algorithm which allows us to model wavefields with frequency-domain
	methods using a much smaller number of frequencies than that typically
	required by the classical sampling theory in order to obtain an alias-free
	result. The foundation of the algorithm is the recent results on
	the compressed sensing, which state that data can be successfully
	recovered from an incomplete measurement if the data is sufficiently
	sparse. Results from numerical experiment show that only 30\% of
	the total frequency spectrum is need to capture the full wavefield
	information when working in the hard 2D synthetic Marmousi model.},
  keywords = {Presentation,SLIM, SEG},
  optdoi = {10.1190/1.3059307},
  optmonth = {11/2008},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf }
}

@CONFERENCE{lin10SEGspm,
  author = {Tim T.Y. Lin and Ning Tu and Felix J. Herrmann},
  title = {Sparsity-promoting migration from surface-related multiples},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  volume = {29},
  pages = {3333-3337},
  organization = {SEG},
  abstract = {Seismic imaging typically begins with the removal of multiple energy
	in the data, out of fear that it may introduce erroneous structure.
	However, seismic multiples have effectively seen more of the earth{\textquoteright}s
	structure, and if treated correctly can potential supply more information
	to a seismic image compared to primaries. Past approaches to accomplish
	this leave ample room for improvement; they either require extensive
	modification to standard migration techniques, rely too much on prior
	information, require extensive pre-processing, or resort to full-waveform
	inversion. We take some valuable lessons from these efforts and present
	a new approach balanced in terms of ease of implementation, robustness,
	efficiency and well-posedness, involving a sparsity-promoting inversion
	procedure using standard Born migration and a data-driven multiple
	modeling approach based on the focal transform.},
  keywords = {Presentation,SEG,Processing},
  optdoi = {10.1190/1.3513540},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm.pdf}
}

@CONFERENCE{mansour2012SSPwspgl1,
  author = {Hassan Mansour},
  title = {Beyond $\ell_1$ norm minimization for sparse signal recovery},
  booktitle = {2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
  year = {2012},
  address = {Ann Arbor, Michigan, USA},
  organization = {IEEE},
  abstract = {Sparse signal recovery has been dominated by the basis pursuit denoise
	(BPDN) problem formulation for over a decade. In this paper, we propose
	an algorithm that outperforms BPDN in finding sparse solutions to
	underdetermined linear systems of equations at no additional computational
	cost. Our algorithm, called WSPGL1, is a modification of the spectral
	projected gradient for $\ell_1$ minimization (SPGL1) algorithm in
	which the sequence of LASSO subproblems are replaced by a sequence
	of weighted LASSO subproblems with constant weights applied to a
	support estimate. The support estimate is derived from the data and
	is updated at every iteration. The algorithm also modifies the Pareto
	curve at every iteration to reflect the new weighted $\ell_1$ minimization
	problem that is being solved. We demonstrate through extensive simulations
	that the sparse recovery performance of our algorithm is superior
	to that of $\ell_1$ minimization and approaches the recovery performance
	of iterative re-weighted $\ell_1$ (IRWL1) minimization of Cand{\`e}s,
	Wakin, and Boyd. Moreover, our algorithm has the computational cost
	of a single BPDN problem.},
  keywords = {Sparse recovery, compressed sensing, iterative algorithms, weighted
	$\ell_1$ minimization, partial support recovery},
  optmonth = {03/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1.pdf}
}

@CONFERENCE{mansour2011SBGFcspsma,
  author = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title = {A compressive sensing perspective on simultaneous marine acquisition},
  booktitle = {SBGF},
  year = {2011},
  organization = {SBGF},
  abstract = {The high cost of acquiring seismic data in Marine environments compels
	the adoption of simultaneous- source acquisition - an emerging technology
	that is stimulating both geophysical research and commercial efforts.
	In this paper, we discuss the properties of randomized simultaneous
	acquisition matrices and demonstrate that sparsity-promoting recovery
	improves the quality of the reconstructed seismic data volumes. Simultaneous
	Marine acquisition calls for the development of a new set of design
	principles and post-processing tools. Leveraging established findings
	from the field of compressed sensing, the recovery from simultaneous
	sources depends on a sparsifying transform that compresses seismic
	data, is fast, and reasonably incoherent with the compressive sampling
	matrix. To achieve this incoherence, we use random time dithering
	where sequential acquisition with a single airgun is replaced by
	continuous acquisition with multiple airguns firing at random times
	and at random locations. We demonstrate our results with simulations
	of simultaneous Marine acquisition using periodic and randomized
	time dithering.},
  keywords = {Presentation,SBGF,Acquisition,Compressive Sensing},
  presenation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SBGF/2011/Mansour11SBGFcspsma/Mansour11SBGFcspsma.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SBGF/2011/Mansour11SBGFcspsma/Mansour11SBGFcspsma.pdf}
}

@CONFERENCE{mansour2012ICASSadapt,
  author = {Hassan Mansour and Ozgur Yilmaz},
  title = {Adaptive compressed sensing for video acquisition},
  booktitle = {ICASSP},
  year = {2012},
  organization = {ICASSP},
  abstract = {In this paper, we propose an adaptive compressed sensing scheme that
	utilizes a support estimate to focus the measurements on the large
	valued coefficients of a compressible signal. We embed a “sparse-filtering”
	stage into the measure- ment matrix by weighting down the contribution
	of signal coefficients that are outside the support estimate. We
	present an application which can benefit from the proposed sampling
	scheme, namely, video compressive acquisition. We demonstrate that
	our proposed adaptive CS scheme results in a significant improvement
	in reconstruction quality compared with standard CS as well as adaptive
	recovery using weighted $\ell$1 minimization.},
  keywords = {ICASSP},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPaCS/MansourYilmazICASSPaCS.pdf}
}

@CONFERENCE{mansour2012ICASSsupport,
  author = {Hassan Mansour and Ozgur Yilmaz},
  title = {Support driven reweighted $\ell_1$ minimization},
  booktitle = {ICASSP},
  year = {2012},
  organization = {ICASSP},
  abstract = {In this paper, we propose a support driven reweighted $\ell$_1 minimization
	algorithm (SDRL1) that solves a sequence of weighted $\ell$_1 problems
	and relies on the support estimate accu- racy. Our SDRL1 algorithm
	is related to the IRL1 algorithm proposed by Cande`s, Wakin, and
	Boyd. We demonstrate that it is sufficient to find support estimates
	with good accuracy and apply constant weights instead of using the
	inverse coefficient magnitudes to achieve gains similar to those
	of IRL1. We then prove that given a support estimate with sufficient
	accuracy, if the signal decays according to a specific rate, the
	solution to the weighted $\ell$_1 minimization problem results in
	a support estimate with higher accuracy than the initial estimate.
	We also show that under certain conditions, it is possible to achieve
	higher estimate accuracy when the inter- section of support estimates
	is considered. We demonstrate the performance of SDRL1 through numerical
	simulations and compare it with that of IRL1 and standard $\ell$_1
	minimization.},
  keywords = {ICASSP},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPwL1/MansourYilmazICASSPwL1.pdf}
}

@CONFERENCE{maysami2006SINBADrro,
  author = {Mohammad Maysami},
  title = {Recent results on seismic deconvolution},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {One of the important steps in seismic imaging is to provide suitable
	information about boundaries. Sharp variation of physical properties
	at a layer boundary cause reflection the wavefield. In previous work
	done by C. M. Dupuis, seismic signal characterization is divided
	into two steps: detection and estimation. In the detection phase,
	the goal is to find all singularities in a seismic section regardless
	of their order and then to categorize the data to different events
	by windowing each singularity. In the estimation step, we determine
	the order of singularity more precisely by using a rough estimate
	based on the detection phase. Traditionally, a redundant dictionary
	method is employed for the detection part. However, we attempt to
	instead use a new L1-solver developed by D.L. Donoho: the Stagewise
	Orthogonal Matching Pursuit (StOMP). It approximates the solution
	to inverse problems while promoting the sparsity in the solution
	vector. This algorithm will allow us to experimentally confirm the
	recent analysis by S. Mallat on spiky deconvolution limits, which
	imposes a required minimum distance between spikes. This required
	minimum distance between different spikes is dependent on the number
	of spikes as well as the width of the chosen source wavelet used
	in convolution with the train. These results allow for the design
	of more robust and accurate detection schemes for seismic signal
	characterization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/maysami06SINBADrro/maysami06SINBADrro_pres.pdf}
}

@CONFERENCE{maysami2008SEGlcf,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Lithological constraints from seismic waveforms: application to opal-A
	to opal-CT transition},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2011-2015},
  organization = {SEG},
  abstract = {In this paper, we present a new method for seismic waveform characterization
	whose aim is threefold, namely (i) extraction of detailed information
	on the sharpness of transitions in the subsurface from seismic waveforms,
	(ii) reflector modeling, based on binary-mixture and percolation
	theory, and (iii) establishment of well-seismic ties, through parameterizations
	of our waveform and critical reflector model. We test this methodology
	on the opal-A (Amorphous) to opal-CT (Cristobalite/Tridymite) transition
	imaged in a migrated section of North Sea field data West of the
	Shetlands.},
  keywords = {SEG, SLIM},
  optdoi = {10.1190/1.3059400},
  optmonth = {November},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/maysami08SEGlcf/maysami08SEGlcf.pdf }
}

@CONFERENCE{maysami2007EAGEsrc,
  author = {Mohammad Maysami and Felix J. Herrmann},
  title = {Seismic reflector characterization by a multiscale detection-estimation
	method},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {Seismic transitions of the subsurface are typically considered as
	zero-order singularities (step functions). According to this model,
	the conventional deconvolution problem aims at recovering the seismic
	reflectivity as a sparse spike train. However, recent multiscale
	analysis on sedimentary records revealed the existence of accumulations
	of varying order singularities in the subsurface, which give rise
	to fractional-order discontinuities. This observation not only calls
	for a richer class of seismic reflection waveforms, but it also requires
	a different methodology to detect and characterize these reflection
	events. For instance, the assumptions underlying conventional deconvolution
	no longer hold. Because of the bandwidth limitation of seismic data,
	multiscale analysis methods based on the decay rate of wavelet coefficients
	may yield ambiguous results. We avoid this problem by formulating
	the estimation of the singularity orders by a parametric nonlinear
	inversion method.},
  keywords = {Presentation, SLIM, EAGE},
  optmonth = {June},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc.pdf }
}


@CONFERENCE{modzelewski2008SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {The SLIM group is actively developing software for seismic imaging.
	This talk will give a general overview of the software development
	during SINBAD project with focus on the final release in February
	2008. The covered topics will include: 1) adopting Python for object-oriented
	programming, 2) including parallelism into the algorithms used in
	seismic imaging/modeling, 3) in-house algorithms for seismic imaging,
	and 4) contributions to Madagascar (RSF). The talk will serve as
	an introduction to the other presentations in the session "SINBAD
	Software releases".},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/modzelewski2008SINBADdas/modzelewski2008SINBADdas.pdf}
}

@CONFERENCE{modzelewski2006SINBADdas,
  author = {Henryk Modzelewski},
  title = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/modzelewski2006SINBADdas/modzelewski2006SINBADdas.pdf}
}

@CONFERENCE{moghaddam2008SINBADrtm,
  author = {Peyman P. Moghaddam},
  title = {Reverse-time Migration Amplitude Recovery with Curvelets},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We recover the amplitude of a seismic image by approximating the normal
	(demigration-migration) operator. In this approximation, we make
	use of the property that curvelets remain invariant under the action
	of the normal operator. We propose a seismic amplitude recovery method
	that employs an eigenvalue like decomposition for the normal operator
	using curvelets as eigenvectors. Subsequently, we propose an approximate
	nonlinear singularity-preserving solution to the least-squares seismic
	imaging problem with sparseness in the curvelet domain and spatial
	continuity constraints. Our method is tested with a reverse-time
	{\textquoteright}wave-equation{\textquoteright} migration code simulating
	the acoustic wave equation.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/moghaddam2008SINBADrtm/moghaddam2008SINBADrtm.pdf
}
}

@CONFERENCE{moghaddam2006SINBADioa,
  author = {Peyman P. Moghaddam},
  title = {Imaging operator approximation using Curvelets},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {In this presentation, the normal (demigation-migration) operator is
	studied in terms of a pseudo-differential operator. The invariance
	of curvelets under this operator and their sparsity on the seismic
	images is used to precondition the migration operator. A brief overview
	will be given on some of the theory from micro-local analysis which
	proofs that curvelets remain approximately invariant under the operator.
	The proper setting for which a diagonal approximation in the curvelet
	domain is accurate is discussed together with different methods that
	estimate this diagonal from of-the-shelf migration operators. This
	is joint work with Chris Stolk.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/moghaddam2006SINBADioa/moghaddam2006SINBADioa.pdf}
}

@CONFERENCE{moghaddam2006SINBADsac,
  author = {Peyman P. Moghaddam},
  title = {Sparsity- and continuity-promoting norms for seismic images},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {During this presentation, the importance of sparsity and continuity
	enhancing energy norms is emphasized for seismic imaging and inversion.
	The continuity promoting energy norm is justified by the apparent
	smoothness of reflectors in the direction along and the oscillatory
	behavior across the interfaces. This energy norm is called anisotropic
	diffusion and will be defined mathematically. Denoising examples
	will be given during which seismic images are recovered from the
	noise by a joint norm-one and continuity promoting minimization.},
  keywords = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/moghaddam2006SINBADsac/moghaddam2006SINBADsac.pdf}
}

@CONFERENCE{moghaddam2008SEGcbm,
  author = {Peyman P. Moghaddam and Cody R. Brown and Felix J. Herrmann},
  title = {Curvelet-based migration preconditioning},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2211-2215},
  organization = {SEG},
  abstract = {In this paper, we introduce a preconditioner for seismic imaging{\textendash}-i.e.,
	the inversion of the linearized Born scattering operator. This preconditioner
	approximately corrects for the {\textquoteleft}{\textquoteleft}square
	root{\textquoteright}{\textquoteright} of the normal{\textendash}-i.e.,
	the demigration-migration operator. This approach consists of three
	parts, namely (i) a left preconditoner, defined by a fractional time
	integration designed to make the migration operator zero order, and
	two right preconditioners that apply (ii) a scaling in the physical
	domain accounting for a spherical spreading, and (iii) a curvelet-domain
	scaling that corrects for spatial and reflector-dip dependent amplitude
	errors. We show that a combination of these preconditioners lead
	to a significant improvement of the convergence for iterative least-squares
	solutions to the seismic imaging problem based on reverse-time migration
	operators.},
  keywords = {Presentation,SLIM,SEG},
  optdoi = {10.1190/1.3059325},
  optmonth = {November},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm.pdf }
}

@CONFERENCE{moghaddam2010SEGrfw,
  author = {Peyman P. Moghaddam and Felix J. Herrmann},
  title = {Randomized full-waveform inversion: a dimenstionality-reduction approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2010},
  volume = {29},
  pages = {977-982},
  organization = {SEG},
  abstract = {Full-waveform inversion relies on the collection of large multi-experiment
	data volumes in combination with a sophisticated back-end to create
	high-fidelity inversion results. While improvements in acquisition
	and inversion have been extremely successful, the current trend of
	incessantly pushing for higher quality models in increasingly complicated
	regions of the Earth reveals fundamental shortcomings in our ability
	to handle increasing problem sizes numerically. Two main culprits
	can be identified. First, there is the so-called {\textquoteleft}{\textquoteleft}curse
	of dimensionality{\textquoteright}{\textquoteright} exemplified by
	Nyquist{\textquoteright}s sampling criterion, which puts disproportionate
	strain on current acquisition and processing systems as the size
	and desired resolution increases. Secondly, there is the recent {\textquoteleft}{\textquoteleft}departure
	from Moore{\textquoteright}s law{\textquoteright}{\textquoteright}
	that forces us to develop algorithms that are amenable to parallelization.
	In this paper, we discuss different strategies that address these
	issues via randomized dimensionality reduction.},
  keywords = {Presentation,SEG,Full-waveform inversion,Optimization},
  optdoi = {10.1190/1.3513940},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw.pdf}
}

@CONFERENCE{moghaddam2004SEGmpw,
  author = {Peyman P. Moghaddam and Felix J. Herrmann},
  title = {Migration preconditioning with curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2004},
  volume = {23},
  pages = {2204-2207},
  organization = {SEG},
  abstract = {In this paper, the property of Curvelet transforms for preconditioning
	the migration and normal operators is investigated. These operators
	belong to the class of Fourier integral operators and pseudo-differential
	operator, respectively. The effect of this pre-conditioner is shown
	in term of improvement of sparsity, convergence rate, number of iteration
	for the Krylov-subspace solver and clustering of singular(eigen)
	values. The migration operator, which we employed in this work is
	the common-offset Kirchoff-Born migration. {\copyright}2004 Society
	of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  optdoi = {10.1190/1.1845213},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Moghaddam04SEGmpw/Moghaddam04SEGmpw_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Moghaddam04SEGmpw/Moghaddam04SEGmpw_paper.pdf}
}

@CONFERENCE{moghaddam2007CSEGmar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Migration amplitude recovery using curvelets},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2007},
  organization = {CSEG},
  abstract = {In this paper, we recover the amplitude of a seismic image by approximating
	the normal operator and subsequently inverting it. Normal operator
	(migration followed by modeling) is an example of pseudo-differential.
	curvelets are proven to be invariant under the action of pseudo-differential
	operators under certain conditions. Subsequently, curvelets are forming
	as eigen-vectors for such an operator. We propose a seismic amplitude
	recovery method that employs an eigen-value decomposition for normal
	operator using curvelets as eigen-vectors and to be estimated eigenvalues.
	A post-stack reverse-time, wave-equation migration is used for evaluation
	of the proposed method.},
  file = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf:PDF},
  keywords = {SLIM},
  optmonth = {May},
  url = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf}
}

@CONFERENCE{moghaddam2007CSEGsac,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Sparsity and continuity enhancing seismic imaging},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2007},
  organization = {CSEG},
  abstract = {A non-linear singularity-preserving solution to the least-squares
	seismic imaging problem with sparseness and continuity constraints
	is proposed. The applied formalism explores curvelets as a directional
	frame that, by their sparsity on the image, and their invariance
	under the imaging operators, allows for a stable recovery of the
	amplitudes. Our method is based on the estimation of the normal operator
	in the form of an {\textquoteright}eigenvalue{\textquoteright} decompsoition
	with curvelets as the eigenvectors{\textquoteright}. Subsequently,
	we propose an inversion method that derives from estimation of the
	normal operator and is formulated as a convex optimization problem.
	Sparsity in the curvelet domain as well as continuity along the reflectors
	in the image domain are promoted as part of this optimization. Our
	method is tested with a reverse-time {\textquoteright}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation.},
  file = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf:PDF},
  keywords = {SLIM},
  optmonth = {May},
  url = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf}
}

@CONFERENCE{moghaddam2007EAGEsar,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Seismic amplitude recovery with curvelets},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {A non-linear singularity-preserving solution to the least-squares
	seismic imaging problem with sparseness and continuity constraints
	is proposed. The applied formalism explores curvelets as a directional
	frame that, by their sparsity on the image, and their invariance
	under the imaging operators, allows for a stable recovery of the
	amplitudes. Our method is based on the estimation of the normal operator
	in the form of an {\textquoteright}eigenvalue{\textquoteright} decompsoition
	with curvelets as the {\textquoteright}eigenvectors{\textquoteright}.
	Subsequently, we propose an inversion method that derives from estimation
	of the normal operator and is formulated as a convex optimization
	problem. Sparsity in the curvelet domain as well as continuity along
	the reflectors in the image domain are promoted as part of this optimization.
	Our method is tested with a reverse-time {\textquoteright}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation.},
  keywords = {SLIM, EAGE},
  optmonth = {June},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/moghaddam07EAGEsar/moghaddam07EAGEsar.pdf }
}

@CONFERENCE{moghaddam2007SEGrsi,
  author = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title = {Robust seismic-images amplitude recovery using curvelets},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2225-2229},
  organization = {SEG},
  abstract = {In this paper, we recover the amplitude of a seismic image by approximating
	the normal (demigration-migration) operator. In this approximation,
	we make use of the property that curvelets remain invariant under
	the action of the normal operator. We propose a seismic amplitude
	recovery method that employs an eigenvalue like decomposition for
	the normal operator using curvelets as eigen-vectors. Subsequently,
	we propose an approximate non-linear singularity-preserving solution
	to the least-squares seismic imaging problem with sparseness in the
	curvelet domain and spatial continuity constraints. Our method is
	tested with a reverse-time {\textquoteleft}wave-equation{\textquoteright}
	migration code simulating the acoustic wave equation on the SEG-AA
	salt model. {\copyright}2007 Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  optdoi = {10.1190/1.2792928},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi.pdf }
}

@CONFERENCE{min2012CSEGrgfe,
  author = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
  title = {Re-establishment of gradient in frequency-domain elastic waveform
	inversion},
  booktitle = {CSEG technical program},
  year = {2012},
  abstract = {To obtain solutions close to global minimum in waveform inversion,
	the gradients computed at each frequency need to be weighted to appropriately
	describe the residuals between modeled and field data. While the
	low-frequency components of the gradients should be weighted to recover
	the long- wavelength structures, the high-frequency components of
	the gradients need to be weighted when the short-wavelength structures
	are restored. However, the conventional elastic waveform inversion
	algorithms cannot properly weight the gradients computed at each
	frequency. When gradients are scaled using the pseudo-Hessian matrix
	inside the frequency loop, gradients obtained at high frequencies
	are over-emphasized. When the gradients are scaled outside the frequency
	loop, gradients are weighted by the source spectra. In this study,
	we propose applying weighting factors to the gradients obtained at
	each frequency so that gradients can properly reflect the differences
	between the true and assumed models satisfying the general inverse
	theory. The weighting factors are composed by the backpropagated
	residuals. Numerical examples for the simple rectangular-shaped model
	and the modified version of the Marmousi-2 model show that the weighting
	method enhances gradient images and inversion results compared to
	the conventional inversion algorithms.},
  keywords = {elastic, waveform inversion, frequency-domain, weighting factors},
  optmonth = {02/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2012/min2012CSEGrgfe/min2012CSEGrgfe.pdf}
}

@CONFERENCE{min2012EAGEefwi,
  author = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
  title = {Frequency-domain elastic waveform inversion using weighting factors
	related to source-deconvolved residuals},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {One of the limitations in seismic waveform inversion is that inversion
	results are very sensitive to initial guesses, which may be because
	the gradients computed at each frequency are not properly weighted
	depending on given models.Analyzingthe conventional waveform inversion
	algorithms using the pseudo-Hessian matrix as a pre-conditioner shows
	that the gradientsdo not properly describe the feature of given models
	or high- and low-end frequencies do not contribute the model parameter
	updates due to banded spectra of source wavelet. For a better waveform
	inversion algorithm, we propose applying weighting factors to gradients
	computed at each frequency. The weighting factors are designed using
	the source-deconvolved back-propagated wavefields. Numerical results
	for the SEG/EAGE salt model show that the weighting method improves
	gradient images and its inversion results are compatible with true
	velocities even with poorly estimated initial guesses.},
  keywords = {elastic, waveform inversion, frequency-domain, weighting factors},
  optmonth = {01/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/min2012EAGEefwi/min2012EAGEefwi.pdf}
}

@CONFERENCE{ross2008SINBADsit,
  author = {Sean Ross-Ross},
  title = {Seismic inversion through operator overloading},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Geophysical processing is dominated by many different out of core
	memory software environments (OOCE). Such environments include Madagascar
	and SU and are designed to handle data that can not be operated on
	in memory. Each base operation is created as a main program that
	reads data from disk and writes the result to disk. The main programs
	can also be chained together on stdin/out pipes using a shell only
	writing data to disk at the end. To be efficient, the algorithm using
	an OOCE must chain together the longest pipe to avoid disk I/O, as
	a result it is very difficult to use iterative techniques. The algorithms
	are written in shell scripts can be difficult to read and understand.
	SLIMpy is a software library that contains definitions of coordinate
	free vectors and linear operators. It allows the user to design and
	run algorithms with any out of core package, in a Matlab style interface
	while maintaining optimal efficiency and speed. SLIMpy looks at each
	main program of each OOCE as a Matrix vector operation or vector
	reduction/transformation operation. It uses operator overloading
	to generate an abstract syntax tree (AST) which can be optimized
	in many ways before executing its commands. The AST also provides
	a pathway for embarrassingly parallel applications by splitting the
	tree over different nodes and processors. SLIMpy provides an interface
	to these OOCE that allows for optimal construction of commands and
	allows for iterative techniques. It smoothes the transition from
	other languages such as Matlab and allows the algorithm designer
	to write readable and reusable code. SLIMpy also adds to OOCE by
	allowing for easy parallelization.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/ross2008SINBADsit/ross2008SINBADsit.pdf}
}

@CONFERENCE{saab2008SINBADcps,
  author = {Rayan Saab},
  title = {Curvelet-Based Primary-Multiple Separation from a Bayesian Perspective},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a novel primary-multiple separation scheme which makes
	use of the sparsity of both primaries and multiples in a transform
	domain, such as the curvelet transform, to provide estimates of each.
	The proposed algorithm utilizes seismic data as well as the output
	of a preliminary step that provides (possibly) erroneous predictions
	of the multiples. The algorithm separates the signal components,
	i.e., the primaries and multiples, by solving an optimization problem
	that assumes noisy input data and can be derived from a Bayesian
	perspective. More precisely, the optimization problem can be arrived
	at via an assumption of a weighted Laplacian distribution for the
	primary and multiple coefficients in the transform domain and of
	white Gaussian noise contaminating both the seismic data and the
	preliminary prediction of the multiples, which both serve as input
	to the algorithm.},
  date-modified = {2008-08-22 12:45:53 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/saab2008SINBADcps/saab2008SINBADcp.pdf}
}

@CONFERENCE{saab2008ICASSPssa,
  author = {Rayan Saab and Rick Chartrand and Ozgur Yilmaz},
  title = {Stable sparse approximations via nonconvex optimization},
  booktitle = {ICASSP},
  year = {2008},
  organization = {ICASSP},
  abstract = {We present theoretical results pertaining to the ability of lp minimization
	to recover sparse and compressible signals from incomplete and noisy
	measurements. In particular, we extend the results of Cande`s, Romberg
	and Tao [1] to the p < 1 case. Our results indicate that depending
	on the restricted isometry constants (see, e.g.,[2] and [3]) and
	the noise level, lp minimization with certain values of p < 1 provides
	better theoretical guarantees in terms of stability and robustness
	than l1 minimization does. This is especially true when the restricted
	isometry constants are relatively large.},
  keywords = {ICASSP},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2008/saab08ICASSPssa/saab08ICASSPssa.pdf }
}

@CONFERENCE{saab2007SEGcbp,
  author = {Rayan Saab and Deli Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Curvelet-based primary-multiple separation from a {B}ayesian perspective},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2510-2514},
  organization = {SEG},
  abstract = {In this abstract, we present a novel primary-multiple separation scheme
	which makes use of the sparsity of both primaries and multiples in
	a transform domain, such as the curvelet transform, to provide estimates
	of each. The proposed algorithm utilizes seismic data as well as
	the output of a preliminary step that provides (possibly) erroneous
	predictions of the multiples. The algorithm separates the signal
	components, i.e., the primaries and multiples, by solving an optimization
	problem that assumes noisy input data and can be derived from a Bayesian
	perspective. More precisely, the optimization problem can be arrived
	at via an assumption of a weighted Laplacian distribution for the
	primary and multiple coefficients in the transform domain and of
	white Gaussian noise contaminating both the seismic data and the
	preliminary prediction of the multiples, which both serve as input
	to the algorithm. {\copyright}2007 Society of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  optdoi = {10.1190/1.2792988},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp.pdf }
}

@CONFERENCE{saab2009SAMPTAnccs,
  author = {Rayan Saab and Ozgur Yilmaz},
  title = {A short note on non-convex compressed sensing},
  booktitle = {SAMPTA technical program},
  year = {2009},
  organization = {SAMPTA},
  abstract = {In this note, we summarize the results we recently proved in\cite{SY08}
	on the theoretical performance guarantees of the decoders $¿î_p$.
	These decoders rely on $\ell^p$ minimization with $p {\i}n (0,1)$
	to recover estimates of sparse and compressible signals from incomplete
	and inaccurate measurements. Our guarantees generalize the results
	of \cite{CRT05} and \cite{Wojtaszczyk08} about decoding by $\ell_p$
	minimization with $p=1$, to the setting where $p {\i}n (0,1)$ and
	are obtained under weaker sufficient conditions. We also present
	novel extensions of our results in \cite{SY08} that follow from the
	recent work of DeVore et al. in \cite{DPW08}. Finally, we show some
	insightful numerical experiments displaying the trade-off in the
	choice of $p {\i}n (0,1]$ depending on certain properties of the
	input signal.},
  keywords = {Presentation},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2009/saab09SAMPTAnccs/saab09SAMPTAnccs.pdf}
}

@CONFERENCE{sastry2007SINBADnor,
  author = {Challa S. Sastry},
  title = {Norm-one recovery from irregular sampled data},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Seismic traces are sampled irregularly and insufficiently due to practical
	and economical limitations. The use of such data in seismic imaging
	results in image artifacts and poor spatial resolution. Therefore,
	before being used, the measurements are to be interpolated onto a
	regular grid. One of the methods achieving this objective is based
	on the Fourier reconstruction, which deals with the under-determined
	system of equations. The recent pursuit techniques (namely, basis
	pursuit, matching pursuit etc) admit certain promising features such
	as faster and simpler implementation even in large scale settings.
	The presentation discusses the application of the pursuit algorithms
	to the Fourier-based interpolation problem for the signals that have
	sparse Fourier spectra. In particular, the objective of the presentation
	includes: 1). studying the performance of the algorithm if, and how
	far, the measurement coordinates can be shifted from uniform distribution
	on the continuous interval. 2). studying what could be the allowable
	misplacement in the measurement coordinates that does not alter the
	quality of the reconstruction process},
  keywords = {SLIM, SINBAD, Presentation}
}

@CONFERENCE{challa2007EAGEsrf,
  author = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title = {Signal reconstruction from incomplete and misplaced measurements},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {Constrained by practical and economical considerations, one often
	uses seismic data with missing traces. The use of such data results
	in image artifacts and poor spatial resolution. Sometimes due to
	practical limitations, measurements may be available on a perturbed
	grid, instead of on the designated grid. Due to algorithmic requirements,
	when such measurements are viewed as those on the designated grid,
	the recovery procedures may result in additional artifacts. This
	paper interpolates incomplete data onto regular grid via the Fourier
	domain, using a recently developed greedy algorithm. The basic objective
	is to study experimentally as to what could be the size of the perturbation
	in measurement coordinates that allows for the measurements on the
	perturbed grid to be considered as on the designated grid for faithful
	recovery. Our experimental work shows that for compressible signals,
	a uniformly distributed perturbation can be offset with slightly
	more number of measurements.},
  keywords = {SLIM, EAGE},
  optmonth = {June},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/challa07EAGEsrf/challa07EAGEsrf.pdf}
}

@CONFERENCE{sastry2007SINBADrfu,
  author = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title = {Recovery from unstructured data},
  booktitle = {SINBAD 2006},
  year = {2006},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/sastry2007SINBADrfu/sastry2007SINBADrfu.pdf}
}

@CONFERENCE{shahidi2009SEGcmf,
  author = {Reza Shahidi and Felix J. Herrmann},
  title = {Curvelet-domain matched filtering with frequency-domain regularization},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {3645-3650},
  organization = {SEG},
  abstract = {In Herrmann et al. (2008), it is shown that zero-order pseudodifferential
	operators, which model the migration-demigration operator and the
	operator mapping the predicted multiples to the true multiples, can
	be represented by a diagonal weighting in the curvelet domain. In
	that paper, a smoothness constraint was introduced in the phase space
	of the operator in order to regularize the solution to make it unique.
	In this paper, we use recent results in Demanet and Ying (2008) on
	the discrete symbol calculus to impose a further smoothness constraint,
	this time in the frequency domain. It is found that with this additional
	constraint, faster convergence is realized. Results on a synthetic
	pseudodifferential operator as well as on an example of primary-multiple
	separation in seismic data are included, comparing the model with
	and without the new smoothness constraint, from which it is found
	that results of improved quality are also obtained.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255624},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/shahidi09SEGcmf/shahidi09segcmf.pdf}
}


@CONFERENCE{tang2009SEGhdb,
  author = {Gang Tang and Reza Shahidi and Felix J. Herrmann and Jianwei Ma},
  title = {Higher dimensional blue-noise sampling schemes for curvelet-based
	seismic data recovery},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {191-195},
  organization = {SEG},
  abstract = {In combination with compressive sensing, a successful reconstruction
	scheme called Curvelet-based Recovery by Sparsity-promoting Inversion
	(CRSI) has been developed, and has proven to be useful for seismic
	data processing. One of the most important issues for CRSI is the
	sampling scheme, which can greatly affect the quality of reconstruction.
	Unlike usual regular undersampling, stochastic sampling can convert
	aliases to easy-to-eliminate noise. Some stochastic sampling methods
	have been developed for CRSI, e.g. jittered sampling, however most
	have only been applied to 1D sampling along a line. Seismic datasets
	are usually higher dimensional and very large, thus it is desirable
	and often necessary to develop higher dimensional sampling methods
	to deal with these data. For dimensions higher than one, few results
	have been reported, except uniform random sampling, which does not
	perform well. In the present paper, we explore 2D sampling methodologies
	for curvelet-based reconstruction, possessing sampling spectra with
	blue noise characteristics, such as Poisson Disk sampling, Farthest
	Point Sampling, and the 2D extension of jittered sampling. These
	sampling methods are shown to lead to better recovery and results
	are compared to the other more traditional sampling protocols.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255230},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/tang09SEGhdb/tang09SEGhdb.pdf}
}

@CONFERENCE{thomson2006SINBADlss,
  author = {Darren Thomson},
  title = {Large-scale seismic data recovery by the Parallel Windowed Curvelet
	Transform},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {We propose using overlapping, tapered windows to process seismic data
	in parallel. This method consists of numerically tight linear operators
	and adjoints that are suitable for use in iterative algorithms. This
	method is also highly scalable and makes parallelprocessing of large
	seismic data sets feasible. We use this scheme to define the Parallel
	Windowed Fast Discrete Curvelet Transform (PWFDCT), which we have
	applied to a seismic data interpolation algorithm. Some preliminary
	results will be shown. Henryk Modzeleweski: Design and specifications
	for SLIMPy's software framework The SLIM group is actively developing
	software for seismic imaging. This talk will give a general overview
	of the software development philosophy adopted by SLIM. The covered
	topics will include: 1) adopting Python for object-oriented programming,
	2) including parallelism into the algorithms used in seismic imaging/modeling,
	3) in-house algorithms for seismic imaging, and 4) contributions
	to Madagascar (RSF). The talk will serve as an introduction to the
	other presentations in the session ``SINBAD Software releases".},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/thomson2006SINBADlss/thomson2006SINBADlss.pdf}
}

@CONFERENCE{thomson2006SINBADppe,
  author = {Darren Thomson},
  title = {(P)SLIMPy: parallel extension},
  booktitle = {SINBAD 2006},
  year = {2006},
  abstract = {The parallel extensions to the SLIMpy environment enable pipe-based
	processing of large data sets in an MPI-based parallel environment.
	Parallel processing can be done by straightforward slicing of data,
	or by using an overlapping domain decomposition that requires communication
	between different processors. The principal aim of the parallel extensions
	is to leave abstract numerical algorithms (ANA's) and applications
	programmed for use in SLIMpy untouched when moving to parallel processing.
	The object-oriented functionality of Python makes this possible.},
  keywords = {SLIM, SINBAD, Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2006/thomson2006SINBADppe/thomson2006SINBADppe.pdf}
}

@CONFERENCE{thomson2006SEGpwfd,
  author = {Darren Thomson and Gilles Hennenfent and Henryk Modzelewski and Felix
	J. Herrmann},
  title = {A parallel windowed fast discrete curvelet transform applied to seismic
	processing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  volume = {25},
  pages = {2767-2771},
  organization = {SEG},
  abstract = {We propose using overlapping, tapered windows to process seismic data
	in parallel. This method consists of numerically tight linear oper
	ators and adjoints that are suitable for use in iterative algorithms.
	This method is also highly scalable and makes parallel processing
	of large seismic data sets feasible. We use this scheme to define
	the Parallel Windowed Fast Discrete Curvelet Transform (PWFDCT),
	which we apply to a seismic data interpolation algorithm. The successful
	performance of our parallel processing scheme and algorithm on a
	two-dimensional synthetic data is shown.},
  keywords = {SEG},
  optdoi = {10.1190/1.2370099},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/thomson06SEGpwfd/thomson06SEGpwfd.pdf }
}

@CONFERENCE{tu2011EAGElmf,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Least-squares migration of full wavefield with source encoding},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {Multiples can provide valuable information that is missing in primaries,
	and there is a growing interest in using them for seismic imaging.
	In our earlier work, we proposed to combine primary estimation and
	migration to image from the total up-going wavefield. The method
	proves to be effective but computationally expensive. In this abstract,
	we propose to reduce the computational cost by removing the multi-dimensional
	convolution required by primary estimation, and reducing the number
	of PDE solves in migration by introducing simultaneous sources with
	source renewal. We gain great performance boost without compromising
	the quality of the image.},
  keywords = {EAGE, depth migration, surface-related multiples},
  optmonth = {01/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/tu2011EAGElmf/tu2011EAGElmf_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/tu2011EAGElmf/tu2011EAGElmf.pdf}
}

@CONFERENCE{tu2012SEGima,
  author = {Ning Tu and Felix J. Herrmann},
  title = {Imaging with multiples accelerated by message passing},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  organization = {SEG},
  abstract = {With the growing realization that multiples can provide valuable information,
	there is a paradigm shift from removing them to using them. For instance,
	primary estimation by sparse inversion has demonstrated its superiority
	over surface-related multiple removal in many aspects. Inspired by
	this shift, we propose a method to image directly from the total
	up-going wavefield, including surface-related multiples, by sparse
	inversion. To address the high computational cost associated with
	this method, we propose to speed up the inversion by having the wave-equation
	solver carry out the multi-dimensional convolutions implicitly and
	cheaply by randomized subsampling. We improve the overall performance
	of this algorithm by selecting new independent copies of the randomized
	modeling operator, which leads to a cancellation of correlations
	that hamper the speed of convergence of the solver. We show the merits
	of our approach on a number of examples.},
  keywords = {SEG, Imaging, Multiples},
  optmonth = {04/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima.pdf}
}

@CONFERENCE{tu2011EAGEspmsrm,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Sparsity-promoting migration with surface-related multiples},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2011},
  organization = {EAGE},
  abstract = {Multiples, especially the surface-related multiples, form a significant
	part of the total up-going wave- field. If not properly dealt with,
	they can lead to false reflectors in the final image. So conventionally
	practitioners remove them prior to migration. Recently research has
	revealed that multiples can actually provide extra illumination so
	different methods are proposed to address the issue that how to use
	multiples in seismic imaging, but with various kinds of limitations.
	In this abstract, we combine primary estimation and sparsity-promoting
	migration into one convex-optimization process to include information
	from multiples. Synthetic examples show that multiples do make active
	contributions to seismic migration. Also by this combination, we
	can benefit from better recoveries of the GreenÕs function by using
	sparsity-promoting algorithms since reflectivity is sparser than
	the GreenÕs function.},
  keywords = {Presentation,EAGE,Imaging,Processing},
  optmonth = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm.pdf}
}

@CONFERENCE{tu2011SEGmult,
  author = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title = {Migration with surface-related multiples from incomplete seismic
	data},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  volume = {30},
  pages = {3222-3227},
  organization = {SEG},
  abstract = {Seismic acquisition is confined by limited aperture that leads to
	finite illumination, which, together with other factors, hin- ders
	imaging of subsurface objects in complex geological set- tings such
	as salt structures. Conventional processing, includ- ing surface-related
	multiple elimination, further reduces the amount of information we
	can get from seismic data. With the growing consensus that multiples
	carry valuable informa- tion that is missing from primaries, we are
	motivated to exploit the extra illumination provided by multiples
	to image the sub- surface. In earlier research, we proposed such
	a method by combining primary estimation and sparsity-promoting migra-
	tion to invert for model perturbations directly from the total up-going
	wavefield. In this abstract, we focus on a particular case. By exploiting
	the extra illumination from surface-related multiples, we mitigate
	the effects caused by migrating from in- complete data with missing
	sources and missing near-offsets.},
  keywords = {Presentation,SEG,Imaging,Processing},
  optdoi = {10.1190/1.3627865},
  optmonth = {04/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult.pdf}
}

@CONFERENCE{vandenberg2008IAMesr,
  author = {Ewout {van den Berg}},
  title = {Exact sparse reconstruction and neighbourly polytopes},
  booktitle = {IAM},
  year = {2008},
  bdsk-url-1 = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/IAM/2008/vandenberg2008IAMesr/vandenberg2008IAMesr.pdf},
  date-added = {2008-08-26 15:44:44 -0700},
  date-modified = {2008-08-26 15:45:58 -0700},
  keywords = {SLIM, IAM, Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/IAM/2008/vandenberg2008IAMesr/vandenberg2008IAMesr.pdf}
}

@CONFERENCE{vandenberg2008SINBADsat,
  author = {Ewout {van den Berg}},
  title = {Sparco: A testing framework for sparse reconstruction},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Sparco is a framework for testing and benchmarking algorithms for
	sparse reconstruction. It includes a large collection of sparse reconstruction
	problems drawn from the imaging, compressed sensing, and geophysics
	literature. Sparco is also a framework for implementing new test
	problems and can be used as a tool for reproducible research. We
	describe the software environment, and demonstrate its usefulness
	for testing and comparing solvers for sparse reconstruction.},
  date-modified = {2008-08-22 12:54:25 -0700},
  keywords = {SLIM, SINBAD, Presentation},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/vandenberg2008SINBADsat/vandenberg2008SINBADsat.pdf}
}

@CONFERENCE{friedlander2009SCAIMspot,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {Spot: A linear-operator toolbox for Matlab},
  booktitle = {SCAIM},
  year = {2009},
  address = {University of British Columbia},
  organization = {SCAIM Seminar},
  keywords = {minimization, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2009Nov/friedlander2009SCAIMspot/friedlander2009SCAIMspot.pdf}
}

@CONFERENCE{vandenberg2007SINBADipo1,
  author = {Ewout {van den Berg} and Michael P. Friedlander},
  title = {In Pursuit of a Root},
  booktitle = {2007 Von Neumann Symposium},
  year = {2007},
  keywords = {minimization, Presentation, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2007/vandenberg2007SINBADipo1/vandenberg2007SINBADipo1.pdf}
}

@CONFERENCE{vandenberg2009SLIMocf,
  author = {Ewout {van den Berg} and Mark Schmidt and Michael P. Friedlander
	and K. Murphy},
  title = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory
	Projected Quasi-Newton Algorithm},
  booktitle = {SLIM},
  year = {2009},
  volume = {12},
  series = {Twelfth International Conference on Artificial Intelligence and Statistics},
  abstract = {An optimization algorithm for minimizing a smooth function over a
	convex set is described. Each iteration of the method computes a
	descent direction by minimizing, over the original constraints, a
	diagonal-plus low-rank quadratic approximation to the function. The
	quadratic approximation is constructed using a limited-memory quasi-Newton
	update. The method is suitable for large-scale problems where evaluation
	of the function is substan- tially more expensive than projection
	onto the constraint set. Numerical experiments on one- norm regularized
	test problems indicate that the proposed method is competitve with
	state- of-the-art methods such as bound-constrained L-BFGS and orthant-wise
	descent. We further show that the method generalizes to a wide class
	of problems, and substantially improves on state-of-the-art methods
	for problems such as learning the structure of Gaussian graphi- cal
	models (involving positive-definite matrix constraints) and Markov
	random fields (in- volving second-order cone constraints).},
  date-added = {2009-01-29 17:16:34 -0800},
  date-modified = {2009-01-29 17:16:34 -0800},
  keywords = {SLIM},
  optmonth = {04/2009},
  url = {http://www.cs.ubc.ca/~mpf/papers/SchmidtBergFriedMurph09.pdf}
}

@CONFERENCE{vanderneut2012EAGEdecomp,
  author = {Joost {van der Neut} and Felix J. Herrmann},
  title = {Up / down wavefield decomposition by sparse inversion},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {Expressions have been derived for the decomposition of multi-component
	seismic recordings into up- and down-going constituents. However,
	these expressions contain singularities at critical angles and can
	be sensitive for noise. By interpreting wavefield decomposition as
	an inverse problem and imposing constraints on the sparseness of
	the solution, we arrive at a robust formalism that can be applied
	to noisy data. The method is demonstrated on synthetic data with
	multi-component receivers in a horizontal borehole, but can also
	be applied for different configurations, including OBC and dual-sensor
	streamers.},
  keywords = {EAGE,wavefield decomposition, sparse inversion},
  optmonth = {01/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp.pdf}
}

@CONFERENCE{vanderneut2012SEGirs,
  author = {Joost {van der Neut} and Felix J. Herrmann and Kees Wapenaar},
  title = {Interferometric redatuming with simultaneous and missing sources
	using sparsity promotion in the curvelet domain},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2012},
  organization = {SEG},
  abstract = {Interferometric redatuming is a velocity-independent method to turn
	downhole receivers into virtual sources. Accurate redatuming involves
	solving an inverse problem, which can be highly ill-posed, especially
	in the presence of noise, incomplete data and limited aperture. We
	address these issues by combining interferometric redatuming with
	transform-domain sparsity promotion, leading to a formulation that
	deals with data imperfections. We show that sparsity promotion improves
	the retrieval of virtual shot records under a salt flank. To reduce
	acquisition costs, it can be beneficial to reduce the number of sources
	or shoot them simultaneously. It is shown that sparse inversion can
	still provide a stable solution in such cases.},
  keywords = {Processing,Imaging,Optimization,Interferometry, SEG},
  optmonth = {04/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/vanderneut2012SEGirs/vanderneut2012SEGirs.pdf}
}

@CONFERENCE{vanleeuwen2012EAGEcarpcg,
  author = {Tristan {van Leeuwen} and Dan Gordon and Rachel Gordon and Felix
	J. Herrmann},
  title = {Preconditioning the Helmholtz equation via row-projections},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {3D frequency-domain full waveform inversion relies on being able to
	efficiently solve the 3D Helmholtz equation. Iterative methods require
	sophisticated preconditioners because the Helmholtz matrix is typically
	indefinite. We review a preconditioning technique that is based on
	row-projections. Notable advantages of this preconditioner over existing
	ones are that it has low algorithmic complexity, is easily parallelizable
	and extendable to time-harmonic vector equations.},
  keywords = {EAGE, Helmholtz equation, precondition},
  optmonth = {01/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg.pdf}
}

@CONFERENCE{vanleeuwen2012EAGEext,
  author = {Tristan {van Leeuwen} and Felix J. Herrmann},
  title = {Wave-equation extended images: computation and velocity continuation},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {An extended image is a multi-dimensional correlation of source and
	receiver wavefields. For a kinematically correct velocity, most of
	the energy will be concentrated at zero offset. Because of the computational
	cost involved in correlating the wavefields for all offsets, such
	exteded images are computed for a subsurface offset that is aligned
	with the local dip. In this paper, we present an efficient way to
	compute extended images for all subsurface offsets without explicitly
	calculating the receiver wavefields, thus making it computationally
	feasible to compute such extended images. We show how more conventional
	image gathers, where the offset is aligned with the dip, can be extracted
	from this extended image. We also present a velocity continuation
	procedure that allows us to compute the extended image for a given
	velocity without recomputing all the source wavefields.},
  keywords = {EAGE, extended image, velocity continuation},
  optmonth = {01/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext.pdf}
}

@CONFERENCE{verschuur2007SEGmmp,
  author = {D. J. Verschuur and Deli Wang and Felix J. Herrmann},
  title = {Multiterm multiple prediction using separated reflections and diffractions
	combined with curvelet-based subtraction},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2535-2539},
  organization = {SEG},
  abstract = {The surface-related multiple elimination (SRME) method has proven
	to be successful on a large number of data cases. Most of the applications
	are still 2D, as the full 3D implementation is still expensive and
	under development. However, the earth is a 3D medium, such that 3D
	effects are difficult to avoid. Most of the 3D effects come from
	diffractive structures, whereas the specular reflections normally
	have less of a 3D behavior. By separating the seismic data in a specular
	reflecting and a diffractive part, multiple prediction can be carried
	out with these different subsets of the input data, resulting in
	several categories of predicted multiples. Because each category
	of predicted multiples can be subtracted from the input data with
	different adaptation filters, a more flexible SRME procedure is obtained.
	Based on some initial results from a Gulf of Mexico dataset, the
	potential of this approach is investigated. {\copyright}2007 Society
	of Exploration Geophysicists},
  keywords = {Presentation, SLIM, SEG},
  optdoi = {10.1190/1.2792993},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp.pdf }
}

@CONFERENCE{wang2008SINBADrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present a nonlinear curvelet-based sparsity-promoting formulation
	for the primary-multiple separation problem. We show that these coherent
	signal components can be separated robustly by explicitly exploiting
	the locality of curvelets in phase space (space-spatial frequency
	plane) and their ability to compress data volumes that contain wavefronts.
	This work is an extension of earlier results and the presented algorithms
	are shown to be stable under noise and moderately erroneous multiple
	predictions.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/wang2008SINBADrri/wang2008SINBADrri.pdf}
}

@CONFERENCE{wang2007SEGrri,
  author = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title = {Recent results in curvelet-based primary-multiple separation: application
	to real data},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2007},
  volume = {26},
  pages = {2500-2504},
  organization = {SEG},
  abstract = {In this abstract, we present a nonlinear curvelet-based sparsity-promoting
	formulation for the primary-multiple separation problem. We show
	that these coherent signal components can be separated robustly by
	explicitly exploting the locality of curvelets in phase space (space-spatial
	frequency plane) and their ability to compress data volumes that
	contain wavefronts. This work is an extension of earlier results
	and the presented algorithms are shown to be stable under noise and
	moderately erroneous multiple predictions. {\copyright}2007 Society
	of Exploration Geophysicists},
  keywords = {Presentation, SLIM},
  optdoi = {10.1190/1.2792986},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri.pdf }
}

@CONFERENCE{wason2012CSEGode,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Only dither: efficient simultaneous marine acquisition},
  booktitle = {CSEG technical program},
  year = {2012},
  abstract = {Simultaneous-source acquisition is an emerging technology that is
	stimulating both geophysical research and commercial efforts. Simultaneous
	marine acquisition calls for the development of a new set of design
	principles and post-processing tools. The focus here is on simultaneous-source
	marine acquisition design and sparsity-promoting sequential-source
	data recovery. We propose a pragmatic simultaneous-source, randomized
	marine acquisition scheme where multiple vessels sail across an ocean-bottom
	array firing airguns at — sequential locations and randomly time-dithered
	instances. By leveraging established findings from the field of compressive
	sensing, where the choice of the sparsifying transform needs to be
	incoherent with the compressive sampling matrix, we can significantly
	impact the reconstruction quality, and demonstrate that the compressive
	sampling matrix resulting from the proposed sampling scheme is sufficiently
	incoherent with the curvelet transform to yield successful recovery
	by sparsity promotion. Results are illustrated with simulations of
	“purely” random marine acquisition, which requires an airgun
	to be located at each source location, and random time-dithering
	marine acquisition with one and two source vessels. Size of the collected
	data volumes in all cases is the same. Compared to the recovery from
	the former acquisition scheme (SNR = 10.5dB), we get good results
	by dithering with only one source vessel (SNR = 8.06dB) in the latter
	scheme, which improve at the cost of having an additional source
	vessel (SNR = 9.85dB).},
  keywords = {CSEG, acquisition, marine, simultaneous},
  optmonth = {02/2012},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2012/wason2012CSEGode/wason2012CSEGode.pdf}
}

@CONFERENCE{wason2012EAGEode,
  author = {Haneet Wason and Felix J. Herrmann},
  title = {Only dither: efficient simultaneous marine acquisition},
  booktitle = {EAGE technical program},
  year = {2012},
  organization = {EAGE},
  abstract = {Simultaneous-source acquisition is an emerging technology that is
	stimulating both geophysical research and commercial efforts. The
	focus here is on simultaneous-source marine acquisition design and
	sparsity-promoting sequential-source data recovery. We propose a
	pragmatic simultaneous-source, randomized marine acquisition scheme
	where multiple vessels sail across an ocean-bottom array firing airguns
	at—sequential locations and randomly time-dithered instances. Within
	the context of compressive sensing, where the choice of the sparsifying
	transform needs to be incoherent with the compressive sampling matrix,
	we can significantly impact the reconstruction quality, and demonstrate
	that the compressive sampling matrix resulting from the proposed
	sampling scheme is sufficiently incoherent with the curvelet transform
	to yield successful recovery by sparsity promotion. Results are illustrated
	with simulations of “purely” random marine acquisition, which
	requires an airgun to be located at each source location, and random
	time-dithering marine acquisition with one and two source vessels.
	Size of the collected data volumes in all cases is the same. Compared
	to the recovery from the former acquisition scheme (SNR = 10.5dB),
	we get good results by dithering with only one source vessel (SNR
	= 8.06dB) in the latter scheme, which improve at the cost of having
	an additional source vessel (SNR = 9.44dB).},
  keywords = {EAGE, acquisition, marine, simultaneous},
  optmonth = {01/2012},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode.pdf}
}

@CONFERENCE{wason2011SEGsprsd,
  author = {Haneet Wason and Felix J. Herrmann and Tim T.Y. Lin},
  title = {Sparsity-promoting recovery from simultaneous data: a compressive
	sensing approach},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2011},
  volume = {30},
  pages = {6-10},
  organization = {SEG},
  abstract = {Seismic data acquisition forms one of the main bottlenecks in seismic
	imaging and inversion. The high cost of acquisition work and collection
	of massive data volumes compel the adoption of simultaneous-source
	seismic data acquisition - an emerging technology that is developing
	rapidly, stimulating both geophysical research and commercial efforts.
	Aimed at improving the performance of marine- and land-acquisition
	crews, simultaneous acquisition calls for development of a new set
	of design principles and post-processing tools. Leveraging developments
	from the field of compressive sensing the focus here is on simultaneous-acquisition
	design and sequential-source data recovery. Apart from proper compressive
	sensing sampling schemes, the recovery from simultaneous simulations
	depends on a sparsifying transform that compresses seismic data,
	is fast, and reasonably incoherent with the compressive-sampling
	matrix. Using the curvelet transform, in which seismic data can be
	represented parsimoniously, the recovery of the sequential-source
	data volumes is achieved using the sparsity-promoting program {\textemdash}
	SPGL1, a solver based on projected spectral gradients. The main outcome
	of this approach is a new technology where acquisition related costs
	are no longer determined by the stringent Nyquist sampling criteria.},
  keywords = {Presentation,SEG,Acquisition,Compressive Sensing},
  optdoi = {10.1190/1.3628174},
  optmonth = {04/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd.pdf}
}

@CONFERENCE{yan2008SINBADwru,
  author = {Jiupeng Yan},
  title = {Wavefield Reconstruction Using Simultaneous Denoising Interpolation
	vs. Denoising after Interpolation},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {This report represents and compares two methods of wavefield reconstruction
	from noisy seismic data with missing traces. The two methods are
	(i) First interpolate incomplete noisy data to get complete noisy
	data and then denoise, and (ii) Interpolate and denoise the incomplete
	noisy data simultaneously. A sample test of synthetic data will be
	presented. The results of tests show that denoising after interpolation
	is better than simultaneous denoising and interpolation if the parameter
	of the denoising problem is chosen appropriately.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/yan2008SINBADwru/yan2008SINBADwru.pdf}
}

@CONFERENCE{yan2009SEGgpb,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain
	matched filtering},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  volume = {28},
  pages = {3297-3301},
  organization = {SEG},
  abstract = {The removal of groundroll in land based seismic data is a critical
	step for seismic imaging. In this paper, we introduce a work flow
	to predict the groundroll by interferometry and then separate the
	groundroll in the curvelet domain. Thus workflow is similar to the
	workflow of surface-related multiple elimination (SRME). By exploiting
	the adaptability and sparsity of curvelets, we are able to significantly
	improve the separation of groundroll in comparison to results yielded
	by frequency-domain adaptive subtraction methods. We provide synthetic
	data example to illustrate our claim.},
  keywords = {Presentation,SEG},
  optdoi = {10.1190/1.3255544},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb_pres.pdf},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb.pdf}
}

@CONFERENCE{yan2009SEGgpb2,
  author = {Jiupeng Yan and Felix J. Herrmann},
  title = {Groundroll prediction by interferometry and separation by curvelet-domain
	filtering. Presented at the 79th SEG Meeting, Houston},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2009},
  abstract = {The removal of groundroll in land based seismic data is a critical
	step for seismic imaging. In this paper, we introduce a work flow
	to predict the groundroll by interferometry and then separate the
	groundroll in the curvelet domain. Thus workflow is similar to the
	workflow of surface-related multiple elimination (SRME). By exploiting
	the adaptability and sparsity of curvelets, we are able to significantly
	improve the separation of groundroll in comparison to results yielded
	by frequency-domain adaptive subtraction methods. We provide synthetic
	data example to illustrate our claim.},
  keywords = {Presentation},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan2009SEGgpb2/yan2009SEGgpb2.pdf}
}

@CONFERENCE{yarham2008SINBADbss,
  author = {Carson Yarham},
  title = {Bayesian signal separation applied to ground-roll removal},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {Accurate and adaptive noise removal is a critical part in seismic
	processing. Recent developments in signal separation methods have
	allowed a more flexible and accurate framework in which to perform
	ground roll and reflector separation. The use of a new Bayesian separation
	scheme developed at the SLIM group that contains control parameters
	to adjust for the uniqueness of specific problems is used. The sensitivity
	and variation of the control parameters is examined and this method
	is applied to synthetic and real data and the results are compared
	to previous methods.},
  date-modified = {2008-08-22 12:42:58 -0700},
  keywords = {Presentation,SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/yarham2008SINBADbss/yarham2008SINBADbss.pdf}
}

@CONFERENCE{yarham2007SINBADnsw,
  author = {Carson Yarham},
  title = {Nonlinear surface wave prediction and separation},
  booktitle = {SINBAD 2007},
  year = {2007},
  abstract = {Removal of surface waves is an integral step in seismic processing.
	There are many standard techniques for removal of this type of coherent
	noise, such as f-k filtering, but these methods are not always effective.
	One of the common problems with removal of surface waves is that
	they tend to be aliased in the frequency domain. This can make removal
	difficult and affect the frequency content of the reflector signals,
	as this signals will not be completely separated. As seen in (Hennenfent,
	G. and F. Herrmann, 2006, Application of stable signal recovery to
	seismic interpolation) interpolation can be used effectively to resample
	the seismic record thus dealiasing the surface waves. This separates
	the signals in the frequency domain allowing for a more precise and
	complete removal. The use of this technique with curvelet based surface
	wave predictions and an iterative L1 separation scheme can be used
	to remove surface waves from shot records more completely that with
	standard techniques.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2007/yarham2007SINBADnsw/yarham2007SINBADnsw.pdf}
}

@CONFERENCE{yarham2006SEGcgrr,
  author = {Carson Yarham and Urs Boeniger and Felix J. Herrmann},
  title = {Curvelet-based ground roll removal},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2006},
  volume = {25},
  pages = {2777-2782},
  organization = {SEG},
  abstract = {We have effectively identified and removed ground roll through a two-
	step process. The first step is to identify the major components
	of the ground roll through various methods including multiscale separation,
	directional or frequency filtering or by any other method that identifies
	the ground roll. Given this estimate for ground roll, the recorded
	signal is separated during the second step through a block-coordinate
	relaxation method that seeks the sparsest set for weighted curvelet
	coefficients of the ground roll and the sought-after reflectivity.
	The combination of these two methods allows us to separate out the
	ground roll signal while preserving the reflector information. Since
	our method is iterative, we have control of the separation process.
	We successfully tested our algorithm on a real data set with a complex
	ground roll and reflector structure.},
  keywords = {SEG},
  optdoi = {10.1190/1.2370101},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/yarham06SEGcgrr/yarham06SEGcgrr.pdf }
}

@CONFERENCE{yarham2007EAGEcai,
  author = {Carson Yarham and Gilles Hennenfent and Felix J. Herrmann},
  title = {Curvelet applications in surface wave removal},
  booktitle = {EAGE Technical Program Expanded Abstracts},
  year = {2007},
  organization = {EAGE},
  abstract = {Ground roll removal of seismic signals can be a challenging prospect.
	Dealing with undersampleing causing aliased waves amplitudes orders
	of magnitude higher than reflector signals and low frequency loss
	of information due to band ...},
  keywords = {SLIM, EAGE},
  optmonth = {June},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/yarham07EAGEcai/yarham07EAGEcai.pdf}
}

@CONFERENCE{yarham2008SEGbgr,
  author = {Carson Yarham and Felix J. Herrmann},
  title = {Bayesian ground-roll seperation by curvelet-domain sparsity promotion},
  booktitle = {SEG Technical Program Expanded Abstracts},
  year = {2008},
  volume = {27},
  pages = {2576-2580},
  organization = {SEG},
  abstract = {The removal of coherent noise generated by surface waves in land based
	seismic is a prerequisite to imaging the subsurface. These surface
	waves, termed as ground roll, overlay important reflector information
	in both the t-x and f-k domains. Standard techniques of ground-roll
	removal commonly alter reflector information. We propose the use
	of the curvelet domain as a sparsifying transform in which to preform
	signal-separation techniques that preserves reflector information
	while increasing ground-roll removal. We look at how this method
	preforms on synthetic data for which we can build quantitative results
	and a real field data set.},
  keywords = {Presentation,SLIM},
  optdoi = {10.1190/1.3063878},
  optmonth = {November},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr.pdf }
}

@CONFERENCE{yarham2004CSEGgrr,
  author = {Carson Yarham and Felix J. Herrmann and Daniel Trad},
  title = {Ground Roll Removal Using Non-Separable Wavelet Transforms },
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2004},
  keywords = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGgrr/Yarham04CSEGgrr.pdf
	}
}

@CONFERENCE{yarham2004CSEGcpa,
  author = {Carson Yarham and Daniel Trad and Felix J. Herrmann},
  title = {Curvelet processing and imaging: adaptive ground roll removal},
  booktitle = {CSEG Technical Program Expanded Abstracts},
  year = {2004},
  organization = {CSEG},
  abstract = {In this paper we present examples of ground roll attenuation for synthetic
	and real data gathers by using Contourlet and Curvelet transforms.
	These non-separable wavelet transforms are locoalized both (x,t)-
	and (k,f)-domains and allow for adaptive seperation of signal and
	ground roll. Both linear and non-linear filtering are discussed using
	the unique properties of these basis that allow for simultaneous
	localization in the both domains. Eventhough, the linear filtering
	techniques are encouraging the true added value of these basis-function
	techniques becomes apparent when we use these decompositions to adaptively
	substract modeled ground roll from data using a non-linear thesholding
	procedure. We show real and synthetic examples and the results suggest
	that these directional-selective basis functions provide a usefull
	tool for the removal of coherent noise such as ground roll.},
  keywords = {Presentation, SLIM, CSEG},
  optmonth = {05/2005},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/Yarham04CSEGcpa_pres.pdf
	},
  url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/yarham04csegcpa.pdf}
}

@CONFERENCE{yilmaz2008SINBADsse,
  author = {Ozgur Yilmaz},
  title = {Stable sparse expansions via non-convex optimization},
  booktitle = {SINBAD 2008},
  year = {2008},
  abstract = {We present theoretical results pertaining to the ability of p-(quasi)norm
	minimization to recover sparse and compressible signals from incomplete
	and noisy measurements. In particular, we extend the results of Candes,
	Romberg and Tao for 1-norm to the p<1 case. Our results indicate
	that depending on the restricted isometry constants and the noise
	level, p-norm minimization with certain values of p<1 provides better
	theoretical guarantees in terms of stability and robustness compared
	to 1-norm minimization. This is especially true when the restricted
	isometry constants are relatively large, or equivalently, when the
	data is significantly undersampled.},
  keywords = {Presentation, SINBAD, SLIM},
  url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2008/yilmaz2008SINBADsse/yilmaz2008SINBADsse.pdf}
}

