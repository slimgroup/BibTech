%-------------------------------------------------2012-------------------------------------------------

@conference{herrmann2012SINBADfwi,
        title = {Our findings on the Chevron benchmark dataset},
        booktitle = {SINBAD consortium talks},
        year = {2012},
        organization = {SINBAD},
        abstract = {During this presentation, we will review our findings working with the synthetic GOM data released as part of the post SEG workshop:"Gulf of Mexico Imaging Challenges: What Can Full Waveform Inversion Achieve?". This is joint work with Andrew J. Calvert, Ian Hanlon, Mostafa Javanmehri, Rajiv Kumar, Tristan van Leeuwen, Xiang Li, Brendan Smithyman, Eric Takam Takougang, and Haneet Wason.},
        keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
        author = {Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv Kumar and Tristan van Leeuwen and Xiang Li and Brendan Smithyman and Eric Takam Takougang and Haneet Wason and Felix J. Herrmann}	
}


@conference{herrmann2012SINBADals,
        title = {Fast sparsity-promoting imaging with message passing},
        booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
        abstract = {To meet current-day challenges, exploration seismology increasingly relies on more and more sophisticated algorithms that require multiple paths through all data. This requirement leads to problems because the size of seismic data volumes is increasing exponentially, exposing bottlenecks in IO and computational capability. To overcome these bottlenecks, we follow recent trends in machine learning and compressive sensing by proposing a sparsity-promoting inversion technique that works on small randomized subsets of data only. We boost the performance of this algorithm significantly by modifying a state-of-the-art l1-norm solver to benefit from message passing, which breaks the build up of correlations between model iterates and the randomized linear forward model. We demonstrate the performance of this algorithm on a toy sparse-recovery problem and on a realistic reverse-time-migration example with random source encoding. The improvements in speed, memory use, and output quality are truly remarkable.},
       keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
       url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
       author = {Felix J. Herrmann}
}


@conference{peters2012SINBADfde, 
        title = {Frequency domain 3D elastic wave propagation in general anisotropic media},
        booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
        abstract = {Elastic wave propagation in 3 spatial dimensions is modeled using a wave equation containing the full stiffness tensor consisting of 21 independent components. This allows modeling in general anisotropic media. The wave equation is discretized on several Cartesian and rotated Cartesian staggered finite-difference grids (using a 2nd order approximation). The grids are linearly combined and, in combination with a antilumped mass strategy, minimize numerical dispersion while requiring a low number of grid points per wavelength. In case not all 21 components need to be modeled, an approximation of the stiffness tensor can be used (e.g., orthorhombic anisotropy, TTI, ...). This results in a linear system of equations, which is solved using an iterative method. The modeling of all 21 components of the stiffness tensor (or an approximation) enables the development of new waveform inversion functionalities.},
        keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Bas Peters}
}
        

@conference {Akalin2012SINBADlss,
	title = {Large scale seismic data interpolation with matrix completion},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Seismic surveys amass large and incomplete data sets, and designing algorithms to interpolate the missing data at very large scales poses a daunting and critical challenge.  We study how to apply scalable matrix completion methods to such interpolation problems.  Recent studies in matrix completion have shown that a matrix that has low rank can be exactly completed when only a small number of observations are available. However, there are two challenges to applying matrix completion to seismic data. Matrix completion is typically applied to two dimensional or dyadic data whereas seismic data is often tensorial. Also successful matrix completion requires a low-rank matrix structure. We address these problems by organizing the seismic data on a matrix grid which exhibits a low-rank structure. This encoding allows us to apply the Jellyfish algorithm, developed at the University of Wisconsin, which achieves state-of-the-art performance for large-scale matrix completion. The proposed framework makes it possible to complete high-SNR interpolations of gigabytes of 4-D seismic data in minutes on standard multicore workstations. Our preliminary experimental results suggest that matrix completion provides a promising new approach to the seismic data interpolation problem.
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Okan Akalin}
}

@conference {xiang2012SINBADfgn,
	title = {Fast Gauss-Newton full-waveform inversion with sparsity regularization},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Full-waveform inversion (FWI) can be considered as a controlled data fitting process, in which we approximately fit observed data by iteratively updating the initial velocity model, we expect the final model can reveal subsurface structure till the wavefield misfit can converge to designed tolerance.  The conventional FWI approach is expensive since it requires the inversion of a linear system, which involves extremely large multi-experiment data volumes. To overcome this issue we percent a curvetlet based sparsity-promoting Gauss-Newton inversion method. In this presentation we invert for the model updates by replacing the normal Gauss-Newton linearized subproblem for subsampled FWI with a sparsity promoting FWI formulation. We speed up the algorithm and avoid over fitting the data by solving the problem approximately. Aside from this, we control wavefield dispersion by gradually increasing grid size as we move to higher frequencies. Our approach is successful because it reduces the size of seismic data volumes without loss of information. With this reduction, we can compute a Newton-like update with the reduced data volume at the cost of roughly one gradient update for the fully sampled wavefield.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Xiang Li}
}

@conference {krislock2012SINBADwsn,
	title = {Wireless Sensor Network Localization},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Locating the position of sensors connected together in a wireless network given only the position of a small number of the sensors and estimates of some of the distances between the sensors is a difficult problem with many modern applications. Within the last few years research in wireless sensor network localization has greatly increased due to the many new applications using wireless sensors, from lightweight sensors used to monitor the environment to ocean-bottom sensors used in geophysical applications. A second reason for this increased interest is our recent ability to efficiently solve these problems using modern semidefinite optimization solvers. We will discuss how semidefinite optimization can be used to solve such problems and possible directions for future work.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Nathan Krislock}
}

@conference {miao2012SINBADtjs,
	title = {Towards joint sparsity in seismic imaging},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {As has been widely explored in l1 migration scheme, seismic image is sparse under proper transform bases, for example, curvelet. Accordingly, as suggested by seismic imaging condition, similar sparse pattern is expected among ray parameters in prestack image gather, providing a joint sparsity scheme for prestack l1 constrained migration. By moving from poststack sparse migraion to prestack joint sparse migration, we show here the recovery result are more informative. In terms of
wave extrapolation, a spectral projector is introduced to eliminate evanescent wave mode. The spectral projector is designed as a recursive matrix polynomial, which is accelerated by exploiting hieratically semi-separable matrix approximation. This hss acceleration, by itself has more broad application potential.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Lina Miao}
}

@conference {miao2012SINBADasp,
	title = {Accelerating on sparse promoting recovery and its benefits in seismic application},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Sparse promoting recovery problem arises more and more frequently with the broad application of compressed sensing tool in exploration seismology. Because of the curse of dimensionality, the prohibitive computation burden on iteratively evaluating objective functions is one of the key issues that constrain high performance l1 solver. In this paper, we try to further improve the convergence performance of SPGl1, one of the state-of-the-art large scale sparse recovery solver, and as a result limit the number of objective function evaluations by introducing a projected quasi Newton method. Examples showing acceleration on seismic data collection, data processing as well as inversion are included.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Lina Miao}
}

@conference {friedlander2012SINBADrsh,
	title = {Randomized sampling: How confident are you?},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {At last year's consortium meeting, I described an inexact gradient method and sampling scheme for data fitting. The randomization method has good convergence properties, at least as measured by the distance to the solution----in expectation. But as one insightful critic rightly pointed out, we don't usually observe the expectation, at least not in a single run. In this talk I will characterize the convergence of the method in terms of bounds on the probability of being too far away from the solution.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Michael P. Friedlander}
}

@conference {tamalet2012SINBADvpe,
	title = { Variance parameters estimation - Application to Full Waveform Inversion},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Many inverse problems include nuisance parameters. While not of direct interest, these parameters are required to recover primary parameters. In order to estimate these nuisance parameters as well as the primary parameters in large-scale inverse problems, a method based on variable projection, which consists in projecting out a subset over the variables, has been developed. We present here the application of this method to the problem of variance parameters estimation in multiple datasets, which is an important problem in many areas including geophysics. More precisely, we apply the method to Full Waveform Inversion and demonstrate the improvement in recovery of the model parameters in the case where the variance of the noise increases with the frequency.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Anais Tamalet}
}

@conference {yilmaz2012SINBADwms,
	title = {Weighted methods in sparse recovery},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {In the recent years we have successfully employed "weighted" algorithms to recover sparse signals from few linear, non-adaptive measurements. The general principle here is to use prior knowledge about the signal to be recovered, e.g., approximate locations of large-in-magnitude transform coefficients, if such information is available. An example for this is the use of weighted 1-norm minimization to improve wavefield reconstruction from randomized (sub)sampling. We will review these results and outline some new directions we have explored during the last year, such as weighted non-convex sparse recovery (see Ghadermarzy's talk), weighted analysis-based recovery (see Hargreaves's talk), and a weighted randomized Kaczmarz algorithm for solving large overdetermined systems of equations that are known to admit a (nearly) sparse solution. Various examples in seismic will be shown.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Ozgur Yilmaz}
}

@conference {Lin2012SINBADrdr,
	title = {Recent developments on the robust estimation of primaries by sparse inversion},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Robust estimation of primaries by sparse inversion is a next-generation surface multiple removal technique with an objective to truly invert an operator that models the free-surface. Key to the success of this approach is the imposition of a sparsity constraint on the primary impulse response in the time domain. This is accomplished by carefully applying large-scale convex optimization techniques on an extended L1 minimization problem. One of the benefits of our approach is that many extensions to the algorithm can be devised under this optimization framework to improve the quality of the solution given fixed computational costs and mitigating various shortcomings in field data. This talk will first review the basic technique of Robust EPSI and follow with some highlights on recent further developments of the algorithm, including a discussion on the role of regularization by reciprocity and the interpolation of near-offset data, as well as investigations into optimality and robustness to data outliers.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Tim T.Y. Lin}
}

@conference {lin2012SINBADics,
	title = {An introduction to cosparse signal reconstruction},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Undersampling techniques in exploration seismology usually relies on the assumption that seismic records and images permit sparse approximations under certain representations, such as Curvelet coefficients. Recent findings have suggested that for redundant representations (of which Curvelet is an example), the analysis operator that maps the physical signal to coefficients may also play a crucial role in recovering data from incomplete observations. In particular, the number of zero-valued coefficients given by the analysis operator acting on the signal, referred to as its "cosparsity", have an analogous role to the sparsity of the signal in terms of the coefficients. The cosparsity of the signal permits recovery guarantees that are completely separate from sparsity-based models, and gives rise to distinct sets of reconstruction algorithms and performances compared to sparsity-based approaches. We present in this talk some initial findings on the viability of cosparse reconstruction for a variety of seismic applications that previously relied on sparse signal reconstruction, such as data interpolation and source separation.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Tim T.Y. Lin}
}

@conference {ning2012SINBADfim,
	title = {Fast imaging with multiples},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {If correctly used, multiple energy can be mapped to the correct subsurface locations. However, simply applying the cross-correlation imaging condition will introduce non-causal artifacts into the final image. Here we propose an inversion approach to image primaries and multiples simultaneously that yields an artifact-free image. To address the high computational cost associated with inversion, we propose to: i) have the wave-equation solver carry out the multi-dimensional convolutions implicitly, and ii) reduce the number of PDE solves by randomized subsampling. We then propose to improve the overall performance of this algorithm by a process called rerandomization, which helps to cancel the correlation built between model iterate and the subsampling operator. We show the merits of our approach on a number of examples.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Ning Tu}
}

@conference {hargreaves2012SINBADavs,
	title = {Analysis versus synthesis in weighted sparse recovery},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
        abstract = {The synthesis model for compressive sensing has been the model of choice for many years and various weighting schemes have been shown to improve it's performance (see Yilmaz, Mansour, and Ghadermarzy talks). However, there is a counterpart model to synthesis, namely the analysis model, which has been less popular but recently attracted more attention (see Lin's talk). In this talk, weighting in the analysis model is discussed and applied to the seismic trace interpolation problem.},
	keywords = {Presentation, SINBAD,  SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Brock Hargreaves}
}


@conference {wason2012SINBADobs,
	title = {Ocean bottom seismic acquisition via jittered sampling},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {We present a pragmatic marine acquisition scheme where a single (or multiple) vessel sails across an ocean-bottom array firing airguns at — optimally jittered source locations and instances in time. Following the principles of compressive sensing, we can significantly impact the reconstruction quality of conventional seismic data (from jittered data) and demonstrate successful recovery by sparsity promotion. In contrast to random (under)sampling, acquisition via jittered (under)sampling helps in controlling the maximum gap size, which is a practical requirement of wavefield reconstruction with localized sparsifying transforms. Results are illustrated with simulations of optimally jittered marine acquisition, and periodic time-dithering marine acquisition.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Haneet Wason}
}


@conference {petrenko2012SINBADcarp,
	title = {CARP-CG: A computational study},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Forward modelling of the wave equation is a key ingredient in seismic full waveform inversion (FWI). Simulation in the time domain and solution of the wave equation in the frequency domain are two competing approaches to modelling. Frequency domain approaches can further be categorized as using either direct or iterative solvers. For 3D FWI, iterative solvers in the frequency domain are attractive, partly because they require less memory than the other methods. This is due to the fact that there is no need to store the wavefield at each time step, or compute a factorization of the Helmholtz operator that will not be as sparse as the original matrix.
One iterative solver that has been applied to the Helmholtz equation is CARP-CG. CARP-CG uses Kaczmarz row projections for each block of a domain decomposition scheme to precondition the Helmholtz system into being symmetric and positive semidefinite. The method of conjugate gradients is then used to solve the preconditioned system.

We present a comparison of the performance of CARP-CG implemented in several languages (MATLAB, C, FORTRAN, julia) and in two different hardware environments: the LIMA HPC cluster hosted at UBC, and the Checkers cluster which is part of the Westgrid consortium. Parallelization of the algorithm via domain decomposition implemented with MPI (distributed memory) and OMP (shared memory) is also examined.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Art Petrenko}
}

@conference {mansour2012SINBADsti,
	title = {Seismic trace interpolation via sparsity promoting reweighted algorithms},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Missing-trace interpolation aims to reconstruct regularly sampled wavefields from periodically sampled data with gaps caused by physical constraints. While transform-domain sparsity promotion has proven to be an effective tool to solve this recovery problem, current recovery techniques make no use of a priori information on the transform-domain coefficients. To overcome these vulnerabilities in solving the recovery problem for large-scale problems, we propose recovery by weighted one-norm minimization, which exploits correlations between locations of significant coefficients of different partitions, e.g., shot records, common-offset gathers, or frequency slices, of the acquired data. Moreover, in situations where no prior support estimate is available, we propose the WSPGL1 algorithm that outperforms standard $\ell_1$ minimization in finding sparse solutions to underdetermined linear systems of equations. Our algorithm is a modification of the SPGL1 algorithm and enjoys better sparse recovery performance at no additional computational cost. We illustrate the improved recovery using WSPGL1 for randomly subsampled seismic traces.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Hassan Mansour}
}

@conference {vanleeuwen2012SINBADyap,
	title = {Yet another perspective on image volumes},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {An extended image is defined as the multi-dimensional cross-correlation of the source and receiver wavefields used for imaging.  This extended image will reveal velocity errors by de-focusing and can thus be used for velocity analysis. However, for optimal sensitivity to velocity errors, the subsurface offset has to be aligned with the local dip. As this dip is not known a priori, we consider forming the extended image for subsurface offsets in all directions. However, computing and storing such a large image volume is not computationally feasible.  We organize the image volume in a matrix and use matrix-probing techniques to glean information form the matrix without explicitly forming it.  A matrix-vector multiply with the image-volume matrix can be performed at the cost of two wave-equation solves and does not require any explicit cross-correlations of the wavefields.  Such techniques can also be used to evaluate focusing penalties without forming the whole image volume.  Finally, the matrix-viewpoint allows us to derive a 2-way equivalent of the DSR equation in a straightforward manner and provides a possible avenue for developing new velocity-continuation techniques.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Tristan van Leeuwen}
}

@conference {vanleeuwen2012SINBAD3dfd,
	title = {3D Frequency-domain waveform inversion using a row-projected Helmholtz solver},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {3D frequency-domain full waveform inversion relies on being able to efficiently solve the 3D Helmholtz equation.  Iterative methods require sophisticated preconditioners because the Helmholtz matrix is typically indefinite. In the first part of the talk I review a preconditioning technique that is based on row-projections. Notable advantages of this preconditioner over existing ones are that it has low algorithmic complexity, is easily parallelizable and extendable to time-harmonic vector equations. In the second part of the talk I discuss how the row-projected solver can be used in the context of waveform inversion. Key aspects are: the use of block-iterative methods for multiple sources and adapting the accuracy of the solver.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Tristan van Leeuwen}
}

@conference {oghenekohwo2012SINBADcs,
	title = {Compressed sensing:  a tool for eliminating repeatability in acquisition of 4D (time-lapse) seismic data},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {In 4D (time-lapse) seismic data acquisition, a very significant step is the repeatability of the acquisition process.  In other words, the geophones must be placed at the exact location as they were, during baseline survey and acquisition. This condition is required to be able to produce an image of the same location over time and this enhances a proper reservoir characterization. The cost of repeating the seismic acquisition is very expensive, as geophones (receivers) have to be left at the same location over the period for which the data will be acquired. In this talk, we attempt to highlight the effort of Compressed Sensing, to eliminate this condition of repeatability of the acquisition . We show that a random sampling of the shots or random placement of the geophones is able to reproduce the same image over time, hence eliminating any acquisition imprints on the final seismic image. },
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Felix Oghenekohwo}
}


@conference {kumar2012SINBADsdi,
	title = {Seismic data interpolation using SVD Free Pareto Curve based Low Rank Optimization},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Seismic data acquisition is cursed by missing data caused by physical and/or budget constraints. Aim of interpolation technique is to spatially transform irregularly acquired data to regularly sampled data while maintaining the events coherency. While transform-domain sparsity promotion has proven to be an effective tool to solve this recovery problem, recent developments in Rank penalizing techniques opens new horizon to improved recovery by exploiting low-rank structure. A major downside of current state of the art techniques is their reliance on the SVD of seismic data structures, which can be prohibitively expensive. Fortunately, recent work allows us to circumvent this problem by working with matrix factorizations. We review a novel approach to rank penalization, and successfully apply it to the seismic interpolation problem by exploiting the low-rank structure of seismic data. Experiments for the recovery of 2D and 3D acquisition support the feasibility and potential of the new approach.},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Rajiv Kumar}
}


@conference {au-yeung2012SINBADcs,
	title = {Compressed sensing, random Fourier matrix and Jitter sampling},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = { Compressed sensing is an emerging signal processing technique that allows signals to be sampled well below the Nyquist rate, when the signal has a sparse representation in an orthonormal basis. By using a random Fourier matrix or a Gaussian matrix as our measurement matrix, we can reconstruct a signal from far fewer measurements than required by Shannon sampling theorem. In this talk, we will discuss the role of uniform versus jitter sampling, both in a theoretical and practical viewpoint.
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Enrico Au-Yeung and Hassan Mansour and Ozgur Yilmaz}
}

@conference {dasilva2012SINBADhtt,
	title = {Hierarchical Tucker Tensor Optimization - Applications to 4D Seismic Data Interpolation},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {There has been a swell of research in the scientific computing community in the last couple of years which tries to extend notions of linear algebra (rank, the SVD, linear systems, etc.) to higher dimensional arrays, or tensors. Much work has been proposed to try to overcome the so called "curse of dimensionality", the O(N^d) storage required for a d-dimensional array, where N is the size of each dimension. The hierarchical Tucker format is one such tensor representation which manages to decompose a hierarchy of dimensions into parameter matrices of very manageable size, requiring at most dNK + (d - 2)K^3 + K^2 parameters, where K is an internal rank parameter. In this work, we extend ideas of matrix completion to the tensor case, where we only know a small number of randomly distributed entries from various 4D frequency slices, and try to recover the fully sampled tensor based on the knowledge that it has low hierarchical tucker rank in a particular arrangement of dimensions. Using this approach, we exploit the multi-dimensional dependencies within the full data in order to achieve very promising interpolation results even from heavily subsampled data. 
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Curt Da Silva}
}

@conference {aravkin2012SINBADenp,
	title = {Estimating Nuisance Parameters in Inverse Problems},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {Many inverse problems include nuisance parameters which, while not of direct interest, are required to recover primary parameters. In this talk, we present the idea of "projecting out" these variables, and how this idea allows us to design methods for solving a broad class of problems with nuisance parameters, such as variance or degrees of freedom. We then discuss several geophysical applications, including including estimation of unknown variance parameters in the Gaussian model for full waveform inversion, degree of freedom (d.o.f.) parameter estimation in the context of robust imaging problems, and robust source estimation.
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Aleksandr Y. Aravkin}
}

@conference {aravkin2012SINBADgsf,
	title = {Generalized SPGL1: from theory to applications},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {The SPGL1 solver has been effectively used for many geophysical applications, including curvelet data interpolation, imaging, and as a subroutine in full waveform inversion. In this talk, we present an overview of the theoretical foundation of the solver, along with a broad generalization of this foundation. We then introduce several applications, including robust & sparse imaging, sparse deconvolution, and data interpolation by matrix completion.  
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Aleksandr Y. Aravkin}
}

@conference {ghadermarzy2012SINBADncc,
	title = {Non-convex compressed sensing using partial support information},
	booktitle = {SINBAD consortium talks},
	year = {2012},
	organization = {SINBAD},
	abstract = {In this talk, we will address the recovery conditions of weighted $\ell_p$ minimization for signal reconstruction from compressed sensing measurements when (possibly inaccurate) partial support information is available. First we will motivate the use of (weighted) $\ell_p$ minimization with $p<1$ and point out its advantages over weighted $\ell_1$ minimization when there is prior information on the support of the signal that is possibly partial and inaccurate. Then we will provide theoretical guarantees of sufficient recovery conditions for weighted $\ell_p$ minimization, which are better than those for (unweighted) $\ell_p$ minimization as well as those for weighted $\ell_1$. In the last part of the talk, we will illustrate our results with some numerical experiments stylized applications.
},
	keywords = {Presentation, SINBAD, SINBADFALL2012, SLIM},
	url = {https://www.slim.eos.ubc.ca/Publications/Private/Conferences/SINBAD/2012/},
	author = {Navid Ghadermarzy}
}

@conference{herrmann2012UW,
author = {Felix J. Herrmann},
year = {2012},
booktitle = {Talk at University of Wisconsin},
title = {Compressive sensing and sparse recovery in exploration seismology},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Seminar/2012/herrmann2012UW/herrmann2012UW_pres.pdf}
}

@conference{min2012CSEGrgfe,
author = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
booktitle = {CSEG technical program},
title = {Re-establishment of gradient in frequency-domain elastic waveform inversion},
abstract = {To obtain solutions close to global minimum in waveform 
inversion, the gradients computed at each frequency need to be 
weighted to appropriately describe the residuals between modeled and 
field data. While the low-frequency components of the gradients should 
be weighted to recover the long- wavelength structures, the high-frequency 
components of the gradients need to be weighted when the short-wavelength 
structures are restored. However, the conventional elastic waveform inversion 
algorithms cannot properly weight the gradients computed at each frequency. 
When gradients are scaled using the pseudo-Hessian matrix inside the 
frequency loop, gradients obtained at high frequencies are over-emphasized. 
When the gradients are scaled outside the frequency loop, gradients are 
weighted by the source spectra. In this study, we propose applying weighting 
factors to the gradients obtained at each frequency so that gradients can 
properly reflect the differences between the true and assumed models satisfying 
the general inverse theory. The weighting factors are composed by the 
backpropagated residuals. Numerical examples for the simple rectangular-shaped 
model and the modified version of the Marmousi-2 model show that the 
weighting method enhances gradient images and inversion results compared 
to the conventional inversion algorithms.},
keywords = {elastic, waveform inversion, frequency-domain, weighting factors},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2012/min2012CSEGrgfe/min2012CSEGrgfe.pdf}
}

@conference{wason2012CSEGode,
author = {Haneet Wason and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
booktitle = {CSEG technical program},
title = {Only dither: efficient simultaneous marine acquisition},
abstract = {Simultaneous-source acquisition is an emerging technology
that is stimulating both geophysical research and commercial efforts. 
Simultaneous marine acquisition calls for the development of a new set
of design principles and post-processing tools. The focus here is on 
simultaneous-source marine acquisition design and sparsity-promoting 
sequential-source data recovery. We propose a pragmatic simultaneous-source, 
randomized marine acquisition scheme where multiple vessels sail across 
an ocean-bottom array firing airguns at — sequential locations and randomly 
time-dithered instances. By leveraging established findings from the field of 
compressive sensing, where the choice of the sparsifying transform needs 
to be incoherent with the compressive sampling matrix, we can significantly 
impact the reconstruction quality, and demonstrate that the compressive 
sampling matrix resulting from the proposed sampling scheme is sufficiently 
incoherent with the curvelet transform to yield successful recovery by sparsity 
promotion. Results are illustrated with simulations of “purely” random marine 
acquisition, which requires an airgun to be located at each source location, 
and random time-dithering marine acquisition with one and two source vessels. 
Size of the collected data volumes in all cases is the same. Compared to the 
recovery from the former acquisition scheme (SNR = 10.5dB), we get good 
results by dithering with only one source vessel (SNR = 8.06dB) in the latter 
scheme, which improve at the cost of having an additional source vessel 
(SNR = 9.85dB).},
keywords = {CSEG, acquisition, marine, simultaneous},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2012/wason2012CSEGode/wason2012CSEGode.pdf}
}

@conference{li2011CSEGefimag,
author = {Xiang Li and Felix J. Herrmann},
year = {2012},
optmonth = {02/2012},
booktitle = {CSEG technical program},
title = {Efficient full-waveform inversion with marine acquisition geometry},
abstract = {Full-waveform inversion (FWI) is a nonlinear data fitting procedure 
based on seismic data to derive a accurate velocity model. With the increasing 
demand for high resolution images in complex geological settings, the 
importance of improvements in acquisition and inversion become more and 
more critical. However, these improvements will be obtained at high 
computational cost, as a typical marine survey contains thousands of shot 
and receiver positions, and FWI needs several passes through massive seismic 
data. Computational cost of FWI will grow exponentially as the size of seismic 
data and desired resolution increase. In this paper we present a modified 
Gauss-Newton (GN) method that borrows ideas from compressive sensing, 
where we compute the GN updates from a few randomly selected sequential 
shots. Each subproblem is solved by using a sparsity promoting algorithm. 
With this approach, we dramatically reduce the size and hence the 
computational costs of the problem, whilst we control information loss by 
redrawing a different set of sequential shots for each subproblem.},
keywords = {CSEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2012/li2011CSEGefimag/li2011CSEGefimag.pdf}
}


@conference{herrmann2012SEGfwi,
author = {Andrew J. Calvert and Ian Hanlon and Mostafa Javanmehri and Rajiv Kumar and Tristan van Leeuwen and Xiang Li and Brendan Smithyman and Eric Takam Takougang and Haneet Wason and Felix J. Herrmann},
title = {FWI from the West Coasts: lessons learned from "Gulf of Mexico Imaging Challenges: What Can Full Waveform Inversion Achieve?"},
year = {2012},
booktitle = {SEG Technical Program Expanded Abstracts},
organization = {SEG},
keywords = {workshop, fwi, SEG},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/herrmann2012SEGfwi/herrmann2012SEGfwi_pres.pdf}
}


@conference{herrmann2012SEGals,
author = {Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {Accelerated large-scale inversion with message passing},
abstract = {To meet current-day challenges, exploration seismology increasingly 
relies on more and more sophisticated algorithms that require 
multiple paths through all data. This requirement leads to problems 
because the size of seismic data volumes is increasing exponentially, 
exposing bottlenecks in IO and computational capability. To overcome 
these bottlenecks, we follow recent trends in machine learning and 
compressive sensing by proposing a sparsity-promoting inversion 
technique that works on small randomized subsets of data only. We 
boost the performance of this algorithm significantly by modifying a 
state-of-the-art l1-norm solver to benefit from message 
passing, which breaks the build up of correlations between model 
iterates and the randomized linear forward model. We demonstrate the 
performance of this algorithm on a toy sparse-recovery problem and 
on a realistic reverse-time-migration example with random source 
encoding. The improvements in speed, memory use, and output quality 
are truly remarkable.},
keywords = {Imaging, Optimization, Compressive Sensing, SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals.pdf},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/herrmann2012SEGals/herrmann2012SEGals_pres.pdf}
}

@conference{vanderneut2012SEGirs,
author = {Joost {van der Neut} and Felix J. Herrmann and Kees Wapenaar},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {Interferometric redatuming with simultaneous and missing 
sources using sparsity promotion in the curvelet domain},
abstract = {Interferometric redatuming is a velocity-independent 
method to turn downhole receivers into virtual sources. Accurate 
redatuming involves solving an inverse problem, which can be 
highly ill-posed, especially in the presence of noise, incomplete 
data and limited aperture. We address these issues by combining 
interferometric redatuming with transform-domain sparsity 
promotion, leading to a formulation that deals with data imperfections. 
We show that sparsity promotion improves the retrieval of virtual 
shot records under a salt flank. To reduce acquisition costs, it can 
be beneficial to reduce the number of sources or shoot them 
simultaneously. It is shown that sparse inversion can still provide 
a stable solution in such cases.},
keywords = {Processing,Imaging,Optimization,Interferometry, SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/vanderneut2012SEGirs/vanderneut2012SEGirs.pdf}
}

@conference{li2012SEGspmamp,
author = {Xiang Li and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {Sparsity-promoting migration accelerated by message passing},
abstract = {Seismic imaging via linearized inversion requires multiple 
iterations to minimize the least-squares misfit as a function of the 
medium perturbation. Unfortunately, the cost for these iterations 
are prohibitive because each iteration requires many wave-equation 
simulations, which without direct solvers require an expensive 
separate solve for each source. To overcome this problem, we use 
dimensionality-reduction to decrease the size of seismic imaging 
problem by turning the large number of sequential shots into a much 
small number of simultaneous shots. In our approach, we take 
advantage of sparsifying transforms to remove source crosstalk 
resulting from randomly weighting and stacking sequential shots into 
a few super shots. We also take advantage of the fact that the 
convergence of large-scale sparsity-promoting solvers can be 
improved significantly by borrowing ideas from message passing, 
which are designed to break correlation built up between the linear 
system and the model iterate. In this way, we arrive at a 
formulation where we run the sparsity-promoting solver for a 
relatively large number of very iterations. Aside from leading to a 
significant speed up, our approach had the advantage of greatly 
reducing the memory imprint and IO requirements. We demonstrate this 
feature by solving a sparsity-promoting imaging problem with 
operators of reverse-time migration, which is computationally 
infeasible without the dimensionality reduction.},
keywords = {SEG, Imaging, Inversion},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp.pdf},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/li2012SEGspmamp/li2012SEGspmamp_pres.pdf}
}


@conference{aravkin2012SEGST,
author = {Aleksandr Y. Aravkin and Tristan {van Leeuwen} and Kenneth Bube and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {On Non-Uniqueness of the Student's t-formulation for Linear Inverse Problems},
abstract = {We review the statistical interpretation of inverse problem 
formulations, and the motivations for selecting non-convex penalties 
for robust behaviour with respect to measurement outliers or artifacts 
in the data. An important downside of using non-convex formulations 
such as the Student's t is the potential for non-uniqueness, and we 
present a simple example where the Student's t penalty can be made 
to have many local minima by appropriately selecting the degrees of 
freedom parameter. 
On the other hand, the non-convexity of the Student's t is precisely 
what gives it the ability to ignore artifacts in the data. We explain this 
idea, and present a stylized imaging experiment, where the Student's t 
is able to recover a velocity perturbation from data contaminated by 
a very peculiar artifact --- data from a different velocity perturbation. 
The performance of Student's t inversion is investigated empirically 
for different values of the degrees of freedom parameter, and 
different initial conditions.},
keywords = {student's t, robust, non-convex, uniqueness, SEG},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/aravkin2012SEGST/aravkin2012SEGST.pdf}
}

@conference{tu2012SEGima,
author = {Ning Tu and Felix J. Herrmann},
year = {2012},
optmonth = {04/2012},
organization = {SEG},
booktitle    = {SEG Technical Program Expanded Abstracts},
title = {Imaging with multiples accelerated by message passing},
abstract = {With the growing realization that multiples can provide 
valuable information, there is a paradigm shift from removing them 
to using them. For instance, primary estimation by sparse inversion 
has demonstrated its superiority over surface-related multiple removal 
in many aspects. Inspired by this shift, we propose a method to 
image directly from the total up-going wavefield, including surface-related 
multiples, by sparse inversion. To address the high computational cost 
associated with this method, we propose to speed up the inversion by 
having the wave-equation solver carry out the multi-dimensional 
convolutions implicitly and cheaply by randomized subsampling. We 
improve the overall performance of this algorithm by selecting new 
independent copies of the randomized modeling operator, which 
leads to a cancellation of correlations that hamper the speed of 
convergence of the solver. We show the merits of our approach on 
a number of examples.},
keywords = {SEG, Imaging, Multiples},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima.pdf},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2012/tu2012SEGima/tu2012SEGima_pres.pdf}
}

@conference{mansour2012SSPwspgl1,
author = {Hassan Mansour},
year = {2012},
optmonth = {03/2012},
title = {Beyond $\ell_1$ norm minimization for sparse signal recovery},
booktitle={2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
address={Ann Arbor, Michigan, USA},
organization={IEEE},
abstract = {Sparse signal recovery has been dominated by the basis 
pursuit denoise (BPDN) problem formulation for over a decade. In this 
paper, we propose an algorithm that outperforms BPDN in finding 
sparse solutions to underdetermined linear systems of equations at no 
additional computational cost. Our algorithm, called WSPGL1, is a 
modification of the spectral projected gradient for $\ell_1$ minimization 
(SPGL1) algorithm in which the sequence of LASSO subproblems are 
replaced by a sequence of weighted LASSO subproblems with constant 
weights applied to a support estimate. The support estimate is derived 
from the data and is updated at every iteration. The algorithm also 
modifies the Pareto curve at every iteration to reflect the new weighted 
$\ell_1$ minimization problem that is being solved. We demonstrate through 
extensive simulations that the sparse recovery performance of our 
algorithm is superior to that of $\ell_1$ minimization and approaches the 
recovery performance of iterative re-weighted $\ell_1$ (IRWL1) minimization 
of Cand{\`e}s, Wakin, and Boyd. Moreover, our algorithm has the 
computational cost of a single BPDN problem.},
keywords = {Sparse recovery, compressed sensing, iterative algorithms, weighted $\ell_1$ minimization, partial support recovery},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1_pres.pdf},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/mansour2012SSPwspgl1/mansour2012SSPwspgl1.pdf}
}

@conference{herrmann2012SSPamp,
author = {Felix J. Herrmann},
year = {2012},
optmonth = {03/2012},
title = {Approximate message passing meets exploration seismology},
booktitle={2012 IEEE Statistical Signal Processing Workshop (SSP) (SSP'12)},
address={Ann Arbor, Michigan, USA},
organization={IEEE},
abstract = {Data collection, data processing, and imaging in exploration 
seismology increasingly hinge on large-scale sparsity promoting solvers 
to remove artifacts caused by efforts to reduce costs. We show how the 
inclusion of a 'message term' in the calculation of the residuals improves 
the convergence of these iterative solvers by breaking correlations that 
develop between the model iterate and the linear system that needs to 
be inverted. We compare this message-passing scheme to state-of-the-art 
solvers for problems in missing-trace interpo- lation and in 
dimensionality-reduced imaging with phase en- coding.},
keywords = {Exploration seismology, compressive sensing, transform-domain sparsity promotion, seismic imaging},
presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp_pres.pdf},
url = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SSP/2012/herrmann2012SSPamp/herrmann2012SSPamp.pdf}
}

@CONFERENCE{herrmann2012EAGEcsm,
  author       = {Felix J. Herrmann and Haneet Wason},
  title        = {Compressive sensing in marine acquisition and beyond},
  year         = {2012},
  optmonth     = {02/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {workshop, acquisition, marine},
  abstract     = {Simultaneous-source marine acquisition is an example
                  of compressive sensing where acquisition with a
                  single vessel is replaced by simultaneous
                  acquisition by multiple vessels with sources that
                  fire at randomly dithered times. By identifying
                  simultaneous acquisition as compressive sensing, we
                  are able to design acquisitions that favour recovery
                  by sparsity promotion. Compared to conventional
                  processing that yields estimates for sequential
                  data, sparse recovery leads to significantly
                  improved results for simultaneous data volumes that
                  are collected in shorter times. These improvements
                  are the result of proper design of the acquisition,
                  selection of the appropriate transform domain, and
                  solution of the recovery problem by sparsity
                  promotion. During this talk, we will show how these
                  design principles can be applied to marine
                  acquisition and to other problems in exploration
                  seismology that can benefit from compressive
                  sensing.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEcsm/herrmann2012EAGEcsm.pdf}
}


@CONFERENCE{min2012EAGEefwi,
  author       = {Ju-Won Oh and Dong-Joo Min and Felix J. Herrmann},
  title        = {Frequency-domain elastic waveform inversion using weighting factors related to source-deconvolved residuals},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {elastic, waveform inversion, frequency-domain, weighting factors},
  abstract     = {One of the limitations in seismic waveform inversion
                  is that inversion results are very sensitive to
                  initial guesses, which may be because the gradients
                  computed at each frequency are not properly weighted
                  depending on given models.Analyzingthe conventional
                  waveform inversion algorithms using the
                  pseudo-Hessian matrix as a pre-conditioner shows
                  that the gradientsdo not properly describe the
                  feature of given models or high- and low-end
                  frequencies do not contribute the model parameter
                  updates due to banded spectra of source wavelet. For
                  a better waveform inversion algorithm, we propose
                  applying weighting factors to gradients computed at
                  each frequency. The weighting factors are designed
                  using the source-deconvolved back-propagated
                  wavefields.  Numerical results for the SEG/EAGE salt
                  model show that the weighting method improves
                  gradient images and its inversion results are
                  compatible with true velocities even with poorly
                  estimated initial guesses.},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/min2012EAGEefwi/min2012EAGEefwi.pdf}
}


@CONFERENCE{dasilva2012EAGEprobingprecond,
  author       = {Curt {Da Silva} and Felix J. Herrmann},
  title        = {Matrix Probing and Simultaneous Sources: A New Approach for Preconditioning the Hessian},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, matrix probing, pseudo-differential operator},
  abstract     = {Recent advances based on the mathematical
                  understanding of the Hessian as, under certain
                  conditions, a pseudo-differential operator have
                  resulted in a new preconditioner by L. Demanet et
                  al. Basing their approach on a suitable basis
                  expansion for the Hessian, by suitably 'probing' the
                  Hessian, i.e. applying the Hessian to a small number
                  of randomized model perturbations, one can obtain an
                  approximation to the inverse Hessian in an efficient
                  manner.  Building upon this approach, we consider
                  this preconditioner in the context of least-squares
                  migration and Full Waveform Inversion and
                  specifically dimensionality reduction techniques in
                  these domains.  By utilizing previous work in
                  simultaneous sources, we are able to develop an
                  efficient least-squares migration scheme which
                  recovers higher quality images and hence higher
                  quality search directions in the context of a
                  Gauss-Newton method for Full Waveform Inversion
                  while simultaneously avoiding inordinate amounts of
                  additional work.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/dasilva2012EAGEprobingprecond/dasilva2012EAGEprobingprecond.pdf}
}


@CONFERENCE{wason2012EAGEode,
  author       = {Haneet Wason and Felix J. Herrmann},
  title        = {Only dither: efficient simultaneous marine acquisition},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, acquisition, marine, simultaneous},
  abstract     = {Simultaneous-source acquisition is an emerging
                  technology that is stimulating both geophysical
                  research and commercial efforts. The focus here is
                  on simultaneous-source marine acquisition design and
                  sparsity-promoting sequential-source data
                  recovery. We propose a pragmatic
                  simultaneous-source, randomized marine acquisition
                  scheme where multiple vessels sail across an
                  ocean-bottom array firing airguns at—sequential
                  locations and randomly time-dithered
                  instances. Within the context of compressive
                  sensing, where the choice of the sparsifying
                  transform needs to be incoherent with the
                  compressive sampling matrix, we can significantly
                  impact the reconstruction quality, and demonstrate
                  that the compressive sampling matrix resulting from
                  the proposed sampling scheme is sufficiently
                  incoherent with the curvelet transform to yield
                  successful recovery by sparsity promotion. Results
                  are illustrated with simulations of “purely” random
                  marine acquisition, which requires an airgun to be
                  located at each source location, and random
                  time-dithering marine acquisition with one and two
                  source vessels. Size of the collected data volumes
                  in all cases is the same. Compared to the recovery
                  from the former acquisition scheme (SNR = 10.5dB),
                  we get good results by dithering with only one
                  source vessel (SNR = 8.06dB) in the latter scheme,
                  which improve at the cost of having an additional
                  source vessel (SNR = 9.44dB).},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/wason2012EAGEode/wason2012EAGEode.pdf}
}


@CONFERENCE{tu2011EAGElmf,
  author       = {Ning Tu and Felix J. Herrmann},
  title        = {Least-squares migration of full wavefield with source encoding},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, depth migration, surface-related multiples},
  abstract     = {Multiples can provide valuable information that is
                  missing in primaries, and there is a growing
                  interest in using them for seismic imaging.  In our
                  earlier work, we proposed to combine primary
                  estimation and migration to image from the total
                  up-going wavefield. The method proves to be
                  effective but computationally expensive. In this
                  abstract, we propose to reduce the computational
                  cost by removing the multi-dimensional convolution
                  required by primary estimation, and reducing the
                  number of PDE solves in migration by introducing
                  simultaneous sources with source renewal. We gain
                  great performance boost without compromising the
                  quality of the image.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/tu2011EAGElmf/tu2011EAGElmf_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/tu2011EAGElmf/tu2011EAGElmf.pdf}
}


@CONFERENCE{aravkin2012EAGErobust,
  author       = {Aleksandr Y. Aravkin and Tristan {van Leeuwen} and Henri Calandra and Felix J. Herrmann},
  title        = {Source estimation for frequency-domain FWI with robust penalties},
  year         = {2012},
  optmonth     = {01/2012},
  keywords     = {EAGE},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  abstract     = {Source estimation is an essential component of full
                  waveform inversion. In the standard frequency domain
                  formulation, there is closed form solution for the
                  the optimal source weights, which can thus be
                  cheaply estimated on the fly. A growing body of work
                  underscores the importance of robust modeling for
                  data with large outliers or artifacts that are not
                  captured by the forward model. Effectively, the
                  least-squares penalty on the residual is replaced by
                  a robust penalty, such as Huber, Hybrid `1-`2 or
                  Student’s t. As we will demonstrate, it is essential
                  to use the same robust penalty for source
                  estimation. In this abstract, we present a general
                  approach to robust waveform inversion with robust
                  source estimation. In this general formulation,
                  there is no closed form solution for the optimal
                  source weights so we need to solve a scalar
                  optimization problem to obtain these weights. We can
                  efficiently solve this optimization problem with a
                  Newton-like method in a few iterations. The
                  computational cost involved is of the same order as
                  the usual least-squares source estimation
                  procedure. We show numerical examples illustrating
                  robust source estimation and robust waveform
                  inversion on synthetic data with outliers.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/aravkin2012EAGErobust/aravkin2012EAGErobust.pdf}
}


@CONFERENCE{vanleeuwen2012EAGEext,
  author       = {Tristan {van Leeuwen} and Felix J. Herrmann},
  title        = {Wave-equation extended images: computation and velocity continuation},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, extended image, velocity continuation},
  abstract     = {An extended image is a multi-dimensional correlation
                  of source and receiver wavefields. For a
                  kinematically correct velocity, most of the energy
                  will be concentrated at zero offset. Because of the
                  computational cost involved in correlating the
                  wavefields for all offsets, such exteded images are
                  computed for a subsurface offset that is aligned
                  with the local dip. In this paper, we present an
                  efficient way to compute extended images for all
                  subsurface offsets without explicitly calculating
                  the receiver wavefields, thus making it
                  computationally feasible to compute such extended
                  images.  We show how more conventional image
                  gathers, where the offset is aligned with the dip,
                  can be extracted from this extended image. We also
                  present a velocity continuation procedure that
                  allows us to compute the extended image for a given
                  velocity without recomputing all the source
                  wavefields.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEext/vanleeuwen2012EAGEext.pdf}
}


@CONFERENCE{vanleeuwen2012EAGEcarpcg,
  author       = {Tristan {van Leeuwen} and Dan Gordon and Rachel Gordon and Felix J. Herrmann},
  title        = {Preconditioning the Helmholtz equation via row-projections},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, Helmholtz equation, precondition},
  abstract     = {3D frequency-domain full waveform inversion relies
                  on being able to efficiently solve the 3D Helmholtz
                  equation. Iterative methods require sophisticated
                  preconditioners because the Helmholtz matrix is
                  typically indefinite. We review a preconditioning
                  technique that is based on row-projections.  Notable
                  advantages of this preconditioner over existing ones
                  are that it has low algorithmic complexity, is
                  easily parallelizable and extendable to
                  time-harmonic vector equations.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanleeuwen2012EAGEcarpcg/vanleeuwen2012EAGEcarpcg.pdf}
}


@CONFERENCE{vanderneut2012EAGEdecomp,
  author       = {Joost {van der Neut} and Felix J. Herrmann},
  title        = {Up / down wavefield decomposition by sparse inversion},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE,wavefield decomposition, sparse inversion},
  abstract     = {Expressions have been derived for the decomposition
                  of multi-component seismic recordings into up- and
                  down-going constituents. However, these expressions
                  contain singularities at critical angles and can be
                  sensitive for noise. By interpreting wavefield
                  decomposition as an inverse problem and imposing
                  constraints on the sparseness of the solution, we
                  arrive at a robust formalism that can be applied to
                  noisy data. The method is demonstrated on synthetic
                  data with multi-component receivers in a horizontal
                  borehole, but can also be applied for different
                  configurations, including OBC and dual-sensor
                  streamers.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/vanderneut2012EAGEdecomp/vanderneut2012EAGEdecomp.pdf}
}


@CONFERENCE{herrmann2012EAGEpmr,
  author       = {Felix J. Herrmann},
  title        = {Pass on the message: recent insights in large-scale sparse recovery},
  year         = {2012},
  optmonth     = {01/2012},
  booktitle    = {EAGE technical program},
  organization = {EAGE},
  keywords     = {EAGE, message passing, sparse inversion},
  abstract     = {Data collection, data processing, and imaging in
                  exploration seismology increasingly hinge on
                  large-scale sparsity promoting solvers to remove
                  artifacts caused by efforts to reduce costs. We show
                  how the inclusion of a “message term“ in the
                  calculation of the residuals improves the
                  convergence of these iterative solvers by breaking
                  correlations that develop between the model iterate
                  and the linear system that needs to be inverted. We
                  compare this message-passing scheme to
                  state-of-the-art solvers for problems in
                  missing-trace interpolation and in
                  dimensionality-reduced imaging with phase encoding.},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2012/herrmann2012EAGEpmr/herrmann2012EAGEpmr.pdf}
}


@CONFERENCE{aravkin2012ICASSProbustb,
  author       = {Aleksandr Y. Aravkin and Michael P. Friedlander and Tristan van Leeuwen },
  booktitle    = {ICASSP},
  Keywords     = {ICASSP},
  Organization = {ICASSP},
  Title        = {Robust inversion via semistochastic dimensionality reduction},
  Year         = {2012},
  pages        = {5245-5248},
  optdoi       = {10.1109/ICASSP.2012.6289103},
  Abstract     = {We consider a class of inverse problems where it is
                  possible to aggregate the results of multiple
                  experiments. This class includes problems where the
                  forward model is the solution operator to linear
                  ODEs or PDEs. The tremendous size of such problems
                  motivates the use dimensionality reduction (DR)
                  techniques based on randomly mixing
                  experiments. These techniques break down, however,
                  when robust data-fitting formulations are used,
                  which are essential in cases of missing data,
                  unusually large errors, and systematic features in
                  the data unexplained by the forward model. We survey
                  robust methods within a statistical framework, and
                  propose a sampling optimization approach that allows
                  DR. The efficacy of the methods are demonstrated for
                  a large-scale seismic inverse problem using the
                  robust Student's t-distribution, where a useful
                  synthetic velocity model is recovered in the extreme
                  scenario of 60% corrupted data. The sampling
                  approach achieves this recovery using 20% of the
                  effort required by a direct robust approach.},
  keywords     = {SLIM,Optimization,Full-waveform inversion},
  URL          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/AravkinFriedlanderLeeuwen/AravkinFriedlanderLeeuwen.pdf }
}


@CONFERENCE{aravkin2012ICASSPfastseis,
  Author       = {Aleksandr Y. Aravkin and Xiang Li and Felix J. Herrmann },
  Title        = {Fast seismic imaging for marine data},
  booktitle    = {ICASSP},
  Abstract     = {Seismic imaging can be formulated as a linear
                  inverse problem where a medium perturbation is
                  obtained via minimization of a least-squares misfit
                  functional. The demand for higher resolution images
                  in more geophysically complex areas drives the need
                  to develop techniques that handle problems of
                  tremendous size with limited computational
                  resources. While seismic imaging is amenable to
                  dimensionality reduction techniques that collapse
                  the data volume into a smaller set of “super-shots”,
                  these techniques break down for complex acquisition
                  geometries such as marine acquisition, where sources
                  and receivers move during acquisition. To meet these
                  challenges, we propose a novel method that combines
                  sparsity-promoting (SP) solvers with random sub- set
                  selection of sequential shots, yielding a SP
                  algorithm that only ever sees a small portion of the
                  full data, enabling its application to very
                  large-scale problems. Application of this technique
                  yields excellent results for a complicated
                  synthetic, which underscores the robustness of
                  sparsity promotion and its suitability for seismic
                  imaging.},
  Keywords     = {ICASSP},
  Organization = {ICASSP},
  Year         = {2012},
  keywords     = {SLIM,Imaging,Optimization,Compressive Sensing},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/AravkinLiHerrmann/AravkinLiHerrmann.pdf}
}


@CONFERENCE{mansour2012ICASSadapt,
  Author       = {Hassan Mansour and Ozgur Yilmaz},
  Title        = {Adaptive compressed sensing for video acquisition},  
  booktitle    = {ICASSP},
  Abstract     = {In this paper, we propose an adaptive compressed
                  sensing scheme that utilizes a support estimate to
                  focus the measurements on the large valued
                  coefficients of a compressible signal. We embed a
                  “sparse-filtering” stage into the measure- ment
                  matrix by weighting down the contribution of signal
                  coefficients that are outside the support
                  estimate. We present an application which can
                  benefit from the proposed sampling scheme, namely,
                  video compressive acquisition. We demonstrate that
                  our proposed adaptive CS scheme results in a
                  significant improvement in reconstruction quality
                  compared with standard CS as well as adaptive
                  recovery using weighted $\ell$1 minimization.},
  Keywords     = {ICASSP},
  Organization = {ICASSP},
  Year         = {2012},
  keywords     = {Compressive Sensing},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPaCS/MansourYilmazICASSPaCS.pdf}
}


@CONFERENCE{mansour2012ICASSsupport,
  Author       = {Hassan Mansour and Ozgur Yilmaz},
  Title        = {Support driven reweighted $\ell_1$ minimization},
  booktitle    = {ICASSP},
  Abstract     = {In this paper, we propose a support driven
                  reweighted $\ell$_1 minimization algorithm (SDRL1)
                  that solves a sequence of weighted $\ell$_1
                  problems and relies on the support estimate accu-
                  racy. Our SDRL1 algorithm is related to the IRL1
                  algorithm proposed by Cande`s, Wakin, and Boyd. We
                  demonstrate that it is sufficient to find support
                  estimates with good accuracy and apply constant
                  weights instead of using the inverse coefficient
                  magnitudes to achieve gains similar to those of
                  IRL1. We then prove that given a support estimate
                  with sufficient accuracy, if the signal decays
                  according to a specific rate, the solution to the
                  weighted $\ell$_1 minimization problem results in a
                  support estimate with higher accuracy than the
                  initial estimate. We also show that under certain
                  conditions, it is possible to achieve higher
                  estimate accuracy when the inter- section of support
                  estimates is considered. We demonstrate the
                  performance of SDRL1 through numerical simulations
                  and compare it with that of IRL1 and standard
                  $\ell$_1 minimization.},
  Keywords     = {ICASSP},
  Organization = {ICASSP},
  Year         = {2012},
  keywords     = {Compressive Sensing},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2012/MansourYilmazICASSPwL1/MansourYilmazICASSPwL1.pdf}
}



%-------------------------------------------------2011-------------------------------------------------

@CONFERENCE{herrmann2011SEGffw,
  author       = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen and Xiang Li},
  title        = {Fast full-waveform inversion with compressive sensing},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  keywords     = {SEG,SLIM,Presentation,Full-waveform inversion},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/HerrmannSEG2011fws/HerrmannSEG2011fws.pdf}
}


@CONFERENCE{herrmann2011SPIEmsp,
  author       = {Felix J. Herrmann and Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen},
  title        = {A modified, sparsity promoting, {G}auss-{N}ewton algorithm for seismic waveform inversion},
  booktitle    = {Proc. SPIE},
  year         = {2011},
  number       = {81380V},
  notes        = {TR-2011-05},
  optmonth     = {08/2011},
  issn         = {1},
  url          = {http://slim.eos.ubc.ca/Publications/public/Journals/SPIEreport.pdf},
  optdoi       = {doi:10.1117/12.893861},
  keywords     = {SLIM,Compressive Sensing,Optimization,Full-waveform inversion},
  abstract     = {Images obtained from seismic data are used by the
                  oil and gas industry for geophysical
                  exploration. Cutting-edge methods for transforming
                  the data into interpretable images are moving away
                  from linear approximations and high-frequency
                  asymptotics towards Full Waveform Inversion (FWI), a
                  nonlinear data-fitting procedure based on full data
                  modeling using the wave-equation. The size of the
                  problem, the nonlinearity of the for- ward model,
                  and ill-posedness of the formulation all contribute
                  to a pressing need for fast algorithms and novel
                  regularization techniques to speed up and improve
                  inversion results. In this paper, we design a
                  modified Gauss-Newton algorithm to solve the PDE-
                  constrained optimization problem using ideas from
                  stochastic optimization and com- pressive
                  sensing. More specifically, we replace the
                  Gauss-Newton subproblems by ran- domly subsampled,
                  -$\ell_1$ regularized subproblems. This allows us us
                  significantly reduce the computational cost of
                  calculating the updates and exploit the
                  compressibility of wavefields in Curvelets. We
                  explain the relationships and connections between
                  the new method and stochastic optimization and
                  compressive sensing (CS), and demonstrate the
                  efficacy of the new method on a large-scale
                  synthetic seismic example.}
}


@CONFERENCE{aravkin2011SIAMfwi,
  author       = {Aleksandr Y. Aravkin and Felix J. Herrmann and Tristan van Leeuwen and James V. Burke and Xiang Li},
  title        = {Full Waveform Inversion with Compressive Updates},
  booktitle    = {SIAM},
  year         = {2011},
  organization = {SIAM CS\&E 2011},
  abstract     = {Full-waveform inversion relies on large
                  multi-experiment data volumes.  While improvements
                  in acquisition and inversion have been extremely
                  successful, the current push for higher quality
                  models reveals fundamental shortcomings handling
                  increasing problem sizes numerically. To address
                  this fundamental issue, we propose a randomized
                  dimensionality-reduction strategy motivated by
                  recent developments in stochastic optimization and
                  compressive sensing. In this formulation
                  conventional Gauss-Newton iterations are replaced by
                  dimensionality-reduced sparse recovery problems with
                  source encodings.},
  keywords     = {SLIM,Presentation,Full-waveform inversion}, 
  presentation = {http://slim/Publications/Public/Presentations/2011/Aravkin2.28.2011.pdf}
}


@CONFERENCE{aravkin2011ICIAMspf,
  author       = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and Felix J. Herrmann},
  title        = {Sparsity promoting formulations and algorithms for
                  FWI. Presented at AMP Medical and Seismic Imaging,
                  2011, Vancouver BC.},
  booktitle    = {ICIAM},
  year         = {2011},
  organization = {ICIAM 2011},
  abstract     = {Full Waveform Inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. FWI is typically formulated as a nonlinear
                  least squares optimization problem, and various
                  regularization techniques are used to guide the
                  optimization because the problem is ill-posed. We
                  propose a novel sparse regularization which exploits
                  the ability of curvelets to efficiently represent
                  geophysical images.  We then formulate a
                  corresponding sparsity promoting constrained
                  optimization problem, which we solve using an open
                  source algorithm.  The techniques are applicable to
                  any inverse problem where sparsity modeling is
                  appropriate.  We demonstrate the efficacy of the
                  formulation on a toy example (stylized cross-well
                  experiment) and on a realistic Seismic example
                  (partial Marmoussi model). We also discuss the
                  tradeoff between model fit and sparsity promotion,
                  with a view to extend existing techniques for linear
                  inverse problems to the case where the forward model
                  is nonlinear. },
  optmonth     = {07/2011},
  date-added   = {2011-07-15},
  keywords     = {SLIM,Presentation,Full-waveform inversion,Optimization}, 
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/aravkin2011AMP.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/aravkin2011AMP.pdf}
}


@CONFERENCE{aravkin2011ICIAMrfwiu,
  author       = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Robust FWI using Student's t-distribution},
  booktitle    = {ICIAM},
  year         = {2011},
  organization = {ICIAM 2011},
  abstract     = {Iterative inversion algorithms require repeated
                  simulation of 3D time-dependent acoustic, elastic,
                  or electromagnetic wave fields, extending hundreds
                  of wavelengths and hundreds of periods. Also,
                  seismic data is rich in information at every
                  representable scale. Thus simulation-driven
                  optimization approaches to inversion impose great
                  demands on simulator efficiency and accuracy. While
                  computer hardware advances have been of critical
                  importance in bringing inversion closer to practical
                  application, algorithmic advances in simulator
                  methodology have been equally important. Speakers in
                  this two-part session will address a variety of
                  numerical issues arising in the wave simulation, and
                  in its application to inversion. },
  date-added   = {2011-07-20},
  optmonth     = {07/2011},
  keywords     = {SLIM,Presentation,ICIAM,Full-waveform inversion,Optimization}, 
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/aravkin2011WAVES.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/aravkin2011WAVES.pdf}
}


@CONFERENCE{aravkin2011EAGEspfwi,
  author       = {Aleksandr Y. Aravkin and Tristan van Leeuwen and James V. Burke and Felix J. Herrmann},
  title        = {Sparsity Promoting Formulations and Algorithms for FWI},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/Aravkin2011EAGEspfwi/Aravkin2011EAGEspfwi.pdf },
  keywords     = {EAGE,Presentation, Full-waveform inversion}
}


@CONFERENCE{aravkin2011EAGEnspf,
  author       = {Aleksandr Y. Aravkin and James V. Burke and Felix J. Herrmann and Tristan van Leeuwen},
  title        = {A Nonlinear Sparsity Promoting Formulation and Algorithm for Full Waveform Inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Full Waveform Inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. FWI is typically formulated as a nonlinear
                  least squares optimization problem, and various
                  regularization techniques are used to guide the
                  optimization because the problem is illposed. In
                  this paper, we propose a novel sparse regularization
                  which exploits the ability of curvelets to
                  efficiently represent geophysical images. We then
                  formulate a corresponding sparsity promoting
                  constrained optimization problem, which we call
                  Nonlinear Basis Pursuit Denoise (NBPDN) and present
                  an algorithm to solve this problem to recover medium
                  parameters. The utility of the NBPDN formulation and
                  efficacy of the algorithm are demonstrated on a
                  stylized cross-well exper- iment, where a sparse
                  velocity perturbation is recovered with higher
                  quality than the standard FWI formulation (solved
                  with LBFGS).  The NBPDN formulation and algorithm
                  can recover the sparse perturbation even when the
                  data volume is compressed to 5 percent of the
                  original size using random superposition.},
  keywords     = {EAGE,Presentation,Full-waveform inversion,Optimization},
  optmonth     = {01/2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/aravkin2011eage.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/aravkin11EAGEnspf/aravkin11EAGEnspf.pdf }
}


@CONFERENCE{aravkin2011SEGrobust,
  author       = {Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Robust full-waveform inversion using the Student's t-distribution},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {2669-2673},
  organization = {SEG},
  abstract     = {Full-waveform inversion (FWI) is a computational
                  procedure to extract medium parameters from seismic
                  data. Robust meth- ods for FWI are needed to
                  overcome sensitivity to noise and in cases where
                  modeling is particularly poor or far from the real
                  data generating process.  We survey previous robust
                  methods from a statistical perspective, and use this
                  perspective to derive a new robust method by
                  assuming the random errors in our model arise from
                  the Student's t-distribution.  We show that in
                  contrast to previous robust methods, the new method
                  progres- sively down-weighs large outliers,
                  effectively ignoring them once they are large
                  enough. This suggests that the new method is more
                  robust and suitable for situations with very poor
                  data quality or modeling. Experiments show that the
                  new method recovers as well or better than previous
                  robust methods, and can recover models with quality
                  comparable to standard meth- ods on noise-free data
                  when some of the data is completely corrupted, and
                  even when a marine acquisition mask is entirely
                  ignored in the modeling. The ability to ignore a
                  marine acqui- sition mask via robust FWI methods
                  offers an opportunity for stochastic optimization
                  methods in marine acquisition.},
  keywords     = {SEG,Full-waveform inversion,Optimization},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3627747},
  timestamp    = {2011-04-06 15:00:00 -0700},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/aravkin11SEGrobust/aravkin11SEGrobust.pdf }
}


@CONFERENCE{herrmann2011ICIAMconvexcompfwi,
  author       = {Felix J. Herrmann and Aleksandr Y. Aravkin and Tristan van Leeuwen and Xiang Li},
  title        = {FWI with sparse recovery: a convex-composite approach},
  booktitle    = {ICIAM},
  year         = {2011},
  organization = {ICIAM 2011},
  abstract     = {Iterative inversion algorithms require repeated
                  simulation of 3D time-dependent acoustic, elastic,
                  or electromagnetic wave fields, extending hundreds
                  of wavelengths and hundreds of periods. Also,
                  seismic data is rich in information at every
                  representable scale. Thus simulation-driven
                  optimization approaches to inversion impose great
                  demands on simulator efficiency and accuracy. While
                  computer hardware advances have been of critical
                  importance in bringing inversion closer to practical
                  application, algorithmic advances in simulator
                  methodology have been equally important. Speakers in
                  this two-part session will address a variety of
                  numerical issues arising in the wave simulation, and
                  in its application to inversion. },
  date-added   = {2011-07-20},
  optmonth     = {07/2011},
  keywords     = {ICIAM,Presentation,Full-waveform inversion,Optimization},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/herrmann2011ICIAM.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/herrmann2011ICIAM.pdf}
}


@CONFERENCE{herrmann2011SLRAfwi,
  author       = {Felix J. Herrmann and Aleksandr Y. Aravkin and Xiang Li and Tristan van Leeuwen},
  title        = {Full Waveform Inversion with Compressive Updates},
  booktitle    = {SLRA},
  year         = {2011},
  organization = {Sparse and Low Rank Approximation 2011},
  abstract     = {Full-waveform inversion relies on large
                  multi-experiment data volumes.  While improvements
                  in acquisition and inversion have been extremely
                  successful, the current push for higher quality
                  models reveals fundamental shortcomings handling
                  increasing problem sizes numerically. To address
                  this fundamental issue, we propose a randomized
                  dimensionality-reduction strategy motivated by
                  recent developments in stochastic optimization and
                  compressive sensing. In this formulation
                  conventional Gauss-Newton iterations are replaced by
                  dimensionality-reduced sparse recovery problems with
                  source encodings.},
  keywords     = {Presentation,Full-waveform inversion},
  presentation = {http://slim/Publications/Public/Presentations/2011/Herrmann2011css.pdf}
}


@CONFERENCE{herrmann2011EAGEefmsp,
  author       = {Felix J. Herrmann and Xiang Li},
  title        = {Efficient least-squares migration with sparsity promotion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Seismic imaging relies on the collection of
                  multi-experimental data volumes in combination with
                  a sophisticated back-end to create high-fidelity
                  inversion results. While significant improve- ments
                  have been made in linearized inversion, the current
                  trend of incessantly pushing for higher quality
                  models in increasingly complicated regions reveals
                  fundamental shortcomings in handling increasing
                  problem sizes numerically.  The so-called Òcurse of
                  dimensionalityÓ is the main culprit because it leads
                  to an exponential growth in the number of sources
                  and the corresponding number of wavefield
                  simulations required by Ôwave-equationÕ
                  migration. We address this issue by reducing the
                  number of sources by a randomized dimensionality
                  reduction technique that combines recent
                  developments in stochastic optimization and
                  compressive sensing.  As a result, we replace the
                  cur- rent formulations of imaging that rely on all
                  data by a sequence of smaller imaging problems that
                  use the output of the previous inversion as input
                  for the next. Empirically, we find speedups of at
                  least one order-of-magnitude when each reduced
                  experiment is considered theoretically as a separate
                  compressive-sensing experiment.},
  keywords     = {Presentation,EAGE,Imaging},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/eage/eage11/herrmann2011eage.pdf},
  optmonth     = {01/2011},
  timestamp    = {2011-01-14},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/herrmann11EAGEefmsp/herrmann11EAGEefmsp.pdf }
}


@CONFERENCE{herrmann2011SLIMsummer1,
  author       = {Felix J. Herrmann},
  title        = {Gene Golub SIAM Summer School July 4 - 15, 2011},
  booktitle    = {SLIM},
  year         = {2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/HerrmannLecture1.pdf},
  timestamp    = {2011.08.05},
  keywords     = {Presentation},
  optmonth     = {08/2011}
}


@CONFERENCE{herrmann11SLIMsummer2,
  author       = {Felix J. Herrmann},
  title        = {Lecture 2. Gene Golub SIAM Summer School July 4 - 15, 2011},
  booktitle    = {SLIM},
  year         = {2011},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/HerrmannLecture2.pdf},
  timestamp    = {2011.08.05},
  keywords     = {Presentation},
  optmonth     = {08/2011}
}


@CONFERENCE{jumah2011SEGdrepsi,
  author       = {Bander Jumah and Felix J. Herrmann},
  title        = {Dimensionality-reduced estimation of primaries by sparse inversion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {3520-3525},
  organization = {SEG},
  abstract     = {Data-driven methods---such as the estimation of
                  primaries by sparse inversion---suffer from the "curse
                  of dimensionality", which leads to disproportional
                  growth in computational and storage demands when
                  moving to realistic 3-D field data. To re- move this
                  fundamental impediment, we propose a dimensional-
                  ity reduction technique where the "data matrix" is
                  approximated adaptively by a randomized low-rank
                  approximation. Com- pared to conventional methods,
                  our approach has the advantage that the cost of the
                  low-rank approximation is reduced significantly,
                  which may lead to considerable reductions in storage
                  and computational costs of the sparse
                  inversion. Application of the proposed formalism to
                  synthetic data shows that significant improvements
                  are achievable at low computational overhead
                  required to compute the low-rank approximations.},
  date-added   = {2011-04-06 15:00:00 -0700},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3627931},
  keywords     = {Presentation,SEG,Processing},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/Jumah11SEGdrepsi/Jumah11SEGdrepsi.pdf}
}


@CONFERENCE{li2011EAGEfwirr,
  author       = {Xiang Li and Aleksandr Y. Aravkin and Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Full-waveform inversion with randomized L1 recovery for the model updates},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Full-waveform inversion (FWI) is a data fitting
                  procedure that relies on the collection of seis- mic
                  data volumes and sophisticated computing to create
                  high-resolution results. With the advent of FWI, the
                  improvements in acquisition and inversion have been
                  substantial, but these improvements come at a high
                  cost because FWI involves extremely large
                  multi-experiment data volumes. The main obstacle is
                  the Ôcurse of dimensionalityÕ exemplified by
                  NyquistÕs sampling criterion, which puts a
                  disproportionate strain on current acquisition and
                  processing systems as the size and desired
                  resolution increases. In this paper, we address the
                  Ôcurse of dimensionalityÕ by randomized dimen-
                  sionality reduction of the FWI problem adapted from
                  the field of CS. We invert for model updates by
                  replacing the Gauss-Newton linearized subproblem for
                  subsampled FWI with a sparsity promoting
                  formulation, and solve this formulation using the
                  SPGl1 algorithm. We speed up the algorithm and avoid
                  overfitting the data by solving for the linearized
                  updates only approximately.  Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we can compute a Newton-like update with
                  the reduced data volume at the cost of roughly one
                  gradient update for the fully sampled wavefield.},
  keywords     = {Presentation,EAGE,Full-waveform inversion},
  optmonth     = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/li11EAGEfwirr/li11EAGEfwirr.pdf}
}


@CONFERENCE{li2011SBGFmgnsu,
  author       = {Xiang Li and Felix J. Herrmann and Tristan van Leeuwen and Aleksandr Y. Aravkin},
  title        = {Modified Gauss-Newton with Sparse Updates},
  booktitle    = {SBGF},
  year         = {2011},
  organization = {SBGF},
  abstract     = {Full-waveform inversion (FWI) is a data fitting
                  procedure that relies on the collection of seismic
                  data volumes and sophisticated computing to create
                  high-resolution models.With the advent of FWI, the
                  improvements in acquisition and inversion have been
                  substantial, but these improvements come at a high
                  cost because FWI involves extremely large
                  multi-experiment data volumes. The main obstacle is
                  the {\textquoteleft}curse of
                  dimensionality{\textquoteright} exemplified by
                  Nyquist{\textquoteright}s sampling criterion, which
                  puts a disproportionate strain on current
                  acquisition and processing systems as the size and
                  desired resolution increases. In this paper, we
                  address the {\textquoteleft}curse of
                  dimensionality{\textquoteright} by using randomized
                  dimensionality reduction of the FWI problem, coupled
                  with a modified Gauss-Newton (GN) method designed to
                  promote curvelet-domain sparsity of model
                  updates. We solve for these updates using the
                  spectral projected gradient method, implemented in
                  the SPG￿1 software package. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we can compute Gauss-Newton updates with
                  the reduced data volume at the cost of roughly one
                  gradient update for the fully sampled wavefield},
  keywords     = {SBGF,Full-waveform inversion},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SBGF/2011/li11SBGFmgnsu/li11SBGFmgnsu.pdf}
}


@CONFERENCE{lin2011EAGEepsic,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Estimating primaries by sparse inversion in a curvelet-like representation domain},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {We present an uplift in the fidelity and wavefront
                  continuity of results obtained from the Estimation
                  of Primaries by Sparse Inversion (EPSI) program by
                  reconstructing the primary events in a hybrid
                  wavelet-curvelet representation domain. EPSI is a
                  multiple removal technique that belongs to the class
                  of wavefield inversion methods, as an alternative to
                  the traditional adaptive-subtraction process. The
                  main assumption is that the correct primary events
                  should be as sparsely-populated in time as
                  possible. A convex reformulation of the original
                  EPSI algorithm allows its convergence property to be
                  preserved even when the solution wavefield is not
                  formed in the physical domain. Since wavefronts and
                  edge-type singularities are sparsely represented in
                  the curvelet domain, sparse solutions formed in this
                  domain will exhibit vastly improved continuity when
                  compared to those formed in the physical domain,
                  especially for the low-energy events at later
                  arrival times. Further- more, a wavelet-type
                  representation domain will preserve sparsity in the
                  reflected events even if they originate from
                  non-zero-order discontinuities in the subsurface,
                  providing an additional level of robustness. This
                  method does not require any changes in the
                  underlying computational algorithm and does not
                  explicitly impose continuity constraints on each
                  update.},
  keywords     = {Presentation,EAGE,Processing},
  optmonth     = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/lin11EAGEepsic/lin11EAGEepsic.pdf}
}


@CONFERENCE{lin2011SEGrssde,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Robust source signature deconvolution and the estimation of primaries by sparse inversion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {4354-4359},
  abstract     = {The past few years had seen some concentrated
                  interest on a particular wavefield-inversion
                  approach to the popular SRME multiple removal
                  technique called Estimation of Primaries by Sparse
                  Inversion (EPSI). EPSI promises greatly improved
                  tol- erance to noise, missing data, edge effect, and
                  other physi- cal phenomenon generally not described
                  by the SRME relation (van Groenestijn and Verschuur,
                  2009a,b). It is based on the premise that it is
                  possible to stably invert for both the primary
                  impulse response and the source signature despite
                  beforehand having no (or very limited) explicit
                  knowledge of latter. The key to successful
                  applications of EPSI, as shown in very recent works
                  (Savels et al., 2010), is a robust way to
                  reconstruct very sparse primary impulse response
                  events as part of the inver- sion process. Based on
                  the various successful demonstrations in literature,
                  there is a very strong sense that EPSI will also
                  play an important role in future developments of
                  source sig- nature deconvolution and the general
                  recovering of wavefield spectrum. },
  organization = {Dept. of Earth and Ocean Sciences, University of British Columbia},
  keywords     = {Presentation,deconvolution, SEG, sparse inversion,Processing},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/lin11SEGrssde/lin11SEGrssde.pdf},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3628116}
}


@CONFERENCE{mansour2011SBGFcspsma,
  author       = {Hassan Mansour and Haneet Wason and Tim T.Y. Lin and Felix J. Herrmann},
  title        = {A compressive sensing perspective on simultaneous marine acquisition},
  booktitle    = {SBGF},
  year         = {2011},
  organization = {SBGF},
  abstract     = {The high cost of acquiring seismic data in Marine
                  environments compels the adoption of simultaneous-
                  source acquisition - an emerging technology that is
                  stimulating both geophysical research and commercial
                  efforts.  In this paper, we discuss the properties
                  of randomized simultaneous acquisition matrices and
                  demonstrate that sparsity-promoting recovery
                  improves the quality of the reconstructed seismic
                  data volumes. Simultaneous Marine acquisition calls
                  for the development of a new set of design
                  principles and post-processing tools. Leveraging
                  established findings from the field of compressed
                  sensing, the recovery from simultaneous sources
                  depends on a sparsifying transform that compresses
                  seismic data, is fast, and reasonably incoherent
                  with the compressive sampling matrix. To achieve
                  this incoherence, we use random time dithering where
                  sequential acquisition with a single airgun is
                  replaced by continuous acquisition with multiple
                  airguns firing at random times and at random
                  locations. We demonstrate our results with
                  simulations of simultaneous Marine acquisition using
                  periodic and randomized time dithering.},
  keywords     = {Presentation,SBGF,Acquisition,Compressive Sensing},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SBGF/2011/Mansour11SBGFcspsma/Mansour11SBGFcspsma.pdf}
}


@CONFERENCE{vanleeuwen2011SIAMGEOmawt,
  author       = {Tristan van Leeuwen and Wim Mulder},
  title        = {Multiscale aspects of waveform tomography},
  booktitle    = {SIAMGEO},
  year         = {2011},
  organization = {SIAM GeoSciences 2011},
  abstract     = {We consider the inference of medium velocity from
                  transmitted acoustic waves. Typically, the
                  measurements are done in a narrow frequency band. As
                  a result the sensitivity of the data with respect to
                  velocity perturbations varies dramatically with the
                  scale of the perturbation.
                  {\textquoteleft}Smooth{\textquoteright}
                  perturbations will cause a phase shift, whereas
                  perturbations that vary on the wavelength-scale
                  cause amplitude variations. We investigate how to
                  incorporate this scale dependent behavior in the
                  formulation of the inverse problem.},
  keywords     = {Presentation},
  presentation = {http://slim/Publications/Public/Presentations/2011/SIAMGS11_MS61_Leeuwen.pdf}
}


@CONFERENCE{tu2011SEGmult,
  author       = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Migration with surface-related multiples from incomplete seismic data},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {3222-3227},
  organization = {SEG},
  abstract     = {Seismic acquisition is confined by limited aperture
                  that leads to finite illumination, which, together
                  with other factors, hin- ders imaging of subsurface
                  objects in complex geological set- tings such as
                  salt structures. Conventional processing, includ-
                  ing surface-related multiple elimination, further
                  reduces the amount of information we can get from
                  seismic data. With the growing consensus that
                  multiples carry valuable informa- tion that is
                  missing from primaries, we are motivated to exploit
                  the extra illumination provided by multiples to
                  image the sub- surface. In earlier research, we
                  proposed such a method by combining primary
                  estimation and sparsity-promoting migra- tion to
                  invert for model perturbations directly from the
                  total up-going wavefield. In this abstract, we focus
                  on a particular case. By exploiting the extra
                  illumination from surface-related multiples, we
                  mitigate the effects caused by migrating from in-
                  complete data with missing sources and missing
                  near-offsets.},
  keywords     = {Presentation,SEG,Imaging,Processing},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3627865},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/tu11SEGmult/tu11SEGmult.pdf}
}


@CONFERENCE{tu2011EAGEspmsrm,
  author       = {Ning Tu and Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Sparsity-promoting migration with surface-related multiples},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Multiples, especially the surface-related multiples,
                  form a significant part of the total up-going wave-
                  field. If not properly dealt with, they can lead to
                  false reflectors in the final image. So
                  conventionally practitioners remove them prior to
                  migration. Recently research has revealed that
                  multiples can actually provide extra illumination so
                  different methods are proposed to address the issue
                  that how to use multiples in seismic imaging, but
                  with various kinds of limitations.  In this
                  abstract, we combine primary estimation and
                  sparsity-promoting migration into one
                  convex-optimization process to include information
                  from multiples. Synthetic examples show that
                  multiples do make active contributions to seismic
                  migration. Also by this combination, we can benefit
                  from better recoveries of the GreenÕs function by
                  using sparsity-promoting algorithms since
                  reflectivity is sparser than the GreenÕs function.},
  keywords     = {Presentation,EAGE,Imaging,Processing},
  optmonth     = {01/2011},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/tu11EAGEspmsrm/tu11EAGEspmsrm.pdf}
}


@CONFERENCE{vanleeuwen2011AMPhsdmwi,
  author       = {Tristan van Leeuwen and Mark Schmidt and Michael P. Friedlander and Felix J. Herrmann},
  title        = {A hybrid stocahstic-deterministic method for
                  waveform inversion. Presented at AMP Medical and
                  Seismic Imaging, 2011, Vancouver BC.},
  booktitle    = {AMP},
  year         = {2011},
  organization = {WAVES 2011},
  abstract     = {A lot of seismic and medical imaging problems can be
                  written as a least-squares data- fitting problem. In
                  particular, we consider the case of multi-experiment
                  data, where the data consists of a large number of
                  ÔindependentÕ measurements. Solving the inverse
                  prob- lem then involves repeatedly forward modeling
                  the data for each of these experiments. In case the
                  number of experiments is large and the modeling
                  kernel expensive to apply, such an approach may be
                  prohibitively expensive. We review techniques from
                  stochastic opti- mization which aim at dramatically
                  reducing the number of experiments that need to be
                  modeled at each iteration. This reduction is
                  typically achieved by randomly subsampling the
                  data. Special care needs to be taken in the
                  optimization to deal with the stochasticity that is
                  introduced in this way. },
  date-added   = {2011-07-15},
  optmonth     = {07/2011},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/AMPleeuwen2011b.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/AMPleeuwen2011b.pdf}
}


@CONFERENCE{vanleeuwen2011EAGEhsdomwi,
  author       = {Tristan van Leeuwen and Felix J. Herrmann and Mark Schmidt and Michael P. Friedlander},
  title        = {A hybrid stochastic-deterministic optimization method for waveform inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2011},
  organization = {EAGE},
  abstract     = {Present-day high quality 3D acquisition can give us
                  lower frequencies and longer offsets with which to
                  invert. However, the computational costs involved in
                  handling this data explosion are
                  tremendous. Therefore, recent developments in
                  full-waveform inversion have been geared towards
                  reducing the computational costs involved. A key
                  aspect of several approaches that have been proposed
                  is a dramatic reduction in the number of sources
                  used in each iteration. A reduction in the number of
                  sources directly translates to less PDE-solves and
                  hence a lower computational cost. Re- cent attention
                  has been drawn towards reducing the sources by
                  randomly combining the sources in to a few
                  supershots, but other strategies are also
                  possible. In all cases, the full data misfit, which
                  involves all the sequential sources, is replaced by
                  a reduced misfit that is much cheaper to evaluate
                  because it involves only a small number of sources
                  (batchsize). The batchsize controls the accuracy
                  with which the reduced misfit approximates the full
                  misfit. The optimization of such an inaccurate, or
                  noisy, misfit is the topic of stochastic
                  optimization. In this paper, we propose an
                  optimization strategy that borrows ideas from the
                  field of stochastic optimization. The main idea is
                  that in the early stage of the optimization, far
                  from the true model, we do not need a very accurate
                  misfit. The strategy consists of gradually
                  increasing the batchsize as the iterations
                  proceed. We test the proposed strategy on a
                  synthetic dataset. We achieve a very reasonable
                  inversion result at the cost of roughly 13
                  evaluations of the full misfit. We observe a
                  speed-up of roughly a factor 20.},
  keywords     = {Presentation,EAGE,Full-waveform inversion,Optimization},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi_pres.pdf},
  optmonth     = {01/2011},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2011/vanleeuwen11EAGEhsdomwi/vanleeuwen11EAGEhsdomwi.pdf}
}


@CONFERENCE{vanleeuwen2011WAVESpeiv,
  author       = {Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Probing the extended image volume for seismic velocity inversion},
  booktitle    = {WAVES},
  year         = {2011},
  organization = {Waves 2011},
  abstract     = {In seismic velocity inversion one aims to
                  reconstruct a kinematically correct subsurface
                  velocity model that can be used as input for further
                  processing and inversion of the data. An important
                  tool in velocity inversion is the prestack image
                  volume. This image volume can be defined as a cross-
                  correlation of the source and receivers wavefields
                  for non-zero space and time lags. If the background
                  velocity is kinematically acceptable, this image
                  volume will have its main contributions at zero lag,
                  even for complex models. Thus, it is an ideal tool
                  for wave-equation migration velocity analysis in the
                  presence of strong lateral heterogeneity. In
                  particular, it allows us to pose migration velocity
                  analysis as a PDE- constrained optimization problem,
                  where the goal is to minimize the energy in the
                  image volume at non-zero lag subject to fitting the
                  data approximately. However, it is computationally
                  infeasible to explicitly form the whole image
                  volume. In this paper, we discuss several ways to
                  reduce the computational costs involved in computing
                  the image volume and evaluating the focusing
                  criterion.  We reduce the costs for calculating the
                  data by randomized source synthesis. We also present
                  an efficient way to subsample the image
                  volume. Finally, we propose an alternative
                  optimization criterion and suggest a multiscale
                  inversion strategy for wave-equation MVA.  },
  date-added   = {2011-07-29},
  optmonth     = {07/2011},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/waves11leeuwen.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/waves11leeuwen.pdf}
}


@CONFERENCE{vanleeuwen2011SEGext,
  author       = {Tristan van Leeuwen and Felix J. Herrmann},
  title        = {Probing the extended image volume},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {4045-4050},
  organization = {SEG},
  abstract     = {The prestack image volume can be defined as a cross-
                  correlation of the source and receivers wavefields
                  for non-zero space and time lags.  If the background
                  velocity is kinemati- cally acceptable, this image
                  volume will have its main contri- butions at zero
                  lag, even for complex models. Thus, it is an ideal
                  tool for wave-equation migration velocity analysis
                  in the presence of strong lateral heterogeneity. In
                  particular, it allows us to pose migration velocity
                  analysis as a PDE-constrained optimization problem,
                  where the goal is to minimize the en- ergy in the
                  image volume at non-zero lag subject to fitting the
                  data approximately.  However, it is computationally
                  infeasi- ble to explicitly form the whole image
                  volume. In this paper, we discuss several ways to
                  reduce the computational costs in- volved in
                  computing the image volume and evaluating the fo-
                  cusing criterion. We reduce the costs for
                  calculating the data by randomized source
                  synthesis. We also present an efficient way to
                  subsample the image volume. Finally, we propose an
                  alternative optimization criterion and suggest a
                  multiscale in- version strategy for wave-equation
                  MVA.},
  keywords     = {SEG,Imaging},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3628051},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/vanleeuwen11SEGext/vanleeuwen11SEGext.pdf}
}


@CONFERENCE{vanleeuwen2011ICIAMcbmcwe,
  author       = {Tristan van Leeuwen},
  title        = {A correlation-based misfit criterion for
                  wave-equation traveltime tomography. Presented at
                  ICIAM 2011, Vancouver BC.},
  booktitle    = {ICIAM},
  year         = {2011},
  organization = {ICIAM 2011},
  abstract     = {The inference of subsurface medium parameters from
                  seismic data can be posed as a PDE-constrained
                  data-fitting procedure. This approach is successful
                  in reconstructing medium perturbations that are in
                  the order of the wavelength. In practice, the data
                  lack low frequency content and this means that one
                  needs a good initial guess of the slowly varying
                  component of the medium. For a wrong starting model
                  an iterative reconstruction procedure is likely to
                  end up in a local minimum. We propose to use a
                  different measure of the misfit that makes the
                  optimization problem well-posed in terms of the
                  slowly varying velocity structures. This procedure
                  can be seen as a generalization of ray-based
                  traveltime tomography. We discuss the theoretical
                  underpinnings of the method and give some numerical
                  examples.},
  date-added   = {2011-07-19},
  optmonth     = {07/2011},
  keywords     = {Presentation,ICIAM,Imaging},
  file         = {http://slim/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf:PDF},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf},
  url          = {http://slim/Publications/Public/Presentations/2011/ICIAM11leeuwen.pdf}
}


@CONFERENCE{wason2011SEGsprsd,
  author       = {Haneet Wason and Felix J. Herrmann and Tim T.Y. Lin},
  title        = {Sparsity-promoting recovery from simultaneous data:
                  a compressive sensing approach},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2011},
  volume       = {30},
  pages        = {6-10},
  organization = {SEG},
  abstract     = {Seismic data acquisition forms one of the main
                  bottlenecks in seismic imaging and inversion. The
                  high cost of acquisition work and collection of
                  massive data volumes compel the adoption of
                  simultaneous-source seismic data acquisition - an
                  emerging technology that is developing rapidly,
                  stimulating both geophysical research and commercial
                  efforts.  Aimed at improving the performance of
                  marine- and land-acquisition crews, simultaneous
                  acquisition calls for development of a new set of
                  design principles and post-processing
                  tools. Leveraging developments from the field of
                  compressive sensing the focus here is on
                  simultaneous-acquisition design and
                  sequential-source data recovery. Apart from proper
                  compressive sensing sampling schemes, the recovery
                  from simultaneous simulations depends on a
                  sparsifying transform that compresses seismic data,
                  is fast, and reasonably incoherent with the
                  compressive-sampling matrix. Using the curvelet
                  transform, in which seismic data can be represented
                  parsimoniously, the recovery of the
                  sequential-source data volumes is achieved using the
                  sparsity-promoting program {\textemdash} SPGL1, a
                  solver based on projected spectral gradients. The
                  main outcome of this approach is a new technology
                  where acquisition related costs are no longer
                  determined by the stringent Nyquist sampling
                  criteria.},
  keywords     = {Presentation,SEG,Acquisition,Compressive Sensing},
  optmonth     = {04/2011},
  optdoi       = {10.1190/1.3628174},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2011/wason11SEGsprsd/wason11SEGsprsd.pdf}
}


%-------------------------------------------------2010-------------------------------------------------

@CONFERENCE{herrmann2010SEGerc, 
  author       = {Felix J. Herrmann},
  title        = {Empirical recovery conditions for seismic sampling},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2010},
  abstract     = {In this paper, we offer an alternative sampling
                  method leveraging recent insights from compressive
                  sensing towards seismic acquisition and processing
                  for data that are traditionally considered to be
                  undersampled.  The main outcome of this approach is
                  a new technology where acquisition and processing
                  related costs are no longer determined by overly
                  stringent sampling criteria, such as Nyquist. At the
                  heart of our approach lies randomized incoherent
                  sampling that breaks subsampling related
                  interferences by turning them into harmless noise,
                  which we subsequently remove by promoting
                  transform-domain sparsity. Now, costs no longer grow
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold.  First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic acquisition.
                  Second, we show that accurate recovery can be
                  accomplished for compressively sampled data volumes
                  sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.  We illustrate this
                  principle by means of number of case studies.},
  keywords     = {SEG},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/herrmann10erc.pdf}
}


@CONFERENCE{frijlink2010EAGEcos,
  author       = {M.O. Frijlink and Reza Shahidi and Felix J. Herrmann and R.G. van Borselen},
  title        = {Comparison of Standard Adaptive Subtraction and
                  Primary-multiple Separation in the Curvelet Domain},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {In recent years, data-driven multiple prediction
                  methods and wavefield extrapolation methods have
                  proven to be powerful methods to attenuate multiples
                  from data acquired in complex 3-D geologic
                  environments.  These methods make use of a two-stage
                  approach, where first the multiples (surface-related
                  and / or internal) multiples are predicted before
                  they are subtracted from the original input data in
                  an adaptively.  The quality of these predicted
                  multiples often raises high expectations for the
                  adaptive subtraction techniques, but for various
                  reasons these expectations are not always met in
                  practice. Standard adaptive subtraction methods use
                  the well-known minimum energy criterion, stating
                  that the total energy after optimal multiple
                  attenuation should be minimal. When primaries and
                  multiples interfere , the minimum energy criterion
                  is no longer appropriate. Also, when multiples of
                  different orders interfere, adaptive energy
                  minimization will lead to a compromise between
                  different amplitudes corrections for the different
                  orders of multiples. This paper investigates the
                  performance of two multiple subtraction schemes for
                  a real data set that exhibits both interference
                  problems. Results from an adaptive subtraction in
                  the real curvelet domain, separating primaries and
                  multiples, are compared to those obtained using a
                  more conventional adaptive subtraction method in the
                  spatial domain.},
  keywords     = {EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/frijlink10EAGEcos/frijlink10EAGEcos.pdf}
}


@CONFERENCE{herrmann2010MATHIAScssr,
  author       = {Felix J. Herrmann},
  title        = {Compressive Sensing and Sparse Recovery in
                  Exploration Seismology. Presented at MATHIAS 2010
                  organized by Total SA. Paris.},
  booktitle    = {MATHIAS},
  year         = {2010},
  abstract     = {During this presentation, I will talk about how
                  recent results from compressive sensing and sparse
                  recovery can be used to solve problems in
                  exploration seismology where incomplete sampling is
                  ubiquitous.  I will also talk about how these ideas
                  apply to dimensionality reduction of full-waveform
                  inversion by randomly phase encoded sources.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2010/herrmann2010total.pdf}
}


@CONFERENCE{herrmann2010EAGErds,
  author       = {Felix J. Herrmann and Xiang Li},
  title        = {Randomized dimensionality reduction for full-waveform inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth continues to reveal
                  fundamental shortcomings in our ability to handle
                  the ever increasing problem size numerically.  Two
                  causes can be identified as the main culprits
                  responsible for this barrier. First, there is the
                  so-called {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase.  Secondly, there is the
                  recent {\textquoteleft}{\textquoteleft}departure
                  from Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this. In this paper, we address this situation by
                  randomized dimensionality reduction, which we adapt
                  from the field of compressive sensing.  In this
                  approach, we combine deliberate randomized
                  subsampling with structure-exploiting
                  transform-domain sparsity promotion. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we compute Newton-like updates at the
                  cost of roughly one gradient update for the
                  fully-sampled wavefield.},
  keywords     = {Presentation,EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds_pres.pdf},  
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErds/herrmann10EAGErds.pdf/}
}


@CONFERENCE{herrmann2010EAGErss,
  author       = {Felix J. Herrmann},
  title        = {Randomized sampling strategies},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {Seismic exploration relies on the collection of
                  massive data volumes that are subsequently mined for
                  information during seismic processing.  While this
                  approach has been extremely successful in the past,
                  the current trend towards higher quality images in
                  increasingly complicated regions continues to reveal
                  fundamental shortcomings in our workflows for
                  high-dimensional data volumes. Two causes can be
                  identified..  First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase.  Secondly, there is the
                  recent {\textquoteleft}{\textquoteleft}departure
                  from Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  randomized subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. As such we end up with a new
                  technology where the costs of acquisition and
                  processing are no longer dictated by the size of the
                  acquisition but by the transform-domain sparsity of
                  the end-product.},
  keywords     = {Presentation,EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss_pres.pdf},  
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/herrmann10EAGErss/herrmann10EAGErss.pdf}
}


@CONFERENCE{herrmann2010IRISsns,
  author       = {Felix J. Herrmann},
  title        = {Sub-Nyquist sampling and sparsity: getting more
                  information from fewer samples. Presented at the
                  IRIS Workshop},
  booktitle    = {IRIS},
  year         = {2010},
  abstract     = {Many seismic exploration techniques rely on the
                  collection of massive data volumes. While this
                  approach has been extremely successful in the past,
                  current efforts toward higher resolution images in
                  increasingly complicated regions of the Earth
                  continue to reveal fundamental shortcomings in our
                  workflows. Chiefly amongst these is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. In this presentation, we offer an
                  alternative sampling method leveraging recent
                  insights from compressive sensing towards seismic
                  acquisition and processing of severely under-sampled
                  data. The main outcome of this approach is a new
                  technology where acquisition and processing related
                  costs are no longer determined by overly stringent
                  sampling criteria, such as Nyquist. At the heart of
                  our approach lies randomized incoherent sampling
                  that breaks subsampling related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting transform-domain
                  sparsity. Now, costs no longer grow significantly
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  exploration. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity. Many seismic exploration
                  techniques rely on the collection of massive data
                  volumes. While this approach has been extremely
                  successful in the past, current efforts toward
                  higher resolution images in increasingly complicated
                  regions of the Earth continue to reveal fundamental
                  shortcomings in our workflows. Chiefly amongst these
                  is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which disproportionately strains current
                  acquisition and processing systems as the size and
                  desired resolution of our survey areas continues to
                  increase. In this presentation, we offer an
                  alternative sampling method leveraging recent
                  insights from compressive sensing towards seismic
                  acquisition and processing of severely under-sampled
                  data. The main outcome of this approach is a new
                  technology where acquisition and processing related
                  costs are no longer determined by overly stringent
                  sampling criteria, such as Nyquist. At the heart of
                  our approach lies randomized incoherent sampling
                  that breaks subsampling related interferences by
                  turning them into harmless noise, which we
                  subsequently remove by promoting transform-domain
                  sparsity. Now, costs no longer grow significantly
                  with resolution and dimensionality of the survey
                  area, but instead depend on transform-domain
                  sparsity only. Our contribution is twofold. First,
                  we demonstrate by means of carefully designed
                  numerical experiments that compressive sensing can
                  successfully be adapted to seismic
                  exploration. Second, we show that accurate recovery
                  can be accomplished for compressively sampled data
                  volumes sizes that exceed the size of conventional
                  transform-domain data volumes by only a small
                  factor. Because compressive sensing combines
                  transformation and encoding by a single linear
                  encoding step, this technology is directly
                  applicable to acquisition and to dimensionality
                  reduction during processing. In either case,
                  sampling, storage, and processing costs scale with
                  transform-domain sparsity.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2010/herrmann2010iris.pdf}
}


@CONFERENCE{johnson2010EAGEeop,
  author       = {James Johnson and Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Estimation of primaries via sparse inversion with reciprocity},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {Accurate removal of surface related multiples is a
                  key step in seismic data processing. The industry
                  standard for removing multiples is SRME, which
                  involves convolving the data with itself to predict
                  the multiples, followed by an adaptive subtraction
                  procedure to recover the primaries (Verschuur and
                  Berkhout, 1997). Other methods involve
                  multidimensional division of the up-going and
                  down-going wavefields (Amundsen, 2001). However,
                  this approach may suffer from stability
                  problems. With the introduction of the
                  {\textquoteleft}{\textquoteleft}estimation of
                  primaries by sparse
                  inversion{\textquoteright}{\textquoteright}(EPSI),
                  van Groenestijn and Verschuur (2009) recentely
                  reformulated SRME to jointly estimate the
                  surface-free impulse response and the source
                  signature directly from the data. The advantage of
                  EPSI is that it recovers the primary response
                  directly, and does not require a second processing
                  step for the subtraction of estimated multiples from
                  the original data. However, because it estimates
                  both the primary impulse response and source
                  signature from the data EPSI must be regularized.
                  Motivated by recent successful application of the
                  curvelet transform in seismic data processing
                  (Herrmann et al., 2007), we formulate EPSI as a
                  bi-convex optimization problem that seeks sparsity
                  on the surface-free Green{\textquoteright}s function
                  and Fourier-domain smoothness on the source
                  wavelet. Our main contribution compared to previous
                  work (Lin and Herrmann, 2009), and the contribution
                  of that author to the proceedings of this
                  meeting(Lin and Herrmann, 2010), is that we employ
                  the physical principle of as source-receiver
                  reciprocity to improve the inversion.},
  keywords     = {EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/johnson10EAGEeop/johnson10EAGEeop.pdf}
}


@CONFERENCE{li2010SEGfwi,
  author       = {Xiang Li and Felix J. Herrmann},
  title        = {Full-waveform inversion from compressively recovered model updates},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2010},
  volume       = {29},
  pages        = {1029-1033},
  organization = {SEG},
  abstract     = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth reveals fundamental
                  shortcomings in our ability to handle increasing
                  problem size numerically. Two main culprits can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution increases. Secondly,
                  there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this. In this paper, we address this situation by
                  randomized dimensionality reduction, which we adapt
                  from the field of compressive sensing.  In this
                  approach, we combine deliberate randomized
                  subsampling with structure-exploiting
                  transform-domain sparsity promotion. Our approach is
                  successful because it reduces the size of seismic
                  data volumes without loss of information. With this
                  reduction, we compute Newton-like updates at the
                  cost of roughly one gradient update for the
                  fully-sampled wavefield.},
  keywords     = {Presentation,SEG,Full-waveform inversion},
  optdoi       = {10.1190/1.3513022},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/li10SEGfwi/li10SEGfwi.pdf}
}


@CONFERENCE{lin2010EAGEseo,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Stabilization of estimation of primaries via sparse inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2010},
  organization = {EAGE},
  abstract     = {Recent works on surface-related multiple removal
                  include a direct estimation method proposed by van
                  Groenestijn and Verschuur (2009), where under a
                  sparsity assumption the primary impulse response is
                  determined directly from a data-driven wavefield
                  inversion process called Estimation of Primaries by
                  Sparse Inversion (EPSI). The authors have shown that
                  this approach is superior to traditional estimation
                  subtraction processes such as SRME on shallow bottom
                  marine data, where by expanding the model to
                  simultaneously invert for the near-offset traces,
                  which are not directly available in most situation
                  but are observable in the data multiples, a large
                  improvement over Radon interpolation is
                  demonstrated. One of the major roadblocks to the
                  widespread adoption of EPSI is that one must have
                  precise knowledge of a time-window that contains
                  multiple-free primaries during each update. There is
                  some anecdotal evidence that the inversion result is
                  unstable under errors in the time-window length, a
                  behavior that runs contrary to the strengths of EPSI
                  and diminishes its effectiveness for shallow-bottom
                  marine data where multiples are closely spaced.
                  Moreover, due to the nuances involved in
                  regularizing the model impulse response in the
                  inverse problem, the EPSI approach has an additional
                  number of inversion parameters to choose and often
                  also does not often lead to a stable solution under
                  perturbations to these parameters.  We show that the
                  specific sparsity constraint on the EPSI updates
                  lead to an inherently intractable problem, and that
                  the time-window and other inversion variables arise
                  as additional regularizations on the unknown towards
                  a meaningful solution. We furthermore suggest a way
                  to remove almost all of these parameters via a L0 to
                  L1 convexification, which stabilizes the inversion
                  while preserving the crucial sparsity assumption in
                  the primary impulse response model.},
  keywords     = {Presentation,EAGE,Processing},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo_pres.pdf},  
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2010/lin10EAGEseo/lin10EAGEseo.pdf}
}


@CONFERENCE{lin10SEGspm,
  author       = {Tim T.Y. Lin and Ning Tu and Felix J. Herrmann},
  title        = {Sparsity-promoting migration from surface-related multiples},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2010},
  volume       = {29},
  pages        = {3333-3337},
  organization = {SEG},
  abstract     = {Seismic imaging typically begins with the removal of
                  multiple energy in the data, out of fear that it may
                  introduce erroneous structure.  However, seismic
                  multiples have effectively seen more of the
                  earth{\textquoteright}s structure, and if treated
                  correctly can potential supply more information to a
                  seismic image compared to primaries. Past approaches
                  to accomplish this leave ample room for improvement;
                  they either require extensive modification to
                  standard migration techniques, rely too much on
                  prior information, require extensive pre-processing,
                  or resort to full-waveform inversion. We take some
                  valuable lessons from these efforts and present a
                  new approach balanced in terms of ease of
                  implementation, robustness, efficiency and
                  well-posedness, involving a sparsity-promoting
                  inversion procedure using standard Born migration
                  and a data-driven multiple modeling approach based
                  on the focal transform.},
  keywords     = {Presentation,SEG,Processing},
  optdoi       = {10.1190/1.3513540},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/lin10SEGspm/lin10SEGspm.pdf}
}


@CONFERENCE{moghaddam2010SEGrfw,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann},
  title        = {Randomized full-waveform inversion: a dimenstionality-reduction approach},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2010},
  volume       = {29},
  pages        = {977-982},
  organization = {SEG},
  abstract     = {Full-waveform inversion relies on the collection of
                  large multi-experiment data volumes in combination
                  with a sophisticated back-end to create
                  high-fidelity inversion results. While improvements
                  in acquisition and inversion have been extremely
                  successful, the current trend of incessantly pushing
                  for higher quality models in increasingly
                  complicated regions of the Earth reveals fundamental
                  shortcomings in our ability to handle increasing
                  problem sizes numerically. Two main culprits can be
                  identified. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution increases. Secondly,
                  there is the recent
                  {\textquoteleft}{\textquoteleft}departure from
                  Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to develop algorithms that are amenable to
                  parallelization.  In this paper, we discuss
                  different strategies that address these issues via
                  randomized dimensionality reduction.},
  keywords     = {Presentation,SEG,Full-waveform inversion,Optimization},
  optdoi       = {10.1190/1.3513940},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2010/moghaddam10SEGrfw/moghaddam10SEGrfw.pdf}
}



%-------------------------------------------------2009-------------------------------------------------

@CONFERENCE{erlangga2009SEGfwi,
  author       = {Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Full-waveform Inversion with Gauss-Newton-Krylov Method},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  abstract     = {This abstract discusses an implicit implementation
                  of the Gauss-Newton method, used for the
                  frequency-domain full-waveform inversion, where the
                  inverse of the Hessian for the update is never
                  formed explicitly.  Instead, the inverse of the
                  Hessian is computed approximately by a conjugate
                  gradient (CG) method, which only requires the action
                  of the Hessian on the CG search direction. This
                  procedure avoids an excessive computer storage,
                  usually needed for storing the Hessian, at the
                  expense of extra computational work in CG. An
                  effective preconditioner for the Hessian is
                  important to improve the convergence of CG, and
                  hence to reduce the overall computational work.},
  keywords     = {Presentation, SEG, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/erlangga09segfwi.pdf}
}


@CONFERENCE{erlangga2009EAGEmwi,
  author       = {Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Migration with implicit solvers for the time-harmonic Helmholtz equation},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2009},
  organization = {EAGE},
  abstract     = {From the measured seismic data, the location and the
                  amplitude of reflectors can be determined via a
                  migration algorithm. Classically, following
                  Claerbout{\textquoteright}s imaging principle [2], a
                  reflector is located at the position where the
                  source{\textquoteright}s forward-propagated
                  wavefield correlates with the backward-propagated
                  wavefield of the receiver data. Lailly and Tarantola
                  later showed that this imaging principle is an
                  instance of inverse problems, with the associated
                  migration operator formulated via a least-squares
                  functional; see [6, 12, 13]. Furthermore, they
                  showed that the migrated image is associated with
                  the gradient of this functional with respect to the
                  image. If the solution of the least-squares
                  functional is done iteratively, the
                  correlation-based image coincides up to a constant
                  with the first iteration of a gradient method. In
                  practice, this migration is done either in the time
                  domain or in the frequency domain. In the
                  frequency-domain migration, the main bottleneck thus
                  far, which renders its full implementation to large
                  scale problems, is the lack of efficient solvers for
                  computing wavefields. Robust direct methods easily
                  run into excessive memory requirements as the size
                  of the problem increases. On the other hand,
                  iterative methods, which are less demanding in terms
                  of memory, suffered from lack of convergence. During
                  the past years, however, progress has been made in
                  the development of an efficient iterative method [4,
                  3] for the frequency-domain wavefield
                  computations. In this paper, we will show the
                  significance of this method (called MKMG) in the
                  context of the frequency-domain migration, where
                  multi-shot-frequency wavefields (of order of 10,000
                  related wavefields) need to be computed.},
  keywords     = {Presentation,EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/erlanggaEAGE2009.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Erlangga09EAGEmwi/Erlangga09EAGEmwi.pdf}
}


@CONFERENCE{erlangga2009SEGswi,
  author       = {Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Seismic waveform inversion with Gauss-Newton-Krylov method},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {2357-2361},
  organization = {SEG},
  abstract     = {This abstract discusses an implicit implementation
                  of the Gauss-Newton method, used for the
                  frequency-domain full-waveform inversion, where the
                  inverse of the Hessian for the update is never
                  formed explicitly.  Instead, the inverse of the
                  Hessian is computed approximately by a conjugate
                  gradient (CG) method, which only requires the action
                  of the Hessian on the CG search direction. This
                  procedure avoids an excessive computer storage,
                  usually needed for storing the Hessian, at the
                  expense of extra computational work in CG. An
                  effective preconditioner for the Hessian is
                  important to improve the convergence of CG, and
                  hence to reduce the overall computational work.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255332},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/erlangga09SEGswi/erlangga09SEGswi.pdf}
}


@CONFERENCE{friedlander2009VIETcsgpa,
  author       = {Michael P. Friedlander},
  title        = {Computing sparse and group-sparse approximations},
  booktitle    = {VIET},
  organization = {2009 High Performance Scientific Computing Conference},
  year         = {2009},
  address      = {Hanoi, Vietnam},
  keywords     = {minimization, SLIM}
}


@CONFERENCE{friedlander2009NUalssr,
  author       = {Michael P. Friedlander},
  title        = {Algorithms for large-scale sparse reconstruction},
  booktitle    = {IEMS},
  organization = {IEMS Colloquim Speaker},
  year         = {2009},
  address      = {Northwestern University},
  keywords     = {minimization, SLIM}
}


@CONFERENCE{friedlander2009SCAIMspot,
  author       = {Ewout {van den Berg} and Michael P. Friedlander},
  title        = {Spot: A linear-operator toolbox for Matlab},
  booktitle    = {SCAIM},
  organization = {SCAIM Seminar},
  year         = {2009},
  address      = {University of British Columbia},
  keywords     = {minimization, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Private/Presentations/sinbad/2010Fall/Thu-13-50-Friedlander.pdf }
}


@CONFERENCE{herrmann2009SEGcib,
  author       = {Felix J. Herrmann},
  title        = {Compressive imaging by wavefield inversion with group sparsity},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {2337-2341},
  organization = {SEG},
  abstract     = {Migration relies on multi-dimensional correlations
                  between source- and residual wavefields. These
                  multi-dimensional correlations are computationally
                  expensive because they involve operations with
                  explicit and full matrices that contain both
                  wavefields. By leveraging recent insights from
                  compressive sampling, we present an alternative
                  method where linear correlation-based imaging is
                  replaced by imaging via multidimensional
                  deconvolutions of compressibly sampled wavefields.
                  Even though this approach goes at the expense of
                  having to solve a sparsity-promotion recovery
                  program for the image, our wavefield inversion
                  approach has the advantage of reducing the system
                  size in accordance to transform-domain sparsity of
                  the image. Because seismic images also exhibit a
                  focusing of the energy towards zero offset, the
                  compressive-wavefield inversion itself is carried
                  out using a recent extension of one-norm solver
                  technology towards matrix-valued problems. These
                  so-called hybrid $(1,\,2)$-norm solvers allow us to
                  penalize pre-stack energy away from zero offset
                  while exploiting joint sparsity amongst near-offset
                  images. Contrary to earlier work to reduce modeling
                  and imaging costs through random phase-encoded
                  sources, our method compressively samples wavefields
                  in model space.  This approach has several
                  advantages amongst which improved system-size
                  reduction, and more flexibility during subsequent
                  inversions for subsurface properties.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255328},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGcib/herrmann09SEGcib.pdf}
}


@CONFERENCE{herrmann2009EAGEcsa,
  author       = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title        = {Compressive sensing applied to full-waveform inversion},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2009},
  organization = {EAGE},
  abstract     = {With the recent resurgence of full-waveform
                  inversion, the computational cost of solving forward
                  modeling problems has become{\textendash}-aside from
                  issues with non-uniqueness{\textendash}-one of the
                  major impediments withstanding successful
                  application of this technology to industry-size data
                  volumes. To overcome this impediment, we argue that
                  further improvements in this area will depend on a
                  problem formulation with a computational complexity
                  that is no longer strictly determined by the size of
                  the discretization but by transform-domain sparsity
                  of its solution. In this new paradigm, we bring
                  computational costs in par with our ability to
                  compress seismic data and images. This premise is
                  related to two recent developments. First, there is
                  the new field of compressive sensing (CS in short
                  throughout the paper, Cand{\textquoteleft}es et al.,
                  2006; Donoho, 2006){\textendash}-where the argument
                  is made, and rigorously proven, that compressible
                  signals can be recovered from severely sub-Nyquist
                  sampling by solving a sparsity promoting
                  program. Second, there is in the seismic community
                  the recent resurgence of simultaneous-source
                  acquisition (Beasley, 2008; Krohn and Neelamani,
                  2008; Herrmann et al., 2009; Berkhout, 2008;
                  Neelamani et al., 2008), and continuing efforts to
                  reduce the cost of seismic modeling, imaging, and
                  inversion through phase encoding of simultaneous
                  sources (Morton and Ober, 1998; Romero et al., 2000;
                  Krohn and Neelamani, 2008; Herrmann et al., 2009),
                  removal of subsets of angular frequencies (Sirgue
                  and Pratt, 2004; Mulder and Plessix, 2004; Lin et
                  al., 2008) or plane waves (Vigh and Starr, 2008). By
                  using CS principles, we remove sub-sampling
                  interferences asocciated with these approaches
                  through a combination of exploiting transform-domain
                  sparsity, properties of certain sub-sampling
                  schemes, and the existence of sparsity promoting
                  solvers.},
  keywords     = {Presentation,EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09eagecs.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/Herrmann09EAGEcsa/Herrmann09EAGEcsa.pdf}
}


@CONFERENCE{herrmann2009SAMPTAcws,
  author       = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title        = {Compressive-wavefield simulations},
  booktitle    = {SAMPTA},
  year         = {2009},
  organization = {SAMPTA},
  abstract     = {Full-waveform inversion{\textquoteright}s high
                  demand on computational resources forms, along with
                  the non-uniqueness problem, the major impediment
                  withstanding its widespread use on industrial-size
                  datasets.  Turning modeling and inversion into a
                  compressive sensing problem{\textendash}-where
                  simulated data are recovered from a relatively small
                  number of independent simultaneous
                  sources{\textendash}-can effectively mitigate this
                  high-cost impediment. The key is in showing that we
                  can design a sub-sampling operator that commutes
                  with the time-harmonic Helmholtz system. As in
                  compressive sensing, this leads to a reduction in
                  simulation cost.  Moreover, this reduction is
                  commensurate with the transform-domain sparsity of
                  the solution, implying that computational costs are
                  no longer determined by the size of the
                  discretization but by transform-domain sparsity of
                  the solution of the CS problem which forms our data.
                  The combination of this sub-sampling strategy with
                  our recent work on implicit solvers for the
                  Helmholtz equation provides a viable alternative to
                  full-waveform inversion schemes based on explicit
                  finite-difference methods.},
  keywords     = {SAMPTA},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2009/Herrmann09SAMPTAcws/Herrmann09SAMPTAcws.pdf}
}


@CONFERENCE{herrmann09EAGEbnrs,
  author       = {Felix J. Herrmann and Gang Tang and Reza Shahidi and Gilles Hennenfent and Tim T.Y. Lin},
  title        = {Beating Nyquist by randomized sampling. Presented at the EAGE (workshop), Amsterdam},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2009},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09eagews.pdf}
}


@CONFERENCE{herrmann2009IAPcsisa,
  author       = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title        = {Compressive seismic imaging with simultaneous
                  acquisition presented at the IAP meeting, Vienna,},
  booktitle    = {IAP},
  year         = {2009},
  abstract     = {The shear size of seismic data volumes forms one of
                  the major impediments for the inversion of seismic
                  data. Turning forward modeling and inversion into a
                  compressive sensing (CS) problem - where simulated
                  data are recovered from a relatively small number of
                  independent sources - can effectively mitigate this
                  high-cost impediment. Our key contribution lies in
                  the design of a sub-sampling operator that commutes
                  with the time-harmonic Helmholtz system. As in
                  compressive sensing, this leads to a reduction of
                  simulation cost. This reduction is commensurate with
                  the transform-domain sparsity of the solution.,
                  implying that computational costs are no longer
                  determined by the size of the discretization but by
                  transform-domain sparsity of the solution of the CS
                  problem that recovers the data. The combination of
                  this sub-sampling strategy with our recent work on
                  preconditioned implicit solvers for the
                  time-harmonic Helmholtz equation provides a viable
                  alternative to full-waveform inversion schemes based
                  on explicit time-domain finite-difference methods.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09AIP1.pdf}
}


@CONFERENCE{herrmann2009SEGsns,
  author       = {Felix J. Herrmann},
  title        = {Sub-Nyquist sampling and sparsity: How to get more information from fewer samples},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3410-3415},
  organization = {SEG},
  abstract     = {Seismic exploration relies on the collection of
                  massive data volumes that are subsequently mined for
                  information during seismic processing.  While this
                  approach has been extremely successful in the past,
                  the current trend of incessantly pushing for higher
                  quality images in increasingly complicated regions
                  of the Earth continues to reveal fundamental
                  shortcomings in our workflows to handle massive
                  high-dimensional data volumes. Two causes can be
                  identified as the main culprits responsible for this
                  barrier. First, there is the so-called
                  {\textquoteleft}{\textquoteleft}curse of
                  dimensionality{\textquoteright}{\textquoteright}
                  exemplified by Nyquist{\textquoteright}s sampling
                  criterion, which puts disproportionate strain on
                  current acquisition and processing systems as the
                  size and desired resolution of our survey areas
                  continues to increase.  Secondly, there is the
                  recent {\textquoteleft}{\textquoteleft}departure
                  from Moore{\textquoteright}s
                  law{\textquoteright}{\textquoteright} that forces us
                  to lower our expectations to compute ourselves out
                  of this curse of dimensionality. In this paper, we
                  offer a way out of this situation by a deliberate
                  \emph{randomized} subsampling combined with
                  structure-exploiting transform-domain sparsity
                  promotion. Our approach is successful because it
                  reduces the size of seismic data volumes without
                  loss of information. Because of this size reduction
                  both impediments are removed and we end up with a
                  new technology where the costs of acquisition and
                  processing are no longer dictated by the \emph{size
                  of the acquisition} but by the transform-domain
                  \emph{sparsity} of the end-product after
                  processing.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255570},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGsns/herrmann09SEGsns.pdf}
}


@CONFERENCE{herrmann2009PIMScssr3,
  author       = {Felix J. Herrmann},
  title        = {Compressed sensing and sparse recovery in
                  exploration seismology. Lecture III presented at the
                  PIMS Summer School on Seismic Imaging. Seattle.},
  booktitle    = {PIMS},
  year         = {2009},
  abstract     = {In this course, I will present how recent results
                  from compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well.  The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS3.pdf}
}


@CONFERENCE{herrmann2009PIMScssr2,
  author       = {Felix J. Herrmann},
  title        = {Compressed sensing and sparse recovery in
                  exploration seismology. Lecture II presented at the
                  PIMS Summer School on Seismic Imaging. Seattle.},
  booktitle    = {PIMS},
  year         = {2009},
  abstract     = {In this course, I will present how recent results
                  from compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well.  The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data.},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS2.pdf}
}


@CONFERENCE{herrmann2009PIMScssr1,
  author       = {Felix J. Herrmann},
  title        = {Compressed sensing and sparse recovery in
                  exploration seismology. Lecture I presented at the
                  PIMS Summer School on Seismic Imaging. Seattle.},
  booktitle    = {PIMS},
  year         = {2009},
  abstract     = {In this course, I will present how recent results
                  from compressed sensing and sparse recovery apply to
                  exploration seismology. During the first lecture, I
                  will present the basic principles of compressive
                  sensing; the importance of random jitter sampling
                  and sparsifying transforms; and large-scale one-norm
                  solvers. I will discuss the application of these
                  techniques to missing trace interpolation. The
                  second lecture will be devoted to coherent signal
                  separation based on curveletdomain matched filtering
                  and Bayesian separation with sparsity
                  promotion. Applications of these techniques to the
                  primary-multiple wavefield-separation problem on
                  real data will be discussed as well.  The third
                  lecture will be devoted towards sparse recovery in
                  seismic modeling and imaging and includes the
                  problem of preconditioning the imaging operators,
                  and the recovery from simultaneous source-acquired
                  data},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Herrmann09PIMS1.pdf}
}


@CONFERENCE{herrmann2009SEGrpl,
  author       = {Felix J. Herrmann},
  title        = {Reflector-preserved lithological upscaling},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3466-3470},
  organization = {SEG},
  abstract     = {By combining Percolation models with lithological
                  smoothing, we arrive at method for upscaling rock
                  elastic constants that preserves reflections.  In
                  this approach, the Percolation model predicts sharp
                  onsets in the elastic moduli of sand-shale mixtures
                  when the shales reach a critical volume fraction. At
                  that point, the shale inclusions form a connected
                  cluster, and the macroscopic rock properties change
                  with the power-law growth of the cluster. This
                  switch-like nonlinearity preserves singularities,
                  and hence reflections, even if no sharp transition
                  exists in the lithology or if they are smoothed out
                  using standard upscaling procedures.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255582},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/herrmann09SEGrpl/herrmann09SEGrpl.pdf}
}


@CONFERENCE{kumar2009SEGins,
  author       = {Vishal Kumar and Felix J. Herrmann},
  title        = {Incoherent noise suppression with curvelet-domain sparsity},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3356-3360},
  organization = {SEG},
  abstract     = {The separation of signal and noise is a key issue in
                  seismic data processing. By noise we refer to the
                  incoherent noise that is present in the data. We use
                  the recently introduced multiscale and
                  multidirectional curvelet transform for suppression
                  of random noise. The curvelet transform decomposes
                  data into directional plane waves that are local in
                  nature. The coherent features of the data occupy the
                  large coefficients in the curvelet domain, whereas
                  the incoherent noise lives in the small
                  coefficients. In other words, signal and noise have
                  minimal overlap in the curvelet domain. This gives
                  us a chance to use curvelets to suppress noise
                  present in data.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255557},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/kumar09SEGins/kumar09SEGins.pdf}
}


@CONFERENCE{lin2009SEGcsf,
  author       = {Tim T.Y. Lin and Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Compressive simultaneous full-waveform simulation},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {2577-2581},
  organization = {SEG},
  abstract     = {The fact that the computational complexity of
                  wavefield simulation is proportional to the size of
                  the discretized model and acquisition geometry, and
                  not to the complexity of the simulated wavefield, is
                  a major impediment within seismic imaging. By
                  turning simulation into a compressive sensing
                  problem{\textendash}where simulated data is
                  recovered from a relatively small number of
                  independent simultaneous sources{\textendash}we
                  remove this impediment by showing that compressively
                  sampling a simulation is equivalent to compressively
                  sampling the sources, followed by solving a reduced
                  system. As in compressive sensing, this allows for a
                  reduction in sampling rate and hence in simulation
                  costs. We demonstrate this principle for the
                  time-harmonic Helmholtz solver. The solution is
                  computed by inverting the reduced system, followed
                  by a recovery of the full wavefield with a sparsity
                  promoting program. Depending on the
                  wavefield{\textquoteright}s sparsity, this approach
                  can lead to a significant cost reduction, in
                  particular when combined with the implicit
                  preconditioned Helmholtz solver, which is known to
                  converge even for decreasing mesh sizes and
                  increasing angular frequencies. These properties
                  make our scheme a viable alternative to explicit
                  time-domain finite-difference.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255381},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/Lin09SEGcsf/Lin09SEGcsf.pdf}
}


@CONFERENCE{lin2009EAGEdsa,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Designing simultaneous acquisitions with compressive sensing},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2009},
  organization = {EAGE},
  abstract     = {The goal of this paper is in designing a functional
                  simultaneous acquisition scheme by applying the
                  principles of compressive sensing. By framing the
                  acquisition in a compressive sensing setting we
                  immediately gain insight into not only how to choose
                  the source signature and shot patterns, but also in
                  how well we can hope to demultiplex the data when
                  given a set amount of reduction in the number of
                  sweeps. The principles of compressive sensing
                  dictates that the quality of the demultiplexed data
                  is closely related to the transform-domain sparsity
                  of the solution. This means that, given an estimate
                  in the complexity of the expectant data wavefield,
                  it is possible to controllably reduce the number of
                  shots that needs to be recorded in the field. We
                  show a proof of concept by introducing an
                  acquisition compatible with compressive sensing
                  based on randomly phase-encoded vibroseis sweeps.},
  keywords     = {Presentation,EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/lin2009eagedsa.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2009/lin09EAGEdsa/lin09EAGEdsa.pdf}
}


@CONFERENCE{lin2009SEGucs,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Unified compressive sensing framework for
                  simultaneous acquisition with primary estimation},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3113-3117},
  organization = {SEG},
  abstract     = {The central promise of simultaneous acquisition is a
                  vastly improved crew efficiency during acquisition
                  at the cost of additional post-processing to obtain
                  conventional source-separated data volumes. Using
                  recent theories from the field of compressive
                  sensing, we present a way to systematically model
                  the effects of simultaneous acquisition.  Our
                  formulation form a new framework in the study of
                  acquisition design and naturally leads to an
                  inversion-based approach for the separation of shot
                  records. Furthermore, we show how other
                  inversion-based methods, such as a recently proposed
                  method from van Groenestijn and Verschuur (2009) for
                  primary estimation, can be processed together with
                  the demultiplexing problem to achieve a better
                  result compared to a separate treatment of these
                  problems.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255502},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/lin09SEGucs/lin09SEGucs.pdf}
}


@CONFERENCE{lin2009DELPHIrwi,
  author       = {Tim T.Y. Lin and Felix J. Herrmann and Yogi A. Erlangga},
  title        = {Randomized wavefield inversion presented at the DELPHI meeting. The Hague.},
  booktitle    = {DELPHI},
  year         = {2009},
  keywords     = {Presentation},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2009/Delphi2009.pdf}
}


@CONFERENCE{saab2009SAMPTAnccs,
  author       = {Rayan Saab and Ozgur Yilmaz},
  title        = {A short note on non-convex compressed sensing},
  year         = {2009},
  booktitle    = {SAMPTA technical program},
  organization = {SAMPTA},
  abstract     = {In this note, we summarize the results we recently
                  proved in\cite{SY08} on the theoretical performance
                  guarantees of the decoders $¿î_p$.  These decoders
                  rely on $\ell^p$ minimization with $p {\i}n (0,1)$
                  to recover estimates of sparse and compressible
                  signals from incomplete and inaccurate
                  measurements. Our guarantees generalize the results
                  of \cite{CRT05} and \cite{Wojtaszczyk08} about
                  decoding by $\ell_p$ minimization with $p=1$, to the
                  setting where $p {\i}n (0,1)$ and are obtained under
                  weaker sufficient conditions. We also present novel
                  extensions of our results in \cite{SY08} that follow
                  from the recent work of DeVore et al. in
                  \cite{DPW08}. Finally, we show some insightful
                  numerical experiments displaying the trade-off in
                  the choice of $p {\i}n (0,1]$ depending on certain
                  properties of the input signal.},
  keywords     = {Presentation},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SAMPTA/2009/saab09SAMPTAnccs/saab09SAMPTAnccs.pdf}
}


@CONFERENCE{shahidi2009SEGcmf,
  author       = {Reza Shahidi and Felix J. Herrmann},
  title        = {Curvelet-domain matched filtering with frequency-domain regularization},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3645-3650},
  organization = {SEG},
  abstract     = {In Herrmann et al. (2008), it is shown that
                  zero-order pseudodifferential operators, which model
                  the migration-demigration operator and the operator
                  mapping the predicted multiples to the true
                  multiples, can be represented by a diagonal
                  weighting in the curvelet domain. In that paper, a
                  smoothness constraint was introduced in the phase
                  space of the operator in order to regularize the
                  solution to make it unique.  In this paper, we use
                  recent results in Demanet and Ying (2008) on the
                  discrete symbol calculus to impose a further
                  smoothness constraint, this time in the frequency
                  domain. It is found that with this additional
                  constraint, faster convergence is realized. Results
                  on a synthetic pseudodifferential operator as well
                  as on an example of primary-multiple separation in
                  seismic data are included, comparing the model with
                  and without the new smoothness constraint, from
                  which it is found that results of improved quality
                  are also obtained.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255624},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/shahidi09SEGcmf/shahidi09segcmf.pdf}
}


@CONFERENCE{tang2009SEGhdb,
  author       = {Gang Tang and Reza Shahidi and Felix J. Herrmann and Jianwei Ma},
  title        = {Higher dimensional blue-noise sampling schemes for curvelet-based seismic data recovery},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {191-195},
  organization = {SEG},
  abstract     = {In combination with compressive sensing, a
                  successful reconstruction scheme called
                  Curvelet-based Recovery by Sparsity-promoting
                  Inversion (CRSI) has been developed, and has proven
                  to be useful for seismic data processing. One of the
                  most important issues for CRSI is the sampling
                  scheme, which can greatly affect the quality of
                  reconstruction.  Unlike usual regular undersampling,
                  stochastic sampling can convert aliases to
                  easy-to-eliminate noise. Some stochastic sampling
                  methods have been developed for CRSI, e.g. jittered
                  sampling, however most have only been applied to 1D
                  sampling along a line. Seismic datasets are usually
                  higher dimensional and very large, thus it is
                  desirable and often necessary to develop higher
                  dimensional sampling methods to deal with these
                  data. For dimensions higher than one, few results
                  have been reported, except uniform random sampling,
                  which does not perform well. In the present paper,
                  we explore 2D sampling methodologies for
                  curvelet-based reconstruction, possessing sampling
                  spectra with blue noise characteristics, such as
                  Poisson Disk sampling, Farthest Point Sampling, and
                  the 2D extension of jittered sampling. These
                  sampling methods are shown to lead to better
                  recovery and results are compared to the other more
                  traditional sampling protocols.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255230},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/tang09SEGhdb/tang09SEGhdb.pdf}
}


@CONFERENCE{vandenberg2009SLIMocf,
  author        = {Ewout {van den Berg} and Mark Schmidt and Michael P. Friedlander and K. Murphy},
  title         = {Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm},
  booktitle     = {SLIM},
  year          = {2009},
  volume        = {12},
  series        = {Twelfth International Conference on Artificial Intelligence and Statistics},
  optmonth      = {04/2009},
  abstract      = {An optimization algorithm for minimizing a smooth
                  function over a convex set is described. Each
                  iteration of the method computes a descent direction
                  by minimizing, over the original constraints, a
                  diagonal-plus low-rank quadratic approximation to
                  the function. The quadratic approximation is
                  constructed using a limited-memory quasi-Newton
                  update. The method is suitable for large-scale
                  problems where evaluation of the function is
                  substan- tially more expensive than projection onto
                  the constraint set. Numerical experiments on one-
                  norm regularized test problems indicate that the
                  proposed method is competitve with state- of-the-art
                  methods such as bound-constrained L-BFGS and
                  orthant-wise descent. We further show that the
                  method generalizes to a wide class of problems, and
                  substantially improves on state-of-the-art methods
                  for problems such as learning the structure of
                  Gaussian graphi- cal models (involving
                  positive-definite matrix constraints) and Markov
                  random fields (in- volving second-order cone
                  constraints).},
  date-added    = {2009-01-29 17:16:34 -0800},
  date-modified = {2009-01-29 17:16:34 -0800},
  keywords      = {SLIM},
  url           = {http://www.cs.ubc.ca/~mpf/papers/SchmidtBergFriedMurph09.pdf}
}


@CONFERENCE{yan2009SEGgpb,
  author       = {Jiupeng Yan and Felix J. Herrmann},
  title        = {Groundroll prediction by interferometry and separation by curvelet-domain matched filtering},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  volume       = {28},
  pages        = {3297-3301},
  organization = {SEG},
  abstract     = {The removal of groundroll in land based seismic data
                  is a critical step for seismic imaging. In this
                  paper, we introduce a work flow to predict the
                  groundroll by interferometry and then separate the
                  groundroll in the curvelet domain. Thus workflow is
                  similar to the workflow of surface-related multiple
                  elimination (SRME). By exploiting the adaptability
                  and sparsity of curvelets, we are able to
                  significantly improve the separation of groundroll
                  in comparison to results yielded by frequency-domain
                  adaptive subtraction methods. We provide synthetic
                  data example to illustrate our claim.},
  keywords     = {Presentation,SEG},
  optdoi       = {10.1190/1.3255544},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2009/yan09SEGgpb/yan09SEGgpb.pdf}
}


@CONFERENCE{yan2009SEGgpb2,
  author       = {Jiupeng Yan and Felix J. Herrmann},
  title        = {Groundroll prediction by interferometry and
                  separation by curvelet-domain filtering. Presented
                  at the 79th SEG Meeting, Houston},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2009},
  abstract     = {The removal of groundroll in land based seismic data
                  is a critical step for seismic imaging. In this
                  paper, we introduce a work flow to predict the
                  groundroll by interferometry and then separate the
                  groundroll in the curvelet domain. Thus workflow is
                  similar to the workflow of surface-related multiple
                  elimination (SRME). By exploiting the adaptability
                  and sparsity of curvelets, we are able to
                  significantly improve the separation of groundroll
                  in comparison to results yielded by frequency-domain
                  adaptive subtraction methods. We provide synthetic
                  data example to illustrate our claim.},
  keywords     = {Presentation},
  url          = {http://slim.eos.ubc.ca/Publications/public/presentations/seg/seg09/yan09seggpi.pdf}
}



%-------------------------------------------------2008-------------------------------------------------

@CONFERENCE{vandenberg2008SINBADsat,
  author        = {Ewout {van den Berg}},
  title         = {Sparco: A testing framework for sparse reconstruction},
  booktitle     = {SINBAD 2008},
  year          = {2008},
  abstract      = {Sparco is a framework for testing and benchmarking
                  algorithms for sparse reconstruction. It includes a
                  large collection of sparse reconstruction problems
                  drawn from the imaging, compressed sensing, and
                  geophysics literature. Sparco is also a framework
                  for implementing new test problems and can be used
                  as a tool for reproducible research. We describe the
                  software environment, and demonstrate its usefulness
                  for testing and comparing solvers for sparse
                  reconstruction.},
  date-modified = {2008-08-22 12:54:25 -0700},
  keywords      = {SLIM, SINBAD, Presentation},
  url           = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ewout_Spar.pdf}
}


@CONFERENCE{erlangga2008SEGaim,
  author       = {Yogi A. Erlangga and Felix J. Herrmann},
  title        = {An iterative multilevel method for computing wavefields in frequency-domain seismic inversion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {1957-1960},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {We describe an iterative multilevel method for
                  solving linear systems representing forward modeling
                  and back propagation of wavefields in
                  frequency-domain seismic inversions. The workhorse
                  of the method is the so-called multilevel Krylov
                  method, applied to a multigrid-preconditioned linear
                  system, and is called multigrid-multilevel Krylov
                  (MKMG) method.  Numerical experiments are presented
                  for 2D Marmousi synthetic model for a range of
                  frequencies. The convergence of the method is fast,
                  and depends only mildly on frequency. The method can
                  be considered as the first viable alternative to LU
                  factorization, which is practically prohibitive for
                  3D seismic inversions.},
  keywords     = {Presentation,SLIM,SEG},
  optdoi       = {10.1190/1.3059279},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim_pres.pdf },
  url          = { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/erlangga08SEGaim/erlangga08SEGaim.pdf }

}


@CONFERENCE{eso2008SEGira,
  author       = {R. A. Eso and S. Napier and Felix J. Herrmann and D. W. Oldenburg},
  title        = {Iterative reconstruction algorithm for non-linear operators},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {579-583},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {Iterative soft thresholding of a models wavelet
                  coefficients can be used to obtain models that are
                  sparse with respect to a known basis function. We
                  generate sparse models for non-linear forward
                  operators by applying the soft thresholding operator
                  to the model obtained through a Gauss-Newton
                  iteration and apply the technique in a synthetic
                  2.5D DC resistivity crosswell tomographic example.},
  keywords     = {SLIM, SEG},
  optdoi       = {10.1190/1.3063719},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/eso08SEGira/eso08SEGira.pdf }
}


@CONFERENCE{friedlander2008SINBADafl,
  author    = {Michael P. Friedlander},
  title     = {Algorithms for Large-Scale Sparse Reconstruction},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {Many signal processing applications seek to approximate
                  a signal as a linear combination of only a few
                  elementary atoms drawn from a large collection. This
                  is known as sparse reconstruction, and the theory of
                  compressed sensing allows us to pose it as a
                  structured convex optimization problem. I will
                  discuss the role of duality in revealing some
                  unexpected and useful properties of these problems,
                  and will show how they can lead to practical,
                  large-scale algorithms.  I will also describe some
                  applications of these algorithms.},
  keywords = {Presentation, SINBAD, SLIM},
  url      = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Michael_Alg.pdf}
}


@CONFERENCE{friedlander2008SIAMasa,
  author       = {Michael P. Friedlander},
  title        = {Active-set Approaches to Basis Pursuit Denoising},
  booktitle    = {SIAM Optimization},
  year         = {2008},
  optmonth     = {05/2008},
  organization = {SIAM Optimization},
  file         = {http//www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  keywords     = {Presentation, SLIM},
  url          = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}


@CONFERENCE{friedlander2008WCOMasm,
  author       = {Michael P. Friedlander and M. A. Saunders},
  title        = {Active-set methods for basis pursuit},
  booktitle    = {WCOM},
  organization = {West Coast Opitmization Meeting (WCOM)},
  year         = {2008},
  optmonth     = {07/2008},
  abstract     = {Many imaging and compressed sensing applications
                  seek sparse solutions to large under-determined
                  least-squares problems. The basis pursuit (BP)
                  approach minimizes the 1-norm of the solution, and
                  the BP denoising (BPDN) approach balances it against
                  the least-squares fit. The duals of these problems
                  are conventional linear and quadratic programs.  We
                  introduce a modified parameterization of the BPDN
                  problem and explore the effectiveness of active-set
                  methods for solving its dual.  Our basic algorithm
                  for the BP dual unifies several existing algorithms
                  and is applicable to large-scale examples.},
  file         = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf:PDF},
  url          = {http://www.cs.ubc.ca/~mpf/public/mpf08siopt.pdf}
}


@CONFERENCE{hennenfent2008SINBADnii2,
  author    = {Gilles Hennenfent},
  title     = {New insights into one-norm solvers from the Pareto curve},
  booktitle = {SINBAD}, 
  year      = {2008},
  abstract  = {Several geophysical ill-posed inverse problems are
                  successfully solved by promoting sparsity using
                  one-norm regularization. The practicality of this
                  approach depends on the effectiveness of the
                  one-norm solver used and on its robustness under
                  limited number of iterations. We propose an approach
                  to understand the behavior and evaluate the
                  performance of one-norm solvers. The technique
                  consists of tracking on a graph the data misfit
                  versus the one norm of successive iterates. By
                  comparing the solution paths to the Pareto curve, we
                  are able to assess the performance of the solvers
                  and the quality of the solutions. Such an assessment
                  is particularly relevant given the renewed interest
                  in one-norm regularization.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_New.pdf}
}


@CONFERENCE{hennenfent2008SEGonri,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {One-norm regularized inversion: learning from the Pareto curve},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  organization = {SEG},
  abstract     = {Geophysical inverse problems typically involve a
                  trade off between data misfit and some prior. Pareto
                  curves trace the optimal trade off between these two
                  competing aims. These curves are commonly used in
                  problems with two-norm priors where they are plotted
                  on a log-log scale and are known as L-curves. For
                  other priors, such as the sparsity-promoting one
                  norm, Pareto curves remain rela- tively
                  unexplored. First, we show how these curves provide
                  an objective criterion to gauge how robust one-norm
                  solvers are when they are limited by a maximum
                  number of matrix-vector products that they can
                  perform. Second, we use Pareto curves and their
                  properties to define and compute one-norm
                  compressibilities.  We argue this notion is key to
                  understand one-norm regularized inversion.  Third,
                  we illustrate the correlation between the one-norm
                  compressibility and the perfor- mance of Fourier and
                  curvelet reconstructions with sparsity promoting
                  inversion.},
  keywords     = {SEG},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/hennenfent08SEGonri/hennenfent08SEGonri.pdf}
}


@CONFERENCE{hennenfent2008SINBADsdw2,
  author    = {Gilles Hennenfent},
  title     = {Simply denoise: wavefield reconstruction via jittered undersampling},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {We present a new discrete undersampling scheme designed
                  to favor wavefield reconstruction by
                  sparsity-promoting inversion with transform elements
                  that are localized in the Fourier domain. Our work
                  is motivated by empirical observations in the
                  seismic community, corroborated by recent results
                  from compressive sampling, which indicate favorable
                  (wavefield) reconstructions from random as opposed
                  to regular undersampling.  As predicted by theory,
                  random undersampling renders coherent aliases into
                  harmless incoherent random noise, effectively
                  turning the interpolation problem into a much
                  simpler denoising problem. A practical requirement
                  of wavefield reconstruction with localized
                  sparsifying transforms is the control on the maximum
                  gap size. Unfortunately, random undersampling does
                  not provide such a control and the main purpose of
                  this paper is to introduce a sampling scheme, coined
                  jittered undersampling, that shares the benefits of
                  random sampling, while offering control on the
                  maximum gap size. Our contribution of jittered
                  sub-Nyquist sampling proofs to be key in the
                  formulation of a versatile wavefield
                  sparsity-promoting recovery scheme that follows the
                  principles of compressive sampling. After studying
                  the behavior of the jittered-undersampling scheme in
                  the Fourier domain, its performance is studied for
                  curvelet recovery by sparsity-promoting inversion
                  (CRSI). Our findings on synthetic and real seismic
                  data indicate an improvement of several decibels
                  over recovery from regularly-undersampled data for
                  the same amount of data collected.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Gilles_jit.pdf}
}


@CONFERENCE{herrmann2008SEGcdm,
  title        = {Curvelet-domain matched filtering},
  author       = {Felix J. Herrmann and Peyman P. Moghaddam and Deli Wang},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  optmonth     = {08/2008},
  year         = {2008},
  abstract     = {Matching seismic wavefields and images lies at the
                  heart of many pre-/post-processing steps part of
                  seismic imaging{\textendash}- whether one is
                  matching predicted wavefield components, such as
                  multiples, to the actual to-be-separated wavefield
                  components present in the data or whether one is
                  aiming to restore migration amplitudes by scaling,
                  using an image-to-remigrated-image matching
                  procedure to calculate the scaling coefficients. The
                  success of these wavefield matching procedures
                  depends on our ability to (i) control possible
                  overfitting, which may lead to accidental removal of
                  energy or to inaccurate image-amplitude corrections,
                  (ii) handle data or images with nonunique dips, and
                  (iii) apply subsequent wavefield separations or
                  migraton amplitude corrections stably. In this
                  paper, we show that the curvelet transform allows us
                  to address all these issues by imposing smoothness
                  in phase space, by using their capability to handle
                  conflicting dips, and by leveraging their ability to
                  represent seismic data and images sparsely. This
                  latter property renders curvelet-domain sparsity
                  promotion an effective prior.},
  keywords     = {SLIM,Presentation, SEG},
  number       = {TR-2008-6},
  organization = {SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm_pres.pdf},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGcdm/herrmann08SEGcdm.pdf }
}


@CONFERENCE{herrmann2008SINBADacd2,
  author       = {Felix J. Herrmann},
  title        = {Adaptive curvelet-domain primary-multiple separation},
  booktitle    = {SINBAD},
  organization = {SINBAD},
  year         = {2008},
  note         = {SINBAD 2008},
  abstract     = {In many exploration areas, successful separation of
                  primaries and multiples greatly determines the
                  quality of seismic imaging. Despite major advances
                  made by Surface-Related Multiple Elimination (SRME),
                  amplitude errors in the predicted multiples remain a
                  problem. When these errors vary for each type of
                  multiple differently (as a function of offset, time
                  and dip), these amplitude errors pose a serious
                  challenge for conventional least-squares matching
                  and for the recently introduced separation by
                  curvelet-domain thresholding. We propose a
                  data-adaptive method that corrects amplitude errors,
                  which vary smoothly as a function of location, scale
                  (frequency band) and angle. In that case, the
                  amplitudes can be corrected by an element-wise
                  curvelet-domain scaling of the predicted
                  multiples. We show that this scaling leads to a
                  successful estimation of the primaries, despite
                  amplitude, sign, timing and phase errors in the
                  predicted multiples. Our results on synthetic and
                  real data show distinct improvements over
                  conventional least-squares matching, in terms of
                  better suppression of multiple energy and
                  high-frequency clutter and better recovery of the
                  estimated primaries.},
  keywords     = {Presentation, SINBAD, SLIM},
  url          = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ada.pdf}
}


@CONFERENCE{herrmann2008SEGcdm3,
  author       = {Felix J. Herrmann},
  title        = {Curvelet-domain matched filtering},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {3643-3649},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {Matching seismic wavefields lies at the heart of
                  seismic processing whether one is adaptively
                  subtracting multiples predictions or groundroll.  In
                  both cases, the predictions are matched to the
                  actual to-be-separated wavefield components in the
                  observed data. The success of these wavefield
                  matching procedures depends on our ability to (i)
                  control possible overfitting, which may lead to
                  accidental removal of primary energy, (ii) handle
                  data with nonunique dips, and (iii) apply wavefield
                  separation after matching stably. In this paper, we
                  show that the curvelet transform allows us to
                  address these issues by imposing smoothness in phase
                  space, by using their capability to handle
                  conflicting dips, and by leveraging their ability to
                  represent seismic data sparsely.},
  keywords     = {SEG, SLIM},
  optdoi       = {10.1190/1.3064089},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/Herrmann08SEGcdm3/Herrmann08SEGcdm3.pdf }
}


@CONFERENCE{herrmann2008IONcsa,
  author       = {Felix J. Herrmann},
  title        = {Compressive sampling: a new paradigm for seismic data acquistion and processing?},
  booktitle    = {ION},
  year         = {2008},
  abstract     = {Seismic data processing and imaging are firmly
                  rooted in the well-established paradigm of regular
                  Nyquist sampling. Faced with a typical uncooperative
                  environment, practitioners of seismic data
                  acquisition make all efforts to comply to this
                  theory by creating regularly-sampled seismic-data
                  volumes that are suitable for Fourier-based
                  processing flows. The current advent of new
                  alternative transform domains{\textendash}- such as
                  the sparsifying curvelet domain, where seismic data
                  is decomposed into localized, multiscale and
                  multidirectional plane waves{\textendash}- opens the
                  possibility to change this paradigm by no longer
                  combating sampling irregularity but by embracing
                  it. During this talk, we show that as long as
                  seismic data volumes permit a compressible
                  representation{\textendash}-i.e., data can be
                  represented as a superposition of relatively few
                  number of elementary waveforms{\textendash}- Nyquist
                  sampling is unnecessary pessimistic. So far, nothing
                  new, we all know from the work on Fourier- or other
                  transform-based seismic-data regularization
                  methodologies that wavefields can be recovered
                  accurately from sub-Nyquist samplings through some
                  sort of optimization procedure. What is new,
                  however, are recent insights from the field of
                  "compressive sampling", which dictate the conditions
                  that guarantee or, at least, in practice provide
                  conditions that favor sparsity-promoting recovery
                  from sub-Nyquist sampling. Random sub-sampling, or
                  to be more precise, jitter sub-sampling creates
                  favorable conditions for curvelet-based recovery. We
                  explain this phenomenon by arguing that this type of
                  sampling leads to noisy data, hence our slogan
                  "Simply denoise: wavefield reconstruction via
                  jittered undersampling", where we bank on separating
                  incoherent sub-sampling noise with curvelet-domain
                  sparsity promotion. During our presentation, we
                  introduce you to what curvelets are, why random
                  jitter sampling is important and why this opens a
                  pathway towards a new paradigm of curvelet-domain
                  seismic data processing. Our claims will be
                  supported by examples on synthetic and field
                  data. This is joint work with Gilles Hennenfent,
                  PhD. student at SLIM.},
  keywords     = {ION, Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08ion_pres.pdf}
}


@CONFERENCE{herrmann2008SINBADfwr,
  author    = {Felix J. Herrmann},
  title     = {(De)-Focused wavefield reconstructions},
  booktitle = {SINBAD 2008},
  year      = {2008},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Dfo.pdf}
}


@CONFERENCE{herrmann2008SEGgbu,
  author       = {Felix J. Herrmann},
  title        = {Seismic noise: the good, the bad, \& the ugly},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  keywords     = {Presentation, SEG, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGgbu/herrmann08SEGgbu.pdf }
}


@CONFERENCE{herrmann2008SINBADpsm,
  author    = {Felix J. Herrmann},
  title     = {Phase-space matched filtering and migration preconditioning},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {During this talk, I will report on new phase-space
                  regularization functionals defined in terms of
                  splines. This spline representation reduces the
                  dimensionality of estimating our phase-space matched
                  filter. We will discuss how this filter can be used
                  in migration preconditioning. This is joint work
                  with Christiaan Stolk.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Pha.pdf}
}


@CONFERENCE{herrmann2008SINBADs2c,
  author    = {Felix J. Herrmann},
  title     = {SINBAD 2008 Consortium meeting},
  booktitle = {SINBAD 2008},
  year      = {2008},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Ope.pdf}
}


@CONFERENCE{herrmann2008SIAMcsm,
  author    = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin},
  title     = {Compressive sampling meets seismic imaging},
  booktitle = {SIAM},
  year      = {2008},
  abstract  = {Compressive sensing has led to fundamental new insights
                  in the recovery of compressible signals from
                  sub-Nyquist samplings. It is shown how jittered
                  subsampling can be used to create favorable recovery
                  conditions.  Applications include mitigation of
                  incomplete acquisitions and wavefield
                  computations. While the former is a direct
                  adaptation of compressive sampling, the latter
                  application represents a new way of compressing
                  wavefield extrapolation operators. Operators are not
                  diagonalized but are compressively sampled reducing
                  the computational costs.},
  keywords  = {Presentation, SIAM, SLIM},
  url       = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2008/herrmann08siam08.pdf}
}


@CONFERENCE{herrmann2008SINBADitc,
  author    = {Felix J. Herrmann and Yogi A. Erlangga and Tim T.Y. Lin and Cody R. Brown},
  title     = {Introduction to compressive (wavefield) computation},
  booktitle = {SINBAD 2008},
  year      = {2008},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Felix_Int.pdf}
}


@CONFERENCE{herrmann2008SEGswi,
  author       = {Felix J. Herrmann and Deli Wang},
  title        = {Seismic wavefield inversion with curvelet-domain sparsity promotion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2497-2501},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {Inverting seismic wavefields lies at the heart of
                  seismic data processing and imaging{\textendash}-
                  whether one is applying
                  {\textquoteleft}{\textquoteleft}a poor
                  man{\textquoteright}s
                  inverse{\textquoteright}{\textquoteright} by
                  correlating wavefields during imaging or whether one
                  inverts wavefields as part of a focal transform
                  interferrometric deconvolution or as part of
                  computing the {\textquoteright}data
                  verse{\textquoteright}.  The success of these
                  wavefield inversions depends on the stability of the
                  inverse with respect to data imperfections such as
                  finite aperture, bandwidth limitation, and missing
                  data. In this paper, we show how curvelet domain
                  sparsity promotion can be used as a suitable prior
                  to invert seismic wavefields. Examples include,
                  seismic data regularization with the focused
                  curvelet-based recovery by sparsity-promoting
                  inversion (fCRSI), which involves the inversion of
                  the primary-wavefield operator, the prediction of
                  multiples by inverting the adjoint of the primary
                  operator, and finally the inversion of the data
                  itself {\textendash}- the so-called
                  {\textquoteright}data inverse{\textquoteright}.  In
                  all cases, curvelet-domain sparsity leads to a
                  stable inversion.},
  keywords     = {Presentation,SLIM},
  optdoi       = {10.1190/1.3063862},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/herrmann08SEGswi/herrmann08SEGswi.pdf }
}


@CONFERENCE{johnson2008SINBADsdi,
  author    = {James Johnson and Gilles Hennenfent},
  title     = {Seismic Data Interpolation with Symmetry},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {Due to the physics of reciprocity seismic data sets are
                  symmetric in the source and receiver
                  coordinates. Often seismic data sets are incomplete
                  and the missing data must be
                  interpolated. Typically, missing traces do not occur
                  symmetrically. The purpose of this project is to
                  extend the current formulation for solving the
                  seismic interpolation problems in such a way that
                  they enforce reciprocity. The method decomposes the
                  seismic data volume into symmetric and antisymmetric
                  parts. This decomposition leads to an augmented
                  system of equations for the L1-solver that promotes
                  sparsity in the curvelet domain.  Interpolation is
                  carried out on the entire system during which the
                  asymmetric component of the volume is forced to
                  zero, while the symmetric part of the data volume is
                  matched to the measured data.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_James_Sei.pdf}
}


@CONFERENCE{kumar2008SINBADcd,
  author    = {Vishal Kumar},
  title     = {Curvelet Denoising},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {The separation of signal and noise is an important
                  issue in seismic data processing. By noise we refer
                  to the incoherent noise which is present in the
                  data. In our case, we showed curvelets concentrate
                  seismic signal energy in few significant
                  coefficients unlike noise energy that is spread all
                  over the coefficients. The sparsity of seismic data
                  in the curvelet domain makes curvelets an ideal
                  choice for separating the noise from the seismic
                  data. In our approach the denoising problem is
                  framed as curvelet-regularized inversion problem.
                  After initial processing, we applied the algorithm
                  to the poststack data and compared our results with
                  conventional wavelet denoising.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Den.pdf}
}


@CONFERENCE{kumar2008SINBADcrd,
  author       = {Vishal Kumar},
  title        = {Curvelet-Regularized Deconvolution},
  booktitle    = {SINBAD 2008},
  year         = {2008},
  abstract     = {The removal of source signature from seismic data is
                  an important step in seismic data processing. The
                  Curvelet transform provides sparse representations
                  for images that comprise smooth objects separated by
                  piece-wise smooth discontinuities (e.g. seismic
                  reflectivity).  In this approach the sparseness of
                  reflectivity in Curvelet domain is used as a prior
                  to stabilize the inversion process. Our
                  Curvelet-regularized deconvolution algorithm uses
                  recently developed SPGL1 solver which does adaptive
                  sampling of the trade-off curve. We applied the
                  algorithm on a synthetic example and compared our
                  results with that of Spiky deconvolution approach.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SINBAD/2008/kumar08SINBADcrd/kumar08SINBADcrd_pres.pdf },
  url          = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Vishal_Dec.pdf}
}


@CONFERENCE{kumar2008CSEGcrs,
  author       = {Vishal Kumar and Felix J. Herrmann},
  title        = {Curvelet-regularized seismic deconvolution},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2008},
  optmonth     = {05/2008},
  organization = {CSEG},
  abstract     = {There is an inherent continuity along reflectors of
                  a seismic image.  We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.},
  keywords     = {Presentation,SLIM},
  presentation = {http://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs_pres.pdf},
  url          = {http://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2008/kumar2008CSEGcrs/kumar2008CSEGcrs.pdf}
}


@CONFERENCE{kumar2008SEGdwc,
  author       = {Vishal Kumar and Felix J. Herrmann},
  title        = {Deconvolution with curvelet-domain sparsity},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {1996-2000},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {There is an inherent continuity along reflectors of
                  a seismic image.  We use the recently introduced
                  multiscale and multidirectional curvelet transform
                  to exploit this continuity along reflectors for
                  cases in which the assumption of spiky reflectivity
                  may not hold. We show that such type of seismic
                  reflectivity can be represented in the
                  curvelet-domain by a vector whose entries decay
                  rapidly. This curvelet-domain compression of
                  reflectivity opens new perspectives towards solving
                  classical problems in seismic processing including
                  the deconvolution problem. In this paper, we present
                  a formulation that seeks curvelet-domain sparsity
                  for non-spiky reflectivity and we compare our
                  results with those of spiky deconvolution.},
  keywords     = {SLIM,Presentation, SEG},
  optdoi       = {10.1190/1.3059287},
  presentation = { https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/kumar08SEGdwc/kumar08SEGdwc.pdf }
}


@CONFERENCE{lebed2008SINBADaoc,
  author    = {Evgeniy Lebed},
  title     = {Curvelet / Surfacelet comparison},
  booktitle = {SINBAD},
  year      = {2008},
  abstract  = {Curvelets and Surfacelets are two transforms that aim
                  to achieve a multiscale and a multidirectional
                  decomposition of arbitrary N-dimensional ($N>=2$)
                  signals. While both transforms are Fourier-based,
                  their construction is intrinsically different. In
                  this talk we will give and overview of the
                  construction of the two transforms, and explore
                  their properties such as frequency domain / spatial
                  domain coherence, sparsity, redundancy and
                  computational complexity.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_Curv.pdf}
}


@CONFERENCE{lebed2008SEGhggt,
  author       = {Evgeniy Lebed and Felix J. Herrmann},
  title        = {A hitchhiker{\textquoteright}s guide to the galaxy of transform-domain sparsification},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  organization = {SEG},
  abstract     = {The ability to efficiently and sparsely represent
                  seismic data is becoming an increasingly important
                  problem in geophysics. Over the last decade many
                  transforms such as wavelets, curvelets, contourlets,
                  surfacelets, shearlets, and many other types of
                  {\textquoteright}x-lets{\textquoteright} have been
                  developed to try to resolve this issue. In this
                  abstract we compare the properties of four of these
                  commonly used transforms, namely the shift-invariant
                  wavelets, complex wavelets, curvelets and
                  surfacelets.  We also briefly explore the
                  performance of these transforms for the problem of
                  recovering seismic wavefields from incomplete
                  measurements.},
  keywords     = {SEG},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lebed08SEGhggt/lebed08SEGhggt.pdf}
}


@CONFERENCE{lebed2008SINBADaoc1,
  author    = {Evgeniy Lebed},
  title     = {Applications of Curvelets/Surfacelets to seismic data processing},
  booktitle = {SINBAD},
  year      = {2008},
  abstract  = {In this talk we explore several applications of the
                  curvelet and surfacelet transforms to seismic data
                  processing. The first application is stable signal
                  recovery in the physical domain - seismic data
                  acquisition is often limited by physical and
                  economic constraints, and the goal is to interpolate
                  the data from a given subset of seismic traces.  The
                  second application is signal recovery in a transform
                  domain - we assume that our data comes in a form of
                  a random subset of temporal frequencies and the goal
                  is to recover the missing frequencies from this
                  data. Since seismic signals are generally not
                  bandwidth limited, this in fact becomes an
                  anti-aliasing problem. In both these problems the
                  recovery is resolved via a robust l_1 solver that
                  exploits the sparsity of the signals in
                  curvelet/surfacelet domains. In the last application
                  we explore the problem of primary-multiple
                  separation by simple thresholding.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Evgeniy_App.pdf}
}


@CONFERENCE{lin2008SINBADcwe,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Compressed wavefield extrapolation},
  booktitle    = {SINBAD},
  year         = {2008},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/sites/data/Papers/lin08cwe.pdf}
}


@CONFERENCE{lin2008SEGiso,
  author       = {Tim T.Y. Lin and Evgeniy Lebed and Yogi A. Erlangga and Felix J. Herrmann},
  title        = {Interpolating solutions of the Helmholtz equation with compressed sensing},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2122-2126},
  optmonth     = {11/2008},
  organization = {SEG},
  abstract     = {We present an algorithm which allows us to model
                  wavefields with frequency-domain methods using a
                  much smaller number of frequencies than that
                  typically required by the classical sampling theory
                  in order to obtain an alias-free result. The
                  foundation of the algorithm is the recent results on
                  the compressed sensing, which state that data can be
                  successfully recovered from an incomplete
                  measurement if the data is sufficiently
                  sparse. Results from numerical experiment show that
                  only 30\% of the total frequency spectrum is need to
                  capture the full wavefield information when working
                  in the hard 2D synthetic Marmousi model.},
  keywords     = {Presentation,SLIM, SEG},
  optdoi       = {10.1190/1.3059307},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/lin08SEGiso/lin08SEGiso.pdf } 
}


@CONFERENCE{maysami2008SEGlcf,
  author       = {Mohammad Maysami and Felix J. Herrmann},
  title        = {Lithological constraints from seismic waveforms: application to opal-A to opal-CT transition},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2011-2015},
  optmonth     = {November},
  organization = {SEG},
  abstract     = {In this paper, we present a new method for seismic
                  waveform characterization whose aim is threefold,
                  namely (i) extraction of detailed information on the
                  sharpness of transitions in the subsurface from
                  seismic waveforms, (ii) reflector modeling, based on
                  binary-mixture and percolation theory, and (iii)
                  establishment of well-seismic ties, through
                  parameterizations of our waveform and critical
                  reflector model. We test this methodology on the
                  opal-A (Amorphous) to opal-CT
                  (Cristobalite/Tridymite) transition imaged in a
                  migrated section of North Sea field data West of the
                  Shetlands.}, 
  keywords     = {SEG, SLIM},
  optdoi       = {10.1190/1.3059400},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/maysami08SEGlcf/maysami08SEGlcf.pdf }
}


@CONFERENCE{modzelewski2008SINBADdas,
  author    = {Henryk Modzelewski},
  title     = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {The SLIM group is actively developing software for
                  seismic imaging.  This talk will give a general
                  overview of the software development during SINBAD
                  project with focus on the final release in February
                  2008. The covered topics will include: 1) adopting
                  Python for object-oriented programming, 2) including
                  parallelism into the algorithms used in seismic
                  imaging/modeling, 3) in-house algorithms for seismic
                  imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session "SINBAD Software
                  releases".},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Henryk_Des.pdf}
}


@CONFERENCE{moghaddam2008SINBADrtm,
  author    = {Peyman P. Moghaddam},
  title     = {Reverse-time Migration Amplitude Recovery with Curvelets},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {We recover the amplitude of a seismic image by
                  approximating the normal (demigration-migration)
                  operator. In this approximation, we make use of the
                  property that curvelets remain invariant under the
                  action of the normal operator. We propose a seismic
                  amplitude recovery method that employs an eigenvalue
                  like decomposition for the normal operator using
                  curvelets as eigenvectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Peyman_Mig.pdf}
}


@CONFERENCE{moghaddam2008SEGcbm,
  author       = {Peyman P. Moghaddam and Cody R. Brown and Felix J. Herrmann},
  title        = {Curvelet-based migration preconditioning},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2211-2215},
  optmonth     = {November},
  organization = {SEG},
  abstract     = {In this paper, we introduce a preconditioner for
                  seismic imaging{\textendash}-i.e., the inversion of
                  the linearized Born scattering operator. This
                  preconditioner approximately corrects for the
                  {\textquoteleft}{\textquoteleft}square
                  root{\textquoteright}{\textquoteright} of the
                  normal{\textendash}-i.e., the demigration-migration
                  operator. This approach consists of three parts,
                  namely (i) a left preconditoner, defined by a
                  fractional time integration designed to make the
                  migration operator zero order, and two right
                  preconditioners that apply (ii) a scaling in the
                  physical domain accounting for a spherical
                  spreading, and (iii) a curvelet-domain scaling that
                  corrects for spatial and reflector-dip dependent
                  amplitude errors. We show that a combination of
                  these preconditioners lead to a significant
                  improvement of the convergence for iterative
                  least-squares solutions to the seismic imaging
                  problem based on reverse-time migration operators.},
  keywords     = {Presentation,SLIM,SEG},
  optdoi       = {10.1190/1.3059325},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/moghaddam08SEGcbm/moghaddam08SEGcbm.pdf }
}


@CONFERENCE{ross2008SINBADsit,
  author    = {Sean Ross-Ross},
  title     = {Seismic inversion through operator overloading},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {Geophysical processing is dominated by many different
                  out of core memory software environments
                  (OOCE). Such environments include Madagascar and SU
                  and are designed to handle data that can not be
                  operated on in memory. Each base operation is
                  created as a main program that reads data from disk
                  and writes the result to disk. The main programs can
                  also be chained together on stdin/out pipes using a
                  shell only writing data to disk at the end. To be
                  efficient, the algorithm using an OOCE must chain
                  together the longest pipe to avoid disk I/O, as a
                  result it is very difficult to use iterative
                  techniques. The algorithms are written in shell
                  scripts can be difficult to read and understand.
                  SLIMpy is a software library that contains
                  definitions of coordinate free vectors and linear
                  operators. It allows the user to design and run
                  algorithms with any out of core package, in a Matlab
                  style interface while maintaining optimal efficiency
                  and speed. SLIMpy looks at each main program of each
                  OOCE as a Matrix vector operation or vector
                  reduction/transformation operation. It uses operator
                  overloading to generate an abstract syntax tree
                  (AST) which can be optimized in many ways before
                  executing its commands. The AST also provides a
                  pathway for embarrassingly parallel applications by
                  splitting the tree over different nodes and
                  processors. SLIMpy provides an interface to these
                  OOCE that allows for optimal construction of
                  commands and allows for iterative techniques. It
                  smoothes the transition from other languages such as
                  Matlab and allows the algorithm designer to write
                  readable and reusable code. SLIMpy also adds to OOCE
                  by allowing for easy parallelization.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Sean_Sli.pdf}
}


@CONFERENCE{saab2008ICASSPssa,
  author       = {Rayan Saab and  Rick Chartrand and Ozgur Yilmaz},
  title        = {Stable sparse approximations via nonconvex optimization},
  booktitle    = {ICASSP},
  year         = {2008},
  organization = {ICASSP},
  keywords     = {ICASSP}, 
  abstract     = {We present theoretical results pertaining to the
                  ability of lp minimization to recover sparse and
                  compressible signals from incomplete and noisy
                  measurements. In particular, we extend the results
                  of Cande`s, Romberg and Tao [1] to the p < 1
                  case. Our results indicate that depending on the
                  restricted isometry constants (see, e.g.,[2] and
                  [3]) and the noise level, lp minimization with
                  certain values of p < 1 provides better theoretical
                  guarantees in terms of stability and robustness than
                  l1 minimization does. This is especially true when
                  the restricted isometry constants are relatively
                  large.},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2008/saab08ICASSPssa/saab08ICASSPssa.pdf }
}


@CONFERENCE{saab2008SINBADcps,
  author        = {Rayan Saab},
  title         = {Curvelet-Based Primary-Multiple Separation from a Bayesian Perspective},
  booktitle     = {SINBAD 2008},
  year          = {2008},
  abstract      = {We present a novel primary-multiple separation
                  scheme which makes use of the sparsity of both
                  primaries and multiples in a transform domain, such
                  as the curvelet transform, to provide estimates of
                  each.  The proposed algorithm utilizes seismic data
                  as well as the output of a preliminary step that
                  provides (possibly) erroneous predictions of the
                  multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm.},
  date-modified = {2008-08-22 12:45:53 -0700},
  keywords      = {SLIM, SINBAD, Presentation},
  url           = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Rayan_Curv.pdf}
}


@CONFERENCE{vandenberg2008IAMesr,
  author        = {Ewout {van den Berg}},
  title         = {Exact sparse reconstruction and neighbourly polytopes},
  booktitle     = {IAM},
  year          = {2008},
  bdsk-url-1    = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf},
  date-added    = {2008-08-26 15:44:44 -0700},
  date-modified = {2008-08-26 15:45:58 -0700},
  keywords      = {SLIM, IAM, Presentation},
  presentation  = {http://slim.eos.ubc.ca/Publications/Private/Presentations/2008/vandenberg08iam_pres.pdf}
}


@CONFERENCE{wang2008SINBADrri,
  author    = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title     = {Recent results in curvelet-based primary-multiple separation},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {We present a nonlinear curvelet-based
                  sparsity-promoting formulation for the
                  primary-multiple separation problem. We show that
                  these coherent signal components can be separated
                  robustly by explicitly exploiting the locality of
                  curvelets in phase space (space-spatial frequency
                  plane) and their ability to compress data volumes
                  that contain wavefronts.  This work is an extension
                  of earlier results and the presented algorithms are
                  shown to be stable under noise and moderately
                  erroneous multiple predictions.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Deli_Rec.pdf}
}


@CONFERENCE{yan2008SINBADwru,
  author    = {Jiupeng Yan},
  title     = {Wavefield Reconstruction Using Simultaneous Denoising Interpolation vs. Denoising after Interpolation},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {This report represents and compares two methods of
                  wavefield reconstruction from noisy seismic data
                  with missing traces. The two methods are (i) First
                  interpolate incomplete noisy data to get complete
                  noisy data and then denoise, and (ii) Interpolate
                  and denoise the incomplete noisy data
                  simultaneously. A sample test of synthetic data will
                  be presented. The results of tests show that
                  denoising after interpolation is better than
                  simultaneous denoising and interpolation if the
                  parameter of the denoising problem is chosen
                  appropriately.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Jiapeng_Wav.pdf}
}


@CONFERENCE{yarham2008SINBADbss,
  author        = {Carson Yarham},
  title         = {Bayesian signal separation applied to ground-roll removal},
  booktitle     = {SINBAD 2008},
  year          = {2008},
  abstract      = {Accurate and adaptive noise removal is a critical
                  part in seismic processing. Recent developments in
                  signal separation methods have allowed a more
                  flexible and accurate framework in which to perform
                  ground roll and reflector separation. The use of a
                  new Bayesian separation scheme developed at the SLIM
                  group that contains control parameters to adjust for
                  the uniqueness of specific problems is used. The
                  sensitivity and variation of the control parameters
                  is examined and this method is applied to synthetic
                  and real data and the results are compared to
                  previous methods.},
  date-modified = {2008-08-22 12:42:58 -0700},
  keywords      = {Presentation,SLIM},
  url           = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Carson_Gro.pdf}
}


@CONFERENCE{yarham2008SEGbgr,
  author       = {Carson Yarham and Felix J. Herrmann},
  title        = {Bayesian ground-roll seperation by curvelet-domain sparsity promotion},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2008},
  volume       = {27},
  pages        = {2576-2580},
  optmonth     = {November},
  organization = {SEG},
  abstract     = {The removal of coherent noise generated by surface
                  waves in land based seismic is a prerequisite to
                  imaging the subsurface. These surface waves, termed
                  as ground roll, overlay important reflector
                  information in both the t-x and f-k
                  domains. Standard techniques of ground-roll removal
                  commonly alter reflector information. We propose the
                  use of the curvelet domain as a sparsifying
                  transform in which to preform signal-separation
                  techniques that preserves reflector information
                  while increasing ground-roll removal. We look at how
                  this method preforms on synthetic data for which we
                  can build quantitative results and a real field data
                  set.},
  keywords     = {Presentation,SLIM},
  optdoi       = {10.1190/1.3063878},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr_pres.pdf  },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2008/yarham08SEGbgr/yarham08SEGbgr.pdf  }
}


@CONFERENCE{yilmaz2008SINBADsse,
  author    = {Ozgur Yilmaz},
  title     = {Stable sparse expansions via non-convex optimization},
  booktitle = {SINBAD 2008},
  year      = {2008},
  abstract  = {We present theoretical results pertaining to the
                  ability of p-(quasi)norm minimization to recover
                  sparse and compressible signals from incomplete and
                  noisy measurements. In particular, we extend the
                  results of Candes, Romberg and Tao for 1-norm to the
                  p<1 case. Our results indicate that depending on the
                  restricted isometry constants and the noise level,
                  p-norm minimization with certain values of p<1
                  provides better theoretical guarantees in terms of
                  stability and robustness compared to 1-norm
                  minimization. This is especially true when the
                  restricted isometry constants are relatively large,
                  or equivalently, when the data is significantly
                  undersampled.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/SINBAD2008/Program_files/SINBAD2008_Ozgur_Sta.pdf}
}



%-------------------------------------------------2007-------------------------------------------------

@CONFERENCE{challa2007EAGEsrf,
  author       = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title        = {Signal reconstruction from incomplete and misplaced measurements},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Constrained by practical and economical
                  considerations, one often uses seismic data with
                  missing traces. The use of such data results in
                  image artifacts and poor spatial
                  resolution. Sometimes due to practical limitations,
                  measurements may be available on a perturbed grid,
                  instead of on the designated grid. Due to
                  algorithmic requirements, when such measurements are
                  viewed as those on the designated grid, the recovery
                  procedures may result in additional artifacts. This
                  paper interpolates incomplete data onto regular grid
                  via the Fourier domain, using a recently developed
                  greedy algorithm. The basic objective is to study
                  experimentally as to what could be the size of the
                  perturbation in measurement coordinates that allows
                  for the measurements on the perturbed grid to be
                  considered as on the designated grid for faithful
                  recovery. Our experimental work shows that for
                  compressible signals, a uniformly distributed
                  perturbation can be offset with slightly more number
                  of measurements.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/challa07EAGEsrf/challa07EAGEsrf.pdf}
}


@CONFERENCE{fomel2007ICASSPrepro,
  author       = {Sergey Fomel and Gilles Hennenfent},
  title        = {Reproducible computational experiments using scons},
  booktitle    = {ICASSP},
  organization = {ICASSP},
  year         = {2007},
  keywords     = {ICASSP},
  abstract     = {SCons (from Software Construction) is a well-known
                  open- source program designed primarily for building
                  software. In this paper, we describe our method of
                  extending SCons for managing data processing flows
                  and reproducible computational experiments. We
                  demonstrate our usage of SCons with a simple
                  example.},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/ICASSP/2007/fomel07ICASSPrepro/fomel07ICASSPrepro.pdf  }
}


@CONFERENCE{hennenfent2007SINBADjdn,
  author    = {Gilles Hennenfent},
  title     = {Just denoise. Nonlinear recovery from randomly sampled data},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {In this talk, we turn the interpolation problem of
                  coarsely-sampled data into a denoising problem. From
                  this point of view, we illustrate the benefit of
                  random sampling at sub-Nyquist rate over regular
                  sampling at the same rate. We show that, using
                  nonlinear sparsity-promoting optimization, coarse
                  random sampling may actually lead to significantly
                  better wavefield reconstruction than equivalent
                  regularly sampled data.},
  keywords  = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{hennenfent2007EAGEcrw,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Curvelet reconstruction with sparsity-promoting inversion: successes and challenges},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {In this overview of the recent Curvelet
                  Reconstruction with Sparsity-promoting Inver- sion
                  (CRSI) method, we present our latest 2-D and 3-D
                  interpolation results on both synthetic and real
                  datasets. We compare these results to interpolated
                  data using other ex- isting methods. Finally, we
                  discuss the challenges related to sparsity-promoting
                  solvers for the large-scale problems the industry
                  faces.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEcrw/hennenfent07EAGEcrw_pres.pdf }
}


@CONFERENCE{hennenfent2007EAGEisf,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Irregular sampling: from aliasing to noise},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Seismic data is often irregularly and/or sparsely
                  sampled along spatial coordinates. We show that
                  these acquisition geometries are not necessarily a
                  source of adversity in order to accurately
                  reconstruct adequately-sampled data. We use two
                  examples to illustrate that it may actually be
                  better than equivalent regularly subsampled
                  data. This comment was already made in earlier works
                  by other authors. We explain this behavior by two
                  key observations. Firstly, a noise-free
                  underdetermined problem can be seen as a noisy
                  well-determined problem. Secondly, regularly
                  subsampling creates strong coherent acquisition
                  noise (aliasing) difficult to remove unlike the
                  noise created by irregularly subsampling that is
                  typically weaker and Gaussian-like.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07EAGEisf_pres.pdf },
  url2         = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/hennenfent07EAGEisf/hennenfent07eage_pres.pdf }
}


@CONFERENCE{hennenfent2007SINBADrii,
  author    = {Gilles Hennenfent and Felix J. Herrmann},
  title     = {Recent insights in $L_1$ solvers},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {During this talk, an overview is given on our work on
                  norm-one solvers as part of the DNOISE
                  project. Gilles will explain the ins and outs of our
                  iterative thresholding solver based on log cooling
                  while Felix will present the work of Michael
                  Friedlander "A Newton root-finding algorithms for
                  large-scale basis pursuit denoise". Both approaches
                  involve the solution of the basis pursuit problem
                  that seeks a minimum one-norm solution of an
                  underdetermined least-squares problem. Basis pursuit
                  denoise (BPDN) fits the least-squares problem only
                  approximately, and a single parameter determines a
                  curve that traces the trade-off between the
                  least-squares fit and the one-norm of the
                  solution. In the work of Friedlander, it is shown
                  show that the function that describes this curve is
                  convex and continuously differentiable over all
                  points of interest. They describe an efficient
                  procedure for evaluating this function and its
                  derivatives. As a result, they can compute arbitrary
                  points on this curve. Their method is suitable for
                  large-scale problems. Only matrix-vector operations
                  are required.  This is joint work with Ewout van der
                  Berg and Michael P. Friedlander},
  keywords  = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{hennenfent2007SEGrsn,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Random sampling: New insights into the reconstruction of coarsely sampled wavefields},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2575-2579},
  organization = {SEG},
  abstract     = {In this paper, we turn the interpolation problem of
                  coarsely-sampled data into a denoising problem. From
                  this point of view, we illustrate the benefit of
                  random sampling at sub-Nyquist rate over regular
                  sampling at the same rate. We show that, using
                  nonlinear sparsity-promoting optimization, coarse
                  random sampling may actually lead to significantly
                  better wavefield reconstruction than equivalent
                  regularly sampled data. {\copyright}2007 Society of
                  Exploration Geophysicists},
  optdoi       = {10.1190/1.2793002},
  keywords     = {Presentation, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/hennenfent07SEGrsn/hennenfent07SEGrsn_pres.pdf }
}


@CONFERENCE{herrmann2007AMScsi,
  author       = {Felix J. Herrmann},
  title        = {Compressive seismic imaging},
  booktitle    = {AMS Von Neumann},
  year         = {2007},
  abstract     = {Seismic imaging involves the solution of an
                  inverse-scattering problem during which the energy
                  of (extremely) large data volumes is collapsed onto
                  the Earth's reflectors. We show how the ideas from
                  "compressive sampling" can alleviate this task by
                  exploiting the curvelet transform's "wavefront-set
                  detection" capability and "invariance" property
                  under wave propagation. First, a wavelet-vaguellete
                  technique is reviewed, where seismic amplitudes are
                  recovered from complete data by diagonalizing the
                  Gramm matrix of the linearized scattering
                  problem. Next, we show how the recovery of seismic
                  wavefields from incomplete data can be cast into a
                  compressive sampling problem, followed by a proposal
                  to compress wavefield extrapolation operators via
                  compressive sampling in the modal domain. During the
                  latter approach, we explicitly exploit the mutual
                  incoherence between the eigenfunctions of the
                  Helmholtz operator and the curvelet frame elements
                  that compress the extrapolated wavefield. This is
                  joint work with Gilles Hennenfent, Peyman Moghaddam,
                  Tim Lin, Chris Stolk and Deli Wang.},
  keywords     = {AMS Von Neumann, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/vonNeuman/2007/herrmann07AMScsi/herrmann07AMScsi_pres.pdf }
}


@CONFERENCE{herrmann2007PIMScsm,
  author       = {Felix J. Herrmann},
  title        = {Compressive sampling meets seismic imaging},
  booktitle    = {PIMS},
  year         = {2007},
  keywords     = {PIMS, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/PIMS/2007/herrmann07PIMScsm/herrmann07PIMScsm_pres.pdf  }
}


@CONFERENCE{herrmann2007SEGmpf,
  author       = {Felix J. Herrmann},
  title        = {Multiple prediction from incomplete data with the focused curvelet transform},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2505-2600},
  abstract     = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach. During the first step, the
                  recenly developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation.},
  keywords     = {SEG, Presentation, SLIM}, 
  optdoi       = {10.1190/1.2792987},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGmpf/herrmann07SEGmpf_pres.pdf}
}


@CONFERENCE{herrmann2007SINBADcwe,
  author    = {Felix J. Herrmann},
  title     = {Compressed wavefield extrapolation},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {An explicit algorithm for the extrapolation of one-way
                  wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation.  Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright}, we are
                  able to formulate the (inverse) wavefield
                  extrapolation problem on small subsets of the data
                  volume, thereby reducing the size of the
                  operators. According to compressed sensing theory,
                  signals can successfully be recovered from an
                  incomplete set of measurements when the measurement
                  basis is incoherent with the representation in which
                  the wavefield is sparse. In this new approach, the
                  eigenfunctions of the Helmholtz operator are
                  recognized as a basis that is incoherent with
                  curvelets that are known to compress seismic
                  wavefields. By casting the wavefield extrapolation
                  problem in this framework, wavefields can
                  successfully be extrapolated in the modal domain via
                  a computationally cheaper operation. A proof of
                  principle for the
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. This is joint
                  work with Tim Lin.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/Publications/Public/Journals/CompressedExtrap.pdf}
}


@CONFERENCE{herrmann2007SINBADfrw,
  author       = {Felix J. Herrmann},
  title        = {Focused recovery with the curvelet transform},
  booktitle    = {SINBAD 2007},
  year         = {2007},
  abstract     = {Incomplete data represents a major challenge for a
                  successful prediction and subsequent removal of
                  multiples. In this paper, a new method will be
                  represented that tackles this challenge in a
                  two-step approach.  During the first step, the
                  recently developed curvelet-based recovery by
                  sparsity-promoting inversion (CRSI) is applied to
                  the data, followed by a prediction of the
                  primaries. During the second high-resolution step,
                  the estimated primaries are used to improve the
                  frequency content of the recovered data by combining
                  the focal transform, defined in terms of the
                  estimated primaries, with the curvelet
                  transform. This focused curvelet transform leads to
                  an improved recovery, which can subsequently be used
                  as input for a second stage of multiple prediction
                  and primary-multiple separation. This is joint work
                  with Deli Wang and Gilles Hennenfent.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/sites/data/Papers/Herrmann2007SINBADfoc.pdf }
}


@CONFERENCE{herrmann2007SLIMfsd,
  author    = {Felix J. Herrmann},
  title     = {From seismic data to the composition of rocks: an
                  interdisciplinary and multiscale approach to
                  exploration seismology},
  booktitle = {Berkhout{\textquoteright}s valedictory address: the
                  conceptual approach of understanding},
  year      = {2007},
  abstract  = {In this essay, a nonlinear and multidisciplinary
                  approach is presented that takes seismic data to the
                  composition of rocks. The presented work has deep
                  roots in the
                  {\textquoteleft}gedachtengoed{\textquoteright}
                  (philosophy) of Delphi spearheaded by Guus
                  Berkhout. Central themes are multiscale,
                  object-orientation and a multidisciplinary
                  approach.},
  keywords  = {SLIM},
  url       = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Misc/herrmann07SLIMfsd/herrmann07SLIMfsd.pdf }
}


@CONFERENCE{herrmann2007COIPpti,
  author       = {Felix J. Herrmann},
  title        = {Phase transitions in explorations seismology: statistical mechanics meets information theory},
  booktitle    = {COIP},
  year         = {2007},
  abstract     = {In this paper, two different applications of phase
                  transitions to exploration seismology will be
                  discussed. The first application concerns a phase
                  diagram ruling the recovery conditions for seismic
                  data volumes from incomplete and noisy data while
                  the second phase transition describes the behavior
                  of bi-compositional mixtures as a function of the
                  volume fraction. In both cases, the phase
                  transitions are the result of randomness in large
                  system of equations in combination with
                  nonlinearity. The seismic recovery problem from
                  incomplete data involves the inversion of a
                  rectangular matrix. Recent results from the field of
                  "compressive sensing" provide the conditions for a
                  successful recovery of functions that are sparse in
                  some basis (wavelet) or frame (curvelet)
                  representation, by means of a sparsity
                  ($\ell_1$-norm) promoting nonlinear program. The
                  conditions for a successful recovery depend on a
                  certain randomness of the matrix and on two
                  parameters that express the matrix{\textquoteright}
                  aspect ratio and the ratio of the number of nonzero
                  entries in the coefficient vector for the sparse
                  signal representation over the number of
                  measurements. It appears that the ensemble average
                  for the success rate for the recovery of the sparse
                  transformed data vector by a nonlinear sparsity
                  promoting program, can be described by a phase
                  transition, demarcating the regions for the two
                  ratios for which recovery of the sparse entries is
                  likely to be successful or likely to
                  fail. Consistent with other phase transition
                  phenomena, the larger the system the sharper the
                  transition. The randomness in this example is
                  related to the construction of the matrix, which for
                  the recovery of spike trains corresponds to the
                  randomly restricted Fourier matrix. It is shown,
                  that these ideas can be extended to the curvelet
                  recovery by sparsity-promoting inversion (CRSI)
                  . The second application of phase transitions in
                  exploration seismology concerns the upscaling
                  problem. To counter the intrinsic smoothing of
                  singularities by conventional equivalent medium
                  upscaling theory, a percolation-based nonlinear
                  switch model is proposed. In this model, the
                  transport properties of bi-compositional mixture
                  models for rocks undergo a sudden change in the
                  macroscopic transport properties as soon as the
                  volume fraction of the stronger material reaches a
                  critical point. At this critical point, the stronger
                  material forms a connected cluster, which leads to
                  the creation of a cusp-like singularity in the
                  elastic moduli, which in turn give rise to specular
                  reflections. In this model, the reflectivity is no
                  longer explicitly due to singularities in the rocks
                  composition.  Instead, singularities are created
                  whenever the volume fraction exceeds the critical
                  point. We will show that this concept can be used
                  for a singularity-preserved lithological upscaling.},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/COIP/2007/herrmann07COIPpti/herrmann07COIPpti_pres.pdf }
}


@CONFERENCE{herrmann2007EAGErdi,
  author       = {Felix J. Herrmann},
  title        = {Recent developments in curvelet-based seismic processing},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Combinations of parsimonious signal representations
                  with nonlinear sparsity promoting programs hold the
                  key to the next-generation of seismic data
                  processing algorithms ... Since they allow for a
                  formulation that is stable w.r.t. noise \&
                  incomplete data do not require prior information on
                  the velocity or locations and dips of the events},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGErdi/herrmann07EAGErdi.pdf }
}


@CONFERENCE{herrmann2007SINBADrdi2,
  author       = {Felix J. Herrmann},
  title        = {Recent developments in primary-multiple separation},
  booktitle    = {SINBAD 2007},
  year         = {2007},
  abstract     = {In this talk, we present a novel primary-multiple
                  separation scheme which makes use of the sparsity of
                  both primaries and multiples in a transform domain,
                  such as the curvelet transform, to provide estimates
                  of each. The proposed algorithm utilizes seismic
                  data as well as the output of a preliminary step
                  that provides (possibly) erroneous predictions of
                  the multiples. The algorithm separates the signal
                  components, i.e., the primaries and multiples, by
                  solving an optimization problem that assumes noisy
                  input data and can be derived from a Bayesian
                  perspective. More precisely, the optimization
                  problem can be arrived at via an assumption of a
                  weighted Laplacian distribution for the primary and
                  multiple coefficients in the transform domain and of
                  white Gaussian noise contaminating both the seismic
                  data and the preliminary prediction of the
                  multiples, which both serve as input to the
                  algorithm. Time permitted, we will also briefly
                  discuss a propasal for adaptive curvelet-domain
                  matched filtering. This is joint work with Deli
                  Wang, Rayan Saaba, {\o}zgur Yilmaz and Eric
                  Verschuur.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SINBAD/2007/herrmann07SINBADrdi2/herrmann07SINBADrdi2_pres.pdf }
}


@CONFERENCE{herrmann2007SINBADsia2,
  author    = {Felix J. Herrmann},
  title     = {Seismic image amplitude recovery},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {In this talk, we recover the amplitude of a seismic
                  image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate nonlinear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave equation
                  on the SEG-AA salt model. This is joint work with
                  Peyman Moghaddam and Chris Stolk (University of
                  Twente)},
  keywords  = {Presentation, SINBAD, SLIM}
}


@CONFERENCE{herrmann2007AIPsit,
  author       = {Felix J. Herrmann},
  title        = {Seismic inversion through operator overloading},
  booktitle    = {AIP},
  year         = {2007},
  abstract     = {Inverse problems in (exploration) seismology are
                  known for their large to very large scale. For
                  instance, certain sparsity-promoting inversion
                  techniques involve vectors that easily exceed 230
                  unknowns while seismic imaging involves the
                  construction and application of matrix-free
                  discretized operators where single matrix-vector
                  evaluations may require hours, days or even weeks on
                  large compute clusters. For these reasons, software
                  development in this field has remained the domain of
                  highly technical codes programmed in low-level
                  languages with little eye for easy development, code
                  reuse and integration with (nonlinear) programs that
                  solve inverse problems. Following ideas from the
                  Symes{\textquoteright} Rice Vector Library and
                  Bartlett{\textquoteright}s C++ object-oriented
                  interface, Thyra, and Reduction/Transformation
                  operators (both part of the Trilinos software
                  package), we developed a software-development
                  environment based on overloading. This environment
                  provides a pathway from in-core prototype
                  development to out-of-core and MPI
                  {\textquoteright}production{\textquoteright} code
                  with a high level of code reuse. This code reuse is
                  accomplished by integrating the out-of-core and MPI
                  functionality into the dynamic object-oriented
                  programming language Python. This integration is
                  implemented through operator overloading and allows
                  for the development of a coordinate-free solver
                  framework that (i) promotes code reuse; (ii)
                  analyses the statements in an abstract syntax tree
                  and (iii) generates executable statements. In the
                  current implementation, we developed an interface to
                  generate executable statements for the out-of-core
                  unix-pipe based (seismic) processing package
                  RSF-Madagascar (rsf.sf.net). The modular design
                  allows for interfaces to other seismic processing
                  packages and to in-core Python packages such as
                  numpy. So far, the implementation overloads linear
                  operators and elementwise reduction/transformation
                  operators. We are planning extensions towards
                  nonlinear operators and integration with existing
                  (parallel) solver frameworks such as Trilinos.},
  keywords     = {AIP, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/AIP/2007/herrmann07AIPsit/herrmann07AIPsit.pdf }
}


@CONFERENCE{herrmann2007CYBERsmc,
  author       = {Felix J. Herrmann},
  title        = {Seismology meets compressive sampling presented at the joint NSF-IPAM meeting. Los Angeles. October, 2007.},
  booktitle    = {Cyber},
  year         = {2007},
  abstract     = {Presented at Cyber-Enabled Discovery and Innovation:
                  Knowledge Extraction as a success story lecture. See
                  for more detail
                  https://www.ipam.ucla.edu/programs/cdi2007/},
  keywords     = {Cyber, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/Cyber/2007/herrmann07CYBERsmc/herrmann07CYBERsmc.pdf }
}


@CONFERENCE{herrmann2007EAGEsrm,
  author       = {Felix J. Herrmann},
  title        = {Surface related multiple prediction from incomplete data},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmont      = {06/2007},
  organization = {EAGE},
  abstract     = {Incomplete data, unknown source-receiver signatures
                  and free-surface reflectivity represent challenges
                  for a successful prediction and subsequent removal
                  of multiples. In this paper, a new method will be
                  represented that tackles these challenges by
                  combining what we know about wavefield
                  (de-)focussing, by weighted
                  convolutions/correlations, and recently developed
                  curvelet-based recovery by sparsity-promoting
                  inversion (CRSI). With this combination, we are able
                  to leverage recent insights from wave physics to-
                  wards a nonlinear formulation for the
                  multiple-prediction problem that works for
                  incomplete data and without detailed knowledge on
                  the surface effects.},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsrm/herrmann07EAGEsrm.pdf}
}


@CONFERENCE{herrmann2007AIPssd,
  author       = {Felix J. Herrmann},
  title        = {Stable seismic data recovery},
  booktitle    = {AIP},
  year         = {2007},
  abstract     = {In this talk, directional frames, known as
                  curvelets, are used to recover seismic data and
                  images from noisy and incomplete data. Sparsity and
                  invariance properties of curvelets are exploited to
                  formulate the recovery by a {\textquoteleft}1-norm
                  promoting program. It is shown that our data
                  recovery approach is closely linked to the recent
                  theory of
                  {\textquoteleft}{\textquoteleft}compressive
                  sensing{\textquoteright}{\textquoteright} and can be
                  seen as a first step towards a nonlinear sampling
                  theory for wavefields. The second problem that will
                  be discussed concerns the recovery of the amplitudes
                  of seismic images in clutter. There, the invariance
                  of curvelets is used to approximately invert the
                  Gramm operator of seismic imaging. In the
                  high-frequency limit, this Gramm matrix corresponds
                  to a pseudo-differential operator, which is near
                  diagonal in the curvelet domain.In this talk,
                  directional frames, known as curvelets, are used to
                  recover seismic data and images from noisy and
                  incomplete data. Sparsity and invariance properties
                  of curvelets are exploited to formulate the recovery
                  by a l1-norm promoting program. It is shown that our
                  data recovery approach is closely linked to the
                  recent theory of
                  {\textquoteleft}{\textquoteleft}compressive
                  sensing{\textquoteright}{\textquoteright} and can be
                  seen as a first step towards a nonlinear sampling
                  theory for wavefields. The second problem that will
                  be discussed concerns the recovery of the amplitudes
                  of seismic images in clutter. There, the invariance
                  of curvelets is used to approximately invert the
                  Gramm operator of seismic imaging.  In the
                  high-frequency limit, this Gramm matrix corresponds
                  to a pseudo-differential operator, which is near
                  diagonal in the curvelet domain.},
  keywords     = {AIP, Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/AIP/2007/herrmann07AIPssd/herrmann07AIPssd.pdf }
}


@CONFERENCE{herrmann2007EAGEsia,
  author       = {Felix J. Herrmann and Gilles Hennenfent and Peyman P. Moghaddam},
  title        = {Seismic imaging and processing with curvelets},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {06/2007},
  organization = {EAGE},
  abstract     = {In this paper, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for three problems in
                  seismic processing and imaging namely, seismic data
                  regularization from data with large percentages of
                  traces missing; seismic amplitude recovery for
                  sub-salt images obtained by reverse-time migration
                  and primary-multiple separation, given an inaccurate
                  multiple prediction. We argue why these nonlinear
                  formulations are beneficial.},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia_pres.pdf },  
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/herrmann07EAGEsia/herrmann07EAGEsia.pdf }
}


@CONFERENCE{herrmann2007EAGEjda,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Just diagonalize: a curvelet-based approach to seismic amplitude recovery},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {In his presentation we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  the recovery of seismic amplitudes. We show that the
                  curvelet's wavefront detection capability and
                  invariance under wave propagation lead to a
                  formulation of this recovery problem that is stable
                  under noise and missing data. {\copyright}2007
                  Society of Exploration Geophysicists},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/2007/herrmann07EAGEWSIM.pdf},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Presentations/cseg/herrmann07jda.pdf}
}


@CONFERENCE{herrmann2007SEGsdp,
  author       = {Felix J. Herrmann and Deli Wang and Gilles Hennenfent and Peyman P. Moghaddam},
  title        = {Seismic data processing with curvelets: a multiscale and nonlinear approach},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2220-2224},
  organization = {SEG},
  abstract     = {In this abstract, we present a nonlinear
                  curvelet-based sparsity-promoting formulation of a
                  seismic processing flow, consisting of the following
                  steps: seismic data regularization and the
                  restoration of migration amplitudes. We show that
                  the curvelet{\textquoteright}s wavefront detection
                  capability and invariance under the
                  migration-demigration operator lead to a formulation
                  that is stable under noise and missing
                  data. {\copyright}2007 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.2792927},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsdp/herrmann07SEGsdp.pdf }
}


@CONFERENCE{herrmann2007SEGsnt,
  author    = {Felix J. Herrmann and D. Wilkinson},
  title     = {Seismic noise: the good, the bad and the ugly},
  booktitle = {SEG Summer Research Workshop: Seismic Noise: Origins Preventions, Mitigation, Utilization},
  year      = {2007},
  note      = {Presented at SEG Summer Research Workshop: Seismic Noise: Origins, Prevention, Mitigation, Utilization},
  abstract  = {In this paper, we present a nonlinear curvelet-based
                  sparsity-promoting formulation for three problems
                  related to seismic noise, namely the
                  {\textquoteright}good{\textquoteright},
                  corresponding to noise generated by random sampling;
                  the {\textquoteright}bad{\textquoteright},
                  corresponding to coherent noise for which
                  (inaccurate) predictions exist and the
                  {\textquoteright}ugly{\textquoteright} for which no
                  predictions exist.  We will show that the
                  compressive capabilities of curvelets on seismic
                  data and images can be used to tackle these three
                  categories of noise-related problems.},
  keywords  = {SLIM, SEG},
  url       = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/herrmann07SEGsnt/herrmann07SEGsnt.pdf }
}


@CONFERENCE{lin2007SEGcwe1,
  author       = {Tim T.Y. Lin and Felix J. Herrmann},
  title        = {Compressed wavefield extrapolation with curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {1997-2001},
  organization = {SEG},
  abstract     = {An explicit algorithm for the extrapolation of
                  one-way wavefields is proposed which combines recent
                  developments in information theory and theoretical
                  signal processing with the physics of wave
                  propagation.  Because of excessive memory
                  requirements, explicit formulations for wave
                  propagation have proven to be a challenge in 3-D. By
                  using ideas from
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright}, we are
                  able to formulate the (inverse) wavefield
                  extrapolation problem on small subsets of the data
                  volume, thereby reducing the size of the
                  operators. According to compressed sensing theory,
                  signals can successfully be recovered from an
                  imcomplete set of measurements when the measurement
                  basis is incoherent with the representation in which
                  the wavefield is sparse. In this new approach, the
                  eigenfunctions of the Helmholtz operator are
                  recognized as a basis that is incoherent with
                  curvelets that are known to compress seismic
                  wavefields. By casting the wavefield extrapolation
                  problem in this framework, wavefields can
                  successfully be extrapolated in the modal domain via
                  a computationally cheaper operation. A proof of
                  principle for the
                  {\textquoteleft}{\textquoteleft}compressed
                  sensing{\textquoteright}{\textquoteright} method is
                  given for wavefield extrapolation in 2-D. The
                  results show that our method is stable and produces
                  identical results compared to the direct application
                  of the full extrapolation operator. {\copyright}2007
                  Society of Exploration Geophysicists},
  optdoi       = {10.1190/1.2792882},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/lin07SEGcwe1/lin07SEGcwe1.pdf }
}


@CONFERENCE{maysami2007EAGEsrc,
  author       = {Mohammad Maysami and Felix J. Herrmann},
  title        = {Seismic reflector characterization by a multiscale detection-estimation method},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Seismic transitions of the subsurface are typically
                  considered as zero-order singularities (step
                  functions). According to this model, the
                  conventional deconvolution problem aims at
                  recovering the seismic reflectivity as a sparse
                  spike train. However, recent multiscale analysis on
                  sedimentary records revealed the existence of
                  accumulations of varying order singularities in the
                  subsurface, which give rise to fractional-order
                  discontinuities. This observation not only calls for
                  a richer class of seismic reflection waveforms, but
                  it also requires a different methodology to detect
                  and characterize these reflection events. For
                  instance, the assumptions underlying conventional
                  deconvolution no longer hold. Because of the
                  bandwidth limitation of seismic data, multiscale
                  analysis methods based on the decay rate of wavelet
                  coefficients may yield ambiguous results. We avoid
                  this problem by formulating the estimation of the
                  singularity orders by a parametric nonlinear
                  inversion method.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/maysami07EAGEsrc/maysami07EAGEsrc_pres.pdf }
}


@CONFERENCE{moghaddam2007CSEGmar,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title        = {Migration amplitude recovery using curvelets},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {In this paper, we recover the amplitude of a seismic
                  image by approximating the normal operator and
                  subsequently inverting it. Normal operator
                  (migration followed by modeling) is an example of
                  pseudo-differential.  curvelets are proven to be
                  invariant under the action of pseudo-differential
                  operators under certain conditions. Subsequently,
                  curvelets are forming as eigen-vectors for such an
                  operator. We propose a seismic amplitude recovery
                  method that employs an eigen-value decomposition for
                  normal operator using curvelets as eigen-vectors and
                  to be estimated eigenvalues.  A post-stack
                  reverse-time, wave-equation migration is used for
                  evaluation of the proposed method.},
  file         = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf:PDF},
  keywords     = {SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/168S0131.pdf}
}


@CONFERENCE{moghaddam2007SEGrsi,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title        = {Robust seismic-images amplitude recovery using curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2225-2229},
  organization = {SEG},
  abstract     = {In this paper, we recover the amplitude of a seismic
                  image by approximating the normal
                  (demigration-migration) operator. In this
                  approximation, we make use of the property that
                  curvelets remain invariant under the action of the
                  normal operator. We propose a seismic amplitude
                  recovery method that employs an eigenvalue like
                  decomposition for the normal operator using
                  curvelets as eigen-vectors. Subsequently, we propose
                  an approximate non-linear singularity-preserving
                  solution to the least-squares seismic imaging
                  problem with sparseness in the curvelet domain and
                  spatial continuity constraints. Our method is tested
                  with a reverse-time
                  {\textquoteleft}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave equation
                  on the SEG-AA salt model. {\copyright}2007 Society
                  of Exploration Geophysicists},
  optdoi       = {10.1190/1.2792928},
  keywords     = {Presentation, SLIM, SEG},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/moghaddam07SEGrsi/moghaddam07SEGrsi_pres.pdf }
}


@CONFERENCE{moghaddam2007CSEGsac,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title        = {Sparsity and continuity enhancing seismic imaging},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {A non-linear singularity-preserving solution to the
                  least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes. Our method is based on the estimation of
                  the normal operator in the form of an
                  {\textquoteright}eigenvalue{\textquoteright}
                  decompsoition with curvelets as the
                  eigenvectors{\textquoteright}. Subsequently, we
                  propose an inversion method that derives from
                  estimation of the normal operator and is formulated
                  as a convex optimization problem.  Sparsity in the
                  curvelet domain as well as continuity along the
                  reflectors in the image domain are promoted as part
                  of this optimization. Our method is tested with a
                  reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  file         = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf:PDF},
  keywords     = {SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2007/2007abstracts/091S0130.pdf}
}


@CONFERENCE{moghaddam2007EAGEsar,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann and C. C. Stolk},
  title        = {Seismic amplitude recovery with curvelets},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {A non-linear singularity-preserving solution to the
                  least-squares seismic imaging problem with
                  sparseness and continuity constraints is
                  proposed. The applied formalism explores curvelets
                  as a directional frame that, by their sparsity on
                  the image, and their invariance under the imaging
                  operators, allows for a stable recovery of the
                  amplitudes. Our method is based on the estimation of
                  the normal operator in the form of an
                  {\textquoteright}eigenvalue{\textquoteright}
                  decompsoition with curvelets as the
                  {\textquoteright}eigenvectors{\textquoteright}.
                  Subsequently, we propose an inversion method that
                  derives from estimation of the normal operator and
                  is formulated as a convex optimization
                  problem. Sparsity in the curvelet domain as well as
                  continuity along the reflectors in the image domain
                  are promoted as part of this optimization.  Our
                  method is tested with a reverse-time
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code simulating the acoustic wave
                  equation.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/moghaddam07EAGEsar/moghaddam07EAGEsar.pdf }
}


@CONFERENCE{saab2007SEGcbp,
  author       = {Rayan Saab and Deli Wang and Ozgur Yilmaz and Felix J. Herrmann},
  title        = {Curvelet-based primary-multiple separation from a {B}ayesian perspective},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2510-2514},
  organization = {SEG},
  abstract     = {In this abstract, we present a novel
                  primary-multiple separation scheme which makes use
                  of the sparsity of both primaries and multiples in a
                  transform domain, such as the curvelet transform, to
                  provide estimates of each. The proposed algorithm
                  utilizes seismic data as well as the output of a
                  preliminary step that provides (possibly) erroneous
                  predictions of the multiples. The algorithm
                  separates the signal components, i.e., the primaries
                  and multiples, by solving an optimization problem
                  that assumes noisy input data and can be derived
                  from a Bayesian perspective. More precisely, the
                  optimization problem can be arrived at via an
                  assumption of a weighted Laplacian distribution for
                  the primary and multiple coefficients in the
                  transform domain and of white Gaussian noise
                  contaminating both the seismic data and the
                  preliminary prediction of the multiples, which both
                  serve as input to the algorithm. {\copyright}2007
                  Society of Exploration Geophysicists},
  optdoi       = {10.1190/1.2792988},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/saab07SEGcbp/saab07SEGcbp.pdf }
}


@CONFERENCE{sastry2007SINBADnor,
  author    = {Challa S. Sastry},
  title     = {Norm-one recovery from irregular sampled data},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {Seismic traces are sampled irregularly and
                  insufficiently due to practical and economical
                  limitations. The use of such data in seismic imaging
                  results in image artifacts and poor spatial
                  resolution. Therefore, before being used, the
                  measurements are to be interpolated onto a regular
                  grid. One of the methods achieving this objective is
                  based on the Fourier reconstruction, which deals
                  with the under-determined system of equations. The
                  recent pursuit techniques (namely, basis pursuit,
                  matching pursuit etc) admit certain promising
                  features such as faster and simpler implementation
                  even in large scale settings.  The presentation
                  discusses the application of the pursuit algorithms
                  to the Fourier-based interpolation problem for the
                  signals that have sparse Fourier spectra. In
                  particular, the objective of the presentation
                  includes: 1). studying the performance of the
                  algorithm if, and how far, the measurement
                  coordinates can be shifted from uniform distribution
                  on the continuous interval. 2). studying what could
                  be the allowable misplacement in the measurement
                  coordinates that does not alter the quality of the
                  reconstruction process},
  keywords  = {SLIM, SINBAD, Presentation}
}


@CONFERENCE{vandenberg2007SINBADipo1,
  author    = {Ewout {van den Berg} and Michael P. Friedlander},
  title     = {In Pursuit of a Root},
  booktitle = {2007 Von Neumann Symposium},
  year      = {2007},
  keywords  = {minimization, Presentation, SLIM},
  url       = {http://slim.eos.ubc.ca/Publications/Private/Presentations/sinbad/2007/friedlander07ipo.pdf}
}


@CONFERENCE{verschuur2007SEGmmp,
  author       = {D. J. Verschuur and Deli Wang and Felix J. Herrmann},
  title        = {Multiterm multiple prediction using separated
                  reflections and diffractions combined with
                  curvelet-based subtraction},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2535-2539},
  organization = {SEG},
  abstract     = {The surface-related multiple elimination (SRME)
                  method has proven to be successful on a large number
                  of data cases. Most of the applications are still
                  2D, as the full 3D implementation is still expensive
                  and under development. However, the earth is a 3D
                  medium, such that 3D effects are difficult to
                  avoid. Most of the 3D effects come from diffractive
                  structures, whereas the specular reflections
                  normally have less of a 3D behavior. By separating
                  the seismic data in a specular reflecting and a
                  diffractive part, multiple prediction can be carried
                  out with these different subsets of the input data,
                  resulting in several categories of predicted
                  multiples. Because each category of predicted
                  multiples can be subtracted from the input data with
                  different adaptation filters, a more flexible SRME
                  procedure is obtained.  Based on some initial
                  results from a Gulf of Mexico dataset, the potential
                  of this approach is investigated. {\copyright}2007
                  Society of Exploration Geophysicists},
  optdoi       = {10.1190/1.2792993},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/verschuur07SEGmmp/verschuur07SEGmmp.pdf }
}


@CONFERENCE{wang2007SEGrri,
  author       = {Deli Wang and Rayan Saab and Ozgur Yilmaz and Felix J. Herrmann},
  title        = {Recent results in curvelet-based primary-multiple separation: application to real data},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2007},
  volume       = {26},
  pages        = {2500-2504},
  organization = {SEG},
  abstract     = {In this abstract, we present a nonlinear
                  curvelet-based sparsity-promoting formulation for
                  the primary-multiple separation problem. We show
                  that these coherent signal components can be
                  separated robustly by explicitly exploting the
                  locality of curvelets in phase space (space-spatial
                  frequency plane) and their ability to compress data
                  volumes that contain wavefronts. This work is an
                  extension of earlier results and the presented
                  algorithms are shown to be stable under noise and
                  moderately erroneous multiple
                  predictions. {\copyright}2007 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.2792986},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2007/wang07SEGrri/wang07SEGrri.pdf }
}


@CONFERENCE{yarham2007SINBADnsw,
  author    = {Carson Yarham},
  title     = {Nonlinear surface wave prediction and separation},
  booktitle = {SINBAD 2007},
  year      = {2007},
  abstract  = {Removal of surface waves is an integral step in seismic
                  processing.  There are many standard techniques for
                  removal of this type of coherent noise, such as f-k
                  filtering, but these methods are not always
                  effective.  One of the common problems with removal
                  of surface waves is that they tend to be aliased in
                  the frequency domain. This can make removal
                  difficult and affect the frequency content of the
                  reflector signals, as this signals will not be
                  completely separated. As seen in (Hennenfent, G. and
                  F. Herrmann, 2006, Application of stable signal
                  recovery to seismic interpolation) interpolation can
                  be used effectively to resample the seismic record
                  thus dealiasing the surface waves. This separates
                  the signals in the frequency domain allowing for a
                  more precise and complete removal. The use of this
                  technique with curvelet based surface wave
                  predictions and an iterative L1 separation scheme
                  can be used to remove surface waves from shot
                  records more completely that with standard
                  techniques.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {http://slim.eos.ubc.ca/Publications/Private/Conferences/EAGE/2007/yarham07eage.pdf}
}


@CONFERENCE{yarham2007EAGEcai,
  author       = {Carson Yarham and Gilles Hennenfent and Felix J. Herrmann},
  title        = {Curvelet applications in surface wave removal},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2007},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Ground roll removal of seismic signals can be a
                  challenging prospect.  Dealing with undersampleing
                  causing aliased waves amplitudes orders of magnitude
                  higher than reflector signals and low frequency loss
                  of information due to band ...},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2007/yarham07EAGEcai/yarham07EAGEcai.pdf}
}



%-------------------------------------------------2006-------------------------------------------------

@CONFERENCE{hennenfent2006SINBADapo,
  author       = {Gilles Hennenfent},
  title        = {A primer on stable signal recovery},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation an introduction will be
                  given on the method of stable recovery from noisy
                  and incomplete data. Strong recovery conditions that
                  guarantee the recovery for arbitrary acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/4-Felix3.pdf}
}


@CONFERENCE{hennenfent2006SINBADros,
  author       = {Gilles Hennenfent},
  title        = {Recovery of seismic data: practical considerations},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {We propose a method for seismic data interpolation
                  based on 1) the reformulation of the problem as a
                  stable signal recovery problem and 2) the fact that
                  seismic data is sparsely represented by curvelets.
                  This method does not require information on the
                  seismic velocities.  Most importantly, this
                  formulation potentially leads to an explicit
                  recovery condition. We also propose a large-scale
                  problem solver for the l1-regularization
                  minimization involved in the recovery and
                  successfully illustrate the performance of our
                  algorithm on 2D synthetic and real examples.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/5-Gilles2.pdf}
}


@CONFERENCE{hennenfent2006SINBADtnf,
  author       = {Gilles Hennenfent},
  title        = {The Nonuniform Fast Discrete Curvelet Transform (NFDCT)},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {The authors present an extension of the fast
                  discrete curvelet transform (FDCT) to nonuniformly
                  sampled data. This extension not only restores
                  curvelet compression rates for nonuniformly sampled
                  data but also removes noise and maps the data to a
                  regular grid.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/10-Gilles3.pdf}
}


@CONFERENCE{hennenfent2006SEGaos,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Application of stable signal recovery to seismic data interpolation},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2006},
  volume       = {25},
  pages        = {2797-2801},
  organization = {SEG},
  abstract     = {We propose a method for seismic data interpolation
                  based on 1) the reformulation of the problem as a
                  stable signal recovery problem and 2) the fact that
                  seismic data is sparsely represented by curvelets.
                  This method does not require information on the
                  seismic velocities.  Most importantly, this
                  formulation potentially leads to an explicit
                  recovery condition. We also propose a large-scale
                  problem solver for the 1-regularization minimization
                  involved in the recovery and successfully illustrate
                  the performance of our algorithm on 2D synthetic and
                  real examples. {\copyright}2006 Society of
                  Exploration Geophysicists},
  optdoi       = {10.1190/1.2370105},
  keywords     = {Presentation, SLIM, curvelets, interpolation,
                  seismic data, regularization minimization, iterative
                  thresholding, amplitude, SEG, continuity, fast
                  transform},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/hennenfent06SEGaos/hennenfent06SEGaos.pdf },
  url2         = {http://dx.doi.org/10.1190/1.2370105 }
}


@CONFERENCE{herrmann2006SINBADapo1,
  author       = {Felix J. Herrmann},
  title        = {A primer on sparsity transforms: curvelets and wave atoms},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation an overview will be given
                  on the different sparsity transforms that are used
                  at SLIM. Emphasis will be on two directional and
                  multiscale wavelet transforms, namely the curvelet
                  and the recently introduced wave-atom
                  transforms. The main properties of these transforms
                  will be listed and their performance on seismic data
                  will be discussed.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/4-Felix3.pdf}
}


@CONFERENCE{herrmann2006SINBADapow,
  author       = {Felix J. Herrmann},
  title        = {A primer on weak conditions for stable recovery},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation an introduction will be
                  given on the method of stable recovery from noisy
                  and incomplete data. Weak recovery conditions that
                  guarantee the recovery for typical acquisition
                  geometries will be reviewed and numerical recovery
                  examples will be presented.  The advantage of these
                  weak conditions is that they are less pessimistic
                  and {\textquoteleft}verifiable{\textquoteright} or
                  very large-scale acquisition geometries.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/6-Felix4.pdf}
}


@CONFERENCE{herrmann2006SINBADmpf,
  author       = {Felix J. Herrmann},
  title        = {Multiple prediction from incomplete data},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/16-Felix7.pdf}
}


@CONFERENCE{herrmann2006SINBADom,
  author       = {Felix J. Herrmann},
  title        = {Opening meeting},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/1-Felix1.pdf}
}


@CONFERENCE{herrmann2006SINBADpms,
  author       = {Felix J. Herrmann and Urs Boeniger and D. J.  Verschuur},
  title        = {Primary-multiple separation by curvelet frames},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, during which
                  multiples are predicted from seismic data, and a
                  primary-multiple separation step, during which the
                  predicted multiples are
                  {\textquoteright}matched{\textquoteright} with the
                  true multiples in the data and subsequently
                  removed. The last step is crucial in practice: an
                  incorrect separation will cause residual multiple
                  energy in the result or may lead to a distortion of
                  the primaries, or both. To reduce these adverse
                  effects, a new transformed-domain method is proposed
                  where primaries and multiples are separated rather
                  than matched. This separation is carried out on the
                  basis of differences in the multiscale and
                  multidirectional characteristics of these two signal
                  components. Our method uses the curvelet transform,
                  which maps multidimensional data volumes into almost
                  orthogonal localized multidimensional prototype
                  waveforms that vary in directional and
                  spatio-temporal content. Primaries-only and
                  multiples-only signal components are recovered from
                  the total data volume by a nonlinear optimization
                  scheme that is stable under noisy input data. During
                  the optimization, the two signal components are
                  separated by enhancing sparseness (through weighted
                  l1-norms) in the transformed domain subject to
                  fitting the observed data as the sum of the
                  separated components to within a user-defined
                  tolerance level. Whenever the prediction for the two
                  signal components in the transformed domain
                  correlate, the recovery is suppressed while for
                  regions where the correlation is small the method
                  seeks the sparsest set of coefficients that
                  represent each signal component. Our algorithm does
                  not seek a matched filter and as such it differs
                  fundamentally from traditional adaptive subtraction
                  methods. The method derives its stability from the
                  sparseness obtained by a non-parametric multiscale
                  and multidirectional overcomplete signal
                  representation. This sparsity serves as prior
                  information and allows for a Bayesian interpretation
                  of our method during which the log-likelihood
                  function is minimized while the two signal
                  components are assumed to be given by a
                  superposition of prototype waveforms, drawn
                  independently from a probability function that is
                  weighted by the predicted primaries and
                  multiples. In this paper, the predictions are based
                  on the data-driven surface-related multiple
                  elimination (SRME) method. Synthetic and field data
                  examples show a clean separation leading to a
                  considerable improvement in multiple suppression
                  compared to the conventional method of adaptive
                  matched filtering. This improved separation
                  translates into an improved stack.},
  keywords     = {Presentation, SINBAD, SLIM},
  optdoi       = {10.1111/j.1365-246X.2007.03360.x},
  Volume       = {170},
  pages        = {781-799},
  organization = {Geophysical Journal International},
  url          = {http://slim.eos.ubc.ca/Publications/Public/Journals/herrmann07npm.pdf}
}


@CONFERENCE{herrmann2006SINBADsac,
  author       = {Felix J. Herrmann},
  title        = {Sparsity- and continuity-promoting seismic image recovery with curvelets},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {A nonlinear singularity-preserving solution to
                  seismic image recovery with sparseness and
                  continuity constraints is proposed. The method
                  explicitly explores the curvelet transform as a
                  directional frame expansion that, by virtue of its
                  sparsity on seismic images and its invariance under
                  the Hessian of the linearized imaging problem,
                  allows for a stable recovery of the migration
                  amplitudes from noisy data.  The method corresponds
                  to a preconditioning that corrects the amplitudes
                  during a post-processing step. The solution is
                  formulated as a nonlinear optimization problem where
                  sparsity in the curvelet domain as well as
                  continuity along the imaged reflectors are jointly
                  promoted. To enhance sparsity, the l1-norm on the
                  curvelet coefficients is minimized while continuity
                  is promoted by minimizing an anisotropic diffusion
                  norm on the image. The performance of the recovery
                  scheme is evaluated with
                  {\textquoteright}wave-equation{\textquoteright}
                  migration code on a synthetic dataset. This is joint
                  work with Peyman Moghaddam.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/14-Felix6.pdf}
}


@CONFERENCE{herrmann2006SINBADsra,
  author       = {Felix J. Herrmann},
  title        = {Stable recovery and separation of seismic data},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation an overview will be given
                  on how seismic data regularization and separation
                  problems can be cast into the framework of stable
                  signal recovery. It is shown that the successful
                  solution of these two problems depends on the
                  existence of signal expansions that are
                  compressible. Preliminary examples will be shown.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/3-Felix2.pdf}
}


@CONFERENCE{lin2006SINBADci,
  author       = {Tim T.Y. Lin},
  title        = {Compressed imaging},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {In 1998 Grimbergen et. al. introduced a new method
                  for computing wavefield propagation which improved
                  on the previously employed local explicit operator
                  method in that it exhibited no dip limitation,
                  accurately handled laterally varying background
                  ground velocity models, and is unconditionally
                  stable. These desirable properties are mainly
                  attributed to bringing the propagation problem into
                  an eigenvector basis that diagonalizes the
                  propagation operators. This modal-transform method,
                  however, requires at each depth-level the solution
                  of a large-scale sparse eigenvalue problem to
                  compute the square-root of the Helmholtz
                  operator. By using recent results from compressed
                  sensing, we hope to reduce these computational costs
                  that typically involve the synthesizes of the
                  imaging operators and the cost of matrix-vector
                  products.  To reduce these costs, we compress the
                  extrapolation operators by using only a fraction of
                  the positive eigenvalues and temporal frequencies.
                  This reduction not only leads to smaller matrices
                  but also to reduced synthesis costs. These
                  reductions go at the expense of solving a recovery
                  problem from incomplete data. During the
                  presentation, we show that wavefields can accurately
                  be extrapolated with a compressed operators and
                  competitive costs.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/17-Tim1.pdf}
}


@CONFERENCE{maysami2006SINBADrro,
  author    = {Mohammad Maysami},
  title     = {Recent results on seismic deconvolution},
  booktitle = {SINBAD 2006},
  year      = {2006},
  abstract  = {One of the important steps in seismic imaging is to
                  provide suitable information about boundaries. Sharp
                  variation of physical properties at a layer boundary
                  cause reflection the wavefield. In previous work
                  done by C. M. Dupuis, seismic signal
                  characterization is divided into two steps:
                  detection and estimation. In the detection phase,
                  the goal is to find all singularities in a seismic
                  section regardless of their order and then to
                  categorize the data to different events by windowing
                  each singularity. In the estimation step, we
                  determine the order of singularity more precisely by
                  using a rough estimate based on the detection
                  phase. Traditionally, a redundant dictionary method
                  is employed for the detection part. However, we
                  attempt to instead use a new L1-solver developed by
                  D.L. Donoho: the Stagewise Orthogonal Matching
                  Pursuit (StOMP). It approximates the solution to
                  inverse problems while promoting the sparsity in the
                  solution vector. This algorithm will allow us to
                  experimentally confirm the recent analysis by
                  S. Mallat on spiky deconvolution limits, which
                  imposes a required minimum distance between
                  spikes. This required minimum distance between
                  different spikes is dependent on the number of
                  spikes as well as the width of the chosen source
                  wavelet used in convolution with the train. These
                  results allow for the design of more robust and
                  accurate detection schemes for seismic signal
                  characterization.},
  keywords  = {Presentation, SINBAD, SLIM},
  url       = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SINBAD/2006/maysami06SINBADrro/maysami06SINBADrro_pres.pdf }
}


@CONFERENCE{modzelewski2006SINBADdas,
  author       = {Henryk Modzelewski},
  title        = {Design and specifications for SLIM{\textquoteright}s software framework},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/19-Henryk1.pdf}
}


@CONFERENCE{moghaddam2006SINBADioa,
  author       = {Peyman P. Moghaddam},
  title        = {Imaging operator approximation using Curvelets},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {In this presentation, the normal
                  (demigation-migration) operator is studied in terms
                  of a pseudo-differential operator. The invariance of
                  curvelets under this operator and their sparsity on
                  the seismic images is used to precondition the
                  migration operator. A brief overview will be given
                  on some of the theory from micro-local analysis
                  which proofs that curvelets remain approximately
                  invariant under the operator.  The proper setting
                  for which a diagonal approximation in the curvelet
                  domain is accurate is discussed together with
                  different methods that estimate this diagonal from
                  of-the-shelf migration operators. This is joint work
                  with Chris Stolk.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/15-Peyman2.pdf}
}


@CONFERENCE{moghaddam2006SINBADsac,
  author       = {Peyman P. Moghaddam},
  title        = {Sparsity- and continuity-promoting norms for seismic images},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {During this presentation, the importance of sparsity
                  and continuity enhancing energy norms is emphasized
                  for seismic imaging and inversion.  The continuity
                  promoting energy norm is justified by the apparent
                  smoothness of reflectors in the direction along and
                  the oscillatory behavior across the interfaces. This
                  energy norm is called anisotropic diffusion and will
                  be defined mathematically. Denoising examples will
                  be given during which seismic images are recovered
                  from the noise by a joint norm-one and continuity
                  promoting minimization.},
  keywords     = {Presentation, SINBAD, SLIM},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/13-Peyman1.pdf}
}


@CONFERENCE{sastry2007SINBADrfu,
  author       = {Challa S. Sastry and Gilles Hennenfent and Felix J. Herrmann},
  title        = {Recovery from unstructured data},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  keywords     = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/11-Sastry1.pdf}
}


@CONFERENCE{thomson2006SINBADlss,
  author       = {Darren Thomson},
  title        = {Large-scale seismic data recovery by the Parallel Windowed Curvelet Transform},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {We propose using overlapping, tapered windows to
                  process seismic data in parallel. This method
                  consists of numerically tight linear operators and
                  adjoints that are suitable for use in iterative
                  algorithms. This method is also highly scalable and
                  makes parallelprocessing of large seismic data sets
                  feasible. We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we have applied to a seismic data
                  interpolation algorithm. Some preliminary results
                  will be shown. Henryk Modzeleweski: Design and
                  specifications for SLIMPy's software framework The
                  SLIM group is actively developing software for
                  seismic imaging. This talk will give a general
                  overview of the software development philosophy
                  adopted by SLIM. The covered topics will include: 1)
                  adopting Python for object-oriented programming, 2)
                  including parallelism into the algorithms used in
                  seismic imaging/modeling, 3) in-house algorithms for
                  seismic imaging, and 4) contributions to Madagascar
                  (RSF). The talk will serve as an introduction to the
                  other presentations in the session ``SINBAD Software
                  releases".},
  keywords     = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/18-Darren1.pdf}
}


@CONFERENCE{thomson2006SEGpwfd,
  author       = {Darren Thomson and Gilles Hennenfent and Henryk Modzelewski and Felix J. Herrmann},
  title        = {A parallel windowed fast discrete curvelet transform applied to seismic processing},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2006},
  volume       = {25},
  pages        = {2767-2771},
  organization = {SEG},
  abstract     = {We propose using overlapping, tapered windows to
                  process seismic data in parallel. This method
                  consists of numerically tight linear oper ators and
                  adjoints that are suitable for use in iterative
                  algorithms.  This method is also highly scalable and
                  makes parallel processing of large seismic data sets
                  feasible. We use this scheme to define the Parallel
                  Windowed Fast Discrete Curvelet Transform (PWFDCT),
                  which we apply to a seismic data interpolation
                  algorithm. The successful performance of our
                  parallel processing scheme and algorithm on a
                  two-dimensional synthetic data is shown.},
  keywords     = {SEG},
  optdoi       = {10.1190/1.2370099},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/thomson06SEGpwfd/thomson06SEGpwfd.pdf }
}


@CONFERENCE{thomson2006SINBADppe,
  author       = {Darren Thomson},
  title        = {(P)SLIMPy: parallel extension},
  booktitle    = {SINBAD 2006},
  year         = {2006},
  abstract     = {The parallel extensions to the SLIMpy environment
                  enable pipe-based processing of large data sets in
                  an MPI-based parallel environment.  Parallel
                  processing can be done by straightforward slicing of
                  data, or by using an overlapping domain
                  decomposition that requires communication between
                  different processors. The principal aim of the
                  parallel extensions is to leave abstract numerical
                  algorithms (ANA's) and applications programmed for
                  use in SLIMpy untouched when moving to parallel
                  processing.  The object-oriented functionality of
                  Python makes this possible.},
  keywords     = {SLIM, SINBAD, Presentation},
  presentation = {http://slim.eos.ubc.ca/SINBAD2006/SINBAD2006/Program_files/21-Darren2.pdf}
}


@CONFERENCE{yarham2006SEGcgrr,
  author       = {Carson Yarham and Urs Boeniger and Felix J. Herrmann},
  title        = {Curvelet-based ground roll removal},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2006},
  volume       = {25},
  pages        = {2777-2782},
  organization = {SEG},
  abstract     = {We have effectively identified and removed ground
                  roll through a two- step process. The first step is
                  to identify the major components of the ground roll
                  through various methods including multiscale
                  separation, directional or frequency filtering or by
                  any other method that identifies the ground
                  roll. Given this estimate for ground roll, the
                  recorded signal is separated during the second step
                  through a block-coordinate relaxation method that
                  seeks the sparsest set for weighted curvelet
                  coefficients of the ground roll and the sought-after
                  reflectivity.  The combination of these two methods
                  allows us to separate out the ground roll signal
                  while preserving the reflector information. Since
                  our method is iterative, we have control of the
                  separation process.  We successfully tested our
                  algorithm on a real data set with a complex ground
                  roll and reflector structure.},
  keywords     = {SEG},
  optdoi       = {10.1190/1.2370101},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2006/yarham06SEGcgrr/yarham06SEGcgrr.pdf } 
}



%-------------------------------------------------2005-------------------------------------------------

@CONFERENCE{beyreuther2005SEGcot,
  author       = {Moritz Beyreuther and Jamin Cristall and Felix J. Herrmann},
  title        = {Computation of time-lapse differences with {3-D} directional frames},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2005},
  volume       = {24},
  pages        = {2488-2491},
  organization = {SEG},
  abstract     = {We present an alternative method of extracting
                  production related differences from time-lapse
                  seismic data sets. Our method is not based on the
                  actual subtraction of the two data sets, risking the
                  enhancement of noise and introduction of artifacts
                  due to local phase rotation and slightly misaligned
                  events. Rather, it mutes events of the monitor
                  survey with respect to the baseline survey based on
                  the magnitudes of coefficients in a sparse and local
                  atomic decomposition.  Our technique is demonstrated
                  to be an effective tool for enhancing the time-lapse
                  signal from surveys which have been cross-equalized.
                  {\copyright}2005 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.2148227},
  keywords     = {SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Beyreuther05SEGcot/Beyreuther05SEGcot.pdf },   
  url2         = {http://dx.doi.org/10.1190/1.2148227}
}


@CONFERENCE{hennenfent2005SEGscd,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Sparseness-constrained data continuation with
                  frames: applications to missing traces and aliased
                  signals in {2/3-D}},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2005},
  volume       = {24},
  pages        = {2162-2165},
  organization = {SEG},
  abstract     = {We present a robust iterative sparseness-constrained
                  interpolation algorithm using 2-/3-D curvelet frames
                  and Fourier-like transforms that exploits continuity
                  along reflectors in seismic data. By choosing
                  generic transforms, we circumvent the necessity to
                  make parametric assumptions (e.g. through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. Simulation and real
                  data examples for data with moderately sized gaps
                  demonstrate that our algorithm provides interpolated
                  traces that accurately reproduce the wavelet shape
                  as well as the AVO behavior. Our method also shows
                  good results for de-aliasing judged by the behavior
                  of the ($f-k$)-spectrum before and after
                  regularization. {\copyright}2005 Society of
                  Exploration Geophysicists},
  optdoi       = {10.1190/1.2148142},
  keywords     = {Presentation, SEG, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2005/Hennenfent05SEGscd/Hennenfent05SEGscd_pres.pdf  }
}


@CONFERENCE{hennenfent2005CSEGscs,
  author       = {Gilles Hennenfent and Felix J. Herrmann and R. Neelamani},
  title        = {Sparseness-constrained seismic deconvolution with curvelets},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {Continuity along reflectors in seismic images is
                  used via Curvelet representation to stabilize the
                  convolution operator inversion. The Curvelet
                  transform is a new multiscale transform that
                  provides sparse representations for images that
                  comprise smooth objects separated by piece-wise
                  smooth discontinuities (e.g. seismic images). Our
                  iterative Curvelet-regularized deconvolution
                  algorithm combines conjugate gradient-based
                  inversion with noise regularization performed using
                  non-linear Curvelet coefficient thresholding. The
                  thresholding operation enhances the sparsity of
                  Curvelet representations. We show on a synthetic
                  example that our algorithm provides improved
                  resolution and continuity along reflectors as well
                  as reduced ringing effect compared to the iterative
                  Wiener-based deconvolution approach.},
  keywords     = {Presentation, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Hennenfent05CSEGscs/Hennenfent05CSEGscs_pres.pdf }
}


@CONFERENCE{hennenfent2005EAGEsdr,
  author       = {Gilles Hennenfent and R. Neelamani and Felix J. Herrmann},
  title        = {Seismic deconvolution revisited with curvelet frames},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {We propose an efficient iterative
                  curvelet-regularized deconvolution algorithm that
                  exploits continuity along reflectors in seismic
                  images.  Curvelets are a new multiscale transform
                  that provides sparse representations for images
                  (such as seismic images) that comprise smooth
                  objects separated by piece-wise smooth
                  discontinuities. Our technique combines conjugate
                  gradient-based convolution operator inversion with
                  noise regularization that is performed using
                  non-linear curvelet coefficient shrinkage
                  (thresholding). The shrinkage operation leverages
                  the sparsity of curvelets
                  representations. Simulations demonstrate that our
                  algorithm provides improved resolution compared to
                  the traditional Wiener-based deconvolution
                  approach.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Hennenfent05EAGEsdr/Hennenfent05EAGEsdr.pdf },
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/hennenfent05eage_poster.pdf}
}




@CONFERENCE{herrmann2005CSEGnld,
  author       = {Felix J. Herrmann and Gilles Hennenfent},
  title        = {Non-linear data continuation with redundant frames},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {05/2005},
  organization = {CSEG},
  abstract     = {We propose an efficient iterative data interpolation
                  method using continuity along reflectors in seismic
                  images via curvelet and discrete cosine
                  transforms. The curvelet transform is a new
                  multiscale transform that provides sparse
                  representations for images that comprise smooth
                  objects separated by piece-wise smooth
                  discontinuities (e.g. seismic images). The advantage
                  of using curvelets is that these frames are sparse
                  for high-frequency caustic-free solutions of the
                  wave-equation. Since we are dealing with less than
                  ideal data (e.g. bandwidth-limited), we compliment
                  the curvelet frames with the discrete cosine
                  transform. The latter is motivated by the successful
                  data continuation with the discrete Fourier
                  transform. By choosing generic basis functions we
                  circumvent the necessity to make parametric
                  assumptions (e.g. through linear/parabolic Radon or
                  demigration) regarding the shape of events in
                  seismic data. Synthetic and real data examples
                  demonstrate that our algorithm provides interpolated
                  traces that accurately reproduce the wavelet shape
                  as well as the AVO behavior along events in shot
                  gathers.},
  keywords     = {Presentation, SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2005/2005abstracts/101S0201-Herrmann_F_Non_Linear_Data_Continuation.pdf},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/herrmann05cseg_pres.pdf}
}


@CONFERENCE{herrmann2005CSEGnlr,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Non-linear regularization in seismic imaging},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {05/2005},
  organization = {CSEG},
  abstract     = {Two complementary solution strategies to the
                  least-squares imaging problem with sparseness \&
                  continuity continuity constraints are proposed. The
                  applied formalism explores the sparseness of
                  curvelets coefficients of the reflectivity and their
                  invariance under the demigration-migration
                  operator. We achieve the solution by jointly
                  minimizing a weighted l1-norm on the curvelet
                  coefficients and an anisotropic difussion or total
                  variation norm on the imaged reflectivity model. The
                  l1-norm exploits the sparsenss of the reflectivity
                  in the curvelet domain whereas the anisotropic norm
                  enhances the continuity along the reflections while
                  removing artifacts residing in between
                  reflectors. While the two optimization methods
                  (convex versus non-convex) share the same type of
                  regularization, they differ in flexibility how to
                  handle additional constraints on the coefficients of
                  the imaged reflectivity and in computational
                  expense. A brief sketch of the theory is provided
                  along with a number of synthetic examples.},
  keywords     = {Presentation, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr.pdf},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2005/Herrmann05CSEGnlr/Herrmann05CSEGnlr_pres.pdf }
}


@CONFERENCE{herrmann2005EAGEosf,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam and R. Kirlin},
  title        = {Optimization strategies for sparseness- and continuity-enhanced imaging: theory},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Two complementary solution strategies to the
                  least-squares migration problem with sparseness- and
                  continuity constraints are proposed.  The applied
                  formalism explores the sparseness of curvelets on
                  the reflectivity and their invariance under the
                  demigration-migration operator. Sparseness is
                  enhanced by (approximately) minimizing a (weighted)
                  l1-norm on the curvelet coefficients. Continuity
                  along imaged reflectors is brought out by minimizing
                  the anisotropic diffusion or total variation norm
                  which penalizes variations along and in between
                  reflectors. A brief sketch of the theory is provided
                  as well as a number of synthetic examples. Technical
                  details on the implementation of the optimization
                  strategies are deferred to an accompanying paper:
                  implementation.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGEosf/Herrmann05EAGEosf.pdf}
}


@CONFERENCE{herrmann2005EAGErcd,
  author       = {Felix J. Herrmann and Gilles Hennenfent},
  title        = {Robust curvelet-domain data continuation with sparseness constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {06/2005},
  organization = {EAGE},
  abstract     = {A robust data interpolation method using curvelets
                  frames is presented.  The advantage of this method
                  is that curvelets arguably provide an optimal sparse
                  representation for solutions of wave equations with
                  smooth coefficients. As such curvelets frames
                  circum- vent {\textendash} besides the assumption of
                  caustic-free data {\textendash} the necessity to
                  make parametric assumptions (e.g. through
                  linear/parabolic Radon or demigration) regarding the
                  shape of events in seismic data. A brief sketch of
                  the theory is provided as well as a number of
                  examples on synthetic and real data.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd/Herrmann05EAGErcd.pdf}
}


@CONFERENCE{herrmann2005EAGErcd1,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Robust curvelet-domain primary-multiple separation with sparseness constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2005},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {A non-linear primary-multiple separation method
                  using curvelets frames is presented. The advantage
                  of this method is that curvelets arguably provide an
                  optimal sparse representation for both primaries and
                  multiples.  As such curvelets frames are ideal
                  candidates to separate primaries from multiples
                  given inaccurate predictions for these two data
                  components.  The method derives its robustness
                  regarding the presence of noise; errors in the
                  prediction and missing data from the curvelet
                  frame{\textquoteright}s ability (i) to represent
                  both signal components with a limited number of
                  multi-scale and directional basis functions; (ii) to
                  separate the components on the basis of differences
                  in location, orientation and scales and (iii) to
                  minimize correlations between the coefficients of
                  the two components. A brief sketch of the theory is
                  provided as well as a number of examples on
                  synthetic and real data.},
  keywords     = {SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2005/Herrmann05EAGErcd1/Herrmann05EAGErcd1.pdf}
}



%-------------------------------------------------2004-------------------------------------------------

@CONFERENCE{beyreuther2004EAGEcdo,
  author       = {Moritz Beyreuther and Felix J. Herrmann and Jamin Cristall},
  title        = {Curvelet denoising of {4-D} seismic},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {06/2004},
  organization = {EAGE},
  abstract     = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging is an emerging technology that holds great
                  promise to better monitor and optimise reservoir
                  production. The basic idea behind 4D seismic is that
                  when multiple 3D surveys are acquired at separate
                  calendar times over a producing field, the reservoir
                  geology will not change from survey to survey but
                  the state of the reservoir fluids will change. Thus,
                  taking the difference between two 3D surveys should
                  remove the static geologic contribution to the data
                  and isolate the time- varying fluid flow
                  component. However, a major challenge in 4D seismic
                  is that acquisition and processing differences
                  between 3D surveys often overshadow the changes
                  caused by fluid flow. This problem is compounded
                  when 4D effects are sought to be derived from
                  vintage 3D data sets that were not originally
                  acquired with 4D in mind. The goal of this study is
                  to remove the acquisition and imaging artefacts from
                  a 4D seismic difference cube using Curveket
                  processing techniques.},
  keywords     = {Presentation, SLIM, EAGE},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Beyreuther04EAGEcdo/Beyreuther04EAGEcdo_pres.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/EAGE4D2004.pdf},
  url2         = {https://circle.ubc.ca/bitstream/handle/2429/453/EAGE4D2004.pdf?sequence=1}
}


@CONFERENCE{cristall2004CSEGcpa,
  author       = {Jamin Cristall and Moritz Beyreuther and Felix J. Herrmann},
  title        = {Curvelet processing and imaging: {4-D} adaptive subtraction},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {With burgeoning world demand and a limited rate of
                  discovery of new reserves, there is increasing
                  impetus upon the industry to optimize recovery from
                  already existing fields. 4D, or time-lapse, seismic
                  imaging holds great promise to better monitor and
                  optimise reservoir production. The basic idea behind
                  4D seismic is that when multiple 3D surveys are
                  acquired at separate calendar times over a producing
                  field, the reservoir geology will not change from
                  survey to survey but the state of the reservoir
                  fluids will change. Thus, taking the difference
                  between two 3D surveys should remove the static
                  geologic contribution to the data and isolate the
                  time-varying fluid flow component. However, a major
                  challenge in 4D seismic is that acquisition and
                  processing differences between 3D surveys often
                  overshadow the changes caused by fluid flow. This
                  problem is compounded when 4D effects are sought to
                  be derived from legacy 3D data sets that were not
                  originally acquired with 4D in mind. The goal of
                  this study is to remove the acquisition and imaging
                  artefacts from a 4D seismic difference cube using
                  Curvelet processing techniques.},
  keywords     = {SLIM},
  url          = {http://www.cseg.ca/conventions/abstracts/2004/2004abstracts/059S0201-Cristall_J_Curvelet_4D.pdf}
}


@CONFERENCE{hennenfent2004SEGtta,
  author       = {Gilles Hennenfent and Felix J. Herrmann},
  title        = {Three-term amplitude-versus-offset ({AVO}) inversion revisited by curvelet and wavelet transforms},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  pages        = {211-214},
  organization = {SEG},
  abstract     = {We present a new method to stabilize the three-term
                  AVO inversion using Curvelet and Wavelet
                  transforms. Curvelets are basis functions that
                  effectively represent otherwise smooth objects
                  having discontinuities along smooth curves. The
                  applied formalism explores them to make the most of
                  the continuity along reflectors in seismic
                  images. Combined with Wavelets, Curvelets are used
                  to denoise the data by penalizing high frequencies
                  and small contributions in the AVO-cube. This
                  approach is based on the idea that rapid amplitude
                  changes along the ray-parameter axis are most likely
                  due to noise. The AVO-inverse problem is linearized,
                  formulated and solved for all (x, z) at once. Using
                  densities and velocities of the Marmousi model to
                  define the fluctuations in the elastic properties,
                  the performance of the proposed method is studied
                  and compared with the smoothing along the
                  ray-parameter direction only. We show that our
                  method better approximates the true data after the
                  denoising step, especially when noise level
                  increases. {\copyright}2004 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.1851201},
  keywords     = {Presentation, SLIM},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta.pdf }, 
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Hennenfent04SEGtta/Hennenfent04SEGtta_pres.pdf }
}


@CONFERENCE{herrmann2004SEGcbn,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-based non-linear adaptive subtraction with sparseness constraints},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  pages        = {1977-1980},
  organization = {SEG},
  abstract     = {In this paper an overview is given on the
                  application of directional basis functions, known
                  under the name Curvelets/Contourlets, to various
                  aspects of seismic processing and imaging, which
                  involve adaptive subtraction. Key concepts in the
                  approach are the use of directional basis functions
                  that localize in both domains (e.g. space and
                  angle); non-linear estimation, which corresponds to
                  localized muting on the coefficients, possibly
                  supplemented by constrained optimization.  We will
                  discuss applications that include multiple,
                  ground-roll removal and migration
                  denoising. {\copyright}2004 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.1851181},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn_pres.pdf  },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcbn/Herrmann04SEGcbn.pdf },
  url2         = {http://slim.eos.ubc.ca/~felix/public/SEGAD2004.pdf}
}


@CONFERENCE{herrmann2004EAGEcdp,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-domain preconditioned 'wave-equation' depth-migration with sparseness and illumination constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal- to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/Publications/Public/TechReports/herrmann04mpw.pdf}
}


@CONFERENCE{herrmann2004EAGEcdl,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet-domain least-squares migration with sparseness constraints},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {May},
  organization = {EAGE},
  abstract     = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is like-wise the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords     = {Presentation, SLIM, EAGE},
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl.pdf },
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/EAGE/2004/Herrmann04EAGEcdl/Herrmann04EAGEcdl_pres.pdf }
}


@CONFERENCE{herrmann2004SEGcdm,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Curvelet-domain multiple elimination with sparseness constraints},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  pages        = {1333-1336},
  organization = {SEG},
  abstract     = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, in which
                  multiples are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.
                  {\copyright}2004 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.1851110},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Herrmann04SEGcdm/Herrmann04SEGcdm.pdf  },
  url2         = {http://slim.eos.ubc.ca/~felix/public/SEGM2004.pdf}
}


@CONFERENCE{herrmann2004CSEGcia,
  author       = {Felix J. Herrmann},
  title        = {Curvelet imaging and processing: an overview},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {05/2004},
  organization = {CSEG},
  abstract     = {In this paper an overview is given on the
                  application of directional basis functions, known
                  under the name Curvelets/Contourlets, to various
                  aspects of seismic processing and imaging. Key
                  conceps in the approach are the use of (i)
                  directional basis functions that localize in both
                  domains (e.g. space and angle); (ii) non-linear
                  estimation, which corresponds to localized muting on
                  the coefficients, possibly supplemented by
                  constrained optimization (iii) invariance of the
                  basis functions under the imaging operators. We will
                  discuss applications that include multiple and
                  ground roll removal; sparseness-constrained
                  least-squares migration and the computation of 4-D
                  difference cubes.},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia/Herrmann04CSEGcia.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_overview.doc}
}


@CONFERENCE{herrmann2004CSEGcia1,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Curvelet imaging and processing: adaptive multiple elimination},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {05/2004},
  organization = {CSEG},
  abstract     = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, in which
                  multiples are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia1/Herrmann04CSEGcia1.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_multiple.doc}
}


@CONFERENCE{herrmann2004CSEGcia2,
  author       = {Felix J. Herrmann and Peyman P. Moghaddam},
  title        = {Curvelet imaging and processing: sparseness-constrained least-squares migration},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {May},
  organization = {CSEG},
  abstract     = {A non-linear edge-preserving solution to the
                  least-squares migration problem with sparseness
                  constraints is introduced. The applied formalism
                  explores Curvelets as basis functions that, by
                  virtue of their sparseness and locality, not only
                  allow for a reduction of the dimensionality of the
                  imaging problem but which also naturally lead to a
                  non-linear solution with significantly improved
                  signal-to-noise ratio. Additional conditions on the
                  image are imposed by solving a constrained
                  optimization problem on the estimated Curvelet
                  coefficients initialized by thresholding.  This
                  optimization is designed to also restore the
                  amplitudes by (approximately) inverting the normal
                  operator, which is, like-wise to the (de)-migration
                  operators, almost diagonalized by the Curvelet
                  transform.},
  keywords     = {Presentation, SLIM},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Herrmann04CSEGcia2/Herrmann04CSEGcia2.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/Herrmann_F_Curvelet_imaging.doc}
}


@CONFERENCE{herrmann2004EAGEsop,
  author       = {Felix J. Herrmann and D. J. Verschuur},
  title        = {Separation of primaries and multiples by non-linear estimation in the curvelet domain},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {Predictive multiple suppression methods consist of
                  two main steps: a prediction step, in which
                  multiples are predicted from the seismic data, and a
                  subtraction step, in which the predicted multiples
                  are matched with the true multiples in the data. The
                  last step appears crucial in practice: an incorrect
                  adaptive subtraction method will cause multiples to
                  be sub-optimally subtracted or primaries being
                  distorted, or both. Therefore, we propose a new
                  domain for separation of primaries and multiples via
                  the Curvelet transform. This transform maps the data
                  into almost orthogonal localized events with a
                  directional and spatial-temporal component. The
                  multiples are suppressed by thresholding the input
                  data at those Curvelet components where the
                  predicted multiples have large amplitudes. In this
                  way the more traditional filtering of predicted
                  multiples to fit the input data is avoided. An
                  initial field data example shows a considerable
                  improvement in multiple suppression.},
  keywords     = {SLIM},
  url          = {http://slim.eos.ubc.ca/~felix/public/EAGEM2004.pdf}
}


@CONFERENCE{moghaddam2004SEGmpw,
  author       = {Peyman P. Moghaddam and Felix J. Herrmann},
  title        = {Migration preconditioning with curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2004},
  volume       = {23},
  pages        = {2204-2207},
  organization = {SEG},
  abstract     = {In this paper, the property of Curvelet transforms
                  for preconditioning the migration and normal
                  operators is investigated. These operators belong to
                  the class of Fourier integral operators and
                  pseudo-differential operator, respectively. The
                  effect of this pre-conditioner is shown in term of
                  improvement of sparsity, convergence rate, number of
                  iteration for the Krylov-subspace solver and
                  clustering of singular(eigen) values. The migration
                  operator, which we employed in this work is the
                  common-offset Kirchoff-Born
                  migration. {\copyright}2004 Society of Exploration
                  Geophysicists},
  optdoi       = {10.1190/1.1845213},
  keywords     = {Presentation, SLIM, SEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2004/Moghaddam04SEGmpw/Moghaddam04SEGmpw_pres.pdf },
  url          = {http://slim.eos.ubc.ca/~felix/public/SEGLAN2004.pdf}
}


@CONFERENCE{yarham2004CSEGcpa,
  author       = {Carson Yarham and Daniel Trad and Felix J. Herrmann},
  title        = {Curvelet processing and imaging: adaptive ground roll removal},
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  optmonth     = {05/2005},
  organization = {CSEG},
  abstract     = {In this paper we present examples of ground roll
                  attenuation for synthetic and real data gathers by
                  using Contourlet and Curvelet transforms.  These
                  non-separable wavelet transforms are locoalized both
                  (x,t)- and (k,f)-domains and allow for adaptive
                  seperation of signal and ground roll. Both linear
                  and non-linear filtering are discussed using the
                  unique properties of these basis that allow for
                  simultaneous localization in the both
                  domains. Eventhough, the linear filtering techniques
                  are encouraging the true added value of these
                  basis-function techniques becomes apparent when we
                  use these decompositions to adaptively substract
                  modeled ground roll from data using a non-linear
                  thesholding procedure. We show real and synthetic
                  examples and the results suggest that these
                  directional-selective basis functions provide a
                  usefull tool for the removal of coherent noise such
                  as ground roll.},
  keywords     = {Presentation, SLIM, CSEG},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/Yarham04CSEGcpa_pres.pdf },
  url          = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGcpa/yarham04csegcpa.pdf}
}


@CONFERENCE{yarham2004CSEGgrr,
  author       = {Carson Yarham and Felix J. Herrmann and Daniel Trad},
  title        = {Ground Roll Removal Using Non-Separable Wavelet Transforms },
  booktitle    = {CSEG Technical Program Expanded Abstracts},
  year         = {2004},
  keywords     = {Presentation},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/CSEG/2004/Yarham04CSEGgrr/Yarham04CSEGgrr.pdf }
}



%-------------------------------------------------2003-------------------------------------------------

@CONFERENCE{herrmann2003SEGoiw,
  author       = {Felix J. Herrmann},
  title        = {"Optimal" imaging with curvelets},
  booktitle    = {SEG Technical Program Expanded Abstracts},
  year         = {2003},
  volume       = {22},
  pages        = {997-1000},
  abstract     = {In this paper we present a non-linear
                  edge-preserving solution to linear inverse
                  scattering problems based on optimal basis- function
                  decompositions. Optimality of the basis functions
                  allow us to (i) reduce the dimensionality of the
                  inverse problem; (ii) devise non-linear thresholding
                  operators that approximate minimax (minimize the
                  maximal mean square error given the worst possible
                  prior) and that significantly improve the
                  signal-to-noise ratio on the image. We present a
                  reformulation of the standard generalized
                  least-squares formulation of the seismic inversion
                  problem into a formulation based on thresh- olding,
                  where the singular values, vectors and linear
                  estimators are replaced by quasi-singular values,
                  basis-functions and thresholding. To limit the
                  computational burden we use a Monte-Carlo sampling
                  method to compute the quasi-singular values. With
                  the proposed method, we aim to significantly improve
                  the signal-to-noise ratio (SNR) on the model space
                  and hence the resolution of the seismic image. While
                  classical Tikhonov-regularized methods only gain the
                  square-root of the SNR on the data for the SNR on
                  the model our method scales almost linearly. This
                  significant improvement of the SNR allows us to
                  discern events at high frequencies which would
                  normally be in the noise.},
  keywords     = {Presentation, SEG, SLIM},
  optdoi       = {10.1190/1.1818117},
  presentation = {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2003/Herrmann03SEGoiw/Herrmann03SEGoiw_pres.pdf}

}


@CONFERENCE{herrmann2003SPIEmsa,
  author       = {Felix J. Herrmann},
  title        = {Multifractional splines: application to seismic imaging},
  booktitle    = {Proceedings of SPIE Technical Conference on Wavelets: Applications in Signal and Image Processing X},
  year         = {2003},
  editor       = {Michael A. Unser and Akram Aldroubi and Andrew F. Laine},
  volume       = {5207},
  pages        = {240-258},
  organization = {SPIE},
  abstract     = {Seismic imaging commits itself to locating
                  singularities in the elastic properties of the
                  Earth{\textquoteright}s subsurface. Using the
                  high-frequency ray-Born approximation for scattering
                  from non-intersecting smooth interfaces, seismic
                  data can be represented by a generalized Radon
                  transform mapping the singularities in the medium to
                  seismic data.  Even though seismic data are
                  bandwidth limited, signatures of the singularities
                  in the medium carry through this transform and its
                  inverse and this mapping property presents us with
                  the possibility to develop new imaging techniques
                  that preserve and characterize the singularities
                  from incomplete, bandwidth-limited and noisy data.
                  In this paper we propose a non-adaptive
                  Curvelet/Contourlet technique to image and preserve
                  the singularities and a data-adaptive Matching
                  Pursuit method to characterize these imaged
                  singularities by Multi-fractional Splines. This
                  first technique borrows from the ideas within the
                  Wavelet-Vaguelette/Quasi-SVD approach. We use the
                  almost diagonalization of the scattering operator to
                  approximately compensate for (i) the coloring of the
                  noise and hence facilitate estimation; (ii) the
                  normal operator itself. Results of applying these
                  techniques to seismic imaging are encouraging
                  although many open fundamental questions remain.},
  keywords     = {Presentation, SLIM},
  presentation = {http://slim.eos.ubc.ca/Publications/Public/Presentations/herrmann03msa.pdf},
  url          = {http://www.eos.ubc.ca/~felix/Preprint/SPIE03DEF.pdf}

}



%-------------------------------------------------2001-------------------------------------------------

@CONFERENCE{herrmann2001EAGEsas,
  author       = {Felix J. Herrmann},
  title        = {Scaling and seismic reflectivity: implications of scaling on {AVO}},
  booktitle    = {EAGE Technical Program Expanded Abstracts},
  year         = {2001},
  optmonth     = {June},
  organization = {EAGE},
  abstract     = {AVO analysis of seismic data is based on the
                  assumption that transitions in the earth consist of
                  jump discontinuities only. Generalization of these
                  transitions to more realistic transitions shows a
                  drastic change in observed AVO behavior, especially
                  for the large angles currently attained by
                  increasing cable lengths. We propose a simple
                  ities. After renormalization, the inverted
                  fluctuations regain their relative magnitudes which,
                  due to the scaling, may have been significantly
                  distorted.},
  keywords     = {SLIM},
}

